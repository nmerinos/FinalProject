{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "tapos-experiments.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "WhLmNzrS-GgK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "81423695-5276-4df8-a5a7-af11dd0b55df"
   },
   "source": [
    "!pip install -U adapter-transformers\n",
    "!pip install datasets"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting adapter-transformers\n",
      "  Downloading adapter_transformers-2.1.2-py3-none-any.whl (2.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.5 MB 4.1 MB/s \n",
      "\u001B[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3 MB 24.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.41.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.13)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.6.1)\n",
      "Collecting huggingface-hub>=0.0.14\n",
      "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
      "\u001B[K     |████████████████████████████████| 43 kB 2.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001B[K     |████████████████████████████████| 895 kB 31.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, adapter-transformers\n",
      "Successfully installed adapter-transformers-2.1.2 huggingface-hub-0.0.15 sacremoses-0.0.45 tokenizers-0.10.3\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "\u001B[K     |████████████████████████████████| 264 kB 4.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Collecting tqdm>=4.42\n",
      "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001B[K     |████████████████████████████████| 76 kB 5.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Collecting fsspec>=2021.05.0\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "\u001B[K     |████████████████████████████████| 118 kB 60.9 MB/s \n",
      "\u001B[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001B[K     |████████████████████████████████| 243 kB 63.9 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "Successfully installed datasets-1.11.0 fsspec-2021.7.0 tqdm-4.62.0 xxhash-2.0.2\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH9LXceJFR8h"
   },
   "source": [
    "1. Initial Fine Tuning Analysis with Adapters from Adapterhub. Our adapters are [\"nli/multinli@ukp\", \"sts/qqp@ukp\", \"nli/scitail@ukp\"]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3vbzIjZXUdo",
    "outputId": "fb7cce81-23f3-4bcd-a2d1-c48dd2ef3091"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/rct-sample_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 30 \\\n",
    "--output_dir results/rct-sample/ \\\n",
    "--overwrite_output_dir \\\n",
    "--task_name rct_sample \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 00:57:31.490609: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 00:57:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1890\n",
      "{'loss': 0.8724, 'learning_rate': 1.470899470899471e-05, 'epoch': 7.94}\n",
      " 26% 500/1890 [02:45<07:43,  3.00it/s]Saving model checkpoint to results/rct-sample/checkpoint-500\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.4329, 'learning_rate': 9.417989417989418e-06, 'epoch': 15.87}\n",
      " 53% 1000/1890 [05:32<04:56,  3.00it/s]Saving model checkpoint to results/rct-sample/checkpoint-1000\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.2742, 'learning_rate': 4.126984126984127e-06, 'epoch': 23.81}\n",
      " 79% 1500/1890 [08:18<02:10,  2.99it/s]Saving model checkpoint to results/rct-sample/checkpoint-1500\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "100% 1890/1890 [10:27<00:00,  3.47it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 627.8076, 'train_samples_per_second': 23.893, 'train_steps_per_second': 3.01, 'train_loss': 0.46125870961991566, 'epoch': 30.0}\n",
      "100% 1890/1890 [10:27<00:00,  3.01it/s]\n",
      "Saving model checkpoint to results/rct-sample/\n",
      "Configuration saved in results/rct-sample/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/rct-sample/tokenizer_config.json\n",
      "Special tokens file saved in results/rct-sample/special_tokens_map.json\n",
      "08/02/2021 01:08:16 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30212\n",
      "  Batch size = 8\n",
      "100% 3777/3777 [08:22<00:00,  7.51it/s]\n",
      "08/02/2021 01:16:39 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_loss = 0.8159406185150146\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_acc = 0.7770422348735602\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_f1 = 0.7770422348735602\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_precision = 0.7770422348735602\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_recall = 0.7770422348735602\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_runtime = 502.7718\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_samples_per_second = 60.091\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   eval_steps_per_second = 7.512\n",
      "08/02/2021 01:16:39 - INFO - __main__ -   epoch = 30.0\n",
      "08/02/2021 01:16:39 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30135\n",
      "  Batch size = 8\n",
      "100% 3767/3767 [08:21<00:00,  7.51it/s]\n",
      "08/02/2021 01:25:01 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_loss = 0.8588184118270874\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_acc = 0.7706985233117637\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_f1 = 0.7706985233117637\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_precision = 0.7706985233117637\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_recall = 0.7706985233117637\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_runtime = 501.9085\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_samples_per_second = 60.041\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   eval_steps_per_second = 7.505\n",
      "08/02/2021 01:25:01 - INFO - __main__ -   epoch = 30.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3haB0_q7XlbT",
    "outputId": "fe5dbb9f-057a-49ee-ab85-d2946150d70a"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/rct-sample_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 1e-4 \\\n",
    "--num_train_epochs 50 \\\n",
    "--output_dir results/rct-sample/ \\\n",
    "--overwrite_output_dir \\\n",
    "--task_name rct_sample \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 01:25:06.745462: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 01:25:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2100\n",
      "{'loss': 0.53, 'learning_rate': 7.619047619047618e-05, 'epoch': 11.9}\n",
      " 24% 500/2100 [03:59<12:51,  2.07it/s]Saving model checkpoint to results/rct-sample/checkpoint-500\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.1608, 'learning_rate': 5.2380952380952384e-05, 'epoch': 23.81}\n",
      " 48% 1000/2100 [08:00<08:52,  2.07it/s]Saving model checkpoint to results/rct-sample/checkpoint-1000\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.0776, 'learning_rate': 2.857142857142857e-05, 'epoch': 35.71}\n",
      " 71% 1500/2100 [12:00<04:48,  2.08it/s]Saving model checkpoint to results/rct-sample/checkpoint-1500\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.0416, 'learning_rate': 4.7619047619047615e-06, 'epoch': 47.62}\n",
      " 95% 2000/2100 [16:00<00:48,  2.07it/s]Saving model checkpoint to results/rct-sample/checkpoint-2000\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "100% 2100/2100 [16:48<00:00,  2.29it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1008.5202, 'train_samples_per_second': 24.789, 'train_steps_per_second': 2.082, 'train_loss': 0.19416515781765892, 'epoch': 50.0}\n",
      "100% 2100/2100 [16:48<00:00,  2.08it/s]\n",
      "Saving model checkpoint to results/rct-sample/\n",
      "Configuration saved in results/rct-sample/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/rct-sample/tokenizer_config.json\n",
      "Special tokens file saved in results/rct-sample/special_tokens_map.json\n",
      "08/02/2021 01:42:12 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30212\n",
      "  Batch size = 8\n",
      "100% 3777/3777 [08:23<00:00,  7.51it/s]\n",
      "08/02/2021 01:50:35 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_loss = 1.1466201543807983\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_acc = 0.7766119422745928\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_f1 = 0.776611942274593\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_precision = 0.7766119422745928\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_recall = 0.7766119422745928\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_runtime = 503.1818\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_samples_per_second = 60.042\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   eval_steps_per_second = 7.506\n",
      "08/02/2021 01:50:35 - INFO - __main__ -   epoch = 50.0\n",
      "08/02/2021 01:50:35 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30135\n",
      "  Batch size = 8\n",
      "100% 3767/3767 [08:22<00:00,  7.50it/s]\n",
      "08/02/2021 01:58:58 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_loss = 1.200405240058899\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_acc = 0.7676787788286046\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_f1 = 0.7676787788286046\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_precision = 0.7676787788286046\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_recall = 0.7676787788286046\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_runtime = 502.608\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_samples_per_second = 59.957\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   eval_steps_per_second = 7.495\n",
      "08/02/2021 01:58:58 - INFO - __main__ -   epoch = 50.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJYnHXLJXqM9",
    "outputId": "1447fdd3-6c27-4020-8184-a61125a8d4c7"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/rct-sample_ \\\n",
    "--max_seq_length 256 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--num_train_epochs 35 \\\n",
    "--output_dir results/rct-sample/ \\\n",
    "--overwrite_output_dir \\\n",
    "--task_name rct_sample \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 01:59:02.701035: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 01:59:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 35\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1470\n",
      "{'loss': 0.9469, 'learning_rate': 6.598639455782313e-06, 'epoch': 11.9}\n",
      " 34% 500/1470 [04:00<07:49,  2.07it/s]Saving model checkpoint to results/rct-sample/checkpoint-500\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.4821, 'learning_rate': 3.1972789115646264e-06, 'epoch': 23.81}\n",
      " 68% 1000/1470 [08:00<03:47,  2.06it/s]Saving model checkpoint to results/rct-sample/checkpoint-1000\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "100% 1470/1470 [11:47<00:00,  2.27it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 707.2288, 'train_samples_per_second': 24.744, 'train_steps_per_second': 2.079, 'train_loss': 0.6102794413663903, 'epoch': 35.0}\n",
      "100% 1470/1470 [11:47<00:00,  2.08it/s]\n",
      "Saving model checkpoint to results/rct-sample/\n",
      "Configuration saved in results/rct-sample/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/rct-sample/tokenizer_config.json\n",
      "Special tokens file saved in results/rct-sample/special_tokens_map.json\n",
      "08/02/2021 02:11:07 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30212\n",
      "  Batch size = 8\n",
      "100% 3777/3777 [08:23<00:00,  7.50it/s]\n",
      "08/02/2021 02:19:31 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_loss = 0.7667415738105774\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_acc = 0.7373891169071892\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_f1 = 0.7373891169071892\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_precision = 0.7373891169071892\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_recall = 0.7373891169071892\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_runtime = 503.9618\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_samples_per_second = 59.949\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   eval_steps_per_second = 7.495\n",
      "08/02/2021 02:19:31 - INFO - __main__ -   epoch = 35.0\n",
      "08/02/2021 02:19:31 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30135\n",
      "  Batch size = 8\n",
      "100% 3767/3767 [08:22<00:00,  7.50it/s]\n",
      "08/02/2021 02:27:53 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_loss = 0.7953733801841736\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_acc = 0.7301144848183175\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_f1 = 0.7301144848183174\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_precision = 0.7301144848183175\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_recall = 0.7301144848183175\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_runtime = 502.5669\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_samples_per_second = 59.962\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   eval_steps_per_second = 7.496\n",
      "08/02/2021 02:27:53 - INFO - __main__ -   epoch = 35.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YwjQeb-g_X4T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d59272a9-fa52-41d8-edfa-3aed8972ce53"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/rct-sample_ \\\n",
    "--max_seq_length 256 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--num_train_epochs 50 \\\n",
    "--output_dir results/rct-sample/ \\\n",
    "--overwrite_output_dir \\\n",
    "--task_name rct_sample \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-01 23:55:58.789557: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/01/2021 23:56:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3150\n",
      "{'loss': 1.052, 'learning_rate': 8.412698412698414e-06, 'epoch': 7.94}\n",
      " 16% 500/3150 [02:45<14:40,  3.01it/s]Saving model checkpoint to results/rct-sample/checkpoint-500\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.5771, 'learning_rate': 6.825396825396826e-06, 'epoch': 15.87}\n",
      " 32% 1000/3150 [05:31<11:57,  3.00it/s]Saving model checkpoint to results/rct-sample/checkpoint-1000\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.4426, 'learning_rate': 5.2380952380952384e-06, 'epoch': 23.81}\n",
      " 48% 1500/3150 [08:17<09:14,  2.98it/s]Saving model checkpoint to results/rct-sample/checkpoint-1500\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-1500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.319, 'learning_rate': 3.6507936507936507e-06, 'epoch': 31.75}\n",
      " 63% 2000/3150 [11:03<06:26,  2.98it/s]Saving model checkpoint to results/rct-sample/checkpoint-2000\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.2781, 'learning_rate': 2.0634920634920634e-06, 'epoch': 39.68}\n",
      " 79% 2500/3150 [13:50<03:36,  3.00it/s]Saving model checkpoint to results/rct-sample/checkpoint-2500\n",
      "Configuration saved in results/rct-sample/checkpoint-2500/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2500/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2500/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2500/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2500/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2500/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-2500/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-2500/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "{'loss': 0.2392, 'learning_rate': 4.7619047619047623e-07, 'epoch': 47.62}\n",
      " 95% 3000/3150 [16:36<00:49,  3.00it/s]Saving model checkpoint to results/rct-sample/checkpoint-3000\n",
      "Configuration saved in results/rct-sample/checkpoint-3000/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-3000/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-3000/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-3000/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-3000/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-3000/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/checkpoint-3000/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/checkpoint-3000/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "100% 3150/3150 [17:26<00:00,  3.49it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1046.1793, 'train_samples_per_second': 23.896, 'train_steps_per_second': 3.011, 'train_loss': 0.4724377671499101, 'epoch': 50.0}\n",
      "100% 3150/3150 [17:26<00:00,  3.01it/s]\n",
      "Saving model checkpoint to results/rct-sample/\n",
      "Configuration saved in results/rct-sample/multinli/adapter_config.json\n",
      "Module weights saved in results/rct-sample/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/qqp/adapter_config.json\n",
      "Module weights saved in results/rct-sample/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/scitail/adapter_config.json\n",
      "Module weights saved in results/rct-sample/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/rct-sample/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/rct-sample/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/rct-sample/tokenizer_config.json\n",
      "Special tokens file saved in results/rct-sample/special_tokens_map.json\n",
      "08/02/2021 00:13:42 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30212\n",
      "  Batch size = 8\n",
      "100% 3777/3777 [08:23<00:00,  7.50it/s]\n",
      "08/02/2021 00:22:06 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_loss = 0.8730341196060181\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_acc = 0.7737322918045809\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_f1 = 0.7737322918045808\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_precision = 0.7737322918045809\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_recall = 0.7737322918045809\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_runtime = 503.8113\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_samples_per_second = 59.967\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   eval_steps_per_second = 7.497\n",
      "08/02/2021 00:22:06 - INFO - __main__ -   epoch = 50.0\n",
      "08/02/2021 00:22:06 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30135\n",
      "  Batch size = 8\n",
      "100% 3767/3767 [08:23<00:00,  7.49it/s]\n",
      "08/02/2021 00:30:29 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_loss = 0.9223339557647705\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_acc = 0.766583706653393\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_f1 = 0.766583706653393\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_precision = 0.766583706653393\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_recall = 0.766583706653393\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_runtime = 503.2108\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_samples_per_second = 59.885\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   eval_steps_per_second = 7.486\n",
      "08/02/2021 00:30:29 - INFO - __main__ -   epoch = 50.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2jVsFDpNh44",
    "outputId": "39a542f5-52b8-47a6-ee70-2d28f7b6c41a"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b1-8G_133qA",
    "outputId": "0678b7ce-35dc-4ebb-aaca-b92dda387d93"
   },
   "source": [
    "%cd \"/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final/\""
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dUbEenyQR9BW"
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def jsonl2txt(path):\n",
    "    df = pd.DataFrame()\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            df = df.append(json.loads(line), ignore_index=True)\n",
    "    df.text.to_csv(path.replace(\"jsonl\", \"txt\"), header=False, index=False)\n",
    "jsonl2txt('data/ag_train.jsonl')\n",
    "jsonl2txt('data/ag_dev.jsonl')\n",
    "jsonl2txt('data/ag_test.jsonl')"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxCjDBKkF2dl"
   },
   "source": [
    "2. Creating new adapter from ag news dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxHTBLNERofJ",
    "outputId": "f69a27d1-f391-4a0b-a30a-b9a5905dcc8a"
   },
   "source": [
    "!python3 run_mlm.py \\\n",
    "--train_file data/ag_train.txt \\\n",
    "--line_by_line \\\n",
    "--validation_file data/ag_dev.txt \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--mlm_probability 0.15 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--output_dir results/adapters/ag \\\n",
    "--train_adapter \\\n",
    "--num_train_epochs 100 \\\n",
    "--learning_rate 1e-4 \\\n",
    "--logging_steps 50 \\\n",
    "--per_gpu_train_batch_size 8 \\\n",
    "--per_gpu_eval_batch_size 8 \\\n",
    "--gradient_accumulation_steps 8  \\"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001B[1;30;43mStreaming output truncated to the last 5000 lines.\u001B[0m\n",
      "{'loss': 1.233, 'learning_rate': 7.268930957683741e-05, 'epoch': 27.31}\n",
      "{'loss': 1.284, 'learning_rate': 7.266146993318486e-05, 'epoch': 27.34}\n",
      "{'loss': 1.2173, 'learning_rate': 7.26336302895323e-05, 'epoch': 27.37}\n",
      "{'loss': 1.2316, 'learning_rate': 7.260579064587974e-05, 'epoch': 27.39}\n",
      "{'loss': 1.2466, 'learning_rate': 7.257795100222717e-05, 'epoch': 27.42}\n",
      "{'loss': 1.2216, 'learning_rate': 7.255011135857461e-05, 'epoch': 27.45}\n",
      "{'loss': 1.2047, 'learning_rate': 7.252227171492205e-05, 'epoch': 27.48}\n",
      "{'loss': 1.2071, 'learning_rate': 7.24944320712695e-05, 'epoch': 27.51}\n",
      "{'loss': 1.257, 'learning_rate': 7.246659242761693e-05, 'epoch': 27.53}\n",
      "{'loss': 1.219, 'learning_rate': 7.243875278396437e-05, 'epoch': 27.56}\n",
      " 28% 49500/179600 [4:04:46<10:07:08,  3.57it/s][INFO|trainer.py:1989] 2021-08-02 10:14:46,708 >> Saving model checkpoint to results/adapters/ag/checkpoint-49500\n",
      "[INFO|loading.py:59] 2021-08-02 10:14:46,709 >> Configuration saved in results/adapters/ag/checkpoint-49500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:14:46,721 >> Module weights saved in results/adapters/ag/checkpoint-49500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:14:46,722 >> Configuration saved in results/adapters/ag/checkpoint-49500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:14:47,006 >> Module weights saved in results/adapters/ag/checkpoint-49500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:14:47,006 >> Configuration saved in results/adapters/ag/checkpoint-49500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:14:47,319 >> Module weights saved in results/adapters/ag/checkpoint-49500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:14:47,320 >> tokenizer config file saved in results/adapters/ag/checkpoint-49500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:14:47,320 >> Special tokens file saved in results/adapters/ag/checkpoint-49500/special_tokens_map.json\n",
      "{'loss': 1.2237, 'learning_rate': 7.241091314031181e-05, 'epoch': 27.59}\n",
      "{'loss': 1.2368, 'learning_rate': 7.238307349665925e-05, 'epoch': 27.62}\n",
      "{'loss': 1.2841, 'learning_rate': 7.235523385300668e-05, 'epoch': 27.64}\n",
      "{'loss': 1.2166, 'learning_rate': 7.232739420935413e-05, 'epoch': 27.67}\n",
      "{'loss': 1.2535, 'learning_rate': 7.229955456570157e-05, 'epoch': 27.7}\n",
      "{'loss': 1.2361, 'learning_rate': 7.227171492204901e-05, 'epoch': 27.73}\n",
      "{'loss': 1.2142, 'learning_rate': 7.224387527839644e-05, 'epoch': 27.76}\n",
      "{'loss': 1.2018, 'learning_rate': 7.221603563474388e-05, 'epoch': 27.78}\n",
      "{'loss': 1.2258, 'learning_rate': 7.218819599109132e-05, 'epoch': 27.81}\n",
      "{'loss': 1.2295, 'learning_rate': 7.216035634743875e-05, 'epoch': 27.84}\n",
      " 28% 50000/179600 [4:07:15<10:43:49,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 10:17:15,452 >> Saving model checkpoint to results/adapters/ag/checkpoint-50000\n",
      "[INFO|loading.py:59] 2021-08-02 10:17:15,453 >> Configuration saved in results/adapters/ag/checkpoint-50000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:17:15,465 >> Module weights saved in results/adapters/ag/checkpoint-50000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:17:15,466 >> Configuration saved in results/adapters/ag/checkpoint-50000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:17:15,773 >> Module weights saved in results/adapters/ag/checkpoint-50000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:17:15,773 >> Configuration saved in results/adapters/ag/checkpoint-50000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:17:16,117 >> Module weights saved in results/adapters/ag/checkpoint-50000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:17:16,118 >> tokenizer config file saved in results/adapters/ag/checkpoint-50000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:17:16,118 >> Special tokens file saved in results/adapters/ag/checkpoint-50000/special_tokens_map.json\n",
      "{'loss': 1.2564, 'learning_rate': 7.21325167037862e-05, 'epoch': 27.87}\n",
      "{'loss': 1.2622, 'learning_rate': 7.210467706013362e-05, 'epoch': 27.89}\n",
      "{'loss': 1.237, 'learning_rate': 7.207683741648107e-05, 'epoch': 27.92}\n",
      "{'loss': 1.239, 'learning_rate': 7.204899777282851e-05, 'epoch': 27.95}\n",
      "{'loss': 1.2347, 'learning_rate': 7.202115812917595e-05, 'epoch': 27.98}\n",
      "{'loss': 1.2514, 'learning_rate': 7.199331848552338e-05, 'epoch': 28.01}\n",
      "{'loss': 1.2229, 'learning_rate': 7.196547884187082e-05, 'epoch': 28.03}\n",
      "{'loss': 1.2167, 'learning_rate': 7.193763919821827e-05, 'epoch': 28.06}\n",
      "{'loss': 1.2491, 'learning_rate': 7.190979955456571e-05, 'epoch': 28.09}\n",
      "{'loss': 1.2332, 'learning_rate': 7.188195991091314e-05, 'epoch': 28.12}\n",
      " 28% 50500/179600 [4:09:43<10:21:17,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 10:19:43,505 >> Saving model checkpoint to results/adapters/ag/checkpoint-50500\n",
      "[INFO|loading.py:59] 2021-08-02 10:19:43,506 >> Configuration saved in results/adapters/ag/checkpoint-50500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:19:43,517 >> Module weights saved in results/adapters/ag/checkpoint-50500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:19:43,518 >> Configuration saved in results/adapters/ag/checkpoint-50500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:19:43,812 >> Module weights saved in results/adapters/ag/checkpoint-50500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:19:43,813 >> Configuration saved in results/adapters/ag/checkpoint-50500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:19:44,139 >> Module weights saved in results/adapters/ag/checkpoint-50500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:19:44,139 >> tokenizer config file saved in results/adapters/ag/checkpoint-50500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:19:44,140 >> Special tokens file saved in results/adapters/ag/checkpoint-50500/special_tokens_map.json\n",
      "{'loss': 1.2422, 'learning_rate': 7.185412026726058e-05, 'epoch': 28.15}\n",
      "{'loss': 1.2169, 'learning_rate': 7.182628062360802e-05, 'epoch': 28.17}\n",
      "{'loss': 1.2655, 'learning_rate': 7.179844097995547e-05, 'epoch': 28.2}\n",
      "{'loss': 1.2519, 'learning_rate': 7.17706013363029e-05, 'epoch': 28.23}\n",
      "{'loss': 1.2509, 'learning_rate': 7.174276169265034e-05, 'epoch': 28.26}\n",
      "{'loss': 1.2309, 'learning_rate': 7.171492204899778e-05, 'epoch': 28.28}\n",
      "{'loss': 1.209, 'learning_rate': 7.168708240534522e-05, 'epoch': 28.31}\n",
      "{'loss': 1.2372, 'learning_rate': 7.165924276169265e-05, 'epoch': 28.34}\n",
      "{'loss': 1.2599, 'learning_rate': 7.16314031180401e-05, 'epoch': 28.37}\n",
      "{'loss': 1.2251, 'learning_rate': 7.160356347438754e-05, 'epoch': 28.4}\n",
      " 28% 51000/179600 [4:12:11<10:28:45,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 10:22:12,117 >> Saving model checkpoint to results/adapters/ag/checkpoint-51000\n",
      "[INFO|loading.py:59] 2021-08-02 10:22:12,118 >> Configuration saved in results/adapters/ag/checkpoint-51000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:22:12,130 >> Module weights saved in results/adapters/ag/checkpoint-51000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:22:12,131 >> Configuration saved in results/adapters/ag/checkpoint-51000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:22:12,419 >> Module weights saved in results/adapters/ag/checkpoint-51000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:22:12,420 >> Configuration saved in results/adapters/ag/checkpoint-51000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:22:12,741 >> Module weights saved in results/adapters/ag/checkpoint-51000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:22:12,742 >> tokenizer config file saved in results/adapters/ag/checkpoint-51000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:22:12,742 >> Special tokens file saved in results/adapters/ag/checkpoint-51000/special_tokens_map.json\n",
      "{'loss': 1.2175, 'learning_rate': 7.157572383073498e-05, 'epoch': 28.42}\n",
      "{'loss': 1.2564, 'learning_rate': 7.154788418708241e-05, 'epoch': 28.45}\n",
      "{'loss': 1.2124, 'learning_rate': 7.152004454342984e-05, 'epoch': 28.48}\n",
      "{'loss': 1.2331, 'learning_rate': 7.149220489977728e-05, 'epoch': 28.51}\n",
      "{'loss': 1.2016, 'learning_rate': 7.146436525612472e-05, 'epoch': 28.54}\n",
      "{'loss': 1.2068, 'learning_rate': 7.143652561247216e-05, 'epoch': 28.56}\n",
      "{'loss': 1.2279, 'learning_rate': 7.14086859688196e-05, 'epoch': 28.59}\n",
      "{'loss': 1.2074, 'learning_rate': 7.138084632516704e-05, 'epoch': 28.62}\n",
      "{'loss': 1.2107, 'learning_rate': 7.135300668151448e-05, 'epoch': 28.65}\n",
      "{'loss': 1.2397, 'learning_rate': 7.132516703786192e-05, 'epoch': 28.67}\n",
      " 29% 51500/179600 [4:14:39<10:02:56,  3.54it/s][INFO|trainer.py:1989] 2021-08-02 10:24:39,628 >> Saving model checkpoint to results/adapters/ag/checkpoint-51500\n",
      "[INFO|loading.py:59] 2021-08-02 10:24:39,628 >> Configuration saved in results/adapters/ag/checkpoint-51500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:24:39,640 >> Module weights saved in results/adapters/ag/checkpoint-51500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:24:39,641 >> Configuration saved in results/adapters/ag/checkpoint-51500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:24:39,932 >> Module weights saved in results/adapters/ag/checkpoint-51500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:24:39,932 >> Configuration saved in results/adapters/ag/checkpoint-51500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:24:40,255 >> Module weights saved in results/adapters/ag/checkpoint-51500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:24:40,256 >> tokenizer config file saved in results/adapters/ag/checkpoint-51500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:24:40,256 >> Special tokens file saved in results/adapters/ag/checkpoint-51500/special_tokens_map.json\n",
      "{'loss': 1.2273, 'learning_rate': 7.129732739420935e-05, 'epoch': 28.7}\n",
      "{'loss': 1.2188, 'learning_rate': 7.126948775055679e-05, 'epoch': 28.73}\n",
      "{'loss': 1.2739, 'learning_rate': 7.124164810690424e-05, 'epoch': 28.76}\n",
      "{'loss': 1.2298, 'learning_rate': 7.121380846325168e-05, 'epoch': 28.79}\n",
      "{'loss': 1.2237, 'learning_rate': 7.118596881959912e-05, 'epoch': 28.81}\n",
      "{'loss': 1.2365, 'learning_rate': 7.115812917594655e-05, 'epoch': 28.84}\n",
      "{'loss': 1.2414, 'learning_rate': 7.113028953229399e-05, 'epoch': 28.87}\n",
      "{'loss': 1.2233, 'learning_rate': 7.110244988864143e-05, 'epoch': 28.9}\n",
      "{'loss': 1.2557, 'learning_rate': 7.107461024498888e-05, 'epoch': 28.92}\n",
      "{'loss': 1.2504, 'learning_rate': 7.10467706013363e-05, 'epoch': 28.95}\n",
      " 29% 52000/179600 [4:17:08<10:37:10,  3.34it/s][INFO|trainer.py:1989] 2021-08-02 10:27:08,666 >> Saving model checkpoint to results/adapters/ag/checkpoint-52000\n",
      "[INFO|loading.py:59] 2021-08-02 10:27:08,666 >> Configuration saved in results/adapters/ag/checkpoint-52000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:27:08,679 >> Module weights saved in results/adapters/ag/checkpoint-52000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:27:08,679 >> Configuration saved in results/adapters/ag/checkpoint-52000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:27:08,976 >> Module weights saved in results/adapters/ag/checkpoint-52000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:27:08,977 >> Configuration saved in results/adapters/ag/checkpoint-52000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:27:09,312 >> Module weights saved in results/adapters/ag/checkpoint-52000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:27:09,313 >> tokenizer config file saved in results/adapters/ag/checkpoint-52000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:27:09,313 >> Special tokens file saved in results/adapters/ag/checkpoint-52000/special_tokens_map.json\n",
      "{'loss': 1.2617, 'learning_rate': 7.101893095768375e-05, 'epoch': 28.98}\n",
      "{'loss': 1.2509, 'learning_rate': 7.099109131403119e-05, 'epoch': 29.01}\n",
      "{'loss': 1.2328, 'learning_rate': 7.096325167037863e-05, 'epoch': 29.04}\n",
      "{'loss': 1.2311, 'learning_rate': 7.093541202672605e-05, 'epoch': 29.06}\n",
      "{'loss': 1.2142, 'learning_rate': 7.090757238307349e-05, 'epoch': 29.09}\n",
      "{'loss': 1.252, 'learning_rate': 7.087973273942093e-05, 'epoch': 29.12}\n",
      "{'loss': 1.1922, 'learning_rate': 7.085189309576838e-05, 'epoch': 29.15}\n",
      "{'loss': 1.2211, 'learning_rate': 7.082405345211582e-05, 'epoch': 29.18}\n",
      "{'loss': 1.2534, 'learning_rate': 7.079621380846325e-05, 'epoch': 29.2}\n",
      "{'loss': 1.2015, 'learning_rate': 7.076837416481069e-05, 'epoch': 29.23}\n",
      " 29% 52500/179600 [4:19:37<10:09:21,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 10:29:37,734 >> Saving model checkpoint to results/adapters/ag/checkpoint-52500\n",
      "[INFO|loading.py:59] 2021-08-02 10:29:37,734 >> Configuration saved in results/adapters/ag/checkpoint-52500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:29:37,746 >> Module weights saved in results/adapters/ag/checkpoint-52500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:29:37,746 >> Configuration saved in results/adapters/ag/checkpoint-52500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:29:38,036 >> Module weights saved in results/adapters/ag/checkpoint-52500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:29:38,037 >> Configuration saved in results/adapters/ag/checkpoint-52500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:29:38,366 >> Module weights saved in results/adapters/ag/checkpoint-52500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:29:38,366 >> tokenizer config file saved in results/adapters/ag/checkpoint-52500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:29:38,367 >> Special tokens file saved in results/adapters/ag/checkpoint-52500/special_tokens_map.json\n",
      "{'loss': 1.2093, 'learning_rate': 7.074053452115813e-05, 'epoch': 29.26}\n",
      "{'loss': 1.2167, 'learning_rate': 7.071269487750558e-05, 'epoch': 29.29}\n",
      "{'loss': 1.2232, 'learning_rate': 7.0684855233853e-05, 'epoch': 29.31}\n",
      "{'loss': 1.2421, 'learning_rate': 7.065701559020045e-05, 'epoch': 29.34}\n",
      "{'loss': 1.2152, 'learning_rate': 7.062917594654789e-05, 'epoch': 29.37}\n",
      "{'loss': 1.2412, 'learning_rate': 7.060133630289533e-05, 'epoch': 29.4}\n",
      "{'loss': 1.2434, 'learning_rate': 7.057349665924276e-05, 'epoch': 29.43}\n",
      "{'loss': 1.2122, 'learning_rate': 7.05456570155902e-05, 'epoch': 29.45}\n",
      "{'loss': 1.2366, 'learning_rate': 7.051781737193765e-05, 'epoch': 29.48}\n",
      "{'loss': 1.2067, 'learning_rate': 7.048997772828509e-05, 'epoch': 29.51}\n",
      " 30% 53000/179600 [4:22:05<10:29:28,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 10:32:05,605 >> Saving model checkpoint to results/adapters/ag/checkpoint-53000\n",
      "[INFO|loading.py:59] 2021-08-02 10:32:05,606 >> Configuration saved in results/adapters/ag/checkpoint-53000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:32:05,619 >> Module weights saved in results/adapters/ag/checkpoint-53000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:32:05,619 >> Configuration saved in results/adapters/ag/checkpoint-53000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:32:05,924 >> Module weights saved in results/adapters/ag/checkpoint-53000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:32:05,924 >> Configuration saved in results/adapters/ag/checkpoint-53000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:32:06,261 >> Module weights saved in results/adapters/ag/checkpoint-53000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:32:06,262 >> tokenizer config file saved in results/adapters/ag/checkpoint-53000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:32:06,262 >> Special tokens file saved in results/adapters/ag/checkpoint-53000/special_tokens_map.json\n",
      "{'loss': 1.2357, 'learning_rate': 7.046213808463252e-05, 'epoch': 29.54}\n",
      "{'loss': 1.2189, 'learning_rate': 7.043429844097996e-05, 'epoch': 29.57}\n",
      "{'loss': 1.2028, 'learning_rate': 7.04064587973274e-05, 'epoch': 29.59}\n",
      "{'loss': 1.2704, 'learning_rate': 7.037861915367485e-05, 'epoch': 29.62}\n",
      "{'loss': 1.2589, 'learning_rate': 7.035077951002227e-05, 'epoch': 29.65}\n",
      "{'loss': 1.2303, 'learning_rate': 7.032293986636972e-05, 'epoch': 29.68}\n",
      "{'loss': 1.2404, 'learning_rate': 7.029510022271715e-05, 'epoch': 29.7}\n",
      "{'loss': 1.2028, 'learning_rate': 7.026726057906459e-05, 'epoch': 29.73}\n",
      "{'loss': 1.2058, 'learning_rate': 7.023942093541203e-05, 'epoch': 29.76}\n",
      "{'loss': 1.2233, 'learning_rate': 7.021158129175946e-05, 'epoch': 29.79}\n",
      " 30% 53500/179600 [4:24:33<10:02:02,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 10:34:33,538 >> Saving model checkpoint to results/adapters/ag/checkpoint-53500\n",
      "[INFO|loading.py:59] 2021-08-02 10:34:33,539 >> Configuration saved in results/adapters/ag/checkpoint-53500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:34:33,551 >> Module weights saved in results/adapters/ag/checkpoint-53500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:34:33,551 >> Configuration saved in results/adapters/ag/checkpoint-53500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:34:33,839 >> Module weights saved in results/adapters/ag/checkpoint-53500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:34:33,839 >> Configuration saved in results/adapters/ag/checkpoint-53500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:34:34,158 >> Module weights saved in results/adapters/ag/checkpoint-53500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:34:34,158 >> tokenizer config file saved in results/adapters/ag/checkpoint-53500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:34:34,158 >> Special tokens file saved in results/adapters/ag/checkpoint-53500/special_tokens_map.json\n",
      "{'loss': 1.2575, 'learning_rate': 7.01837416481069e-05, 'epoch': 29.82}\n",
      "{'loss': 1.2138, 'learning_rate': 7.015590200445435e-05, 'epoch': 29.84}\n",
      "{'loss': 1.2693, 'learning_rate': 7.012806236080179e-05, 'epoch': 29.87}\n",
      "{'loss': 1.2694, 'learning_rate': 7.010022271714922e-05, 'epoch': 29.9}\n",
      "{'loss': 1.2137, 'learning_rate': 7.007238307349666e-05, 'epoch': 29.93}\n",
      "{'loss': 1.2237, 'learning_rate': 7.00445434298441e-05, 'epoch': 29.95}\n",
      "{'loss': 1.2421, 'learning_rate': 7.001670378619154e-05, 'epoch': 29.98}\n",
      "{'loss': 1.2426, 'learning_rate': 6.998886414253897e-05, 'epoch': 30.01}\n",
      "{'loss': 1.2004, 'learning_rate': 6.996102449888642e-05, 'epoch': 30.04}\n",
      "{'loss': 1.2546, 'learning_rate': 6.993318485523386e-05, 'epoch': 30.07}\n",
      " 30% 54000/179600 [4:27:01<10:18:21,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 10:37:01,659 >> Saving model checkpoint to results/adapters/ag/checkpoint-54000\n",
      "[INFO|loading.py:59] 2021-08-02 10:37:01,660 >> Configuration saved in results/adapters/ag/checkpoint-54000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:37:01,672 >> Module weights saved in results/adapters/ag/checkpoint-54000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:37:01,672 >> Configuration saved in results/adapters/ag/checkpoint-54000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:37:01,986 >> Module weights saved in results/adapters/ag/checkpoint-54000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:37:01,987 >> Configuration saved in results/adapters/ag/checkpoint-54000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:37:02,333 >> Module weights saved in results/adapters/ag/checkpoint-54000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:37:02,334 >> tokenizer config file saved in results/adapters/ag/checkpoint-54000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:37:02,334 >> Special tokens file saved in results/adapters/ag/checkpoint-54000/special_tokens_map.json\n",
      "{'loss': 1.2085, 'learning_rate': 6.99053452115813e-05, 'epoch': 30.09}\n",
      "{'loss': 1.225, 'learning_rate': 6.987750556792873e-05, 'epoch': 30.12}\n",
      "{'loss': 1.2465, 'learning_rate': 6.984966592427617e-05, 'epoch': 30.15}\n",
      "{'loss': 1.2427, 'learning_rate': 6.982182628062362e-05, 'epoch': 30.18}\n",
      "{'loss': 1.2484, 'learning_rate': 6.979398663697106e-05, 'epoch': 30.21}\n",
      "{'loss': 1.2447, 'learning_rate': 6.976614699331849e-05, 'epoch': 30.23}\n",
      "{'loss': 1.2275, 'learning_rate': 6.973830734966593e-05, 'epoch': 30.26}\n",
      "{'loss': 1.223, 'learning_rate': 6.971046770601337e-05, 'epoch': 30.29}\n",
      "{'loss': 1.2641, 'learning_rate': 6.96826280623608e-05, 'epoch': 30.32}\n",
      "{'loss': 1.247, 'learning_rate': 6.965478841870824e-05, 'epoch': 30.35}\n",
      " 30% 54500/179600 [4:29:29<9:30:20,  3.66it/s][INFO|trainer.py:1989] 2021-08-02 10:39:30,408 >> Saving model checkpoint to results/adapters/ag/checkpoint-54500\n",
      "[INFO|loading.py:59] 2021-08-02 10:39:30,408 >> Configuration saved in results/adapters/ag/checkpoint-54500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:39:30,420 >> Module weights saved in results/adapters/ag/checkpoint-54500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:39:30,420 >> Configuration saved in results/adapters/ag/checkpoint-54500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:39:30,717 >> Module weights saved in results/adapters/ag/checkpoint-54500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:39:30,718 >> Configuration saved in results/adapters/ag/checkpoint-54500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:39:31,040 >> Module weights saved in results/adapters/ag/checkpoint-54500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:39:31,041 >> tokenizer config file saved in results/adapters/ag/checkpoint-54500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:39:31,041 >> Special tokens file saved in results/adapters/ag/checkpoint-54500/special_tokens_map.json\n",
      "{'loss': 1.1987, 'learning_rate': 6.962694877505567e-05, 'epoch': 30.37}\n",
      "{'loss': 1.2717, 'learning_rate': 6.959910913140311e-05, 'epoch': 30.4}\n",
      "{'loss': 1.2469, 'learning_rate': 6.957126948775056e-05, 'epoch': 30.43}\n",
      "{'loss': 1.2699, 'learning_rate': 6.9543429844098e-05, 'epoch': 30.46}\n",
      "{'loss': 1.1843, 'learning_rate': 6.951559020044543e-05, 'epoch': 30.48}\n",
      "{'loss': 1.2368, 'learning_rate': 6.948775055679287e-05, 'epoch': 30.51}\n",
      "{'loss': 1.2214, 'learning_rate': 6.945991091314031e-05, 'epoch': 30.54}\n",
      "{'loss': 1.2404, 'learning_rate': 6.943207126948776e-05, 'epoch': 30.57}\n",
      "{'loss': 1.2246, 'learning_rate': 6.940423162583519e-05, 'epoch': 30.6}\n",
      "{'loss': 1.2081, 'learning_rate': 6.937639198218263e-05, 'epoch': 30.62}\n",
      " 31% 55000/179600 [4:31:52<10:17:42,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 10:41:53,431 >> Saving model checkpoint to results/adapters/ag/checkpoint-55000\n",
      "[INFO|loading.py:59] 2021-08-02 10:41:53,432 >> Configuration saved in results/adapters/ag/checkpoint-55000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:41:53,443 >> Module weights saved in results/adapters/ag/checkpoint-55000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:41:53,443 >> Configuration saved in results/adapters/ag/checkpoint-55000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:41:53,734 >> Module weights saved in results/adapters/ag/checkpoint-55000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:41:53,735 >> Configuration saved in results/adapters/ag/checkpoint-55000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:41:54,061 >> Module weights saved in results/adapters/ag/checkpoint-55000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:41:54,061 >> tokenizer config file saved in results/adapters/ag/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:41:54,061 >> Special tokens file saved in results/adapters/ag/checkpoint-55000/special_tokens_map.json\n",
      "{'loss': 1.1922, 'learning_rate': 6.934855233853007e-05, 'epoch': 30.65}\n",
      "{'loss': 1.2515, 'learning_rate': 6.932071269487751e-05, 'epoch': 30.68}\n",
      "{'loss': 1.23, 'learning_rate': 6.929287305122496e-05, 'epoch': 30.71}\n",
      "{'loss': 1.2318, 'learning_rate': 6.926503340757238e-05, 'epoch': 30.73}\n",
      "{'loss': 1.2051, 'learning_rate': 6.923719376391983e-05, 'epoch': 30.76}\n",
      "{'loss': 1.2218, 'learning_rate': 6.920935412026727e-05, 'epoch': 30.79}\n",
      "{'loss': 1.2262, 'learning_rate': 6.918151447661471e-05, 'epoch': 30.82}\n",
      "{'loss': 1.2349, 'learning_rate': 6.915367483296214e-05, 'epoch': 30.85}\n",
      "{'loss': 1.2196, 'learning_rate': 6.912583518930958e-05, 'epoch': 30.87}\n",
      "{'loss': 1.2324, 'learning_rate': 6.909799554565703e-05, 'epoch': 30.9}\n",
      " 31% 55500/179600 [4:34:20<9:55:22,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 10:44:21,407 >> Saving model checkpoint to results/adapters/ag/checkpoint-55500\n",
      "[INFO|loading.py:59] 2021-08-02 10:44:21,408 >> Configuration saved in results/adapters/ag/checkpoint-55500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:44:21,420 >> Module weights saved in results/adapters/ag/checkpoint-55500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:44:21,420 >> Configuration saved in results/adapters/ag/checkpoint-55500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:44:21,721 >> Module weights saved in results/adapters/ag/checkpoint-55500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:44:21,721 >> Configuration saved in results/adapters/ag/checkpoint-55500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:44:22,047 >> Module weights saved in results/adapters/ag/checkpoint-55500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:44:22,047 >> tokenizer config file saved in results/adapters/ag/checkpoint-55500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:44:22,048 >> Special tokens file saved in results/adapters/ag/checkpoint-55500/special_tokens_map.json\n",
      "{'loss': 1.2634, 'learning_rate': 6.907015590200447e-05, 'epoch': 30.93}\n",
      "{'loss': 1.2008, 'learning_rate': 6.904231625835188e-05, 'epoch': 30.96}\n",
      "{'loss': 1.225, 'learning_rate': 6.901447661469933e-05, 'epoch': 30.99}\n",
      "{'loss': 1.2554, 'learning_rate': 6.898663697104677e-05, 'epoch': 31.01}\n",
      "{'loss': 1.2238, 'learning_rate': 6.895879732739421e-05, 'epoch': 31.04}\n",
      "{'loss': 1.2351, 'learning_rate': 6.893095768374165e-05, 'epoch': 31.07}\n",
      "{'loss': 1.2305, 'learning_rate': 6.890311804008908e-05, 'epoch': 31.1}\n",
      "{'loss': 1.219, 'learning_rate': 6.887527839643653e-05, 'epoch': 31.12}\n",
      "{'loss': 1.234, 'learning_rate': 6.884743875278397e-05, 'epoch': 31.15}\n",
      "{'loss': 1.2479, 'learning_rate': 6.881959910913141e-05, 'epoch': 31.18}\n",
      " 31% 56000/179600 [4:36:48<9:35:26,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 10:46:49,011 >> Saving model checkpoint to results/adapters/ag/checkpoint-56000\n",
      "[INFO|loading.py:59] 2021-08-02 10:46:49,012 >> Configuration saved in results/adapters/ag/checkpoint-56000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:46:49,023 >> Module weights saved in results/adapters/ag/checkpoint-56000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:46:49,023 >> Configuration saved in results/adapters/ag/checkpoint-56000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:46:49,328 >> Module weights saved in results/adapters/ag/checkpoint-56000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:46:49,328 >> Configuration saved in results/adapters/ag/checkpoint-56000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:46:49,658 >> Module weights saved in results/adapters/ag/checkpoint-56000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:46:49,659 >> tokenizer config file saved in results/adapters/ag/checkpoint-56000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:46:49,659 >> Special tokens file saved in results/adapters/ag/checkpoint-56000/special_tokens_map.json\n",
      "{'loss': 1.2271, 'learning_rate': 6.879175946547884e-05, 'epoch': 31.21}\n",
      "{'loss': 1.2316, 'learning_rate': 6.876391982182628e-05, 'epoch': 31.24}\n",
      "{'loss': 1.2166, 'learning_rate': 6.873608017817373e-05, 'epoch': 31.26}\n",
      "{'loss': 1.2152, 'learning_rate': 6.870824053452117e-05, 'epoch': 31.29}\n",
      "{'loss': 1.237, 'learning_rate': 6.86804008908686e-05, 'epoch': 31.32}\n",
      "{'loss': 1.2234, 'learning_rate': 6.865256124721604e-05, 'epoch': 31.35}\n",
      "{'loss': 1.1989, 'learning_rate': 6.862472160356348e-05, 'epoch': 31.38}\n",
      "{'loss': 1.2016, 'learning_rate': 6.859688195991092e-05, 'epoch': 31.4}\n",
      "{'loss': 1.262, 'learning_rate': 6.856904231625835e-05, 'epoch': 31.43}\n",
      "{'loss': 1.2383, 'learning_rate': 6.85412026726058e-05, 'epoch': 31.46}\n",
      " 31% 56500/179600 [4:39:17<10:14:27,  3.34it/s][INFO|trainer.py:1989] 2021-08-02 10:49:18,271 >> Saving model checkpoint to results/adapters/ag/checkpoint-56500\n",
      "[INFO|loading.py:59] 2021-08-02 10:49:18,271 >> Configuration saved in results/adapters/ag/checkpoint-56500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:49:18,283 >> Module weights saved in results/adapters/ag/checkpoint-56500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:49:18,284 >> Configuration saved in results/adapters/ag/checkpoint-56500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:49:18,586 >> Module weights saved in results/adapters/ag/checkpoint-56500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:49:18,587 >> Configuration saved in results/adapters/ag/checkpoint-56500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:49:18,920 >> Module weights saved in results/adapters/ag/checkpoint-56500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:49:18,921 >> tokenizer config file saved in results/adapters/ag/checkpoint-56500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:49:18,921 >> Special tokens file saved in results/adapters/ag/checkpoint-56500/special_tokens_map.json\n",
      "{'loss': 1.2513, 'learning_rate': 6.851336302895324e-05, 'epoch': 31.49}\n",
      "{'loss': 1.2016, 'learning_rate': 6.848552338530068e-05, 'epoch': 31.51}\n",
      "{'loss': 1.2154, 'learning_rate': 6.845768374164811e-05, 'epoch': 31.54}\n",
      "{'loss': 1.2398, 'learning_rate': 6.842984409799554e-05, 'epoch': 31.57}\n",
      "{'loss': 1.2476, 'learning_rate': 6.840200445434298e-05, 'epoch': 31.6}\n",
      "{'loss': 1.2489, 'learning_rate': 6.837416481069042e-05, 'epoch': 31.63}\n",
      "{'loss': 1.2768, 'learning_rate': 6.834632516703787e-05, 'epoch': 31.65}\n",
      "{'loss': 1.2137, 'learning_rate': 6.83184855233853e-05, 'epoch': 31.68}\n",
      "{'loss': 1.2066, 'learning_rate': 6.829064587973274e-05, 'epoch': 31.71}\n",
      "{'loss': 1.2193, 'learning_rate': 6.826280623608018e-05, 'epoch': 31.74}\n",
      " 32% 57000/179600 [4:41:45<9:42:56,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 10:51:46,112 >> Saving model checkpoint to results/adapters/ag/checkpoint-57000\n",
      "[INFO|loading.py:59] 2021-08-02 10:51:46,112 >> Configuration saved in results/adapters/ag/checkpoint-57000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:51:46,126 >> Module weights saved in results/adapters/ag/checkpoint-57000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:51:46,127 >> Configuration saved in results/adapters/ag/checkpoint-57000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:51:46,422 >> Module weights saved in results/adapters/ag/checkpoint-57000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:51:46,423 >> Configuration saved in results/adapters/ag/checkpoint-57000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:51:46,752 >> Module weights saved in results/adapters/ag/checkpoint-57000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:51:46,752 >> tokenizer config file saved in results/adapters/ag/checkpoint-57000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:51:46,753 >> Special tokens file saved in results/adapters/ag/checkpoint-57000/special_tokens_map.json\n",
      "{'loss': 1.2392, 'learning_rate': 6.823496659242762e-05, 'epoch': 31.76}\n",
      "{'loss': 1.2249, 'learning_rate': 6.820712694877505e-05, 'epoch': 31.79}\n",
      "{'loss': 1.2184, 'learning_rate': 6.81792873051225e-05, 'epoch': 31.82}\n",
      "{'loss': 1.2022, 'learning_rate': 6.815144766146994e-05, 'epoch': 31.85}\n",
      "{'loss': 1.2158, 'learning_rate': 6.812360801781738e-05, 'epoch': 31.88}\n",
      "{'loss': 1.2579, 'learning_rate': 6.809576837416481e-05, 'epoch': 31.9}\n",
      "{'loss': 1.2355, 'learning_rate': 6.806792873051225e-05, 'epoch': 31.93}\n",
      "{'loss': 1.2518, 'learning_rate': 6.80400890868597e-05, 'epoch': 31.96}\n",
      "{'loss': 1.2333, 'learning_rate': 6.801224944320714e-05, 'epoch': 31.99}\n",
      "{'loss': 1.2576, 'learning_rate': 6.798440979955457e-05, 'epoch': 32.02}\n",
      " 32% 57500/179600 [4:44:13<10:02:30,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 10:54:14,249 >> Saving model checkpoint to results/adapters/ag/checkpoint-57500\n",
      "[INFO|loading.py:59] 2021-08-02 10:54:14,250 >> Configuration saved in results/adapters/ag/checkpoint-57500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:54:14,262 >> Module weights saved in results/adapters/ag/checkpoint-57500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:54:14,263 >> Configuration saved in results/adapters/ag/checkpoint-57500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:54:14,575 >> Module weights saved in results/adapters/ag/checkpoint-57500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:54:14,576 >> Configuration saved in results/adapters/ag/checkpoint-57500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:54:14,917 >> Module weights saved in results/adapters/ag/checkpoint-57500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:54:14,917 >> tokenizer config file saved in results/adapters/ag/checkpoint-57500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:54:14,917 >> Special tokens file saved in results/adapters/ag/checkpoint-57500/special_tokens_map.json\n",
      "{'loss': 1.2262, 'learning_rate': 6.795657015590201e-05, 'epoch': 32.04}\n",
      "{'loss': 1.2451, 'learning_rate': 6.792873051224945e-05, 'epoch': 32.07}\n",
      "{'loss': 1.2069, 'learning_rate': 6.79008908685969e-05, 'epoch': 32.1}\n",
      "{'loss': 1.2287, 'learning_rate': 6.787305122494432e-05, 'epoch': 32.13}\n",
      "{'loss': 1.1969, 'learning_rate': 6.784521158129176e-05, 'epoch': 32.15}\n",
      "{'loss': 1.2367, 'learning_rate': 6.781737193763921e-05, 'epoch': 32.18}\n",
      "{'loss': 1.2083, 'learning_rate': 6.778953229398664e-05, 'epoch': 32.21}\n",
      "{'loss': 1.2378, 'learning_rate': 6.776169265033408e-05, 'epoch': 32.24}\n",
      "{'loss': 1.2582, 'learning_rate': 6.773385300668151e-05, 'epoch': 32.27}\n",
      "{'loss': 1.2051, 'learning_rate': 6.770601336302895e-05, 'epoch': 32.29}\n",
      " 32% 58000/179600 [4:46:42<9:30:40,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 10:56:42,473 >> Saving model checkpoint to results/adapters/ag/checkpoint-58000\n",
      "[INFO|loading.py:59] 2021-08-02 10:56:42,474 >> Configuration saved in results/adapters/ag/checkpoint-58000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:56:42,485 >> Module weights saved in results/adapters/ag/checkpoint-58000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:56:42,485 >> Configuration saved in results/adapters/ag/checkpoint-58000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:56:42,781 >> Module weights saved in results/adapters/ag/checkpoint-58000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:56:42,782 >> Configuration saved in results/adapters/ag/checkpoint-58000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:56:43,111 >> Module weights saved in results/adapters/ag/checkpoint-58000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:56:43,112 >> tokenizer config file saved in results/adapters/ag/checkpoint-58000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:56:43,112 >> Special tokens file saved in results/adapters/ag/checkpoint-58000/special_tokens_map.json\n",
      "{'loss': 1.2335, 'learning_rate': 6.767817371937639e-05, 'epoch': 32.32}\n",
      "{'loss': 1.1943, 'learning_rate': 6.765033407572384e-05, 'epoch': 32.35}\n",
      "{'loss': 1.2198, 'learning_rate': 6.762249443207126e-05, 'epoch': 32.38}\n",
      "{'loss': 1.2355, 'learning_rate': 6.759465478841871e-05, 'epoch': 32.41}\n",
      "{'loss': 1.2442, 'learning_rate': 6.756681514476615e-05, 'epoch': 32.43}\n",
      "{'loss': 1.2286, 'learning_rate': 6.753897550111359e-05, 'epoch': 32.46}\n",
      "{'loss': 1.2239, 'learning_rate': 6.751113585746102e-05, 'epoch': 32.49}\n",
      "{'loss': 1.2293, 'learning_rate': 6.748329621380846e-05, 'epoch': 32.52}\n",
      "{'loss': 1.2395, 'learning_rate': 6.74554565701559e-05, 'epoch': 32.54}\n",
      "{'loss': 1.1944, 'learning_rate': 6.742761692650335e-05, 'epoch': 32.57}\n",
      " 33% 58500/179600 [4:49:10<10:11:13,  3.30it/s][INFO|trainer.py:1989] 2021-08-02 10:59:10,927 >> Saving model checkpoint to results/adapters/ag/checkpoint-58500\n",
      "[INFO|loading.py:59] 2021-08-02 10:59:10,928 >> Configuration saved in results/adapters/ag/checkpoint-58500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:59:10,943 >> Module weights saved in results/adapters/ag/checkpoint-58500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:59:10,943 >> Configuration saved in results/adapters/ag/checkpoint-58500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:59:11,241 >> Module weights saved in results/adapters/ag/checkpoint-58500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 10:59:11,241 >> Configuration saved in results/adapters/ag/checkpoint-58500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 10:59:11,576 >> Module weights saved in results/adapters/ag/checkpoint-58500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 10:59:11,577 >> tokenizer config file saved in results/adapters/ag/checkpoint-58500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 10:59:11,577 >> Special tokens file saved in results/adapters/ag/checkpoint-58500/special_tokens_map.json\n",
      "{'loss': 1.2128, 'learning_rate': 6.739977728285078e-05, 'epoch': 32.6}\n",
      "{'loss': 1.2502, 'learning_rate': 6.737193763919822e-05, 'epoch': 32.63}\n",
      "{'loss': 1.2085, 'learning_rate': 6.734409799554566e-05, 'epoch': 32.66}\n",
      "{'loss': 1.2197, 'learning_rate': 6.73162583518931e-05, 'epoch': 32.68}\n",
      "{'loss': 1.2029, 'learning_rate': 6.728841870824055e-05, 'epoch': 32.71}\n",
      "{'loss': 1.2351, 'learning_rate': 6.726057906458798e-05, 'epoch': 32.74}\n",
      "{'loss': 1.1912, 'learning_rate': 6.723273942093542e-05, 'epoch': 32.77}\n",
      "{'loss': 1.2311, 'learning_rate': 6.720489977728286e-05, 'epoch': 32.79}\n",
      "{'loss': 1.2466, 'learning_rate': 6.717706013363029e-05, 'epoch': 32.82}\n",
      "{'loss': 1.2092, 'learning_rate': 6.714922048997772e-05, 'epoch': 32.85}\n",
      " 33% 59000/179600 [4:51:39<9:54:46,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 11:01:40,291 >> Saving model checkpoint to results/adapters/ag/checkpoint-59000\n",
      "[INFO|loading.py:59] 2021-08-02 11:01:40,292 >> Configuration saved in results/adapters/ag/checkpoint-59000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:01:40,307 >> Module weights saved in results/adapters/ag/checkpoint-59000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:01:40,307 >> Configuration saved in results/adapters/ag/checkpoint-59000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:01:40,604 >> Module weights saved in results/adapters/ag/checkpoint-59000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:01:40,604 >> Configuration saved in results/adapters/ag/checkpoint-59000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:01:40,933 >> Module weights saved in results/adapters/ag/checkpoint-59000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:01:40,934 >> tokenizer config file saved in results/adapters/ag/checkpoint-59000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:01:40,934 >> Special tokens file saved in results/adapters/ag/checkpoint-59000/special_tokens_map.json\n",
      "{'loss': 1.2317, 'learning_rate': 6.712138084632516e-05, 'epoch': 32.88}\n",
      "{'loss': 1.2286, 'learning_rate': 6.70935412026726e-05, 'epoch': 32.91}\n",
      "{'loss': 1.2463, 'learning_rate': 6.706570155902005e-05, 'epoch': 32.93}\n",
      "{'loss': 1.27, 'learning_rate': 6.703786191536749e-05, 'epoch': 32.96}\n",
      "{'loss': 1.2044, 'learning_rate': 6.701002227171492e-05, 'epoch': 32.99}\n",
      "{'loss': 1.216, 'learning_rate': 6.698218262806236e-05, 'epoch': 33.02}\n",
      "{'loss': 1.2571, 'learning_rate': 6.69543429844098e-05, 'epoch': 33.05}\n",
      "{'loss': 1.233, 'learning_rate': 6.692650334075725e-05, 'epoch': 33.07}\n",
      "{'loss': 1.24, 'learning_rate': 6.689866369710468e-05, 'epoch': 33.1}\n",
      "{'loss': 1.2591, 'learning_rate': 6.687082405345212e-05, 'epoch': 33.13}\n",
      " 33% 59500/179600 [4:54:09<9:56:58,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 11:04:10,075 >> Saving model checkpoint to results/adapters/ag/checkpoint-59500\n",
      "[INFO|loading.py:59] 2021-08-02 11:04:10,075 >> Configuration saved in results/adapters/ag/checkpoint-59500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:04:10,088 >> Module weights saved in results/adapters/ag/checkpoint-59500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:04:10,088 >> Configuration saved in results/adapters/ag/checkpoint-59500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:04:10,404 >> Module weights saved in results/adapters/ag/checkpoint-59500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:04:10,404 >> Configuration saved in results/adapters/ag/checkpoint-59500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:04:10,738 >> Module weights saved in results/adapters/ag/checkpoint-59500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:04:10,739 >> tokenizer config file saved in results/adapters/ag/checkpoint-59500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:04:10,739 >> Special tokens file saved in results/adapters/ag/checkpoint-59500/special_tokens_map.json\n",
      "{'loss': 1.2285, 'learning_rate': 6.684298440979956e-05, 'epoch': 33.16}\n",
      "{'loss': 1.2577, 'learning_rate': 6.6815144766147e-05, 'epoch': 33.18}\n",
      "{'loss': 1.1946, 'learning_rate': 6.678730512249443e-05, 'epoch': 33.21}\n",
      "{'loss': 1.2504, 'learning_rate': 6.675946547884187e-05, 'epoch': 33.24}\n",
      "{'loss': 1.215, 'learning_rate': 6.673162583518932e-05, 'epoch': 33.27}\n",
      "{'loss': 1.206, 'learning_rate': 6.670378619153676e-05, 'epoch': 33.3}\n",
      "{'loss': 1.2436, 'learning_rate': 6.667594654788419e-05, 'epoch': 33.32}\n",
      "{'loss': 1.2143, 'learning_rate': 6.664810690423163e-05, 'epoch': 33.35}\n",
      "{'loss': 1.2239, 'learning_rate': 6.662026726057907e-05, 'epoch': 33.38}\n",
      "{'loss': 1.2267, 'learning_rate': 6.659242761692652e-05, 'epoch': 33.41}\n",
      " 33% 60000/179600 [4:56:38<9:54:52,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 11:06:39,297 >> Saving model checkpoint to results/adapters/ag/checkpoint-60000\n",
      "[INFO|loading.py:59] 2021-08-02 11:06:39,298 >> Configuration saved in results/adapters/ag/checkpoint-60000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:06:39,309 >> Module weights saved in results/adapters/ag/checkpoint-60000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:06:39,309 >> Configuration saved in results/adapters/ag/checkpoint-60000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:06:39,702 >> Module weights saved in results/adapters/ag/checkpoint-60000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:06:39,703 >> Configuration saved in results/adapters/ag/checkpoint-60000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:06:40,040 >> Module weights saved in results/adapters/ag/checkpoint-60000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:06:40,041 >> tokenizer config file saved in results/adapters/ag/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:06:40,041 >> Special tokens file saved in results/adapters/ag/checkpoint-60000/special_tokens_map.json\n",
      "{'loss': 1.1851, 'learning_rate': 6.656458797327395e-05, 'epoch': 33.44}\n",
      "{'loss': 1.2437, 'learning_rate': 6.653674832962137e-05, 'epoch': 33.46}\n",
      "{'loss': 1.2146, 'learning_rate': 6.650890868596882e-05, 'epoch': 33.49}\n",
      "{'loss': 1.1773, 'learning_rate': 6.648106904231626e-05, 'epoch': 33.52}\n",
      "{'loss': 1.2418, 'learning_rate': 6.64532293986637e-05, 'epoch': 33.55}\n",
      "{'loss': 1.231, 'learning_rate': 6.642538975501113e-05, 'epoch': 33.57}\n",
      "{'loss': 1.2376, 'learning_rate': 6.639755011135857e-05, 'epoch': 33.6}\n",
      "{'loss': 1.2294, 'learning_rate': 6.636971046770602e-05, 'epoch': 33.63}\n",
      "{'loss': 1.2109, 'learning_rate': 6.634187082405346e-05, 'epoch': 33.66}\n",
      "{'loss': 1.2491, 'learning_rate': 6.631403118040089e-05, 'epoch': 33.69}\n",
      " 34% 60500/179600 [4:59:07<10:09:37,  3.26it/s][INFO|trainer.py:1989] 2021-08-02 11:09:07,802 >> Saving model checkpoint to results/adapters/ag/checkpoint-60500\n",
      "[INFO|loading.py:59] 2021-08-02 11:09:07,803 >> Configuration saved in results/adapters/ag/checkpoint-60500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:09:07,814 >> Module weights saved in results/adapters/ag/checkpoint-60500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:09:07,815 >> Configuration saved in results/adapters/ag/checkpoint-60500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:09:08,117 >> Module weights saved in results/adapters/ag/checkpoint-60500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:09:08,117 >> Configuration saved in results/adapters/ag/checkpoint-60500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:09:08,449 >> Module weights saved in results/adapters/ag/checkpoint-60500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:09:08,449 >> tokenizer config file saved in results/adapters/ag/checkpoint-60500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:09:08,449 >> Special tokens file saved in results/adapters/ag/checkpoint-60500/special_tokens_map.json\n",
      "{'loss': 1.2571, 'learning_rate': 6.628619153674833e-05, 'epoch': 33.71}\n",
      "{'loss': 1.2198, 'learning_rate': 6.625835189309577e-05, 'epoch': 33.74}\n",
      "{'loss': 1.2053, 'learning_rate': 6.623051224944322e-05, 'epoch': 33.77}\n",
      "{'loss': 1.2172, 'learning_rate': 6.620267260579064e-05, 'epoch': 33.8}\n",
      "{'loss': 1.2079, 'learning_rate': 6.617483296213809e-05, 'epoch': 33.82}\n",
      "{'loss': 1.25, 'learning_rate': 6.614699331848553e-05, 'epoch': 33.85}\n",
      "{'loss': 1.1978, 'learning_rate': 6.611915367483297e-05, 'epoch': 33.88}\n",
      "{'loss': 1.2515, 'learning_rate': 6.60913140311804e-05, 'epoch': 33.91}\n",
      "{'loss': 1.2628, 'learning_rate': 6.606347438752784e-05, 'epoch': 33.94}\n",
      "{'loss': 1.1739, 'learning_rate': 6.603563474387529e-05, 'epoch': 33.96}\n",
      " 34% 61000/179600 [5:01:35<9:40:19,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 11:11:35,659 >> Saving model checkpoint to results/adapters/ag/checkpoint-61000\n",
      "[INFO|loading.py:59] 2021-08-02 11:11:35,660 >> Configuration saved in results/adapters/ag/checkpoint-61000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:11:35,672 >> Module weights saved in results/adapters/ag/checkpoint-61000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:11:35,672 >> Configuration saved in results/adapters/ag/checkpoint-61000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:11:35,965 >> Module weights saved in results/adapters/ag/checkpoint-61000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:11:35,965 >> Configuration saved in results/adapters/ag/checkpoint-61000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:11:36,298 >> Module weights saved in results/adapters/ag/checkpoint-61000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:11:36,298 >> tokenizer config file saved in results/adapters/ag/checkpoint-61000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:11:36,299 >> Special tokens file saved in results/adapters/ag/checkpoint-61000/special_tokens_map.json\n",
      "{'loss': 1.2674, 'learning_rate': 6.600779510022273e-05, 'epoch': 33.99}\n",
      "{'loss': 1.2385, 'learning_rate': 6.597995545657016e-05, 'epoch': 34.02}\n",
      "{'loss': 1.2388, 'learning_rate': 6.59521158129176e-05, 'epoch': 34.05}\n",
      "{'loss': 1.2434, 'learning_rate': 6.592427616926503e-05, 'epoch': 34.08}\n",
      "{'loss': 1.2684, 'learning_rate': 6.589643652561247e-05, 'epoch': 34.1}\n",
      "{'loss': 1.2131, 'learning_rate': 6.586859688195991e-05, 'epoch': 34.13}\n",
      "{'loss': 1.2108, 'learning_rate': 6.584075723830734e-05, 'epoch': 34.16}\n",
      "{'loss': 1.2149, 'learning_rate': 6.581291759465479e-05, 'epoch': 34.19}\n",
      "{'loss': 1.2143, 'learning_rate': 6.578507795100223e-05, 'epoch': 34.21}\n",
      "{'loss': 1.2335, 'learning_rate': 6.575723830734967e-05, 'epoch': 34.24}\n",
      " 34% 61500/179600 [5:04:04<9:57:27,  3.29it/s][INFO|trainer.py:1989] 2021-08-02 11:14:04,978 >> Saving model checkpoint to results/adapters/ag/checkpoint-61500\n",
      "[INFO|loading.py:59] 2021-08-02 11:14:04,979 >> Configuration saved in results/adapters/ag/checkpoint-61500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:14:04,992 >> Module weights saved in results/adapters/ag/checkpoint-61500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:14:04,992 >> Configuration saved in results/adapters/ag/checkpoint-61500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:14:05,311 >> Module weights saved in results/adapters/ag/checkpoint-61500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:14:05,311 >> Configuration saved in results/adapters/ag/checkpoint-61500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:14:05,655 >> Module weights saved in results/adapters/ag/checkpoint-61500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:14:05,656 >> tokenizer config file saved in results/adapters/ag/checkpoint-61500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:14:05,656 >> Special tokens file saved in results/adapters/ag/checkpoint-61500/special_tokens_map.json\n",
      "{'loss': 1.2403, 'learning_rate': 6.57293986636971e-05, 'epoch': 34.27}\n",
      "{'loss': 1.2352, 'learning_rate': 6.570155902004454e-05, 'epoch': 34.3}\n",
      "{'loss': 1.204, 'learning_rate': 6.567371937639199e-05, 'epoch': 34.33}\n",
      "{'loss': 1.2279, 'learning_rate': 6.564587973273943e-05, 'epoch': 34.35}\n",
      "{'loss': 1.207, 'learning_rate': 6.561804008908686e-05, 'epoch': 34.38}\n",
      "{'loss': 1.2408, 'learning_rate': 6.55902004454343e-05, 'epoch': 34.41}\n",
      "{'loss': 1.2361, 'learning_rate': 6.556236080178174e-05, 'epoch': 34.44}\n",
      "{'loss': 1.2393, 'learning_rate': 6.553452115812918e-05, 'epoch': 34.47}\n",
      "{'loss': 1.2497, 'learning_rate': 6.550668151447661e-05, 'epoch': 34.49}\n",
      "{'loss': 1.249, 'learning_rate': 6.547884187082406e-05, 'epoch': 34.52}\n",
      " 35% 62000/179600 [5:06:32<9:09:05,  3.57it/s][INFO|trainer.py:1989] 2021-08-02 11:16:32,840 >> Saving model checkpoint to results/adapters/ag/checkpoint-62000\n",
      "[INFO|loading.py:59] 2021-08-02 11:16:32,841 >> Configuration saved in results/adapters/ag/checkpoint-62000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:16:32,852 >> Module weights saved in results/adapters/ag/checkpoint-62000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:16:32,853 >> Configuration saved in results/adapters/ag/checkpoint-62000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:16:33,141 >> Module weights saved in results/adapters/ag/checkpoint-62000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:16:33,142 >> Configuration saved in results/adapters/ag/checkpoint-62000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:16:33,463 >> Module weights saved in results/adapters/ag/checkpoint-62000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:16:33,463 >> tokenizer config file saved in results/adapters/ag/checkpoint-62000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:16:33,464 >> Special tokens file saved in results/adapters/ag/checkpoint-62000/special_tokens_map.json\n",
      "{'loss': 1.2069, 'learning_rate': 6.54510022271715e-05, 'epoch': 34.55}\n",
      "{'loss': 1.2056, 'learning_rate': 6.542316258351894e-05, 'epoch': 34.58}\n",
      "{'loss': 1.2323, 'learning_rate': 6.539532293986638e-05, 'epoch': 34.6}\n",
      "{'loss': 1.2225, 'learning_rate': 6.536748329621381e-05, 'epoch': 34.63}\n",
      "{'loss': 1.2136, 'learning_rate': 6.533964365256125e-05, 'epoch': 34.66}\n",
      "{'loss': 1.2098, 'learning_rate': 6.531180400890868e-05, 'epoch': 34.69}\n",
      "{'loss': 1.2259, 'learning_rate': 6.528396436525613e-05, 'epoch': 34.72}\n",
      "{'loss': 1.217, 'learning_rate': 6.525612472160356e-05, 'epoch': 34.74}\n",
      "{'loss': 1.2387, 'learning_rate': 6.5228285077951e-05, 'epoch': 34.77}\n",
      "{'loss': 1.216, 'learning_rate': 6.520044543429844e-05, 'epoch': 34.8}\n",
      " 35% 62500/179600 [5:09:00<9:34:42,  3.40it/s][INFO|trainer.py:1989] 2021-08-02 11:19:01,037 >> Saving model checkpoint to results/adapters/ag/checkpoint-62500\n",
      "[INFO|loading.py:59] 2021-08-02 11:19:01,037 >> Configuration saved in results/adapters/ag/checkpoint-62500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:19:01,050 >> Module weights saved in results/adapters/ag/checkpoint-62500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:19:01,050 >> Configuration saved in results/adapters/ag/checkpoint-62500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:19:01,349 >> Module weights saved in results/adapters/ag/checkpoint-62500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:19:01,349 >> Configuration saved in results/adapters/ag/checkpoint-62500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:19:01,688 >> Module weights saved in results/adapters/ag/checkpoint-62500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:19:01,688 >> tokenizer config file saved in results/adapters/ag/checkpoint-62500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:19:01,689 >> Special tokens file saved in results/adapters/ag/checkpoint-62500/special_tokens_map.json\n",
      "{'loss': 1.2214, 'learning_rate': 6.517260579064588e-05, 'epoch': 34.83}\n",
      "{'loss': 1.2036, 'learning_rate': 6.514476614699333e-05, 'epoch': 34.85}\n",
      "{'loss': 1.241, 'learning_rate': 6.511692650334075e-05, 'epoch': 34.88}\n",
      "{'loss': 1.2361, 'learning_rate': 6.50890868596882e-05, 'epoch': 34.91}\n",
      "{'loss': 1.2413, 'learning_rate': 6.506124721603564e-05, 'epoch': 34.94}\n",
      "{'loss': 1.2462, 'learning_rate': 6.503340757238308e-05, 'epoch': 34.97}\n",
      "{'loss': 1.2329, 'learning_rate': 6.500556792873051e-05, 'epoch': 34.99}\n",
      "{'loss': 1.2394, 'learning_rate': 6.497772828507795e-05, 'epoch': 35.02}\n",
      "{'loss': 1.2136, 'learning_rate': 6.49498886414254e-05, 'epoch': 35.05}\n",
      "{'loss': 1.2005, 'learning_rate': 6.492204899777284e-05, 'epoch': 35.08}\n",
      " 35% 63000/179600 [5:11:29<9:19:53,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 11:21:29,554 >> Saving model checkpoint to results/adapters/ag/checkpoint-63000\n",
      "[INFO|loading.py:59] 2021-08-02 11:21:29,554 >> Configuration saved in results/adapters/ag/checkpoint-63000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:21:29,566 >> Module weights saved in results/adapters/ag/checkpoint-63000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:21:29,566 >> Configuration saved in results/adapters/ag/checkpoint-63000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:21:29,857 >> Module weights saved in results/adapters/ag/checkpoint-63000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:21:29,858 >> Configuration saved in results/adapters/ag/checkpoint-63000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:21:30,185 >> Module weights saved in results/adapters/ag/checkpoint-63000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:21:30,186 >> tokenizer config file saved in results/adapters/ag/checkpoint-63000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:21:30,186 >> Special tokens file saved in results/adapters/ag/checkpoint-63000/special_tokens_map.json\n",
      "{'loss': 1.2522, 'learning_rate': 6.489420935412027e-05, 'epoch': 35.11}\n",
      "{'loss': 1.2403, 'learning_rate': 6.486636971046771e-05, 'epoch': 35.13}\n",
      "{'loss': 1.2237, 'learning_rate': 6.483853006681515e-05, 'epoch': 35.16}\n",
      "{'loss': 1.2247, 'learning_rate': 6.48106904231626e-05, 'epoch': 35.19}\n",
      "{'loss': 1.1984, 'learning_rate': 6.478285077951002e-05, 'epoch': 35.22}\n",
      "{'loss': 1.2167, 'learning_rate': 6.475501113585747e-05, 'epoch': 35.24}\n",
      "{'loss': 1.2005, 'learning_rate': 6.472717149220491e-05, 'epoch': 35.27}\n",
      "{'loss': 1.21, 'learning_rate': 6.469933184855235e-05, 'epoch': 35.3}\n",
      "{'loss': 1.2141, 'learning_rate': 6.467149220489978e-05, 'epoch': 35.33}\n",
      "{'loss': 1.219, 'learning_rate': 6.464365256124721e-05, 'epoch': 35.36}\n",
      " 35% 63500/179600 [5:13:57<9:58:10,  3.23it/s][INFO|trainer.py:1989] 2021-08-02 11:23:58,192 >> Saving model checkpoint to results/adapters/ag/checkpoint-63500\n",
      "[INFO|loading.py:59] 2021-08-02 11:23:58,192 >> Configuration saved in results/adapters/ag/checkpoint-63500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:23:58,207 >> Module weights saved in results/adapters/ag/checkpoint-63500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:23:58,208 >> Configuration saved in results/adapters/ag/checkpoint-63500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:23:58,517 >> Module weights saved in results/adapters/ag/checkpoint-63500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:23:58,518 >> Configuration saved in results/adapters/ag/checkpoint-63500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:23:58,862 >> Module weights saved in results/adapters/ag/checkpoint-63500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:23:58,862 >> tokenizer config file saved in results/adapters/ag/checkpoint-63500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:23:58,862 >> Special tokens file saved in results/adapters/ag/checkpoint-63500/special_tokens_map.json\n",
      "{'loss': 1.2039, 'learning_rate': 6.461581291759465e-05, 'epoch': 35.38}\n",
      "{'loss': 1.2544, 'learning_rate': 6.45879732739421e-05, 'epoch': 35.41}\n",
      "{'loss': 1.2134, 'learning_rate': 6.456013363028954e-05, 'epoch': 35.44}\n",
      "{'loss': 1.2161, 'learning_rate': 6.453229398663697e-05, 'epoch': 35.47}\n",
      "{'loss': 1.2319, 'learning_rate': 6.450445434298441e-05, 'epoch': 35.5}\n",
      "{'loss': 1.2418, 'learning_rate': 6.447661469933185e-05, 'epoch': 35.52}\n",
      "{'loss': 1.2204, 'learning_rate': 6.44487750556793e-05, 'epoch': 35.55}\n",
      "{'loss': 1.234, 'learning_rate': 6.442093541202672e-05, 'epoch': 35.58}\n",
      "{'loss': 1.2064, 'learning_rate': 6.439309576837417e-05, 'epoch': 35.61}\n",
      "{'loss': 1.2188, 'learning_rate': 6.436525612472161e-05, 'epoch': 35.63}\n",
      " 36% 64000/179600 [5:16:25<9:14:41,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 11:26:26,156 >> Saving model checkpoint to results/adapters/ag/checkpoint-64000\n",
      "[INFO|loading.py:59] 2021-08-02 11:26:26,157 >> Configuration saved in results/adapters/ag/checkpoint-64000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:26:26,168 >> Module weights saved in results/adapters/ag/checkpoint-64000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:26:26,169 >> Configuration saved in results/adapters/ag/checkpoint-64000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:26:26,465 >> Module weights saved in results/adapters/ag/checkpoint-64000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:26:26,465 >> Configuration saved in results/adapters/ag/checkpoint-64000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:26:26,792 >> Module weights saved in results/adapters/ag/checkpoint-64000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:26:26,792 >> tokenizer config file saved in results/adapters/ag/checkpoint-64000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:26:26,792 >> Special tokens file saved in results/adapters/ag/checkpoint-64000/special_tokens_map.json\n",
      "{'loss': 1.2232, 'learning_rate': 6.433741648106905e-05, 'epoch': 35.66}\n",
      "{'loss': 1.2226, 'learning_rate': 6.430957683741648e-05, 'epoch': 35.69}\n",
      "{'loss': 1.2127, 'learning_rate': 6.428173719376392e-05, 'epoch': 35.72}\n",
      "{'loss': 1.2154, 'learning_rate': 6.425389755011137e-05, 'epoch': 35.75}\n",
      "{'loss': 1.213, 'learning_rate': 6.422605790645881e-05, 'epoch': 35.77}\n",
      "{'loss': 1.2076, 'learning_rate': 6.419821826280624e-05, 'epoch': 35.8}\n",
      "{'loss': 1.2256, 'learning_rate': 6.417037861915368e-05, 'epoch': 35.83}\n",
      "{'loss': 1.2099, 'learning_rate': 6.414253897550112e-05, 'epoch': 35.86}\n",
      "{'loss': 1.2549, 'learning_rate': 6.411469933184856e-05, 'epoch': 35.88}\n",
      "{'loss': 1.2044, 'learning_rate': 6.408685968819599e-05, 'epoch': 35.91}\n",
      " 36% 64500/179600 [5:18:53<9:28:50,  3.37it/s][INFO|trainer.py:1989] 2021-08-02 11:28:53,645 >> Saving model checkpoint to results/adapters/ag/checkpoint-64500\n",
      "[INFO|loading.py:59] 2021-08-02 11:28:53,645 >> Configuration saved in results/adapters/ag/checkpoint-64500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:28:53,656 >> Module weights saved in results/adapters/ag/checkpoint-64500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:28:53,657 >> Configuration saved in results/adapters/ag/checkpoint-64500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:28:53,964 >> Module weights saved in results/adapters/ag/checkpoint-64500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:28:53,965 >> Configuration saved in results/adapters/ag/checkpoint-64500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:28:54,293 >> Module weights saved in results/adapters/ag/checkpoint-64500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:28:54,294 >> tokenizer config file saved in results/adapters/ag/checkpoint-64500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:28:54,294 >> Special tokens file saved in results/adapters/ag/checkpoint-64500/special_tokens_map.json\n",
      "{'loss': 1.2451, 'learning_rate': 6.405902004454342e-05, 'epoch': 35.94}\n",
      "{'loss': 1.1895, 'learning_rate': 6.403118040089086e-05, 'epoch': 35.97}\n",
      "{'loss': 1.2788, 'learning_rate': 6.400334075723831e-05, 'epoch': 36.0}\n",
      "{'loss': 1.2364, 'learning_rate': 6.397550111358575e-05, 'epoch': 36.02}\n",
      "{'loss': 1.2357, 'learning_rate': 6.394766146993318e-05, 'epoch': 36.05}\n",
      "{'loss': 1.2078, 'learning_rate': 6.391982182628062e-05, 'epoch': 36.08}\n",
      "{'loss': 1.1962, 'learning_rate': 6.389198218262806e-05, 'epoch': 36.11}\n",
      "{'loss': 1.2133, 'learning_rate': 6.38641425389755e-05, 'epoch': 36.14}\n",
      "{'loss': 1.2242, 'learning_rate': 6.383630289532294e-05, 'epoch': 36.16}\n",
      "{'loss': 1.2461, 'learning_rate': 6.380846325167038e-05, 'epoch': 36.19}\n",
      " 36% 65000/179600 [5:21:22<9:11:13,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 11:31:22,539 >> Saving model checkpoint to results/adapters/ag/checkpoint-65000\n",
      "[INFO|loading.py:59] 2021-08-02 11:31:22,540 >> Configuration saved in results/adapters/ag/checkpoint-65000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:31:22,552 >> Module weights saved in results/adapters/ag/checkpoint-65000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:31:22,552 >> Configuration saved in results/adapters/ag/checkpoint-65000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:31:22,853 >> Module weights saved in results/adapters/ag/checkpoint-65000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:31:22,854 >> Configuration saved in results/adapters/ag/checkpoint-65000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:31:23,187 >> Module weights saved in results/adapters/ag/checkpoint-65000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:31:23,188 >> tokenizer config file saved in results/adapters/ag/checkpoint-65000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:31:23,188 >> Special tokens file saved in results/adapters/ag/checkpoint-65000/special_tokens_map.json\n",
      "{'loss': 1.2258, 'learning_rate': 6.378062360801782e-05, 'epoch': 36.22}\n",
      "{'loss': 1.2141, 'learning_rate': 6.375278396436526e-05, 'epoch': 36.25}\n",
      "{'loss': 1.2111, 'learning_rate': 6.372494432071269e-05, 'epoch': 36.27}\n",
      "{'loss': 1.2288, 'learning_rate': 6.369710467706013e-05, 'epoch': 36.3}\n",
      "{'loss': 1.229, 'learning_rate': 6.366926503340758e-05, 'epoch': 36.33}\n",
      "{'loss': 1.2118, 'learning_rate': 6.364142538975502e-05, 'epoch': 36.36}\n",
      "{'loss': 1.2226, 'learning_rate': 6.361358574610245e-05, 'epoch': 36.39}\n",
      "{'loss': 1.2171, 'learning_rate': 6.358574610244989e-05, 'epoch': 36.41}\n",
      "{'loss': 1.1851, 'learning_rate': 6.355790645879733e-05, 'epoch': 36.44}\n",
      "{'loss': 1.2425, 'learning_rate': 6.353006681514478e-05, 'epoch': 36.47}\n",
      " 36% 65500/179600 [5:23:50<9:26:41,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 11:33:50,670 >> Saving model checkpoint to results/adapters/ag/checkpoint-65500\n",
      "[INFO|loading.py:59] 2021-08-02 11:33:50,671 >> Configuration saved in results/adapters/ag/checkpoint-65500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:33:50,684 >> Module weights saved in results/adapters/ag/checkpoint-65500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:33:50,684 >> Configuration saved in results/adapters/ag/checkpoint-65500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:33:50,988 >> Module weights saved in results/adapters/ag/checkpoint-65500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:33:50,989 >> Configuration saved in results/adapters/ag/checkpoint-65500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:33:51,325 >> Module weights saved in results/adapters/ag/checkpoint-65500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:33:51,326 >> tokenizer config file saved in results/adapters/ag/checkpoint-65500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:33:51,326 >> Special tokens file saved in results/adapters/ag/checkpoint-65500/special_tokens_map.json\n",
      "{'loss': 1.2483, 'learning_rate': 6.350222717149222e-05, 'epoch': 36.5}\n",
      "{'loss': 1.243, 'learning_rate': 6.347438752783965e-05, 'epoch': 36.53}\n",
      "{'loss': 1.2177, 'learning_rate': 6.344654788418709e-05, 'epoch': 36.55}\n",
      "{'loss': 1.2299, 'learning_rate': 6.341870824053452e-05, 'epoch': 36.58}\n",
      "{'loss': 1.2293, 'learning_rate': 6.339086859688196e-05, 'epoch': 36.61}\n",
      "{'loss': 1.2296, 'learning_rate': 6.336302895322939e-05, 'epoch': 36.64}\n",
      "{'loss': 1.2284, 'learning_rate': 6.333518930957683e-05, 'epoch': 36.66}\n",
      "{'loss': 1.2265, 'learning_rate': 6.330734966592428e-05, 'epoch': 36.69}\n",
      "{'loss': 1.2147, 'learning_rate': 6.327951002227172e-05, 'epoch': 36.72}\n",
      "{'loss': 1.2274, 'learning_rate': 6.325167037861916e-05, 'epoch': 36.75}\n",
      " 37% 66000/179600 [5:26:18<9:27:14,  3.34it/s][INFO|trainer.py:1989] 2021-08-02 11:36:19,248 >> Saving model checkpoint to results/adapters/ag/checkpoint-66000\n",
      "[INFO|loading.py:59] 2021-08-02 11:36:19,249 >> Configuration saved in results/adapters/ag/checkpoint-66000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:36:19,260 >> Module weights saved in results/adapters/ag/checkpoint-66000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:36:19,261 >> Configuration saved in results/adapters/ag/checkpoint-66000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:36:19,561 >> Module weights saved in results/adapters/ag/checkpoint-66000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:36:19,562 >> Configuration saved in results/adapters/ag/checkpoint-66000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:36:19,898 >> Module weights saved in results/adapters/ag/checkpoint-66000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:36:19,899 >> tokenizer config file saved in results/adapters/ag/checkpoint-66000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:36:19,899 >> Special tokens file saved in results/adapters/ag/checkpoint-66000/special_tokens_map.json\n",
      "{'loss': 1.2346, 'learning_rate': 6.322383073496659e-05, 'epoch': 36.78}\n",
      "{'loss': 1.2241, 'learning_rate': 6.319599109131403e-05, 'epoch': 36.8}\n",
      "{'loss': 1.2329, 'learning_rate': 6.316815144766148e-05, 'epoch': 36.83}\n",
      "{'loss': 1.2039, 'learning_rate': 6.314031180400892e-05, 'epoch': 36.86}\n",
      "{'loss': 1.1974, 'learning_rate': 6.311247216035635e-05, 'epoch': 36.89}\n",
      "{'loss': 1.1868, 'learning_rate': 6.308463251670379e-05, 'epoch': 36.91}\n",
      "{'loss': 1.2458, 'learning_rate': 6.305679287305123e-05, 'epoch': 36.94}\n",
      "{'loss': 1.2175, 'learning_rate': 6.302895322939867e-05, 'epoch': 36.97}\n",
      "{'loss': 1.2527, 'learning_rate': 6.30011135857461e-05, 'epoch': 37.0}\n",
      "{'loss': 1.2244, 'learning_rate': 6.297327394209355e-05, 'epoch': 37.03}\n",
      " 37% 66500/179600 [5:28:48<9:21:51,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 11:38:48,528 >> Saving model checkpoint to results/adapters/ag/checkpoint-66500\n",
      "[INFO|loading.py:59] 2021-08-02 11:38:48,528 >> Configuration saved in results/adapters/ag/checkpoint-66500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:38:48,540 >> Module weights saved in results/adapters/ag/checkpoint-66500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:38:48,541 >> Configuration saved in results/adapters/ag/checkpoint-66500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:38:48,848 >> Module weights saved in results/adapters/ag/checkpoint-66500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:38:48,849 >> Configuration saved in results/adapters/ag/checkpoint-66500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:38:49,192 >> Module weights saved in results/adapters/ag/checkpoint-66500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:38:49,193 >> tokenizer config file saved in results/adapters/ag/checkpoint-66500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:38:49,193 >> Special tokens file saved in results/adapters/ag/checkpoint-66500/special_tokens_map.json\n",
      "{'loss': 1.2297, 'learning_rate': 6.294543429844099e-05, 'epoch': 37.05}\n",
      "{'loss': 1.2465, 'learning_rate': 6.291759465478843e-05, 'epoch': 37.08}\n",
      "{'loss': 1.214, 'learning_rate': 6.288975501113586e-05, 'epoch': 37.11}\n",
      "{'loss': 1.2199, 'learning_rate': 6.28619153674833e-05, 'epoch': 37.14}\n",
      "{'loss': 1.2585, 'learning_rate': 6.283407572383074e-05, 'epoch': 37.17}\n",
      "{'loss': 1.1942, 'learning_rate': 6.280623608017817e-05, 'epoch': 37.19}\n",
      "{'loss': 1.203, 'learning_rate': 6.277839643652562e-05, 'epoch': 37.22}\n",
      "{'loss': 1.2047, 'learning_rate': 6.275055679287305e-05, 'epoch': 37.25}\n",
      "{'loss': 1.2256, 'learning_rate': 6.272271714922049e-05, 'epoch': 37.28}\n",
      "{'loss': 1.2103, 'learning_rate': 6.269487750556793e-05, 'epoch': 37.3}\n",
      " 37% 67000/179600 [5:31:16<9:24:30,  3.32it/s][INFO|trainer.py:1989] 2021-08-02 11:41:16,496 >> Saving model checkpoint to results/adapters/ag/checkpoint-67000\n",
      "[INFO|loading.py:59] 2021-08-02 11:41:16,497 >> Configuration saved in results/adapters/ag/checkpoint-67000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:41:16,509 >> Module weights saved in results/adapters/ag/checkpoint-67000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:41:16,509 >> Configuration saved in results/adapters/ag/checkpoint-67000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:41:16,811 >> Module weights saved in results/adapters/ag/checkpoint-67000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:41:16,811 >> Configuration saved in results/adapters/ag/checkpoint-67000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:41:17,157 >> Module weights saved in results/adapters/ag/checkpoint-67000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:41:17,157 >> tokenizer config file saved in results/adapters/ag/checkpoint-67000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:41:17,157 >> Special tokens file saved in results/adapters/ag/checkpoint-67000/special_tokens_map.json\n",
      "{'loss': 1.254, 'learning_rate': 6.266703786191537e-05, 'epoch': 37.33}\n",
      "{'loss': 1.2319, 'learning_rate': 6.26391982182628e-05, 'epoch': 37.36}\n",
      "{'loss': 1.2524, 'learning_rate': 6.261135857461024e-05, 'epoch': 37.39}\n",
      "{'loss': 1.2281, 'learning_rate': 6.258351893095769e-05, 'epoch': 37.42}\n",
      "{'loss': 1.2184, 'learning_rate': 6.255567928730513e-05, 'epoch': 37.44}\n",
      "{'loss': 1.2118, 'learning_rate': 6.252783964365256e-05, 'epoch': 37.47}\n",
      "{'loss': 1.2044, 'learning_rate': 6.25e-05, 'epoch': 37.5}\n",
      "{'loss': 1.1944, 'learning_rate': 6.247216035634744e-05, 'epoch': 37.53}\n",
      "{'loss': 1.2083, 'learning_rate': 6.244432071269489e-05, 'epoch': 37.56}\n",
      "{'loss': 1.2239, 'learning_rate': 6.241648106904232e-05, 'epoch': 37.58}\n",
      " 38% 67500/179600 [5:33:43<9:04:46,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 11:43:44,177 >> Saving model checkpoint to results/adapters/ag/checkpoint-67500\n",
      "[INFO|loading.py:59] 2021-08-02 11:43:44,178 >> Configuration saved in results/adapters/ag/checkpoint-67500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:43:44,190 >> Module weights saved in results/adapters/ag/checkpoint-67500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:43:44,190 >> Configuration saved in results/adapters/ag/checkpoint-67500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:43:44,493 >> Module weights saved in results/adapters/ag/checkpoint-67500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:43:44,494 >> Configuration saved in results/adapters/ag/checkpoint-67500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:43:44,825 >> Module weights saved in results/adapters/ag/checkpoint-67500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:43:44,825 >> tokenizer config file saved in results/adapters/ag/checkpoint-67500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:43:44,826 >> Special tokens file saved in results/adapters/ag/checkpoint-67500/special_tokens_map.json\n",
      "{'loss': 1.2156, 'learning_rate': 6.238864142538976e-05, 'epoch': 37.61}\n",
      "{'loss': 1.2368, 'learning_rate': 6.23608017817372e-05, 'epoch': 37.64}\n",
      "{'loss': 1.2271, 'learning_rate': 6.233296213808464e-05, 'epoch': 37.67}\n",
      "{'loss': 1.2042, 'learning_rate': 6.230512249443207e-05, 'epoch': 37.69}\n",
      "{'loss': 1.2133, 'learning_rate': 6.227728285077951e-05, 'epoch': 37.72}\n",
      "{'loss': 1.26, 'learning_rate': 6.224944320712696e-05, 'epoch': 37.75}\n",
      "{'loss': 1.2177, 'learning_rate': 6.22216035634744e-05, 'epoch': 37.78}\n",
      "{'loss': 1.2095, 'learning_rate': 6.219376391982183e-05, 'epoch': 37.81}\n",
      "{'loss': 1.232, 'learning_rate': 6.216592427616926e-05, 'epoch': 37.83}\n",
      "{'loss': 1.2168, 'learning_rate': 6.21380846325167e-05, 'epoch': 37.86}\n",
      " 38% 68000/179600 [5:36:11<9:12:29,  3.37it/s][INFO|trainer.py:1989] 2021-08-02 11:46:12,058 >> Saving model checkpoint to results/adapters/ag/checkpoint-68000\n",
      "[INFO|loading.py:59] 2021-08-02 11:46:12,058 >> Configuration saved in results/adapters/ag/checkpoint-68000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:46:12,070 >> Module weights saved in results/adapters/ag/checkpoint-68000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:46:12,071 >> Configuration saved in results/adapters/ag/checkpoint-68000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:46:12,382 >> Module weights saved in results/adapters/ag/checkpoint-68000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:46:12,383 >> Configuration saved in results/adapters/ag/checkpoint-68000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:46:12,714 >> Module weights saved in results/adapters/ag/checkpoint-68000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:46:12,715 >> tokenizer config file saved in results/adapters/ag/checkpoint-68000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:46:12,715 >> Special tokens file saved in results/adapters/ag/checkpoint-68000/special_tokens_map.json\n",
      "{'loss': 1.2234, 'learning_rate': 6.211024498886414e-05, 'epoch': 37.89}\n",
      "{'loss': 1.2329, 'learning_rate': 6.208240534521159e-05, 'epoch': 37.92}\n",
      "{'loss': 1.2336, 'learning_rate': 6.205456570155901e-05, 'epoch': 37.94}\n",
      "{'loss': 1.2233, 'learning_rate': 6.202672605790646e-05, 'epoch': 37.97}\n",
      "{'loss': 1.2094, 'learning_rate': 6.19988864142539e-05, 'epoch': 38.0}\n",
      "{'loss': 1.2406, 'learning_rate': 6.197104677060134e-05, 'epoch': 38.03}\n",
      "{'loss': 1.1989, 'learning_rate': 6.194320712694877e-05, 'epoch': 38.06}\n",
      "{'loss': 1.195, 'learning_rate': 6.191536748329621e-05, 'epoch': 38.08}\n",
      "{'loss': 1.1837, 'learning_rate': 6.188752783964366e-05, 'epoch': 38.11}\n",
      "{'loss': 1.2159, 'learning_rate': 6.18596881959911e-05, 'epoch': 38.14}\n",
      " 38% 68500/179600 [5:38:40<9:11:51,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 11:48:40,568 >> Saving model checkpoint to results/adapters/ag/checkpoint-68500\n",
      "[INFO|loading.py:59] 2021-08-02 11:48:40,569 >> Configuration saved in results/adapters/ag/checkpoint-68500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:48:40,581 >> Module weights saved in results/adapters/ag/checkpoint-68500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:48:40,581 >> Configuration saved in results/adapters/ag/checkpoint-68500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:48:40,887 >> Module weights saved in results/adapters/ag/checkpoint-68500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:48:40,888 >> Configuration saved in results/adapters/ag/checkpoint-68500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:48:41,234 >> Module weights saved in results/adapters/ag/checkpoint-68500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:48:41,234 >> tokenizer config file saved in results/adapters/ag/checkpoint-68500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:48:41,234 >> Special tokens file saved in results/adapters/ag/checkpoint-68500/special_tokens_map.json\n",
      "{'loss': 1.2574, 'learning_rate': 6.183184855233853e-05, 'epoch': 38.17}\n",
      "{'loss': 1.2301, 'learning_rate': 6.180400890868597e-05, 'epoch': 38.2}\n",
      "{'loss': 1.1909, 'learning_rate': 6.177616926503341e-05, 'epoch': 38.22}\n",
      "{'loss': 1.2251, 'learning_rate': 6.174832962138086e-05, 'epoch': 38.25}\n",
      "{'loss': 1.2163, 'learning_rate': 6.172048997772828e-05, 'epoch': 38.28}\n",
      "{'loss': 1.2169, 'learning_rate': 6.169265033407573e-05, 'epoch': 38.31}\n",
      "{'loss': 1.2378, 'learning_rate': 6.166481069042317e-05, 'epoch': 38.34}\n",
      "{'loss': 1.2228, 'learning_rate': 6.163697104677061e-05, 'epoch': 38.36}\n",
      "{'loss': 1.2091, 'learning_rate': 6.160913140311805e-05, 'epoch': 38.39}\n",
      "{'loss': 1.2238, 'learning_rate': 6.158129175946548e-05, 'epoch': 38.42}\n",
      " 38% 69000/179600 [5:41:07<8:53:17,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 11:51:07,707 >> Saving model checkpoint to results/adapters/ag/checkpoint-69000\n",
      "[INFO|loading.py:59] 2021-08-02 11:51:07,708 >> Configuration saved in results/adapters/ag/checkpoint-69000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:51:07,724 >> Module weights saved in results/adapters/ag/checkpoint-69000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:51:07,724 >> Configuration saved in results/adapters/ag/checkpoint-69000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:51:08,020 >> Module weights saved in results/adapters/ag/checkpoint-69000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:51:08,020 >> Configuration saved in results/adapters/ag/checkpoint-69000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:51:08,359 >> Module weights saved in results/adapters/ag/checkpoint-69000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:51:08,359 >> tokenizer config file saved in results/adapters/ag/checkpoint-69000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:51:08,359 >> Special tokens file saved in results/adapters/ag/checkpoint-69000/special_tokens_map.json\n",
      "{'loss': 1.237, 'learning_rate': 6.155345211581291e-05, 'epoch': 38.45}\n",
      "{'loss': 1.1948, 'learning_rate': 6.152561247216035e-05, 'epoch': 38.47}\n",
      "{'loss': 1.2376, 'learning_rate': 6.14977728285078e-05, 'epoch': 38.5}\n",
      "{'loss': 1.2283, 'learning_rate': 6.146993318485523e-05, 'epoch': 38.53}\n",
      "{'loss': 1.2335, 'learning_rate': 6.144209354120267e-05, 'epoch': 38.56}\n",
      "{'loss': 1.22, 'learning_rate': 6.141425389755011e-05, 'epoch': 38.59}\n",
      "{'loss': 1.2056, 'learning_rate': 6.138641425389755e-05, 'epoch': 38.61}\n",
      "{'loss': 1.2247, 'learning_rate': 6.1358574610245e-05, 'epoch': 38.64}\n",
      "{'loss': 1.2407, 'learning_rate': 6.133073496659243e-05, 'epoch': 38.67}\n",
      "{'loss': 1.1992, 'learning_rate': 6.130289532293987e-05, 'epoch': 38.7}\n",
      " 39% 69500/179600 [5:43:34<8:45:45,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 11:53:35,429 >> Saving model checkpoint to results/adapters/ag/checkpoint-69500\n",
      "[INFO|loading.py:59] 2021-08-02 11:53:35,430 >> Configuration saved in results/adapters/ag/checkpoint-69500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:53:35,441 >> Module weights saved in results/adapters/ag/checkpoint-69500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:53:35,441 >> Configuration saved in results/adapters/ag/checkpoint-69500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:53:35,736 >> Module weights saved in results/adapters/ag/checkpoint-69500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:53:35,736 >> Configuration saved in results/adapters/ag/checkpoint-69500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:53:36,065 >> Module weights saved in results/adapters/ag/checkpoint-69500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:53:36,066 >> tokenizer config file saved in results/adapters/ag/checkpoint-69500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:53:36,066 >> Special tokens file saved in results/adapters/ag/checkpoint-69500/special_tokens_map.json\n",
      "{'loss': 1.2266, 'learning_rate': 6.127505567928731e-05, 'epoch': 38.72}\n",
      "{'loss': 1.203, 'learning_rate': 6.124721603563475e-05, 'epoch': 38.75}\n",
      "{'loss': 1.244, 'learning_rate': 6.121937639198218e-05, 'epoch': 38.78}\n",
      "{'loss': 1.1976, 'learning_rate': 6.119153674832962e-05, 'epoch': 38.81}\n",
      "{'loss': 1.2238, 'learning_rate': 6.116369710467707e-05, 'epoch': 38.84}\n",
      "{'loss': 1.2224, 'learning_rate': 6.113585746102451e-05, 'epoch': 38.86}\n",
      "{'loss': 1.1879, 'learning_rate': 6.110801781737194e-05, 'epoch': 38.89}\n",
      "{'loss': 1.245, 'learning_rate': 6.108017817371938e-05, 'epoch': 38.92}\n",
      "{'loss': 1.2085, 'learning_rate': 6.105233853006682e-05, 'epoch': 38.95}\n",
      "{'loss': 1.1779, 'learning_rate': 6.102449888641426e-05, 'epoch': 38.98}\n",
      " 39% 70000/179600 [5:46:03<9:09:55,  3.32it/s][INFO|trainer.py:1989] 2021-08-02 11:56:03,815 >> Saving model checkpoint to results/adapters/ag/checkpoint-70000\n",
      "[INFO|loading.py:59] 2021-08-02 11:56:03,816 >> Configuration saved in results/adapters/ag/checkpoint-70000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:56:03,828 >> Module weights saved in results/adapters/ag/checkpoint-70000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:56:03,829 >> Configuration saved in results/adapters/ag/checkpoint-70000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:56:04,139 >> Module weights saved in results/adapters/ag/checkpoint-70000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:56:04,140 >> Configuration saved in results/adapters/ag/checkpoint-70000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:56:04,474 >> Module weights saved in results/adapters/ag/checkpoint-70000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:56:04,474 >> tokenizer config file saved in results/adapters/ag/checkpoint-70000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:56:04,474 >> Special tokens file saved in results/adapters/ag/checkpoint-70000/special_tokens_map.json\n",
      "{'loss': 1.2471, 'learning_rate': 6.09966592427617e-05, 'epoch': 39.0}\n",
      "{'loss': 1.1926, 'learning_rate': 6.096881959910914e-05, 'epoch': 39.03}\n",
      "{'loss': 1.2134, 'learning_rate': 6.094097995545658e-05, 'epoch': 39.06}\n",
      "{'loss': 1.1796, 'learning_rate': 6.091314031180401e-05, 'epoch': 39.09}\n",
      "{'loss': 1.211, 'learning_rate': 6.0885300668151445e-05, 'epoch': 39.11}\n",
      "{'loss': 1.2015, 'learning_rate': 6.085746102449889e-05, 'epoch': 39.14}\n",
      "{'loss': 1.2331, 'learning_rate': 6.0829621380846324e-05, 'epoch': 39.17}\n",
      "{'loss': 1.1988, 'learning_rate': 6.0801781737193766e-05, 'epoch': 39.2}\n",
      "{'loss': 1.2057, 'learning_rate': 6.07739420935412e-05, 'epoch': 39.23}\n",
      "{'loss': 1.2336, 'learning_rate': 6.0746102449888644e-05, 'epoch': 39.25}\n",
      " 39% 70500/179600 [5:48:31<8:51:10,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 11:58:31,877 >> Saving model checkpoint to results/adapters/ag/checkpoint-70500\n",
      "[INFO|loading.py:59] 2021-08-02 11:58:31,878 >> Configuration saved in results/adapters/ag/checkpoint-70500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:58:31,889 >> Module weights saved in results/adapters/ag/checkpoint-70500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:58:31,889 >> Configuration saved in results/adapters/ag/checkpoint-70500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:58:32,182 >> Module weights saved in results/adapters/ag/checkpoint-70500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 11:58:32,183 >> Configuration saved in results/adapters/ag/checkpoint-70500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 11:58:32,503 >> Module weights saved in results/adapters/ag/checkpoint-70500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 11:58:32,504 >> tokenizer config file saved in results/adapters/ag/checkpoint-70500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 11:58:32,504 >> Special tokens file saved in results/adapters/ag/checkpoint-70500/special_tokens_map.json\n",
      "{'loss': 1.2089, 'learning_rate': 6.071826280623608e-05, 'epoch': 39.28}\n",
      "{'loss': 1.2208, 'learning_rate': 6.069042316258352e-05, 'epoch': 39.31}\n",
      "{'loss': 1.2095, 'learning_rate': 6.066258351893096e-05, 'epoch': 39.34}\n",
      "{'loss': 1.1999, 'learning_rate': 6.06347438752784e-05, 'epoch': 39.37}\n",
      "{'loss': 1.1775, 'learning_rate': 6.060690423162584e-05, 'epoch': 39.39}\n",
      "{'loss': 1.1918, 'learning_rate': 6.057906458797328e-05, 'epoch': 39.42}\n",
      "{'loss': 1.2102, 'learning_rate': 6.0551224944320715e-05, 'epoch': 39.45}\n",
      "{'loss': 1.2065, 'learning_rate': 6.052338530066816e-05, 'epoch': 39.48}\n",
      "{'loss': 1.2155, 'learning_rate': 6.049554565701559e-05, 'epoch': 39.5}\n",
      "{'loss': 1.2279, 'learning_rate': 6.0467706013363036e-05, 'epoch': 39.53}\n",
      " 40% 71000/179600 [5:50:59<9:06:36,  3.31it/s][INFO|trainer.py:1989] 2021-08-02 12:01:00,178 >> Saving model checkpoint to results/adapters/ag/checkpoint-71000\n",
      "[INFO|loading.py:59] 2021-08-02 12:01:00,179 >> Configuration saved in results/adapters/ag/checkpoint-71000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:01:00,191 >> Module weights saved in results/adapters/ag/checkpoint-71000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:01:00,191 >> Configuration saved in results/adapters/ag/checkpoint-71000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:01:00,490 >> Module weights saved in results/adapters/ag/checkpoint-71000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:01:00,491 >> Configuration saved in results/adapters/ag/checkpoint-71000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:01:00,826 >> Module weights saved in results/adapters/ag/checkpoint-71000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:01:00,827 >> tokenizer config file saved in results/adapters/ag/checkpoint-71000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:01:00,827 >> Special tokens file saved in results/adapters/ag/checkpoint-71000/special_tokens_map.json\n",
      "{'loss': 1.2326, 'learning_rate': 6.043986636971047e-05, 'epoch': 39.56}\n",
      "{'loss': 1.2331, 'learning_rate': 6.0412026726057914e-05, 'epoch': 39.59}\n",
      "{'loss': 1.2135, 'learning_rate': 6.038418708240535e-05, 'epoch': 39.62}\n",
      "{'loss': 1.2209, 'learning_rate': 6.035634743875279e-05, 'epoch': 39.64}\n",
      "{'loss': 1.2008, 'learning_rate': 6.032850779510023e-05, 'epoch': 39.67}\n",
      "{'loss': 1.2133, 'learning_rate': 6.030066815144766e-05, 'epoch': 39.7}\n",
      "{'loss': 1.2303, 'learning_rate': 6.02728285077951e-05, 'epoch': 39.73}\n",
      "{'loss': 1.2465, 'learning_rate': 6.0244988864142536e-05, 'epoch': 39.75}\n",
      "{'loss': 1.1853, 'learning_rate': 6.021714922048998e-05, 'epoch': 39.78}\n",
      "{'loss': 1.1733, 'learning_rate': 6.0189309576837414e-05, 'epoch': 39.81}\n",
      " 40% 71500/179600 [5:53:28<9:02:44,  3.32it/s][INFO|trainer.py:1989] 2021-08-02 12:03:28,479 >> Saving model checkpoint to results/adapters/ag/checkpoint-71500\n",
      "[INFO|loading.py:59] 2021-08-02 12:03:28,479 >> Configuration saved in results/adapters/ag/checkpoint-71500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:03:28,491 >> Module weights saved in results/adapters/ag/checkpoint-71500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:03:28,491 >> Configuration saved in results/adapters/ag/checkpoint-71500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:03:28,780 >> Module weights saved in results/adapters/ag/checkpoint-71500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:03:28,780 >> Configuration saved in results/adapters/ag/checkpoint-71500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:03:29,113 >> Module weights saved in results/adapters/ag/checkpoint-71500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:03:29,113 >> tokenizer config file saved in results/adapters/ag/checkpoint-71500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:03:29,114 >> Special tokens file saved in results/adapters/ag/checkpoint-71500/special_tokens_map.json\n",
      "{'loss': 1.1953, 'learning_rate': 6.0161469933184856e-05, 'epoch': 39.84}\n",
      "{'loss': 1.1938, 'learning_rate': 6.013363028953229e-05, 'epoch': 39.87}\n",
      "{'loss': 1.2335, 'learning_rate': 6.0105790645879735e-05, 'epoch': 39.89}\n",
      "{'loss': 1.2389, 'learning_rate': 6.007795100222717e-05, 'epoch': 39.92}\n",
      "{'loss': 1.2145, 'learning_rate': 6.005011135857461e-05, 'epoch': 39.95}\n",
      "{'loss': 1.2453, 'learning_rate': 6.002227171492205e-05, 'epoch': 39.98}\n",
      "{'loss': 1.2025, 'learning_rate': 5.999443207126949e-05, 'epoch': 40.01}\n",
      "{'loss': 1.2141, 'learning_rate': 5.996659242761693e-05, 'epoch': 40.03}\n",
      "{'loss': 1.2355, 'learning_rate': 5.993875278396437e-05, 'epoch': 40.06}\n",
      "{'loss': 1.2228, 'learning_rate': 5.9910913140311805e-05, 'epoch': 40.09}\n",
      " 40% 72000/179600 [5:55:51<8:44:02,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 12:05:52,165 >> Saving model checkpoint to results/adapters/ag/checkpoint-72000\n",
      "[INFO|loading.py:59] 2021-08-02 12:05:52,166 >> Configuration saved in results/adapters/ag/checkpoint-72000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:05:52,176 >> Module weights saved in results/adapters/ag/checkpoint-72000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:05:52,177 >> Configuration saved in results/adapters/ag/checkpoint-72000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:05:52,477 >> Module weights saved in results/adapters/ag/checkpoint-72000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:05:52,478 >> Configuration saved in results/adapters/ag/checkpoint-72000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:05:52,817 >> Module weights saved in results/adapters/ag/checkpoint-72000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:05:52,818 >> tokenizer config file saved in results/adapters/ag/checkpoint-72000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:05:52,818 >> Special tokens file saved in results/adapters/ag/checkpoint-72000/special_tokens_map.json\n",
      "{'loss': 1.255, 'learning_rate': 5.988307349665925e-05, 'epoch': 40.12}\n",
      "{'loss': 1.1916, 'learning_rate': 5.9855233853006684e-05, 'epoch': 40.14}\n",
      "{'loss': 1.2128, 'learning_rate': 5.9827394209354126e-05, 'epoch': 40.17}\n",
      "{'loss': 1.2063, 'learning_rate': 5.979955456570156e-05, 'epoch': 40.2}\n",
      "{'loss': 1.2231, 'learning_rate': 5.9771714922049005e-05, 'epoch': 40.23}\n",
      "{'loss': 1.2353, 'learning_rate': 5.974387527839645e-05, 'epoch': 40.26}\n",
      "{'loss': 1.2327, 'learning_rate': 5.971603563474388e-05, 'epoch': 40.28}\n",
      "{'loss': 1.2104, 'learning_rate': 5.9688195991091325e-05, 'epoch': 40.31}\n",
      "{'loss': 1.2387, 'learning_rate': 5.966035634743875e-05, 'epoch': 40.34}\n",
      "{'loss': 1.1964, 'learning_rate': 5.963251670378619e-05, 'epoch': 40.37}\n",
      " 40% 72500/179600 [5:58:13<8:37:45,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 12:08:14,311 >> Saving model checkpoint to results/adapters/ag/checkpoint-72500\n",
      "[INFO|loading.py:59] 2021-08-02 12:08:14,311 >> Configuration saved in results/adapters/ag/checkpoint-72500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:08:14,323 >> Module weights saved in results/adapters/ag/checkpoint-72500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:08:14,323 >> Configuration saved in results/adapters/ag/checkpoint-72500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:08:14,621 >> Module weights saved in results/adapters/ag/checkpoint-72500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:08:14,622 >> Configuration saved in results/adapters/ag/checkpoint-72500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:08:14,950 >> Module weights saved in results/adapters/ag/checkpoint-72500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:08:14,950 >> tokenizer config file saved in results/adapters/ag/checkpoint-72500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:08:14,950 >> Special tokens file saved in results/adapters/ag/checkpoint-72500/special_tokens_map.json\n",
      "{'loss': 1.2126, 'learning_rate': 5.9604677060133626e-05, 'epoch': 40.4}\n",
      "{'loss': 1.1917, 'learning_rate': 5.957683741648107e-05, 'epoch': 40.42}\n",
      "{'loss': 1.2126, 'learning_rate': 5.9548997772828504e-05, 'epoch': 40.45}\n",
      "{'loss': 1.2428, 'learning_rate': 5.952115812917595e-05, 'epoch': 40.48}\n",
      "{'loss': 1.2275, 'learning_rate': 5.949331848552339e-05, 'epoch': 40.51}\n",
      "{'loss': 1.2222, 'learning_rate': 5.9465478841870825e-05, 'epoch': 40.53}\n",
      "{'loss': 1.1846, 'learning_rate': 5.943763919821827e-05, 'epoch': 40.56}\n",
      "{'loss': 1.2222, 'learning_rate': 5.9409799554565703e-05, 'epoch': 40.59}\n",
      "{'loss': 1.2171, 'learning_rate': 5.9381959910913146e-05, 'epoch': 40.62}\n",
      "{'loss': 1.2331, 'learning_rate': 5.935412026726058e-05, 'epoch': 40.65}\n",
      " 41% 73000/179600 [6:00:35<8:19:29,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 12:10:35,665 >> Saving model checkpoint to results/adapters/ag/checkpoint-73000\n",
      "[INFO|loading.py:59] 2021-08-02 12:10:35,666 >> Configuration saved in results/adapters/ag/checkpoint-73000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:10:35,678 >> Module weights saved in results/adapters/ag/checkpoint-73000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:10:35,678 >> Configuration saved in results/adapters/ag/checkpoint-73000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:10:35,985 >> Module weights saved in results/adapters/ag/checkpoint-73000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:10:35,986 >> Configuration saved in results/adapters/ag/checkpoint-73000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:10:36,327 >> Module weights saved in results/adapters/ag/checkpoint-73000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:10:36,328 >> tokenizer config file saved in results/adapters/ag/checkpoint-73000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:10:36,328 >> Special tokens file saved in results/adapters/ag/checkpoint-73000/special_tokens_map.json\n",
      "{'loss': 1.2036, 'learning_rate': 5.9326280623608024e-05, 'epoch': 40.67}\n",
      "{'loss': 1.2374, 'learning_rate': 5.929844097995546e-05, 'epoch': 40.7}\n",
      "{'loss': 1.2077, 'learning_rate': 5.92706013363029e-05, 'epoch': 40.73}\n",
      "{'loss': 1.2554, 'learning_rate': 5.924276169265034e-05, 'epoch': 40.76}\n",
      "{'loss': 1.2223, 'learning_rate': 5.921492204899778e-05, 'epoch': 40.78}\n",
      "{'loss': 1.2198, 'learning_rate': 5.918708240534522e-05, 'epoch': 40.81}\n",
      "{'loss': 1.1918, 'learning_rate': 5.915924276169266e-05, 'epoch': 40.84}\n",
      "{'loss': 1.2176, 'learning_rate': 5.9131403118040095e-05, 'epoch': 40.87}\n",
      "{'loss': 1.2211, 'learning_rate': 5.910356347438754e-05, 'epoch': 40.9}\n",
      "{'loss': 1.2101, 'learning_rate': 5.907572383073497e-05, 'epoch': 40.92}\n",
      " 41% 73500/179600 [6:02:56<8:45:47,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 12:12:57,244 >> Saving model checkpoint to results/adapters/ag/checkpoint-73500\n",
      "[INFO|loading.py:59] 2021-08-02 12:12:57,244 >> Configuration saved in results/adapters/ag/checkpoint-73500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:12:57,255 >> Module weights saved in results/adapters/ag/checkpoint-73500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:12:57,256 >> Configuration saved in results/adapters/ag/checkpoint-73500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:12:57,556 >> Module weights saved in results/adapters/ag/checkpoint-73500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:12:57,556 >> Configuration saved in results/adapters/ag/checkpoint-73500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:12:57,893 >> Module weights saved in results/adapters/ag/checkpoint-73500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:12:57,894 >> tokenizer config file saved in results/adapters/ag/checkpoint-73500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:12:57,894 >> Special tokens file saved in results/adapters/ag/checkpoint-73500/special_tokens_map.json\n",
      "{'loss': 1.2219, 'learning_rate': 5.90478841870824e-05, 'epoch': 40.95}\n",
      "{'loss': 1.2034, 'learning_rate': 5.9020044543429845e-05, 'epoch': 40.98}\n",
      "{'loss': 1.2049, 'learning_rate': 5.899220489977728e-05, 'epoch': 41.01}\n",
      "{'loss': 1.1903, 'learning_rate': 5.896436525612472e-05, 'epoch': 41.04}\n",
      "{'loss': 1.1713, 'learning_rate': 5.893652561247216e-05, 'epoch': 41.06}\n",
      "{'loss': 1.2043, 'learning_rate': 5.89086859688196e-05, 'epoch': 41.09}\n",
      "{'loss': 1.1802, 'learning_rate': 5.888084632516704e-05, 'epoch': 41.12}\n",
      "{'loss': 1.2182, 'learning_rate': 5.885300668151448e-05, 'epoch': 41.15}\n",
      "{'loss': 1.2162, 'learning_rate': 5.8825167037861916e-05, 'epoch': 41.17}\n",
      "{'loss': 1.21, 'learning_rate': 5.879732739420936e-05, 'epoch': 41.2}\n",
      " 41% 74000/179600 [6:05:18<8:15:03,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 12:15:19,219 >> Saving model checkpoint to results/adapters/ag/checkpoint-74000\n",
      "[INFO|loading.py:59] 2021-08-02 12:15:19,219 >> Configuration saved in results/adapters/ag/checkpoint-74000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:15:19,232 >> Module weights saved in results/adapters/ag/checkpoint-74000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:15:19,232 >> Configuration saved in results/adapters/ag/checkpoint-74000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:15:19,538 >> Module weights saved in results/adapters/ag/checkpoint-74000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:15:19,538 >> Configuration saved in results/adapters/ag/checkpoint-74000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:15:19,879 >> Module weights saved in results/adapters/ag/checkpoint-74000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:15:19,879 >> tokenizer config file saved in results/adapters/ag/checkpoint-74000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:15:19,880 >> Special tokens file saved in results/adapters/ag/checkpoint-74000/special_tokens_map.json\n",
      "{'loss': 1.1921, 'learning_rate': 5.8769487750556794e-05, 'epoch': 41.23}\n",
      "{'loss': 1.2483, 'learning_rate': 5.8741648106904236e-05, 'epoch': 41.26}\n",
      "{'loss': 1.2198, 'learning_rate': 5.871380846325167e-05, 'epoch': 41.29}\n",
      "{'loss': 1.1923, 'learning_rate': 5.8685968819599115e-05, 'epoch': 41.31}\n",
      "{'loss': 1.1973, 'learning_rate': 5.865812917594655e-05, 'epoch': 41.34}\n",
      "{'loss': 1.2102, 'learning_rate': 5.863028953229399e-05, 'epoch': 41.37}\n",
      "{'loss': 1.2188, 'learning_rate': 5.860244988864143e-05, 'epoch': 41.4}\n",
      "{'loss': 1.2285, 'learning_rate': 5.857461024498887e-05, 'epoch': 41.43}\n",
      "{'loss': 1.205, 'learning_rate': 5.854677060133631e-05, 'epoch': 41.45}\n",
      "{'loss': 1.2412, 'learning_rate': 5.851893095768375e-05, 'epoch': 41.48}\n",
      " 41% 74500/179600 [6:07:41<8:03:33,  3.62it/s][INFO|trainer.py:1989] 2021-08-02 12:17:41,876 >> Saving model checkpoint to results/adapters/ag/checkpoint-74500\n",
      "[INFO|loading.py:59] 2021-08-02 12:17:41,877 >> Configuration saved in results/adapters/ag/checkpoint-74500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:17:41,888 >> Module weights saved in results/adapters/ag/checkpoint-74500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:17:41,888 >> Configuration saved in results/adapters/ag/checkpoint-74500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:17:42,175 >> Module weights saved in results/adapters/ag/checkpoint-74500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:17:42,175 >> Configuration saved in results/adapters/ag/checkpoint-74500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:17:42,495 >> Module weights saved in results/adapters/ag/checkpoint-74500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:17:42,496 >> tokenizer config file saved in results/adapters/ag/checkpoint-74500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:17:42,496 >> Special tokens file saved in results/adapters/ag/checkpoint-74500/special_tokens_map.json\n",
      "{'loss': 1.1929, 'learning_rate': 5.8491091314031185e-05, 'epoch': 41.51}\n",
      "{'loss': 1.1934, 'learning_rate': 5.846325167037863e-05, 'epoch': 41.54}\n",
      "{'loss': 1.218, 'learning_rate': 5.843541202672606e-05, 'epoch': 41.56}\n",
      "{'loss': 1.2, 'learning_rate': 5.840757238307349e-05, 'epoch': 41.59}\n",
      "{'loss': 1.2036, 'learning_rate': 5.8379732739420935e-05, 'epoch': 41.62}\n",
      "{'loss': 1.2271, 'learning_rate': 5.835189309576837e-05, 'epoch': 41.65}\n",
      "{'loss': 1.2411, 'learning_rate': 5.8324053452115814e-05, 'epoch': 41.68}\n",
      "{'loss': 1.2221, 'learning_rate': 5.829621380846325e-05, 'epoch': 41.7}\n",
      "{'loss': 1.2118, 'learning_rate': 5.826837416481069e-05, 'epoch': 41.73}\n",
      "{'loss': 1.2217, 'learning_rate': 5.824053452115813e-05, 'epoch': 41.76}\n",
      " 42% 75000/179600 [6:10:02<8:25:58,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 12:20:03,356 >> Saving model checkpoint to results/adapters/ag/checkpoint-75000\n",
      "[INFO|loading.py:59] 2021-08-02 12:20:03,357 >> Configuration saved in results/adapters/ag/checkpoint-75000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:20:03,369 >> Module weights saved in results/adapters/ag/checkpoint-75000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:20:03,369 >> Configuration saved in results/adapters/ag/checkpoint-75000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:20:03,670 >> Module weights saved in results/adapters/ag/checkpoint-75000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:20:03,670 >> Configuration saved in results/adapters/ag/checkpoint-75000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:20:04,014 >> Module weights saved in results/adapters/ag/checkpoint-75000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:20:04,014 >> tokenizer config file saved in results/adapters/ag/checkpoint-75000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:20:04,015 >> Special tokens file saved in results/adapters/ag/checkpoint-75000/special_tokens_map.json\n",
      "{'loss': 1.2078, 'learning_rate': 5.821269487750557e-05, 'epoch': 41.79}\n",
      "{'loss': 1.2289, 'learning_rate': 5.8184855233853006e-05, 'epoch': 41.81}\n",
      "{'loss': 1.1952, 'learning_rate': 5.815701559020045e-05, 'epoch': 41.84}\n",
      "{'loss': 1.22, 'learning_rate': 5.8129175946547884e-05, 'epoch': 41.87}\n",
      "{'loss': 1.2285, 'learning_rate': 5.810133630289533e-05, 'epoch': 41.9}\n",
      "{'loss': 1.23, 'learning_rate': 5.807349665924276e-05, 'epoch': 41.93}\n",
      "{'loss': 1.215, 'learning_rate': 5.8045657015590205e-05, 'epoch': 41.95}\n",
      "{'loss': 1.2443, 'learning_rate': 5.801781737193764e-05, 'epoch': 41.98}\n",
      "{'loss': 1.2613, 'learning_rate': 5.7989977728285083e-05, 'epoch': 42.01}\n",
      "{'loss': 1.2218, 'learning_rate': 5.796213808463252e-05, 'epoch': 42.04}\n",
      " 42% 75500/179600 [6:12:24<7:49:43,  3.69it/s][INFO|trainer.py:1989] 2021-08-02 12:22:25,206 >> Saving model checkpoint to results/adapters/ag/checkpoint-75500\n",
      "[INFO|loading.py:59] 2021-08-02 12:22:25,207 >> Configuration saved in results/adapters/ag/checkpoint-75500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:22:25,220 >> Module weights saved in results/adapters/ag/checkpoint-75500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:22:25,221 >> Configuration saved in results/adapters/ag/checkpoint-75500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:22:25,505 >> Module weights saved in results/adapters/ag/checkpoint-75500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:22:25,505 >> Configuration saved in results/adapters/ag/checkpoint-75500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:22:25,830 >> Module weights saved in results/adapters/ag/checkpoint-75500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:22:25,831 >> tokenizer config file saved in results/adapters/ag/checkpoint-75500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:22:25,831 >> Special tokens file saved in results/adapters/ag/checkpoint-75500/special_tokens_map.json\n",
      "{'loss': 1.2012, 'learning_rate': 5.793429844097996e-05, 'epoch': 42.07}\n",
      "{'loss': 1.1843, 'learning_rate': 5.79064587973274e-05, 'epoch': 42.09}\n",
      "{'loss': 1.2282, 'learning_rate': 5.787861915367484e-05, 'epoch': 42.12}\n",
      "{'loss': 1.2217, 'learning_rate': 5.785077951002228e-05, 'epoch': 42.15}\n",
      "{'loss': 1.1825, 'learning_rate': 5.782293986636972e-05, 'epoch': 42.18}\n",
      "{'loss': 1.1885, 'learning_rate': 5.779510022271715e-05, 'epoch': 42.2}\n",
      "{'loss': 1.2155, 'learning_rate': 5.776726057906458e-05, 'epoch': 42.23}\n",
      "{'loss': 1.223, 'learning_rate': 5.7739420935412026e-05, 'epoch': 42.26}\n",
      "{'loss': 1.2559, 'learning_rate': 5.771158129175946e-05, 'epoch': 42.29}\n",
      "{'loss': 1.2204, 'learning_rate': 5.7683741648106904e-05, 'epoch': 42.32}\n",
      " 42% 76000/179600 [6:14:46<8:24:13,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 12:24:46,782 >> Saving model checkpoint to results/adapters/ag/checkpoint-76000\n",
      "[INFO|loading.py:59] 2021-08-02 12:24:46,782 >> Configuration saved in results/adapters/ag/checkpoint-76000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:24:46,793 >> Module weights saved in results/adapters/ag/checkpoint-76000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:24:46,794 >> Configuration saved in results/adapters/ag/checkpoint-76000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:24:47,081 >> Module weights saved in results/adapters/ag/checkpoint-76000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:24:47,082 >> Configuration saved in results/adapters/ag/checkpoint-76000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:24:47,397 >> Module weights saved in results/adapters/ag/checkpoint-76000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:24:47,397 >> tokenizer config file saved in results/adapters/ag/checkpoint-76000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:24:47,397 >> Special tokens file saved in results/adapters/ag/checkpoint-76000/special_tokens_map.json\n",
      "{'loss': 1.2223, 'learning_rate': 5.765590200445434e-05, 'epoch': 42.34}\n",
      "{'loss': 1.2574, 'learning_rate': 5.762806236080178e-05, 'epoch': 42.37}\n",
      "{'loss': 1.2021, 'learning_rate': 5.7600222717149225e-05, 'epoch': 42.4}\n",
      "{'loss': 1.2476, 'learning_rate': 5.757238307349666e-05, 'epoch': 42.43}\n",
      "{'loss': 1.1963, 'learning_rate': 5.75445434298441e-05, 'epoch': 42.46}\n",
      "{'loss': 1.1921, 'learning_rate': 5.751670378619154e-05, 'epoch': 42.48}\n",
      "{'loss': 1.2248, 'learning_rate': 5.748886414253898e-05, 'epoch': 42.51}\n",
      "{'loss': 1.2013, 'learning_rate': 5.746102449888642e-05, 'epoch': 42.54}\n",
      "{'loss': 1.1872, 'learning_rate': 5.743318485523386e-05, 'epoch': 42.57}\n",
      "{'loss': 1.1863, 'learning_rate': 5.7405345211581296e-05, 'epoch': 42.59}\n",
      " 43% 76500/179600 [6:17:08<7:46:39,  3.68it/s][INFO|trainer.py:1989] 2021-08-02 12:27:08,855 >> Saving model checkpoint to results/adapters/ag/checkpoint-76500\n",
      "[INFO|loading.py:59] 2021-08-02 12:27:08,856 >> Configuration saved in results/adapters/ag/checkpoint-76500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:27:08,868 >> Module weights saved in results/adapters/ag/checkpoint-76500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:27:08,868 >> Configuration saved in results/adapters/ag/checkpoint-76500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:27:09,156 >> Module weights saved in results/adapters/ag/checkpoint-76500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:27:09,157 >> Configuration saved in results/adapters/ag/checkpoint-76500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:27:09,486 >> Module weights saved in results/adapters/ag/checkpoint-76500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:27:09,486 >> tokenizer config file saved in results/adapters/ag/checkpoint-76500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:27:09,487 >> Special tokens file saved in results/adapters/ag/checkpoint-76500/special_tokens_map.json\n",
      "{'loss': 1.2197, 'learning_rate': 5.737750556792874e-05, 'epoch': 42.62}\n",
      "{'loss': 1.2065, 'learning_rate': 5.7349665924276174e-05, 'epoch': 42.65}\n",
      "{'loss': 1.1949, 'learning_rate': 5.7321826280623616e-05, 'epoch': 42.68}\n",
      "{'loss': 1.2343, 'learning_rate': 5.729398663697105e-05, 'epoch': 42.71}\n",
      "{'loss': 1.2151, 'learning_rate': 5.7266146993318495e-05, 'epoch': 42.73}\n",
      "{'loss': 1.2261, 'learning_rate': 5.723830734966593e-05, 'epoch': 42.76}\n",
      "{'loss': 1.2224, 'learning_rate': 5.721046770601337e-05, 'epoch': 42.79}\n",
      "{'loss': 1.2045, 'learning_rate': 5.71826280623608e-05, 'epoch': 42.82}\n",
      "{'loss': 1.2374, 'learning_rate': 5.715478841870824e-05, 'epoch': 42.84}\n",
      "{'loss': 1.1892, 'learning_rate': 5.712694877505568e-05, 'epoch': 42.87}\n",
      " 43% 77000/179600 [6:19:30<8:04:58,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 12:29:31,041 >> Saving model checkpoint to results/adapters/ag/checkpoint-77000\n",
      "[INFO|loading.py:59] 2021-08-02 12:29:31,042 >> Configuration saved in results/adapters/ag/checkpoint-77000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:29:31,052 >> Module weights saved in results/adapters/ag/checkpoint-77000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:29:31,053 >> Configuration saved in results/adapters/ag/checkpoint-77000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:29:31,339 >> Module weights saved in results/adapters/ag/checkpoint-77000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:29:31,339 >> Configuration saved in results/adapters/ag/checkpoint-77000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:29:31,665 >> Module weights saved in results/adapters/ag/checkpoint-77000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:29:31,666 >> tokenizer config file saved in results/adapters/ag/checkpoint-77000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:29:31,666 >> Special tokens file saved in results/adapters/ag/checkpoint-77000/special_tokens_map.json\n",
      "{'loss': 1.175, 'learning_rate': 5.7099109131403116e-05, 'epoch': 42.9}\n",
      "{'loss': 1.234, 'learning_rate': 5.707126948775056e-05, 'epoch': 42.93}\n",
      "{'loss': 1.2162, 'learning_rate': 5.7043429844097994e-05, 'epoch': 42.96}\n",
      "{'loss': 1.2439, 'learning_rate': 5.701559020044544e-05, 'epoch': 42.98}\n",
      "{'loss': 1.2013, 'learning_rate': 5.698775055679287e-05, 'epoch': 43.01}\n",
      "{'loss': 1.2108, 'learning_rate': 5.6959910913140315e-05, 'epoch': 43.04}\n",
      "{'loss': 1.2152, 'learning_rate': 5.693207126948775e-05, 'epoch': 43.07}\n",
      "{'loss': 1.1899, 'learning_rate': 5.6904231625835194e-05, 'epoch': 43.1}\n",
      "{'loss': 1.2373, 'learning_rate': 5.687639198218263e-05, 'epoch': 43.12}\n",
      "{'loss': 1.2236, 'learning_rate': 5.684855233853007e-05, 'epoch': 43.15}\n",
      " 43% 77500/179600 [6:21:52<7:58:36,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 12:31:52,653 >> Saving model checkpoint to results/adapters/ag/checkpoint-77500\n",
      "[INFO|loading.py:59] 2021-08-02 12:31:52,653 >> Configuration saved in results/adapters/ag/checkpoint-77500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:31:52,665 >> Module weights saved in results/adapters/ag/checkpoint-77500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:31:52,665 >> Configuration saved in results/adapters/ag/checkpoint-77500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:31:52,948 >> Module weights saved in results/adapters/ag/checkpoint-77500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:31:52,949 >> Configuration saved in results/adapters/ag/checkpoint-77500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:31:53,265 >> Module weights saved in results/adapters/ag/checkpoint-77500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:31:53,266 >> tokenizer config file saved in results/adapters/ag/checkpoint-77500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:31:53,266 >> Special tokens file saved in results/adapters/ag/checkpoint-77500/special_tokens_map.json\n",
      "{'loss': 1.2207, 'learning_rate': 5.682071269487751e-05, 'epoch': 43.18}\n",
      "{'loss': 1.1898, 'learning_rate': 5.679287305122495e-05, 'epoch': 43.21}\n",
      "{'loss': 1.2095, 'learning_rate': 5.6765033407572386e-05, 'epoch': 43.23}\n",
      "{'loss': 1.1941, 'learning_rate': 5.673719376391983e-05, 'epoch': 43.26}\n",
      "{'loss': 1.2263, 'learning_rate': 5.6709354120267264e-05, 'epoch': 43.29}\n",
      "{'loss': 1.2161, 'learning_rate': 5.668151447661471e-05, 'epoch': 43.32}\n",
      "{'loss': 1.2143, 'learning_rate': 5.665367483296214e-05, 'epoch': 43.35}\n",
      "{'loss': 1.2136, 'learning_rate': 5.6625835189309585e-05, 'epoch': 43.37}\n",
      "{'loss': 1.2133, 'learning_rate': 5.659799554565702e-05, 'epoch': 43.4}\n",
      "{'loss': 1.2069, 'learning_rate': 5.6570155902004463e-05, 'epoch': 43.43}\n",
      " 43% 78000/179600 [6:24:14<7:43:33,  3.65it/s][INFO|trainer.py:1989] 2021-08-02 12:34:14,565 >> Saving model checkpoint to results/adapters/ag/checkpoint-78000\n",
      "[INFO|loading.py:59] 2021-08-02 12:34:14,565 >> Configuration saved in results/adapters/ag/checkpoint-78000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:34:14,575 >> Module weights saved in results/adapters/ag/checkpoint-78000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:34:14,576 >> Configuration saved in results/adapters/ag/checkpoint-78000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:34:14,862 >> Module weights saved in results/adapters/ag/checkpoint-78000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:34:14,862 >> Configuration saved in results/adapters/ag/checkpoint-78000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:34:15,182 >> Module weights saved in results/adapters/ag/checkpoint-78000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:34:15,183 >> tokenizer config file saved in results/adapters/ag/checkpoint-78000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:34:15,183 >> Special tokens file saved in results/adapters/ag/checkpoint-78000/special_tokens_map.json\n",
      "{'loss': 1.2008, 'learning_rate': 5.654231625835189e-05, 'epoch': 43.46}\n",
      "{'loss': 1.2065, 'learning_rate': 5.651447661469933e-05, 'epoch': 43.49}\n",
      "{'loss': 1.2266, 'learning_rate': 5.648663697104677e-05, 'epoch': 43.51}\n",
      "{'loss': 1.2248, 'learning_rate': 5.6458797327394206e-05, 'epoch': 43.54}\n",
      "{'loss': 1.2256, 'learning_rate': 5.643095768374165e-05, 'epoch': 43.57}\n",
      "{'loss': 1.1857, 'learning_rate': 5.6403118040089085e-05, 'epoch': 43.6}\n",
      "{'loss': 1.1988, 'learning_rate': 5.637527839643653e-05, 'epoch': 43.62}\n",
      "{'loss': 1.2247, 'learning_rate': 5.634743875278396e-05, 'epoch': 43.65}\n",
      "{'loss': 1.177, 'learning_rate': 5.6319599109131406e-05, 'epoch': 43.68}\n",
      "{'loss': 1.2477, 'learning_rate': 5.629175946547884e-05, 'epoch': 43.71}\n",
      " 44% 78500/179600 [6:26:35<7:50:40,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 12:36:36,215 >> Saving model checkpoint to results/adapters/ag/checkpoint-78500\n",
      "[INFO|loading.py:59] 2021-08-02 12:36:36,216 >> Configuration saved in results/adapters/ag/checkpoint-78500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:36:36,226 >> Module weights saved in results/adapters/ag/checkpoint-78500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:36:36,227 >> Configuration saved in results/adapters/ag/checkpoint-78500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:36:36,518 >> Module weights saved in results/adapters/ag/checkpoint-78500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:36:36,519 >> Configuration saved in results/adapters/ag/checkpoint-78500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:36:36,842 >> Module weights saved in results/adapters/ag/checkpoint-78500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:36:36,843 >> tokenizer config file saved in results/adapters/ag/checkpoint-78500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:36:36,843 >> Special tokens file saved in results/adapters/ag/checkpoint-78500/special_tokens_map.json\n",
      "{'loss': 1.2106, 'learning_rate': 5.6263919821826284e-05, 'epoch': 43.74}\n",
      "{'loss': 1.2204, 'learning_rate': 5.623608017817372e-05, 'epoch': 43.76}\n",
      "{'loss': 1.2184, 'learning_rate': 5.620824053452116e-05, 'epoch': 43.79}\n",
      "{'loss': 1.2274, 'learning_rate': 5.61804008908686e-05, 'epoch': 43.82}\n",
      "{'loss': 1.1932, 'learning_rate': 5.615256124721604e-05, 'epoch': 43.85}\n",
      "{'loss': 1.1952, 'learning_rate': 5.6124721603563476e-05, 'epoch': 43.87}\n",
      "{'loss': 1.2239, 'learning_rate': 5.609688195991092e-05, 'epoch': 43.9}\n",
      "{'loss': 1.2072, 'learning_rate': 5.6069042316258355e-05, 'epoch': 43.93}\n",
      "{'loss': 1.2258, 'learning_rate': 5.60412026726058e-05, 'epoch': 43.96}\n",
      "{'loss': 1.2031, 'learning_rate': 5.601336302895323e-05, 'epoch': 43.99}\n",
      " 44% 79000/179600 [6:28:57<8:09:11,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 12:38:58,028 >> Saving model checkpoint to results/adapters/ag/checkpoint-79000\n",
      "[INFO|loading.py:59] 2021-08-02 12:38:58,028 >> Configuration saved in results/adapters/ag/checkpoint-79000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:38:58,038 >> Module weights saved in results/adapters/ag/checkpoint-79000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:38:58,039 >> Configuration saved in results/adapters/ag/checkpoint-79000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:38:58,346 >> Module weights saved in results/adapters/ag/checkpoint-79000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:38:58,347 >> Configuration saved in results/adapters/ag/checkpoint-79000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:38:58,681 >> Module weights saved in results/adapters/ag/checkpoint-79000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:38:58,682 >> tokenizer config file saved in results/adapters/ag/checkpoint-79000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:38:58,682 >> Special tokens file saved in results/adapters/ag/checkpoint-79000/special_tokens_map.json\n",
      "{'loss': 1.248, 'learning_rate': 5.5985523385300676e-05, 'epoch': 44.01}\n",
      "{'loss': 1.2149, 'learning_rate': 5.595768374164812e-05, 'epoch': 44.04}\n",
      "{'loss': 1.2192, 'learning_rate': 5.592984409799554e-05, 'epoch': 44.07}\n",
      "{'loss': 1.1998, 'learning_rate': 5.590200445434298e-05, 'epoch': 44.1}\n",
      "{'loss': 1.1983, 'learning_rate': 5.587416481069042e-05, 'epoch': 44.13}\n",
      "{'loss': 1.188, 'learning_rate': 5.584632516703786e-05, 'epoch': 44.15}\n",
      "{'loss': 1.2006, 'learning_rate': 5.58184855233853e-05, 'epoch': 44.18}\n",
      "{'loss': 1.2209, 'learning_rate': 5.579064587973274e-05, 'epoch': 44.21}\n",
      "{'loss': 1.2164, 'learning_rate': 5.5762806236080175e-05, 'epoch': 44.24}\n",
      "{'loss': 1.1962, 'learning_rate': 5.573496659242762e-05, 'epoch': 44.26}\n",
      " 44% 79500/179600 [6:31:21<7:52:43,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 12:41:22,048 >> Saving model checkpoint to results/adapters/ag/checkpoint-79500\n",
      "[INFO|loading.py:59] 2021-08-02 12:41:22,049 >> Configuration saved in results/adapters/ag/checkpoint-79500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:41:22,062 >> Module weights saved in results/adapters/ag/checkpoint-79500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:41:22,062 >> Configuration saved in results/adapters/ag/checkpoint-79500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:41:22,347 >> Module weights saved in results/adapters/ag/checkpoint-79500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:41:22,348 >> Configuration saved in results/adapters/ag/checkpoint-79500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:41:22,672 >> Module weights saved in results/adapters/ag/checkpoint-79500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:41:22,672 >> tokenizer config file saved in results/adapters/ag/checkpoint-79500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:41:22,673 >> Special tokens file saved in results/adapters/ag/checkpoint-79500/special_tokens_map.json\n",
      "{'loss': 1.2082, 'learning_rate': 5.5707126948775054e-05, 'epoch': 44.29}\n",
      "{'loss': 1.1924, 'learning_rate': 5.5679287305122496e-05, 'epoch': 44.32}\n",
      "{'loss': 1.2031, 'learning_rate': 5.565144766146994e-05, 'epoch': 44.35}\n",
      "{'loss': 1.2382, 'learning_rate': 5.5623608017817374e-05, 'epoch': 44.38}\n",
      "{'loss': 1.2045, 'learning_rate': 5.559576837416482e-05, 'epoch': 44.4}\n",
      "{'loss': 1.1892, 'learning_rate': 5.556792873051225e-05, 'epoch': 44.43}\n",
      "{'loss': 1.2336, 'learning_rate': 5.5540089086859695e-05, 'epoch': 44.46}\n",
      "{'loss': 1.2035, 'learning_rate': 5.551224944320713e-05, 'epoch': 44.49}\n",
      "{'loss': 1.196, 'learning_rate': 5.5484409799554574e-05, 'epoch': 44.52}\n",
      "{'loss': 1.1957, 'learning_rate': 5.545657015590201e-05, 'epoch': 44.54}\n",
      " 45% 80000/179600 [6:33:46<8:04:12,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 12:43:46,653 >> Saving model checkpoint to results/adapters/ag/checkpoint-80000\n",
      "[INFO|loading.py:59] 2021-08-02 12:43:46,653 >> Configuration saved in results/adapters/ag/checkpoint-80000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:43:46,665 >> Module weights saved in results/adapters/ag/checkpoint-80000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:43:46,665 >> Configuration saved in results/adapters/ag/checkpoint-80000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:43:46,968 >> Module weights saved in results/adapters/ag/checkpoint-80000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:43:46,969 >> Configuration saved in results/adapters/ag/checkpoint-80000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:43:47,299 >> Module weights saved in results/adapters/ag/checkpoint-80000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:43:47,300 >> tokenizer config file saved in results/adapters/ag/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:43:47,300 >> Special tokens file saved in results/adapters/ag/checkpoint-80000/special_tokens_map.json\n",
      "{'loss': 1.2146, 'learning_rate': 5.542873051224945e-05, 'epoch': 44.57}\n",
      "{'loss': 1.2213, 'learning_rate': 5.540089086859689e-05, 'epoch': 44.6}\n",
      "{'loss': 1.1815, 'learning_rate': 5.537305122494433e-05, 'epoch': 44.63}\n",
      "{'loss': 1.1918, 'learning_rate': 5.5345211581291766e-05, 'epoch': 44.65}\n",
      "{'loss': 1.2018, 'learning_rate': 5.531737193763921e-05, 'epoch': 44.68}\n",
      "{'loss': 1.2085, 'learning_rate': 5.528953229398664e-05, 'epoch': 44.71}\n",
      "{'loss': 1.1925, 'learning_rate': 5.526169265033407e-05, 'epoch': 44.74}\n",
      "{'loss': 1.1894, 'learning_rate': 5.5233853006681516e-05, 'epoch': 44.77}\n",
      "{'loss': 1.228, 'learning_rate': 5.520601336302895e-05, 'epoch': 44.79}\n",
      "{'loss': 1.2125, 'learning_rate': 5.5178173719376394e-05, 'epoch': 44.82}\n",
      " 45% 80500/179600 [6:36:10<7:54:39,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 12:46:11,069 >> Saving model checkpoint to results/adapters/ag/checkpoint-80500\n",
      "[INFO|loading.py:59] 2021-08-02 12:46:11,069 >> Configuration saved in results/adapters/ag/checkpoint-80500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:46:11,080 >> Module weights saved in results/adapters/ag/checkpoint-80500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:46:11,081 >> Configuration saved in results/adapters/ag/checkpoint-80500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:46:11,373 >> Module weights saved in results/adapters/ag/checkpoint-80500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:46:11,374 >> Configuration saved in results/adapters/ag/checkpoint-80500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:46:11,708 >> Module weights saved in results/adapters/ag/checkpoint-80500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:46:11,708 >> tokenizer config file saved in results/adapters/ag/checkpoint-80500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:46:11,708 >> Special tokens file saved in results/adapters/ag/checkpoint-80500/special_tokens_map.json\n",
      "{'loss': 1.1852, 'learning_rate': 5.515033407572383e-05, 'epoch': 44.85}\n",
      "{'loss': 1.2188, 'learning_rate': 5.512249443207127e-05, 'epoch': 44.88}\n",
      "{'loss': 1.2545, 'learning_rate': 5.509465478841871e-05, 'epoch': 44.9}\n",
      "{'loss': 1.1708, 'learning_rate': 5.506681514476615e-05, 'epoch': 44.93}\n",
      "{'loss': 1.2196, 'learning_rate': 5.5038975501113586e-05, 'epoch': 44.96}\n",
      "{'loss': 1.198, 'learning_rate': 5.501113585746103e-05, 'epoch': 44.99}\n",
      "{'loss': 1.2402, 'learning_rate': 5.4983296213808465e-05, 'epoch': 45.02}\n",
      "{'loss': 1.2009, 'learning_rate': 5.495545657015591e-05, 'epoch': 45.04}\n",
      "{'loss': 1.1885, 'learning_rate': 5.492761692650334e-05, 'epoch': 45.07}\n",
      "{'loss': 1.2118, 'learning_rate': 5.4899777282850786e-05, 'epoch': 45.1}\n",
      " 45% 81000/179600 [6:38:34<7:55:03,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 12:48:35,365 >> Saving model checkpoint to results/adapters/ag/checkpoint-81000\n",
      "[INFO|loading.py:59] 2021-08-02 12:48:35,366 >> Configuration saved in results/adapters/ag/checkpoint-81000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:48:35,380 >> Module weights saved in results/adapters/ag/checkpoint-81000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:48:35,381 >> Configuration saved in results/adapters/ag/checkpoint-81000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:48:35,683 >> Module weights saved in results/adapters/ag/checkpoint-81000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:48:35,683 >> Configuration saved in results/adapters/ag/checkpoint-81000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:48:36,021 >> Module weights saved in results/adapters/ag/checkpoint-81000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:48:36,021 >> tokenizer config file saved in results/adapters/ag/checkpoint-81000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:48:36,021 >> Special tokens file saved in results/adapters/ag/checkpoint-81000/special_tokens_map.json\n",
      "{'loss': 1.2085, 'learning_rate': 5.487193763919822e-05, 'epoch': 45.13}\n",
      "{'loss': 1.2412, 'learning_rate': 5.4844097995545664e-05, 'epoch': 45.16}\n",
      "{'loss': 1.2115, 'learning_rate': 5.48162583518931e-05, 'epoch': 45.18}\n",
      "{'loss': 1.1951, 'learning_rate': 5.478841870824054e-05, 'epoch': 45.21}\n",
      "{'loss': 1.2082, 'learning_rate': 5.476057906458798e-05, 'epoch': 45.24}\n",
      "{'loss': 1.1957, 'learning_rate': 5.473273942093542e-05, 'epoch': 45.27}\n",
      "{'loss': 1.2028, 'learning_rate': 5.4704899777282856e-05, 'epoch': 45.29}\n",
      "{'loss': 1.1998, 'learning_rate': 5.4677060133630285e-05, 'epoch': 45.32}\n",
      "{'loss': 1.2371, 'learning_rate': 5.464922048997773e-05, 'epoch': 45.35}\n",
      "{'loss': 1.2093, 'learning_rate': 5.4621380846325164e-05, 'epoch': 45.38}\n",
      " 45% 81500/179600 [6:40:58<8:02:00,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 12:50:59,202 >> Saving model checkpoint to results/adapters/ag/checkpoint-81500\n",
      "[INFO|loading.py:59] 2021-08-02 12:50:59,203 >> Configuration saved in results/adapters/ag/checkpoint-81500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:50:59,215 >> Module weights saved in results/adapters/ag/checkpoint-81500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:50:59,216 >> Configuration saved in results/adapters/ag/checkpoint-81500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:50:59,509 >> Module weights saved in results/adapters/ag/checkpoint-81500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:50:59,510 >> Configuration saved in results/adapters/ag/checkpoint-81500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:50:59,846 >> Module weights saved in results/adapters/ag/checkpoint-81500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:50:59,847 >> tokenizer config file saved in results/adapters/ag/checkpoint-81500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:50:59,847 >> Special tokens file saved in results/adapters/ag/checkpoint-81500/special_tokens_map.json\n",
      "{'loss': 1.1914, 'learning_rate': 5.4593541202672606e-05, 'epoch': 45.41}\n",
      "{'loss': 1.2133, 'learning_rate': 5.456570155902004e-05, 'epoch': 45.43}\n",
      "{'loss': 1.1886, 'learning_rate': 5.4537861915367484e-05, 'epoch': 45.46}\n",
      "{'loss': 1.2164, 'learning_rate': 5.451002227171492e-05, 'epoch': 45.49}\n",
      "{'loss': 1.2102, 'learning_rate': 5.448218262806236e-05, 'epoch': 45.52}\n",
      "{'loss': 1.2012, 'learning_rate': 5.44543429844098e-05, 'epoch': 45.55}\n",
      "{'loss': 1.2047, 'learning_rate': 5.442650334075724e-05, 'epoch': 45.57}\n",
      "{'loss': 1.2137, 'learning_rate': 5.439866369710468e-05, 'epoch': 45.6}\n",
      "{'loss': 1.196, 'learning_rate': 5.437082405345212e-05, 'epoch': 45.63}\n",
      "{'loss': 1.2192, 'learning_rate': 5.4342984409799555e-05, 'epoch': 45.66}\n",
      " 46% 82000/179600 [6:43:22<7:43:53,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 12:53:23,062 >> Saving model checkpoint to results/adapters/ag/checkpoint-82000\n",
      "[INFO|loading.py:59] 2021-08-02 12:53:23,063 >> Configuration saved in results/adapters/ag/checkpoint-82000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:53:23,078 >> Module weights saved in results/adapters/ag/checkpoint-82000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:53:23,078 >> Configuration saved in results/adapters/ag/checkpoint-82000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:53:23,370 >> Module weights saved in results/adapters/ag/checkpoint-82000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:53:23,370 >> Configuration saved in results/adapters/ag/checkpoint-82000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:53:23,708 >> Module weights saved in results/adapters/ag/checkpoint-82000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:53:23,709 >> tokenizer config file saved in results/adapters/ag/checkpoint-82000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:53:23,709 >> Special tokens file saved in results/adapters/ag/checkpoint-82000/special_tokens_map.json\n",
      "{'loss': 1.2264, 'learning_rate': 5.4315144766147e-05, 'epoch': 45.68}\n",
      "{'loss': 1.228, 'learning_rate': 5.4287305122494433e-05, 'epoch': 45.71}\n",
      "{'loss': 1.1954, 'learning_rate': 5.4259465478841876e-05, 'epoch': 45.74}\n",
      "{'loss': 1.2024, 'learning_rate': 5.423162583518931e-05, 'epoch': 45.77}\n",
      "{'loss': 1.1741, 'learning_rate': 5.4203786191536754e-05, 'epoch': 45.8}\n",
      "{'loss': 1.1837, 'learning_rate': 5.417594654788419e-05, 'epoch': 45.82}\n",
      "{'loss': 1.2349, 'learning_rate': 5.414810690423163e-05, 'epoch': 45.85}\n",
      "{'loss': 1.2222, 'learning_rate': 5.412026726057907e-05, 'epoch': 45.88}\n",
      "{'loss': 1.2357, 'learning_rate': 5.409242761692651e-05, 'epoch': 45.91}\n",
      "{'loss': 1.2169, 'learning_rate': 5.4064587973273954e-05, 'epoch': 45.93}\n",
      " 46% 82500/179600 [6:45:47<7:57:41,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 12:55:47,823 >> Saving model checkpoint to results/adapters/ag/checkpoint-82500\n",
      "[INFO|loading.py:59] 2021-08-02 12:55:47,824 >> Configuration saved in results/adapters/ag/checkpoint-82500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:55:47,835 >> Module weights saved in results/adapters/ag/checkpoint-82500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:55:47,835 >> Configuration saved in results/adapters/ag/checkpoint-82500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:55:48,149 >> Module weights saved in results/adapters/ag/checkpoint-82500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:55:48,150 >> Configuration saved in results/adapters/ag/checkpoint-82500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:55:48,489 >> Module weights saved in results/adapters/ag/checkpoint-82500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:55:48,490 >> tokenizer config file saved in results/adapters/ag/checkpoint-82500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:55:48,490 >> Special tokens file saved in results/adapters/ag/checkpoint-82500/special_tokens_map.json\n",
      "{'loss': 1.2139, 'learning_rate': 5.4036748329621376e-05, 'epoch': 45.96}\n",
      "{'loss': 1.253, 'learning_rate': 5.400890868596882e-05, 'epoch': 45.99}\n",
      "{'loss': 1.2049, 'learning_rate': 5.3981069042316254e-05, 'epoch': 46.02}\n",
      "{'loss': 1.2162, 'learning_rate': 5.3953229398663697e-05, 'epoch': 46.05}\n",
      "{'loss': 1.2218, 'learning_rate': 5.392538975501113e-05, 'epoch': 46.07}\n",
      "{'loss': 1.2146, 'learning_rate': 5.3897550111358575e-05, 'epoch': 46.1}\n",
      "{'loss': 1.2217, 'learning_rate': 5.386971046770601e-05, 'epoch': 46.13}\n",
      "{'loss': 1.1946, 'learning_rate': 5.384187082405345e-05, 'epoch': 46.16}\n",
      "{'loss': 1.1865, 'learning_rate': 5.381403118040089e-05, 'epoch': 46.19}\n",
      "{'loss': 1.1638, 'learning_rate': 5.378619153674833e-05, 'epoch': 46.21}\n",
      " 46% 83000/179600 [6:48:12<7:42:32,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 12:58:13,242 >> Saving model checkpoint to results/adapters/ag/checkpoint-83000\n",
      "[INFO|loading.py:59] 2021-08-02 12:58:13,242 >> Configuration saved in results/adapters/ag/checkpoint-83000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:58:13,257 >> Module weights saved in results/adapters/ag/checkpoint-83000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:58:13,257 >> Configuration saved in results/adapters/ag/checkpoint-83000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:58:13,555 >> Module weights saved in results/adapters/ag/checkpoint-83000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 12:58:13,555 >> Configuration saved in results/adapters/ag/checkpoint-83000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 12:58:13,888 >> Module weights saved in results/adapters/ag/checkpoint-83000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 12:58:13,889 >> tokenizer config file saved in results/adapters/ag/checkpoint-83000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 12:58:13,889 >> Special tokens file saved in results/adapters/ag/checkpoint-83000/special_tokens_map.json\n",
      "{'loss': 1.2186, 'learning_rate': 5.3758351893095774e-05, 'epoch': 46.24}\n",
      "{'loss': 1.1984, 'learning_rate': 5.373051224944321e-05, 'epoch': 46.27}\n",
      "{'loss': 1.1865, 'learning_rate': 5.370267260579065e-05, 'epoch': 46.3}\n",
      "{'loss': 1.2046, 'learning_rate': 5.367483296213809e-05, 'epoch': 46.33}\n",
      "{'loss': 1.2039, 'learning_rate': 5.364699331848553e-05, 'epoch': 46.35}\n",
      "{'loss': 1.2069, 'learning_rate': 5.3619153674832966e-05, 'epoch': 46.38}\n",
      "{'loss': 1.2387, 'learning_rate': 5.359131403118041e-05, 'epoch': 46.41}\n",
      "{'loss': 1.2108, 'learning_rate': 5.3563474387527845e-05, 'epoch': 46.44}\n",
      "{'loss': 1.2329, 'learning_rate': 5.353563474387529e-05, 'epoch': 46.46}\n",
      "{'loss': 1.2196, 'learning_rate': 5.350779510022272e-05, 'epoch': 46.49}\n",
      " 46% 83500/179600 [6:50:36<7:42:52,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 13:00:36,958 >> Saving model checkpoint to results/adapters/ag/checkpoint-83500\n",
      "[INFO|loading.py:59] 2021-08-02 13:00:36,959 >> Configuration saved in results/adapters/ag/checkpoint-83500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:00:36,972 >> Module weights saved in results/adapters/ag/checkpoint-83500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:00:36,973 >> Configuration saved in results/adapters/ag/checkpoint-83500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:00:37,271 >> Module weights saved in results/adapters/ag/checkpoint-83500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:00:37,271 >> Configuration saved in results/adapters/ag/checkpoint-83500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:00:37,597 >> Module weights saved in results/adapters/ag/checkpoint-83500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:00:37,597 >> tokenizer config file saved in results/adapters/ag/checkpoint-83500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:00:37,597 >> Special tokens file saved in results/adapters/ag/checkpoint-83500/special_tokens_map.json\n",
      "{'loss': 1.2354, 'learning_rate': 5.3479955456570166e-05, 'epoch': 46.52}\n",
      "{'loss': 1.2009, 'learning_rate': 5.34521158129176e-05, 'epoch': 46.55}\n",
      "{'loss': 1.1892, 'learning_rate': 5.342427616926503e-05, 'epoch': 46.58}\n",
      "{'loss': 1.2159, 'learning_rate': 5.339643652561247e-05, 'epoch': 46.6}\n",
      "{'loss': 1.2015, 'learning_rate': 5.336859688195991e-05, 'epoch': 46.63}\n",
      "{'loss': 1.2297, 'learning_rate': 5.334075723830735e-05, 'epoch': 46.66}\n",
      "{'loss': 1.2166, 'learning_rate': 5.331291759465479e-05, 'epoch': 46.69}\n",
      "{'loss': 1.202, 'learning_rate': 5.328507795100223e-05, 'epoch': 46.71}\n",
      "{'loss': 1.2228, 'learning_rate': 5.3257238307349665e-05, 'epoch': 46.74}\n",
      "{'loss': 1.2348, 'learning_rate': 5.322939866369711e-05, 'epoch': 46.77}\n",
      " 47% 84000/179600 [6:53:01<7:44:46,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 13:03:01,654 >> Saving model checkpoint to results/adapters/ag/checkpoint-84000\n",
      "[INFO|loading.py:59] 2021-08-02 13:03:01,654 >> Configuration saved in results/adapters/ag/checkpoint-84000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:03:01,666 >> Module weights saved in results/adapters/ag/checkpoint-84000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:03:01,667 >> Configuration saved in results/adapters/ag/checkpoint-84000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:03:01,975 >> Module weights saved in results/adapters/ag/checkpoint-84000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:03:01,976 >> Configuration saved in results/adapters/ag/checkpoint-84000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:03:02,324 >> Module weights saved in results/adapters/ag/checkpoint-84000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:03:02,325 >> tokenizer config file saved in results/adapters/ag/checkpoint-84000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:03:02,325 >> Special tokens file saved in results/adapters/ag/checkpoint-84000/special_tokens_map.json\n",
      "{'loss': 1.2242, 'learning_rate': 5.3201559020044544e-05, 'epoch': 46.8}\n",
      "{'loss': 1.1565, 'learning_rate': 5.3173719376391986e-05, 'epoch': 46.83}\n",
      "{'loss': 1.1805, 'learning_rate': 5.314587973273942e-05, 'epoch': 46.85}\n",
      "{'loss': 1.1952, 'learning_rate': 5.3118040089086864e-05, 'epoch': 46.88}\n",
      "{'loss': 1.2098, 'learning_rate': 5.30902004454343e-05, 'epoch': 46.91}\n",
      "{'loss': 1.2281, 'learning_rate': 5.306236080178174e-05, 'epoch': 46.94}\n",
      "{'loss': 1.1874, 'learning_rate': 5.303452115812918e-05, 'epoch': 46.97}\n",
      "{'loss': 1.2231, 'learning_rate': 5.300668151447662e-05, 'epoch': 46.99}\n",
      "{'loss': 1.2439, 'learning_rate': 5.297884187082406e-05, 'epoch': 47.02}\n",
      "{'loss': 1.1944, 'learning_rate': 5.29510022271715e-05, 'epoch': 47.05}\n",
      " 47% 84500/179600 [6:55:25<7:40:38,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 13:05:26,429 >> Saving model checkpoint to results/adapters/ag/checkpoint-84500\n",
      "[INFO|loading.py:59] 2021-08-02 13:05:26,430 >> Configuration saved in results/adapters/ag/checkpoint-84500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:05:26,442 >> Module weights saved in results/adapters/ag/checkpoint-84500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:05:26,443 >> Configuration saved in results/adapters/ag/checkpoint-84500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:05:26,759 >> Module weights saved in results/adapters/ag/checkpoint-84500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:05:26,760 >> Configuration saved in results/adapters/ag/checkpoint-84500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:05:27,099 >> Module weights saved in results/adapters/ag/checkpoint-84500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:05:27,100 >> tokenizer config file saved in results/adapters/ag/checkpoint-84500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:05:27,100 >> Special tokens file saved in results/adapters/ag/checkpoint-84500/special_tokens_map.json\n",
      "{'loss': 1.197, 'learning_rate': 5.2923162583518935e-05, 'epoch': 47.08}\n",
      "{'loss': 1.2182, 'learning_rate': 5.289532293986638e-05, 'epoch': 47.1}\n",
      "{'loss': 1.2079, 'learning_rate': 5.2867483296213813e-05, 'epoch': 47.13}\n",
      "{'loss': 1.2098, 'learning_rate': 5.2839643652561256e-05, 'epoch': 47.16}\n",
      "{'loss': 1.2265, 'learning_rate': 5.2811804008908685e-05, 'epoch': 47.19}\n",
      "{'loss': 1.2357, 'learning_rate': 5.278396436525612e-05, 'epoch': 47.22}\n",
      "{'loss': 1.2015, 'learning_rate': 5.275612472160356e-05, 'epoch': 47.24}\n",
      "{'loss': 1.2063, 'learning_rate': 5.2728285077951e-05, 'epoch': 47.27}\n",
      "{'loss': 1.2523, 'learning_rate': 5.270044543429844e-05, 'epoch': 47.3}\n",
      "{'loss': 1.189, 'learning_rate': 5.267260579064588e-05, 'epoch': 47.33}\n",
      " 47% 85000/179600 [6:57:50<7:30:30,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 13:07:50,597 >> Saving model checkpoint to results/adapters/ag/checkpoint-85000\n",
      "[INFO|loading.py:59] 2021-08-02 13:07:50,597 >> Configuration saved in results/adapters/ag/checkpoint-85000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:07:50,609 >> Module weights saved in results/adapters/ag/checkpoint-85000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:07:50,609 >> Configuration saved in results/adapters/ag/checkpoint-85000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:07:50,918 >> Module weights saved in results/adapters/ag/checkpoint-85000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:07:50,918 >> Configuration saved in results/adapters/ag/checkpoint-85000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:07:51,258 >> Module weights saved in results/adapters/ag/checkpoint-85000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:07:51,259 >> tokenizer config file saved in results/adapters/ag/checkpoint-85000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:07:51,259 >> Special tokens file saved in results/adapters/ag/checkpoint-85000/special_tokens_map.json\n",
      "{'loss': 1.2197, 'learning_rate': 5.264476614699332e-05, 'epoch': 47.36}\n",
      "{'loss': 1.1778, 'learning_rate': 5.2616926503340756e-05, 'epoch': 47.38}\n",
      "{'loss': 1.1766, 'learning_rate': 5.25890868596882e-05, 'epoch': 47.41}\n",
      "{'loss': 1.2267, 'learning_rate': 5.2561247216035634e-05, 'epoch': 47.44}\n",
      "{'loss': 1.2047, 'learning_rate': 5.2533407572383077e-05, 'epoch': 47.47}\n",
      "{'loss': 1.1965, 'learning_rate': 5.250556792873051e-05, 'epoch': 47.49}\n",
      "{'loss': 1.1826, 'learning_rate': 5.2477728285077955e-05, 'epoch': 47.52}\n",
      "{'loss': 1.2119, 'learning_rate': 5.244988864142539e-05, 'epoch': 47.55}\n",
      "{'loss': 1.161, 'learning_rate': 5.242204899777283e-05, 'epoch': 47.58}\n",
      "{'loss': 1.1984, 'learning_rate': 5.239420935412027e-05, 'epoch': 47.61}\n",
      " 48% 85500/179600 [7:00:14<7:32:33,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 13:10:14,664 >> Saving model checkpoint to results/adapters/ag/checkpoint-85500\n",
      "[INFO|loading.py:59] 2021-08-02 13:10:14,664 >> Configuration saved in results/adapters/ag/checkpoint-85500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:10:14,676 >> Module weights saved in results/adapters/ag/checkpoint-85500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:10:14,676 >> Configuration saved in results/adapters/ag/checkpoint-85500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:10:14,990 >> Module weights saved in results/adapters/ag/checkpoint-85500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:10:14,991 >> Configuration saved in results/adapters/ag/checkpoint-85500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:10:15,335 >> Module weights saved in results/adapters/ag/checkpoint-85500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:10:15,335 >> tokenizer config file saved in results/adapters/ag/checkpoint-85500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:10:15,336 >> Special tokens file saved in results/adapters/ag/checkpoint-85500/special_tokens_map.json\n",
      "{'loss': 1.1999, 'learning_rate': 5.236636971046771e-05, 'epoch': 47.63}\n",
      "{'loss': 1.2016, 'learning_rate': 5.233853006681515e-05, 'epoch': 47.66}\n",
      "{'loss': 1.2131, 'learning_rate': 5.231069042316259e-05, 'epoch': 47.69}\n",
      "{'loss': 1.1856, 'learning_rate': 5.2282850779510026e-05, 'epoch': 47.72}\n",
      "{'loss': 1.1942, 'learning_rate': 5.225501113585747e-05, 'epoch': 47.74}\n",
      "{'loss': 1.2013, 'learning_rate': 5.2227171492204904e-05, 'epoch': 47.77}\n",
      "{'loss': 1.2143, 'learning_rate': 5.2199331848552346e-05, 'epoch': 47.8}\n",
      "{'loss': 1.1963, 'learning_rate': 5.2171492204899775e-05, 'epoch': 47.83}\n",
      "{'loss': 1.2228, 'learning_rate': 5.214365256124721e-05, 'epoch': 47.86}\n",
      "{'loss': 1.2129, 'learning_rate': 5.2115812917594654e-05, 'epoch': 47.88}\n",
      " 48% 86000/179600 [7:02:39<7:12:04,  3.61it/s][INFO|trainer.py:1989] 2021-08-02 13:12:39,597 >> Saving model checkpoint to results/adapters/ag/checkpoint-86000\n",
      "[INFO|loading.py:59] 2021-08-02 13:12:39,598 >> Configuration saved in results/adapters/ag/checkpoint-86000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:12:39,609 >> Module weights saved in results/adapters/ag/checkpoint-86000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:12:39,610 >> Configuration saved in results/adapters/ag/checkpoint-86000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:12:39,904 >> Module weights saved in results/adapters/ag/checkpoint-86000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:12:39,904 >> Configuration saved in results/adapters/ag/checkpoint-86000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:12:40,238 >> Module weights saved in results/adapters/ag/checkpoint-86000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:12:40,239 >> tokenizer config file saved in results/adapters/ag/checkpoint-86000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:12:40,239 >> Special tokens file saved in results/adapters/ag/checkpoint-86000/special_tokens_map.json\n",
      "{'loss': 1.2215, 'learning_rate': 5.208797327394209e-05, 'epoch': 47.91}\n",
      "{'loss': 1.2249, 'learning_rate': 5.206013363028953e-05, 'epoch': 47.94}\n",
      "{'loss': 1.2269, 'learning_rate': 5.203229398663697e-05, 'epoch': 47.97}\n",
      "{'loss': 1.2237, 'learning_rate': 5.200445434298441e-05, 'epoch': 48.0}\n",
      "{'loss': 1.2322, 'learning_rate': 5.1976614699331846e-05, 'epoch': 48.02}\n",
      "{'loss': 1.2321, 'learning_rate': 5.194877505567929e-05, 'epoch': 48.05}\n",
      "{'loss': 1.1852, 'learning_rate': 5.1920935412026724e-05, 'epoch': 48.08}\n",
      "{'loss': 1.1819, 'learning_rate': 5.189309576837417e-05, 'epoch': 48.11}\n",
      "{'loss': 1.206, 'learning_rate': 5.186525612472161e-05, 'epoch': 48.13}\n",
      "{'loss': 1.2005, 'learning_rate': 5.1837416481069045e-05, 'epoch': 48.16}\n",
      " 48% 86500/179600 [7:05:05<7:46:41,  3.32it/s][INFO|trainer.py:1989] 2021-08-02 13:15:05,593 >> Saving model checkpoint to results/adapters/ag/checkpoint-86500\n",
      "[INFO|loading.py:59] 2021-08-02 13:15:05,594 >> Configuration saved in results/adapters/ag/checkpoint-86500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:15:05,606 >> Module weights saved in results/adapters/ag/checkpoint-86500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:15:05,607 >> Configuration saved in results/adapters/ag/checkpoint-86500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:15:05,928 >> Module weights saved in results/adapters/ag/checkpoint-86500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:15:05,929 >> Configuration saved in results/adapters/ag/checkpoint-86500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:15:06,274 >> Module weights saved in results/adapters/ag/checkpoint-86500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:15:06,275 >> tokenizer config file saved in results/adapters/ag/checkpoint-86500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:15:06,275 >> Special tokens file saved in results/adapters/ag/checkpoint-86500/special_tokens_map.json\n",
      "{'loss': 1.2441, 'learning_rate': 5.180957683741649e-05, 'epoch': 48.19}\n",
      "{'loss': 1.2228, 'learning_rate': 5.1781737193763924e-05, 'epoch': 48.22}\n",
      "{'loss': 1.2222, 'learning_rate': 5.1753897550111366e-05, 'epoch': 48.25}\n",
      "{'loss': 1.2052, 'learning_rate': 5.17260579064588e-05, 'epoch': 48.27}\n",
      "{'loss': 1.2113, 'learning_rate': 5.1698218262806244e-05, 'epoch': 48.3}\n",
      "{'loss': 1.1933, 'learning_rate': 5.167037861915368e-05, 'epoch': 48.33}\n",
      "{'loss': 1.2077, 'learning_rate': 5.164253897550112e-05, 'epoch': 48.36}\n",
      "{'loss': 1.1988, 'learning_rate': 5.161469933184856e-05, 'epoch': 48.39}\n",
      "{'loss': 1.1893, 'learning_rate': 5.1586859688196e-05, 'epoch': 48.41}\n",
      "{'loss': 1.2256, 'learning_rate': 5.155902004454343e-05, 'epoch': 48.44}\n",
      " 48% 87000/179600 [7:07:29<7:16:39,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 13:17:29,998 >> Saving model checkpoint to results/adapters/ag/checkpoint-87000\n",
      "[INFO|loading.py:59] 2021-08-02 13:17:29,999 >> Configuration saved in results/adapters/ag/checkpoint-87000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:17:30,010 >> Module weights saved in results/adapters/ag/checkpoint-87000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:17:30,010 >> Configuration saved in results/adapters/ag/checkpoint-87000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:17:30,305 >> Module weights saved in results/adapters/ag/checkpoint-87000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:17:30,306 >> Configuration saved in results/adapters/ag/checkpoint-87000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:17:30,634 >> Module weights saved in results/adapters/ag/checkpoint-87000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:17:30,635 >> tokenizer config file saved in results/adapters/ag/checkpoint-87000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:17:30,635 >> Special tokens file saved in results/adapters/ag/checkpoint-87000/special_tokens_map.json\n",
      "{'loss': 1.2008, 'learning_rate': 5.1531180400890866e-05, 'epoch': 48.47}\n",
      "{'loss': 1.2168, 'learning_rate': 5.150334075723831e-05, 'epoch': 48.5}\n",
      "{'loss': 1.2462, 'learning_rate': 5.1475501113585744e-05, 'epoch': 48.52}\n",
      "{'loss': 1.1948, 'learning_rate': 5.144766146993319e-05, 'epoch': 48.55}\n",
      "{'loss': 1.1749, 'learning_rate': 5.141982182628062e-05, 'epoch': 48.58}\n",
      "{'loss': 1.2101, 'learning_rate': 5.1391982182628065e-05, 'epoch': 48.61}\n",
      "{'loss': 1.1985, 'learning_rate': 5.13641425389755e-05, 'epoch': 48.64}\n",
      "{'loss': 1.2096, 'learning_rate': 5.133630289532294e-05, 'epoch': 48.66}\n",
      "{'loss': 1.2112, 'learning_rate': 5.130846325167038e-05, 'epoch': 48.69}\n",
      "{'loss': 1.2266, 'learning_rate': 5.128062360801782e-05, 'epoch': 48.72}\n",
      " 49% 87500/179600 [7:09:54<7:26:54,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 13:19:54,705 >> Saving model checkpoint to results/adapters/ag/checkpoint-87500\n",
      "[INFO|loading.py:59] 2021-08-02 13:19:54,705 >> Configuration saved in results/adapters/ag/checkpoint-87500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:19:54,716 >> Module weights saved in results/adapters/ag/checkpoint-87500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:19:54,717 >> Configuration saved in results/adapters/ag/checkpoint-87500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:19:55,005 >> Module weights saved in results/adapters/ag/checkpoint-87500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:19:55,005 >> Configuration saved in results/adapters/ag/checkpoint-87500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:19:55,336 >> Module weights saved in results/adapters/ag/checkpoint-87500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:19:55,337 >> tokenizer config file saved in results/adapters/ag/checkpoint-87500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:19:55,337 >> Special tokens file saved in results/adapters/ag/checkpoint-87500/special_tokens_map.json\n",
      "{'loss': 1.2294, 'learning_rate': 5.125278396436526e-05, 'epoch': 48.75}\n",
      "{'loss': 1.1825, 'learning_rate': 5.12249443207127e-05, 'epoch': 48.77}\n",
      "{'loss': 1.2118, 'learning_rate': 5.1197104677060136e-05, 'epoch': 48.8}\n",
      "{'loss': 1.2288, 'learning_rate': 5.116926503340758e-05, 'epoch': 48.83}\n",
      "{'loss': 1.2162, 'learning_rate': 5.1141425389755014e-05, 'epoch': 48.86}\n",
      "{'loss': 1.214, 'learning_rate': 5.1113585746102457e-05, 'epoch': 48.89}\n",
      "{'loss': 1.1857, 'learning_rate': 5.108574610244989e-05, 'epoch': 48.91}\n",
      "{'loss': 1.2032, 'learning_rate': 5.1057906458797335e-05, 'epoch': 48.94}\n",
      "{'loss': 1.2354, 'learning_rate': 5.103006681514477e-05, 'epoch': 48.97}\n",
      "{'loss': 1.1911, 'learning_rate': 5.100222717149221e-05, 'epoch': 49.0}\n",
      " 49% 88000/179600 [7:12:19<7:30:07,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 13:22:19,525 >> Saving model checkpoint to results/adapters/ag/checkpoint-88000\n",
      "[INFO|loading.py:59] 2021-08-02 13:22:19,527 >> Configuration saved in results/adapters/ag/checkpoint-88000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:22:19,540 >> Module weights saved in results/adapters/ag/checkpoint-88000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:22:19,540 >> Configuration saved in results/adapters/ag/checkpoint-88000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:22:19,849 >> Module weights saved in results/adapters/ag/checkpoint-88000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:22:19,849 >> Configuration saved in results/adapters/ag/checkpoint-88000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:22:20,189 >> Module weights saved in results/adapters/ag/checkpoint-88000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:22:20,190 >> tokenizer config file saved in results/adapters/ag/checkpoint-88000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:22:20,190 >> Special tokens file saved in results/adapters/ag/checkpoint-88000/special_tokens_map.json\n",
      "{'loss': 1.209, 'learning_rate': 5.097438752783965e-05, 'epoch': 49.03}\n",
      "{'loss': 1.2204, 'learning_rate': 5.094654788418709e-05, 'epoch': 49.05}\n",
      "{'loss': 1.2041, 'learning_rate': 5.091870824053452e-05, 'epoch': 49.08}\n",
      "{'loss': 1.2008, 'learning_rate': 5.0890868596881956e-05, 'epoch': 49.11}\n",
      "{'loss': 1.1885, 'learning_rate': 5.08630289532294e-05, 'epoch': 49.14}\n",
      "{'loss': 1.1926, 'learning_rate': 5.0835189309576835e-05, 'epoch': 49.16}\n",
      "{'loss': 1.1943, 'learning_rate': 5.080734966592428e-05, 'epoch': 49.19}\n",
      "{'loss': 1.1948, 'learning_rate': 5.077951002227171e-05, 'epoch': 49.22}\n",
      "{'loss': 1.1943, 'learning_rate': 5.0751670378619155e-05, 'epoch': 49.25}\n",
      "{'loss': 1.2229, 'learning_rate': 5.072383073496659e-05, 'epoch': 49.28}\n",
      " 49% 88500/179600 [7:14:43<7:09:24,  3.54it/s][INFO|trainer.py:1989] 2021-08-02 13:24:44,436 >> Saving model checkpoint to results/adapters/ag/checkpoint-88500\n",
      "[INFO|loading.py:59] 2021-08-02 13:24:44,437 >> Configuration saved in results/adapters/ag/checkpoint-88500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:24:44,448 >> Module weights saved in results/adapters/ag/checkpoint-88500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:24:44,448 >> Configuration saved in results/adapters/ag/checkpoint-88500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:24:44,736 >> Module weights saved in results/adapters/ag/checkpoint-88500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:24:44,737 >> Configuration saved in results/adapters/ag/checkpoint-88500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:24:45,058 >> Module weights saved in results/adapters/ag/checkpoint-88500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:24:45,059 >> tokenizer config file saved in results/adapters/ag/checkpoint-88500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:24:45,059 >> Special tokens file saved in results/adapters/ag/checkpoint-88500/special_tokens_map.json\n",
      "{'loss': 1.1955, 'learning_rate': 5.0695991091314034e-05, 'epoch': 49.3}\n",
      "{'loss': 1.2049, 'learning_rate': 5.066815144766147e-05, 'epoch': 49.33}\n",
      "{'loss': 1.2027, 'learning_rate': 5.064031180400891e-05, 'epoch': 49.36}\n",
      "{'loss': 1.254, 'learning_rate': 5.061247216035635e-05, 'epoch': 49.39}\n",
      "{'loss': 1.2203, 'learning_rate': 5.058463251670379e-05, 'epoch': 49.42}\n",
      "{'loss': 1.1956, 'learning_rate': 5.0556792873051226e-05, 'epoch': 49.44}\n",
      "{'loss': 1.2045, 'learning_rate': 5.052895322939867e-05, 'epoch': 49.47}\n",
      "{'loss': 1.2195, 'learning_rate': 5.0501113585746104e-05, 'epoch': 49.5}\n",
      "{'loss': 1.168, 'learning_rate': 5.047327394209355e-05, 'epoch': 49.53}\n",
      "{'loss': 1.1941, 'learning_rate': 5.044543429844098e-05, 'epoch': 49.55}\n",
      " 50% 89000/179600 [7:17:10<7:16:07,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 13:27:11,102 >> Saving model checkpoint to results/adapters/ag/checkpoint-89000\n",
      "[INFO|loading.py:59] 2021-08-02 13:27:11,103 >> Configuration saved in results/adapters/ag/checkpoint-89000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:27:11,115 >> Module weights saved in results/adapters/ag/checkpoint-89000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:27:11,115 >> Configuration saved in results/adapters/ag/checkpoint-89000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:27:11,413 >> Module weights saved in results/adapters/ag/checkpoint-89000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:27:11,413 >> Configuration saved in results/adapters/ag/checkpoint-89000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:27:11,746 >> Module weights saved in results/adapters/ag/checkpoint-89000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:27:11,746 >> tokenizer config file saved in results/adapters/ag/checkpoint-89000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:27:11,746 >> Special tokens file saved in results/adapters/ag/checkpoint-89000/special_tokens_map.json\n",
      "{'loss': 1.2632, 'learning_rate': 5.0417594654788425e-05, 'epoch': 49.58}\n",
      "{'loss': 1.2052, 'learning_rate': 5.038975501113586e-05, 'epoch': 49.61}\n",
      "{'loss': 1.1998, 'learning_rate': 5.0361915367483304e-05, 'epoch': 49.64}\n",
      "{'loss': 1.228, 'learning_rate': 5.033407572383074e-05, 'epoch': 49.67}\n",
      "{'loss': 1.1941, 'learning_rate': 5.030623608017817e-05, 'epoch': 49.69}\n",
      "{'loss': 1.1808, 'learning_rate': 5.027839643652561e-05, 'epoch': 49.72}\n",
      "{'loss': 1.2194, 'learning_rate': 5.0250556792873047e-05, 'epoch': 49.75}\n",
      "{'loss': 1.1833, 'learning_rate': 5.022271714922049e-05, 'epoch': 49.78}\n",
      "{'loss': 1.2131, 'learning_rate': 5.0194877505567925e-05, 'epoch': 49.8}\n",
      "{'loss': 1.2225, 'learning_rate': 5.016703786191537e-05, 'epoch': 49.83}\n",
      " 50% 89500/179600 [7:19:37<7:18:11,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 13:29:37,917 >> Saving model checkpoint to results/adapters/ag/checkpoint-89500\n",
      "[INFO|loading.py:59] 2021-08-02 13:29:37,918 >> Configuration saved in results/adapters/ag/checkpoint-89500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:29:37,929 >> Module weights saved in results/adapters/ag/checkpoint-89500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:29:37,929 >> Configuration saved in results/adapters/ag/checkpoint-89500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:29:38,235 >> Module weights saved in results/adapters/ag/checkpoint-89500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:29:38,236 >> Configuration saved in results/adapters/ag/checkpoint-89500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:29:38,567 >> Module weights saved in results/adapters/ag/checkpoint-89500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:29:38,568 >> tokenizer config file saved in results/adapters/ag/checkpoint-89500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:29:38,568 >> Special tokens file saved in results/adapters/ag/checkpoint-89500/special_tokens_map.json\n",
      "{'loss': 1.2387, 'learning_rate': 5.01391982182628e-05, 'epoch': 49.86}\n",
      "{'loss': 1.1926, 'learning_rate': 5.0111358574610246e-05, 'epoch': 49.89}\n",
      "{'loss': 1.1843, 'learning_rate': 5.008351893095768e-05, 'epoch': 49.92}\n",
      "{'loss': 1.2101, 'learning_rate': 5.0055679287305124e-05, 'epoch': 49.94}\n",
      "{'loss': 1.1662, 'learning_rate': 5.002783964365256e-05, 'epoch': 49.97}\n",
      "{'loss': 1.2153, 'learning_rate': 5e-05, 'epoch': 50.0}\n",
      "{'loss': 1.1983, 'learning_rate': 4.9972160356347445e-05, 'epoch': 50.03}\n",
      "{'loss': 1.2196, 'learning_rate': 4.994432071269488e-05, 'epoch': 50.06}\n",
      "{'loss': 1.1985, 'learning_rate': 4.991648106904232e-05, 'epoch': 50.08}\n",
      "{'loss': 1.2122, 'learning_rate': 4.988864142538976e-05, 'epoch': 50.11}\n",
      " 50% 90000/179600 [7:22:06<7:33:05,  3.30it/s][INFO|trainer.py:1989] 2021-08-02 13:32:06,777 >> Saving model checkpoint to results/adapters/ag/checkpoint-90000\n",
      "[INFO|loading.py:59] 2021-08-02 13:32:06,778 >> Configuration saved in results/adapters/ag/checkpoint-90000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:32:06,789 >> Module weights saved in results/adapters/ag/checkpoint-90000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:32:06,789 >> Configuration saved in results/adapters/ag/checkpoint-90000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:32:07,133 >> Module weights saved in results/adapters/ag/checkpoint-90000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:32:07,134 >> Configuration saved in results/adapters/ag/checkpoint-90000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:32:07,485 >> Module weights saved in results/adapters/ag/checkpoint-90000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:32:07,486 >> tokenizer config file saved in results/adapters/ag/checkpoint-90000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:32:07,486 >> Special tokens file saved in results/adapters/ag/checkpoint-90000/special_tokens_map.json\n",
      "{'loss': 1.242, 'learning_rate': 4.98608017817372e-05, 'epoch': 50.14}\n",
      "{'loss': 1.2033, 'learning_rate': 4.983296213808463e-05, 'epoch': 50.17}\n",
      "{'loss': 1.1946, 'learning_rate': 4.980512249443207e-05, 'epoch': 50.19}\n",
      "{'loss': 1.1949, 'learning_rate': 4.977728285077951e-05, 'epoch': 50.22}\n",
      "{'loss': 1.2065, 'learning_rate': 4.974944320712695e-05, 'epoch': 50.25}\n",
      "{'loss': 1.1957, 'learning_rate': 4.972160356347439e-05, 'epoch': 50.28}\n",
      "{'loss': 1.1728, 'learning_rate': 4.969376391982183e-05, 'epoch': 50.31}\n",
      "{'loss': 1.2319, 'learning_rate': 4.9665924276169265e-05, 'epoch': 50.33}\n",
      "{'loss': 1.181, 'learning_rate': 4.963808463251671e-05, 'epoch': 50.36}\n",
      "{'loss': 1.2056, 'learning_rate': 4.9610244988864144e-05, 'epoch': 50.39}\n",
      " 50% 90500/179600 [7:24:34<7:22:18,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 13:34:35,297 >> Saving model checkpoint to results/adapters/ag/checkpoint-90500\n",
      "[INFO|loading.py:59] 2021-08-02 13:34:35,298 >> Configuration saved in results/adapters/ag/checkpoint-90500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:34:35,310 >> Module weights saved in results/adapters/ag/checkpoint-90500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:34:35,310 >> Configuration saved in results/adapters/ag/checkpoint-90500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:34:35,618 >> Module weights saved in results/adapters/ag/checkpoint-90500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:34:35,619 >> Configuration saved in results/adapters/ag/checkpoint-90500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:34:35,955 >> Module weights saved in results/adapters/ag/checkpoint-90500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:34:35,955 >> tokenizer config file saved in results/adapters/ag/checkpoint-90500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:34:35,956 >> Special tokens file saved in results/adapters/ag/checkpoint-90500/special_tokens_map.json\n",
      "{'loss': 1.2115, 'learning_rate': 4.9582405345211586e-05, 'epoch': 50.42}\n",
      "{'loss': 1.1964, 'learning_rate': 4.955456570155902e-05, 'epoch': 50.45}\n",
      "{'loss': 1.1981, 'learning_rate': 4.952672605790646e-05, 'epoch': 50.47}\n",
      "{'loss': 1.229, 'learning_rate': 4.94988864142539e-05, 'epoch': 50.5}\n",
      "{'loss': 1.1695, 'learning_rate': 4.9471046770601336e-05, 'epoch': 50.53}\n",
      "{'loss': 1.2185, 'learning_rate': 4.944320712694878e-05, 'epoch': 50.56}\n",
      "{'loss': 1.1945, 'learning_rate': 4.9415367483296214e-05, 'epoch': 50.58}\n",
      "{'loss': 1.211, 'learning_rate': 4.938752783964366e-05, 'epoch': 50.61}\n",
      "{'loss': 1.1791, 'learning_rate': 4.935968819599109e-05, 'epoch': 50.64}\n",
      "{'loss': 1.1822, 'learning_rate': 4.9331848552338535e-05, 'epoch': 50.67}\n",
      " 51% 91000/179600 [7:27:02<7:28:33,  3.29it/s][INFO|trainer.py:1989] 2021-08-02 13:37:03,047 >> Saving model checkpoint to results/adapters/ag/checkpoint-91000\n",
      "[INFO|loading.py:59] 2021-08-02 13:37:03,047 >> Configuration saved in results/adapters/ag/checkpoint-91000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:37:03,059 >> Module weights saved in results/adapters/ag/checkpoint-91000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:37:03,059 >> Configuration saved in results/adapters/ag/checkpoint-91000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:37:03,376 >> Module weights saved in results/adapters/ag/checkpoint-91000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:37:03,376 >> Configuration saved in results/adapters/ag/checkpoint-91000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:37:03,725 >> Module weights saved in results/adapters/ag/checkpoint-91000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:37:03,725 >> tokenizer config file saved in results/adapters/ag/checkpoint-91000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:37:03,726 >> Special tokens file saved in results/adapters/ag/checkpoint-91000/special_tokens_map.json\n",
      "{'loss': 1.2052, 'learning_rate': 4.930400890868597e-05, 'epoch': 50.7}\n",
      "{'loss': 1.204, 'learning_rate': 4.9276169265033414e-05, 'epoch': 50.72}\n",
      "{'loss': 1.1956, 'learning_rate': 4.924832962138085e-05, 'epoch': 50.75}\n",
      "{'loss': 1.1925, 'learning_rate': 4.922048997772829e-05, 'epoch': 50.78}\n",
      "{'loss': 1.2201, 'learning_rate': 4.919265033407572e-05, 'epoch': 50.81}\n",
      "{'loss': 1.1862, 'learning_rate': 4.9164810690423163e-05, 'epoch': 50.83}\n",
      "{'loss': 1.1994, 'learning_rate': 4.91369710467706e-05, 'epoch': 50.86}\n",
      "{'loss': 1.2395, 'learning_rate': 4.910913140311804e-05, 'epoch': 50.89}\n",
      "{'loss': 1.1898, 'learning_rate': 4.908129175946548e-05, 'epoch': 50.92}\n",
      "{'loss': 1.197, 'learning_rate': 4.905345211581292e-05, 'epoch': 50.95}\n",
      " 51% 91500/179600 [7:29:30<6:53:13,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 13:39:30,824 >> Saving model checkpoint to results/adapters/ag/checkpoint-91500\n",
      "[INFO|loading.py:59] 2021-08-02 13:39:30,825 >> Configuration saved in results/adapters/ag/checkpoint-91500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:39:30,837 >> Module weights saved in results/adapters/ag/checkpoint-91500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:39:30,837 >> Configuration saved in results/adapters/ag/checkpoint-91500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:39:31,135 >> Module weights saved in results/adapters/ag/checkpoint-91500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:39:31,136 >> Configuration saved in results/adapters/ag/checkpoint-91500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:39:31,476 >> Module weights saved in results/adapters/ag/checkpoint-91500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:39:31,477 >> tokenizer config file saved in results/adapters/ag/checkpoint-91500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:39:31,477 >> Special tokens file saved in results/adapters/ag/checkpoint-91500/special_tokens_map.json\n",
      "{'loss': 1.1987, 'learning_rate': 4.902561247216036e-05, 'epoch': 50.97}\n",
      "{'loss': 1.2194, 'learning_rate': 4.89977728285078e-05, 'epoch': 51.0}\n",
      "{'loss': 1.2083, 'learning_rate': 4.896993318485524e-05, 'epoch': 51.03}\n",
      "{'loss': 1.2019, 'learning_rate': 4.894209354120268e-05, 'epoch': 51.06}\n",
      "{'loss': 1.2025, 'learning_rate': 4.891425389755012e-05, 'epoch': 51.09}\n",
      "{'loss': 1.179, 'learning_rate': 4.888641425389755e-05, 'epoch': 51.11}\n",
      "{'loss': 1.2074, 'learning_rate': 4.885857461024499e-05, 'epoch': 51.14}\n",
      "{'loss': 1.2035, 'learning_rate': 4.8830734966592427e-05, 'epoch': 51.17}\n",
      "{'loss': 1.195, 'learning_rate': 4.880289532293987e-05, 'epoch': 51.2}\n",
      "{'loss': 1.1879, 'learning_rate': 4.8775055679287305e-05, 'epoch': 51.22}\n",
      " 51% 92000/179600 [7:31:58<7:13:26,  3.37it/s][INFO|trainer.py:1989] 2021-08-02 13:41:58,899 >> Saving model checkpoint to results/adapters/ag/checkpoint-92000\n",
      "[INFO|loading.py:59] 2021-08-02 13:41:58,900 >> Configuration saved in results/adapters/ag/checkpoint-92000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:41:58,913 >> Module weights saved in results/adapters/ag/checkpoint-92000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:41:58,913 >> Configuration saved in results/adapters/ag/checkpoint-92000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:41:59,224 >> Module weights saved in results/adapters/ag/checkpoint-92000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:41:59,225 >> Configuration saved in results/adapters/ag/checkpoint-92000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:41:59,571 >> Module weights saved in results/adapters/ag/checkpoint-92000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:41:59,571 >> tokenizer config file saved in results/adapters/ag/checkpoint-92000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:41:59,572 >> Special tokens file saved in results/adapters/ag/checkpoint-92000/special_tokens_map.json\n",
      "{'loss': 1.1643, 'learning_rate': 4.874721603563475e-05, 'epoch': 51.25}\n",
      "{'loss': 1.1947, 'learning_rate': 4.871937639198218e-05, 'epoch': 51.28}\n",
      "{'loss': 1.2077, 'learning_rate': 4.8691536748329626e-05, 'epoch': 51.31}\n",
      "{'loss': 1.204, 'learning_rate': 4.866369710467706e-05, 'epoch': 51.34}\n",
      "{'loss': 1.1897, 'learning_rate': 4.8635857461024504e-05, 'epoch': 51.36}\n",
      "{'loss': 1.1879, 'learning_rate': 4.860801781737194e-05, 'epoch': 51.39}\n",
      "{'loss': 1.2098, 'learning_rate': 4.8580178173719376e-05, 'epoch': 51.42}\n",
      "{'loss': 1.233, 'learning_rate': 4.855233853006682e-05, 'epoch': 51.45}\n",
      "{'loss': 1.2006, 'learning_rate': 4.8524498886414254e-05, 'epoch': 51.48}\n",
      "{'loss': 1.196, 'learning_rate': 4.8496659242761696e-05, 'epoch': 51.5}\n",
      " 52% 92500/179600 [7:34:25<7:05:04,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 13:44:26,306 >> Saving model checkpoint to results/adapters/ag/checkpoint-92500\n",
      "[INFO|loading.py:59] 2021-08-02 13:44:26,307 >> Configuration saved in results/adapters/ag/checkpoint-92500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:44:26,318 >> Module weights saved in results/adapters/ag/checkpoint-92500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:44:26,319 >> Configuration saved in results/adapters/ag/checkpoint-92500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:44:26,622 >> Module weights saved in results/adapters/ag/checkpoint-92500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:44:26,622 >> Configuration saved in results/adapters/ag/checkpoint-92500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:44:26,954 >> Module weights saved in results/adapters/ag/checkpoint-92500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:44:26,954 >> tokenizer config file saved in results/adapters/ag/checkpoint-92500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:44:26,955 >> Special tokens file saved in results/adapters/ag/checkpoint-92500/special_tokens_map.json\n",
      "{'loss': 1.1935, 'learning_rate': 4.846881959910913e-05, 'epoch': 51.53}\n",
      "{'loss': 1.1914, 'learning_rate': 4.8440979955456575e-05, 'epoch': 51.56}\n",
      "{'loss': 1.1911, 'learning_rate': 4.841314031180401e-05, 'epoch': 51.59}\n",
      "{'loss': 1.2149, 'learning_rate': 4.838530066815145e-05, 'epoch': 51.61}\n",
      "{'loss': 1.2125, 'learning_rate': 4.835746102449889e-05, 'epoch': 51.64}\n",
      "{'loss': 1.1816, 'learning_rate': 4.832962138084633e-05, 'epoch': 51.67}\n",
      "{'loss': 1.1928, 'learning_rate': 4.830178173719377e-05, 'epoch': 51.7}\n",
      "{'loss': 1.1533, 'learning_rate': 4.82739420935412e-05, 'epoch': 51.73}\n",
      "{'loss': 1.1814, 'learning_rate': 4.824610244988864e-05, 'epoch': 51.75}\n",
      "{'loss': 1.1859, 'learning_rate': 4.821826280623608e-05, 'epoch': 51.78}\n",
      " 52% 93000/179600 [7:36:53<7:02:56,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 13:46:53,864 >> Saving model checkpoint to results/adapters/ag/checkpoint-93000\n",
      "[INFO|loading.py:59] 2021-08-02 13:46:53,865 >> Configuration saved in results/adapters/ag/checkpoint-93000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:46:53,876 >> Module weights saved in results/adapters/ag/checkpoint-93000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:46:53,876 >> Configuration saved in results/adapters/ag/checkpoint-93000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:46:54,183 >> Module weights saved in results/adapters/ag/checkpoint-93000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:46:54,183 >> Configuration saved in results/adapters/ag/checkpoint-93000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:46:54,512 >> Module weights saved in results/adapters/ag/checkpoint-93000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:46:54,513 >> tokenizer config file saved in results/adapters/ag/checkpoint-93000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:46:54,513 >> Special tokens file saved in results/adapters/ag/checkpoint-93000/special_tokens_map.json\n",
      "{'loss': 1.2143, 'learning_rate': 4.819042316258352e-05, 'epoch': 51.81}\n",
      "{'loss': 1.2119, 'learning_rate': 4.816258351893096e-05, 'epoch': 51.84}\n",
      "{'loss': 1.1983, 'learning_rate': 4.8134743875278395e-05, 'epoch': 51.86}\n",
      "{'loss': 1.2097, 'learning_rate': 4.810690423162584e-05, 'epoch': 51.89}\n",
      "{'loss': 1.185, 'learning_rate': 4.807906458797328e-05, 'epoch': 51.92}\n",
      "{'loss': 1.2648, 'learning_rate': 4.8051224944320716e-05, 'epoch': 51.95}\n",
      "{'loss': 1.2048, 'learning_rate': 4.802338530066816e-05, 'epoch': 51.98}\n",
      "{'loss': 1.2332, 'learning_rate': 4.7995545657015594e-05, 'epoch': 52.0}\n",
      "{'loss': 1.2077, 'learning_rate': 4.796770601336303e-05, 'epoch': 52.03}\n",
      "{'loss': 1.1714, 'learning_rate': 4.7939866369710466e-05, 'epoch': 52.06}\n",
      " 52% 93500/179600 [7:39:20<6:56:41,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 13:49:21,026 >> Saving model checkpoint to results/adapters/ag/checkpoint-93500\n",
      "[INFO|loading.py:59] 2021-08-02 13:49:21,027 >> Configuration saved in results/adapters/ag/checkpoint-93500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:49:21,038 >> Module weights saved in results/adapters/ag/checkpoint-93500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:49:21,039 >> Configuration saved in results/adapters/ag/checkpoint-93500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:49:21,343 >> Module weights saved in results/adapters/ag/checkpoint-93500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:49:21,343 >> Configuration saved in results/adapters/ag/checkpoint-93500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:49:21,691 >> Module weights saved in results/adapters/ag/checkpoint-93500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:49:21,691 >> tokenizer config file saved in results/adapters/ag/checkpoint-93500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:49:21,691 >> Special tokens file saved in results/adapters/ag/checkpoint-93500/special_tokens_map.json\n",
      "{'loss': 1.1726, 'learning_rate': 4.791202672605791e-05, 'epoch': 52.09}\n",
      "{'loss': 1.1933, 'learning_rate': 4.7884187082405344e-05, 'epoch': 52.12}\n",
      "{'loss': 1.2127, 'learning_rate': 4.785634743875279e-05, 'epoch': 52.14}\n",
      "{'loss': 1.188, 'learning_rate': 4.782850779510022e-05, 'epoch': 52.17}\n",
      "{'loss': 1.2233, 'learning_rate': 4.7800668151447665e-05, 'epoch': 52.2}\n",
      "{'loss': 1.1795, 'learning_rate': 4.77728285077951e-05, 'epoch': 52.23}\n",
      "{'loss': 1.1973, 'learning_rate': 4.7744988864142543e-05, 'epoch': 52.25}\n",
      "{'loss': 1.2056, 'learning_rate': 4.771714922048998e-05, 'epoch': 52.28}\n",
      "{'loss': 1.2091, 'learning_rate': 4.768930957683742e-05, 'epoch': 52.31}\n",
      "{'loss': 1.1879, 'learning_rate': 4.766146993318486e-05, 'epoch': 52.34}\n",
      " 52% 94000/179600 [7:41:48<6:46:44,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 13:51:48,797 >> Saving model checkpoint to results/adapters/ag/checkpoint-94000\n",
      "[INFO|loading.py:59] 2021-08-02 13:51:48,797 >> Configuration saved in results/adapters/ag/checkpoint-94000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:51:48,812 >> Module weights saved in results/adapters/ag/checkpoint-94000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:51:48,812 >> Configuration saved in results/adapters/ag/checkpoint-94000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:51:49,115 >> Module weights saved in results/adapters/ag/checkpoint-94000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:51:49,115 >> Configuration saved in results/adapters/ag/checkpoint-94000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:51:49,449 >> Module weights saved in results/adapters/ag/checkpoint-94000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:51:49,449 >> tokenizer config file saved in results/adapters/ag/checkpoint-94000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:51:49,449 >> Special tokens file saved in results/adapters/ag/checkpoint-94000/special_tokens_map.json\n",
      "{'loss': 1.1725, 'learning_rate': 4.763363028953229e-05, 'epoch': 52.37}\n",
      "{'loss': 1.1658, 'learning_rate': 4.7605790645879736e-05, 'epoch': 52.39}\n",
      "{'loss': 1.1889, 'learning_rate': 4.757795100222717e-05, 'epoch': 52.42}\n",
      "{'loss': 1.1962, 'learning_rate': 4.7550111358574614e-05, 'epoch': 52.45}\n",
      "{'loss': 1.193, 'learning_rate': 4.752227171492205e-05, 'epoch': 52.48}\n",
      "{'loss': 1.1905, 'learning_rate': 4.749443207126949e-05, 'epoch': 52.51}\n",
      "{'loss': 1.1891, 'learning_rate': 4.746659242761693e-05, 'epoch': 52.53}\n",
      "{'loss': 1.2373, 'learning_rate': 4.743875278396437e-05, 'epoch': 52.56}\n",
      "{'loss': 1.2123, 'learning_rate': 4.7410913140311807e-05, 'epoch': 52.59}\n",
      "{'loss': 1.1955, 'learning_rate': 4.738307349665925e-05, 'epoch': 52.62}\n",
      " 53% 94500/179600 [7:44:16<6:58:47,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 13:54:16,525 >> Saving model checkpoint to results/adapters/ag/checkpoint-94500\n",
      "[INFO|loading.py:59] 2021-08-02 13:54:16,525 >> Configuration saved in results/adapters/ag/checkpoint-94500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:54:16,537 >> Module weights saved in results/adapters/ag/checkpoint-94500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:54:16,537 >> Configuration saved in results/adapters/ag/checkpoint-94500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:54:16,848 >> Module weights saved in results/adapters/ag/checkpoint-94500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:54:16,848 >> Configuration saved in results/adapters/ag/checkpoint-94500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:54:17,180 >> Module weights saved in results/adapters/ag/checkpoint-94500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:54:17,181 >> tokenizer config file saved in results/adapters/ag/checkpoint-94500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:54:17,181 >> Special tokens file saved in results/adapters/ag/checkpoint-94500/special_tokens_map.json\n",
      "{'loss': 1.1792, 'learning_rate': 4.7355233853006685e-05, 'epoch': 52.64}\n",
      "{'loss': 1.2049, 'learning_rate': 4.732739420935412e-05, 'epoch': 52.67}\n",
      "{'loss': 1.1825, 'learning_rate': 4.7299554565701556e-05, 'epoch': 52.7}\n",
      "{'loss': 1.2097, 'learning_rate': 4.7271714922049e-05, 'epoch': 52.73}\n",
      "{'loss': 1.2031, 'learning_rate': 4.7243875278396435e-05, 'epoch': 52.76}\n",
      "{'loss': 1.1681, 'learning_rate': 4.721603563474388e-05, 'epoch': 52.78}\n",
      "{'loss': 1.1889, 'learning_rate': 4.718819599109131e-05, 'epoch': 52.81}\n",
      "{'loss': 1.1843, 'learning_rate': 4.7160356347438756e-05, 'epoch': 52.84}\n",
      "{'loss': 1.1965, 'learning_rate': 4.71325167037862e-05, 'epoch': 52.87}\n",
      "{'loss': 1.1897, 'learning_rate': 4.7104677060133634e-05, 'epoch': 52.89}\n",
      " 53% 95000/179600 [7:46:43<6:32:38,  3.59it/s][INFO|trainer.py:1989] 2021-08-02 13:56:44,010 >> Saving model checkpoint to results/adapters/ag/checkpoint-95000\n",
      "[INFO|loading.py:59] 2021-08-02 13:56:44,011 >> Configuration saved in results/adapters/ag/checkpoint-95000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:56:44,022 >> Module weights saved in results/adapters/ag/checkpoint-95000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:56:44,022 >> Configuration saved in results/adapters/ag/checkpoint-95000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:56:44,327 >> Module weights saved in results/adapters/ag/checkpoint-95000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:56:44,327 >> Configuration saved in results/adapters/ag/checkpoint-95000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:56:44,666 >> Module weights saved in results/adapters/ag/checkpoint-95000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:56:44,666 >> tokenizer config file saved in results/adapters/ag/checkpoint-95000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:56:44,667 >> Special tokens file saved in results/adapters/ag/checkpoint-95000/special_tokens_map.json\n",
      "{'loss': 1.2185, 'learning_rate': 4.7076837416481076e-05, 'epoch': 52.92}\n",
      "{'loss': 1.1922, 'learning_rate': 4.704899777282851e-05, 'epoch': 52.95}\n",
      "{'loss': 1.1848, 'learning_rate': 4.702115812917595e-05, 'epoch': 52.98}\n",
      "{'loss': 1.2118, 'learning_rate': 4.6993318485523384e-05, 'epoch': 53.01}\n",
      "{'loss': 1.207, 'learning_rate': 4.6965478841870826e-05, 'epoch': 53.03}\n",
      "{'loss': 1.2023, 'learning_rate': 4.693763919821826e-05, 'epoch': 53.06}\n",
      "{'loss': 1.1914, 'learning_rate': 4.6909799554565705e-05, 'epoch': 53.09}\n",
      "{'loss': 1.1816, 'learning_rate': 4.688195991091314e-05, 'epoch': 53.12}\n",
      "{'loss': 1.1932, 'learning_rate': 4.685412026726058e-05, 'epoch': 53.15}\n",
      "{'loss': 1.2386, 'learning_rate': 4.682628062360802e-05, 'epoch': 53.17}\n",
      " 53% 95500/179600 [7:49:10<7:00:40,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 13:59:11,121 >> Saving model checkpoint to results/adapters/ag/checkpoint-95500\n",
      "[INFO|loading.py:59] 2021-08-02 13:59:11,121 >> Configuration saved in results/adapters/ag/checkpoint-95500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:59:11,135 >> Module weights saved in results/adapters/ag/checkpoint-95500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:59:11,135 >> Configuration saved in results/adapters/ag/checkpoint-95500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:59:11,452 >> Module weights saved in results/adapters/ag/checkpoint-95500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 13:59:11,453 >> Configuration saved in results/adapters/ag/checkpoint-95500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 13:59:11,801 >> Module weights saved in results/adapters/ag/checkpoint-95500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 13:59:11,801 >> tokenizer config file saved in results/adapters/ag/checkpoint-95500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 13:59:11,802 >> Special tokens file saved in results/adapters/ag/checkpoint-95500/special_tokens_map.json\n",
      "{'loss': 1.1521, 'learning_rate': 4.679844097995546e-05, 'epoch': 53.2}\n",
      "{'loss': 1.2594, 'learning_rate': 4.67706013363029e-05, 'epoch': 53.23}\n",
      "{'loss': 1.1871, 'learning_rate': 4.674276169265034e-05, 'epoch': 53.26}\n",
      "{'loss': 1.199, 'learning_rate': 4.6714922048997775e-05, 'epoch': 53.28}\n",
      "{'loss': 1.1974, 'learning_rate': 4.668708240534521e-05, 'epoch': 53.31}\n",
      "{'loss': 1.2172, 'learning_rate': 4.6659242761692654e-05, 'epoch': 53.34}\n",
      "{'loss': 1.1953, 'learning_rate': 4.663140311804009e-05, 'epoch': 53.37}\n",
      "{'loss': 1.1546, 'learning_rate': 4.660356347438753e-05, 'epoch': 53.4}\n",
      "{'loss': 1.2331, 'learning_rate': 4.657572383073497e-05, 'epoch': 53.42}\n",
      "{'loss': 1.1847, 'learning_rate': 4.654788418708241e-05, 'epoch': 53.45}\n",
      " 53% 96000/179600 [7:51:37<6:58:51,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 14:01:38,053 >> Saving model checkpoint to results/adapters/ag/checkpoint-96000\n",
      "[INFO|loading.py:59] 2021-08-02 14:01:38,053 >> Configuration saved in results/adapters/ag/checkpoint-96000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:01:38,065 >> Module weights saved in results/adapters/ag/checkpoint-96000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:01:38,065 >> Configuration saved in results/adapters/ag/checkpoint-96000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:01:38,356 >> Module weights saved in results/adapters/ag/checkpoint-96000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:01:38,357 >> Configuration saved in results/adapters/ag/checkpoint-96000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:01:38,686 >> Module weights saved in results/adapters/ag/checkpoint-96000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:01:38,686 >> tokenizer config file saved in results/adapters/ag/checkpoint-96000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:01:38,687 >> Special tokens file saved in results/adapters/ag/checkpoint-96000/special_tokens_map.json\n",
      "{'loss': 1.2062, 'learning_rate': 4.6520044543429846e-05, 'epoch': 53.48}\n",
      "{'loss': 1.1696, 'learning_rate': 4.649220489977729e-05, 'epoch': 53.51}\n",
      "{'loss': 1.1946, 'learning_rate': 4.6464365256124724e-05, 'epoch': 53.54}\n",
      "{'loss': 1.173, 'learning_rate': 4.643652561247217e-05, 'epoch': 53.56}\n",
      "{'loss': 1.1903, 'learning_rate': 4.64086859688196e-05, 'epoch': 53.59}\n",
      "{'loss': 1.1823, 'learning_rate': 4.638084632516704e-05, 'epoch': 53.62}\n",
      "{'loss': 1.2231, 'learning_rate': 4.6353006681514474e-05, 'epoch': 53.65}\n",
      "{'loss': 1.2113, 'learning_rate': 4.632516703786192e-05, 'epoch': 53.67}\n",
      "{'loss': 1.2128, 'learning_rate': 4.629732739420935e-05, 'epoch': 53.7}\n",
      "{'loss': 1.227, 'learning_rate': 4.6269487750556795e-05, 'epoch': 53.73}\n",
      " 54% 96500/179600 [7:54:03<6:49:12,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 14:04:04,286 >> Saving model checkpoint to results/adapters/ag/checkpoint-96500\n",
      "[INFO|loading.py:59] 2021-08-02 14:04:04,287 >> Configuration saved in results/adapters/ag/checkpoint-96500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:04:04,299 >> Module weights saved in results/adapters/ag/checkpoint-96500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:04:04,300 >> Configuration saved in results/adapters/ag/checkpoint-96500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:04:04,609 >> Module weights saved in results/adapters/ag/checkpoint-96500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:04:04,610 >> Configuration saved in results/adapters/ag/checkpoint-96500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:04:04,956 >> Module weights saved in results/adapters/ag/checkpoint-96500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:04:04,957 >> tokenizer config file saved in results/adapters/ag/checkpoint-96500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:04:04,957 >> Special tokens file saved in results/adapters/ag/checkpoint-96500/special_tokens_map.json\n",
      "{'loss': 1.1973, 'learning_rate': 4.624164810690423e-05, 'epoch': 53.76}\n",
      "{'loss': 1.2005, 'learning_rate': 4.621380846325167e-05, 'epoch': 53.79}\n",
      "{'loss': 1.1884, 'learning_rate': 4.6185968819599116e-05, 'epoch': 53.81}\n",
      "{'loss': 1.1857, 'learning_rate': 4.615812917594655e-05, 'epoch': 53.84}\n",
      "{'loss': 1.2163, 'learning_rate': 4.6130289532293994e-05, 'epoch': 53.87}\n",
      "{'loss': 1.1713, 'learning_rate': 4.610244988864143e-05, 'epoch': 53.9}\n",
      "{'loss': 1.2239, 'learning_rate': 4.6074610244988866e-05, 'epoch': 53.92}\n",
      "{'loss': 1.1921, 'learning_rate': 4.60467706013363e-05, 'epoch': 53.95}\n",
      "{'loss': 1.1947, 'learning_rate': 4.6018930957683744e-05, 'epoch': 53.98}\n",
      "{'loss': 1.1877, 'learning_rate': 4.599109131403118e-05, 'epoch': 54.01}\n",
      " 54% 97000/179600 [7:56:30<6:26:47,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 14:06:31,319 >> Saving model checkpoint to results/adapters/ag/checkpoint-97000\n",
      "[INFO|loading.py:59] 2021-08-02 14:06:31,320 >> Configuration saved in results/adapters/ag/checkpoint-97000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:06:31,331 >> Module weights saved in results/adapters/ag/checkpoint-97000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:06:31,331 >> Configuration saved in results/adapters/ag/checkpoint-97000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:06:31,628 >> Module weights saved in results/adapters/ag/checkpoint-97000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:06:31,628 >> Configuration saved in results/adapters/ag/checkpoint-97000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:06:31,956 >> Module weights saved in results/adapters/ag/checkpoint-97000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:06:31,956 >> tokenizer config file saved in results/adapters/ag/checkpoint-97000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:06:31,957 >> Special tokens file saved in results/adapters/ag/checkpoint-97000/special_tokens_map.json\n",
      "{'loss': 1.2071, 'learning_rate': 4.596325167037862e-05, 'epoch': 54.04}\n",
      "{'loss': 1.1878, 'learning_rate': 4.593541202672606e-05, 'epoch': 54.06}\n",
      "{'loss': 1.1718, 'learning_rate': 4.59075723830735e-05, 'epoch': 54.09}\n",
      "{'loss': 1.1969, 'learning_rate': 4.5879732739420936e-05, 'epoch': 54.12}\n",
      "{'loss': 1.1798, 'learning_rate': 4.585189309576838e-05, 'epoch': 54.15}\n",
      "{'loss': 1.2252, 'learning_rate': 4.5824053452115815e-05, 'epoch': 54.18}\n",
      "{'loss': 1.2107, 'learning_rate': 4.579621380846326e-05, 'epoch': 54.2}\n",
      "{'loss': 1.1943, 'learning_rate': 4.576837416481069e-05, 'epoch': 54.23}\n",
      "{'loss': 1.1936, 'learning_rate': 4.574053452115813e-05, 'epoch': 54.26}\n",
      "{'loss': 1.2057, 'learning_rate': 4.571269487750557e-05, 'epoch': 54.29}\n",
      " 54% 97500/179600 [7:58:58<6:49:56,  3.34it/s][INFO|trainer.py:1989] 2021-08-02 14:08:58,752 >> Saving model checkpoint to results/adapters/ag/checkpoint-97500\n",
      "[INFO|loading.py:59] 2021-08-02 14:08:58,753 >> Configuration saved in results/adapters/ag/checkpoint-97500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:08:58,765 >> Module weights saved in results/adapters/ag/checkpoint-97500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:08:58,766 >> Configuration saved in results/adapters/ag/checkpoint-97500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:08:59,066 >> Module weights saved in results/adapters/ag/checkpoint-97500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:08:59,066 >> Configuration saved in results/adapters/ag/checkpoint-97500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:08:59,405 >> Module weights saved in results/adapters/ag/checkpoint-97500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:08:59,406 >> tokenizer config file saved in results/adapters/ag/checkpoint-97500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:08:59,406 >> Special tokens file saved in results/adapters/ag/checkpoint-97500/special_tokens_map.json\n",
      "{'loss': 1.1625, 'learning_rate': 4.568485523385301e-05, 'epoch': 54.31}\n",
      "{'loss': 1.193, 'learning_rate': 4.565701559020045e-05, 'epoch': 54.34}\n",
      "{'loss': 1.1746, 'learning_rate': 4.5629175946547885e-05, 'epoch': 54.37}\n",
      "{'loss': 1.1681, 'learning_rate': 4.560133630289533e-05, 'epoch': 54.4}\n",
      "{'loss': 1.2014, 'learning_rate': 4.5573496659242764e-05, 'epoch': 54.43}\n",
      "{'loss': 1.1801, 'learning_rate': 4.5545657015590206e-05, 'epoch': 54.45}\n",
      "{'loss': 1.2155, 'learning_rate': 4.551781737193764e-05, 'epoch': 54.48}\n",
      "{'loss': 1.2196, 'learning_rate': 4.5489977728285085e-05, 'epoch': 54.51}\n",
      "{'loss': 1.1674, 'learning_rate': 4.5462138084632514e-05, 'epoch': 54.54}\n",
      "{'loss': 1.189, 'learning_rate': 4.5434298440979956e-05, 'epoch': 54.57}\n",
      " 55% 98000/179600 [8:01:24<6:24:26,  3.54it/s][INFO|trainer.py:1989] 2021-08-02 14:11:25,284 >> Saving model checkpoint to results/adapters/ag/checkpoint-98000\n",
      "[INFO|loading.py:59] 2021-08-02 14:11:25,284 >> Configuration saved in results/adapters/ag/checkpoint-98000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:11:25,295 >> Module weights saved in results/adapters/ag/checkpoint-98000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:11:25,295 >> Configuration saved in results/adapters/ag/checkpoint-98000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:11:25,594 >> Module weights saved in results/adapters/ag/checkpoint-98000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:11:25,594 >> Configuration saved in results/adapters/ag/checkpoint-98000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:11:25,930 >> Module weights saved in results/adapters/ag/checkpoint-98000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:11:25,930 >> tokenizer config file saved in results/adapters/ag/checkpoint-98000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:11:25,931 >> Special tokens file saved in results/adapters/ag/checkpoint-98000/special_tokens_map.json\n",
      "{'loss': 1.1753, 'learning_rate': 4.540645879732739e-05, 'epoch': 54.59}\n",
      "{'loss': 1.1935, 'learning_rate': 4.5378619153674834e-05, 'epoch': 54.62}\n",
      "{'loss': 1.2294, 'learning_rate': 4.535077951002227e-05, 'epoch': 54.65}\n",
      "{'loss': 1.1853, 'learning_rate': 4.532293986636971e-05, 'epoch': 54.68}\n",
      "{'loss': 1.1806, 'learning_rate': 4.529510022271715e-05, 'epoch': 54.7}\n",
      "{'loss': 1.1618, 'learning_rate': 4.526726057906459e-05, 'epoch': 54.73}\n",
      "{'loss': 1.1727, 'learning_rate': 4.5239420935412034e-05, 'epoch': 54.76}\n",
      "{'loss': 1.1833, 'learning_rate': 4.521158129175947e-05, 'epoch': 54.79}\n",
      "{'loss': 1.2015, 'learning_rate': 4.518374164810691e-05, 'epoch': 54.82}\n",
      "{'loss': 1.201, 'learning_rate': 4.515590200445434e-05, 'epoch': 54.84}\n",
      " 55% 98500/179600 [8:03:51<6:23:11,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 14:13:51,556 >> Saving model checkpoint to results/adapters/ag/checkpoint-98500\n",
      "[INFO|loading.py:59] 2021-08-02 14:13:51,556 >> Configuration saved in results/adapters/ag/checkpoint-98500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:13:51,570 >> Module weights saved in results/adapters/ag/checkpoint-98500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:13:51,571 >> Configuration saved in results/adapters/ag/checkpoint-98500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:13:51,862 >> Module weights saved in results/adapters/ag/checkpoint-98500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:13:51,863 >> Configuration saved in results/adapters/ag/checkpoint-98500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:13:52,191 >> Module weights saved in results/adapters/ag/checkpoint-98500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:13:52,191 >> tokenizer config file saved in results/adapters/ag/checkpoint-98500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:13:52,192 >> Special tokens file saved in results/adapters/ag/checkpoint-98500/special_tokens_map.json\n",
      "{'loss': 1.2071, 'learning_rate': 4.5128062360801783e-05, 'epoch': 54.87}\n",
      "{'loss': 1.1683, 'learning_rate': 4.510022271714922e-05, 'epoch': 54.9}\n",
      "{'loss': 1.1803, 'learning_rate': 4.507238307349666e-05, 'epoch': 54.93}\n",
      "{'loss': 1.1926, 'learning_rate': 4.50445434298441e-05, 'epoch': 54.95}\n",
      "{'loss': 1.1738, 'learning_rate': 4.501670378619154e-05, 'epoch': 54.98}\n",
      "{'loss': 1.2146, 'learning_rate': 4.4988864142538976e-05, 'epoch': 55.01}\n",
      "{'loss': 1.2183, 'learning_rate': 4.496102449888642e-05, 'epoch': 55.04}\n",
      "{'loss': 1.2266, 'learning_rate': 4.4933184855233854e-05, 'epoch': 55.07}\n",
      "{'loss': 1.1717, 'learning_rate': 4.49053452115813e-05, 'epoch': 55.09}\n",
      "{'loss': 1.1804, 'learning_rate': 4.487750556792873e-05, 'epoch': 55.12}\n",
      " 55% 99000/179600 [8:06:18<6:29:38,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 14:16:18,638 >> Saving model checkpoint to results/adapters/ag/checkpoint-99000\n",
      "[INFO|loading.py:59] 2021-08-02 14:16:18,639 >> Configuration saved in results/adapters/ag/checkpoint-99000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:16:18,652 >> Module weights saved in results/adapters/ag/checkpoint-99000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:16:18,652 >> Configuration saved in results/adapters/ag/checkpoint-99000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:16:18,957 >> Module weights saved in results/adapters/ag/checkpoint-99000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:16:18,958 >> Configuration saved in results/adapters/ag/checkpoint-99000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:16:19,282 >> Module weights saved in results/adapters/ag/checkpoint-99000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:16:19,283 >> tokenizer config file saved in results/adapters/ag/checkpoint-99000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:16:19,283 >> Special tokens file saved in results/adapters/ag/checkpoint-99000/special_tokens_map.json\n",
      "{'loss': 1.1995, 'learning_rate': 4.4849665924276175e-05, 'epoch': 55.15}\n",
      "{'loss': 1.1987, 'learning_rate': 4.482182628062361e-05, 'epoch': 55.18}\n",
      "{'loss': 1.1664, 'learning_rate': 4.4793986636971046e-05, 'epoch': 55.21}\n",
      "{'loss': 1.2138, 'learning_rate': 4.476614699331849e-05, 'epoch': 55.23}\n",
      "{'loss': 1.1893, 'learning_rate': 4.4738307349665925e-05, 'epoch': 55.26}\n",
      "{'loss': 1.1852, 'learning_rate': 4.471046770601337e-05, 'epoch': 55.29}\n",
      "{'loss': 1.1624, 'learning_rate': 4.46826280623608e-05, 'epoch': 55.32}\n",
      "{'loss': 1.1683, 'learning_rate': 4.4654788418708246e-05, 'epoch': 55.35}\n",
      "{'loss': 1.186, 'learning_rate': 4.462694877505568e-05, 'epoch': 55.37}\n",
      "{'loss': 1.2145, 'learning_rate': 4.4599109131403124e-05, 'epoch': 55.4}\n",
      " 55% 99500/179600 [8:08:44<6:11:01,  3.60it/s][INFO|trainer.py:1989] 2021-08-02 14:18:44,957 >> Saving model checkpoint to results/adapters/ag/checkpoint-99500\n",
      "[INFO|loading.py:59] 2021-08-02 14:18:44,957 >> Configuration saved in results/adapters/ag/checkpoint-99500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:18:44,968 >> Module weights saved in results/adapters/ag/checkpoint-99500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:18:44,968 >> Configuration saved in results/adapters/ag/checkpoint-99500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:18:45,263 >> Module weights saved in results/adapters/ag/checkpoint-99500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:18:45,263 >> Configuration saved in results/adapters/ag/checkpoint-99500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:18:45,597 >> Module weights saved in results/adapters/ag/checkpoint-99500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:18:45,598 >> tokenizer config file saved in results/adapters/ag/checkpoint-99500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:18:45,598 >> Special tokens file saved in results/adapters/ag/checkpoint-99500/special_tokens_map.json\n",
      "{'loss': 1.2065, 'learning_rate': 4.457126948775056e-05, 'epoch': 55.43}\n",
      "{'loss': 1.2196, 'learning_rate': 4.4543429844098e-05, 'epoch': 55.46}\n",
      "{'loss': 1.2112, 'learning_rate': 4.451559020044543e-05, 'epoch': 55.48}\n",
      "{'loss': 1.2369, 'learning_rate': 4.4487750556792874e-05, 'epoch': 55.51}\n",
      "{'loss': 1.1876, 'learning_rate': 4.445991091314031e-05, 'epoch': 55.54}\n",
      "{'loss': 1.2022, 'learning_rate': 4.443207126948775e-05, 'epoch': 55.57}\n",
      "{'loss': 1.232, 'learning_rate': 4.440423162583519e-05, 'epoch': 55.6}\n",
      "{'loss': 1.1531, 'learning_rate': 4.437639198218263e-05, 'epoch': 55.62}\n",
      "{'loss': 1.2097, 'learning_rate': 4.4348552338530066e-05, 'epoch': 55.65}\n",
      "{'loss': 1.2, 'learning_rate': 4.432071269487751e-05, 'epoch': 55.68}\n",
      " 56% 100000/179600 [8:11:10<6:34:40,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 14:21:10,729 >> Saving model checkpoint to results/adapters/ag/checkpoint-100000\n",
      "[INFO|loading.py:59] 2021-08-02 14:21:10,729 >> Configuration saved in results/adapters/ag/checkpoint-100000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:21:10,741 >> Module weights saved in results/adapters/ag/checkpoint-100000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:21:10,741 >> Configuration saved in results/adapters/ag/checkpoint-100000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:21:11,052 >> Module weights saved in results/adapters/ag/checkpoint-100000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:21:11,053 >> Configuration saved in results/adapters/ag/checkpoint-100000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:21:11,396 >> Module weights saved in results/adapters/ag/checkpoint-100000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:21:11,396 >> tokenizer config file saved in results/adapters/ag/checkpoint-100000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:21:11,397 >> Special tokens file saved in results/adapters/ag/checkpoint-100000/special_tokens_map.json\n",
      "{'loss': 1.2043, 'learning_rate': 4.429287305122495e-05, 'epoch': 55.71}\n",
      "{'loss': 1.1924, 'learning_rate': 4.426503340757239e-05, 'epoch': 55.73}\n",
      "{'loss': 1.2516, 'learning_rate': 4.423719376391983e-05, 'epoch': 55.76}\n",
      "{'loss': 1.1832, 'learning_rate': 4.420935412026726e-05, 'epoch': 55.79}\n",
      "{'loss': 1.1962, 'learning_rate': 4.41815144766147e-05, 'epoch': 55.82}\n",
      "{'loss': 1.2112, 'learning_rate': 4.415367483296214e-05, 'epoch': 55.85}\n",
      "{'loss': 1.1801, 'learning_rate': 4.412583518930958e-05, 'epoch': 55.87}\n",
      "{'loss': 1.1818, 'learning_rate': 4.4097995545657015e-05, 'epoch': 55.9}\n",
      "{'loss': 1.2116, 'learning_rate': 4.407015590200446e-05, 'epoch': 55.93}\n",
      "{'loss': 1.2205, 'learning_rate': 4.4042316258351894e-05, 'epoch': 55.96}\n",
      " 56% 100500/179600 [8:13:37<6:15:05,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 14:23:37,497 >> Saving model checkpoint to results/adapters/ag/checkpoint-100500\n",
      "[INFO|loading.py:59] 2021-08-02 14:23:37,497 >> Configuration saved in results/adapters/ag/checkpoint-100500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:23:37,509 >> Module weights saved in results/adapters/ag/checkpoint-100500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:23:37,509 >> Configuration saved in results/adapters/ag/checkpoint-100500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:23:37,798 >> Module weights saved in results/adapters/ag/checkpoint-100500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:23:37,799 >> Configuration saved in results/adapters/ag/checkpoint-100500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:23:38,123 >> Module weights saved in results/adapters/ag/checkpoint-100500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:23:38,123 >> tokenizer config file saved in results/adapters/ag/checkpoint-100500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:23:38,124 >> Special tokens file saved in results/adapters/ag/checkpoint-100500/special_tokens_map.json\n",
      "{'loss': 1.2144, 'learning_rate': 4.4014476614699336e-05, 'epoch': 55.99}\n",
      "{'loss': 1.2265, 'learning_rate': 4.398663697104677e-05, 'epoch': 56.01}\n",
      "{'loss': 1.1799, 'learning_rate': 4.3958797327394214e-05, 'epoch': 56.04}\n",
      "{'loss': 1.1737, 'learning_rate': 4.393095768374165e-05, 'epoch': 56.07}\n",
      "{'loss': 1.2077, 'learning_rate': 4.3903118040089086e-05, 'epoch': 56.1}\n",
      "{'loss': 1.2071, 'learning_rate': 4.387527839643653e-05, 'epoch': 56.12}\n",
      "{'loss': 1.2011, 'learning_rate': 4.3847438752783964e-05, 'epoch': 56.15}\n",
      "{'loss': 1.2013, 'learning_rate': 4.381959910913141e-05, 'epoch': 56.18}\n",
      "{'loss': 1.2039, 'learning_rate': 4.379175946547884e-05, 'epoch': 56.21}\n",
      "{'loss': 1.1764, 'learning_rate': 4.3763919821826285e-05, 'epoch': 56.24}\n",
      " 56% 101000/179600 [8:16:03<6:33:22,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 14:26:03,826 >> Saving model checkpoint to results/adapters/ag/checkpoint-101000\n",
      "[INFO|loading.py:59] 2021-08-02 14:26:03,827 >> Configuration saved in results/adapters/ag/checkpoint-101000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:26:03,838 >> Module weights saved in results/adapters/ag/checkpoint-101000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:26:03,839 >> Configuration saved in results/adapters/ag/checkpoint-101000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:26:04,151 >> Module weights saved in results/adapters/ag/checkpoint-101000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:26:04,151 >> Configuration saved in results/adapters/ag/checkpoint-101000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:26:04,495 >> Module weights saved in results/adapters/ag/checkpoint-101000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:26:04,496 >> tokenizer config file saved in results/adapters/ag/checkpoint-101000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:26:04,496 >> Special tokens file saved in results/adapters/ag/checkpoint-101000/special_tokens_map.json\n",
      "{'loss': 1.2261, 'learning_rate': 4.373608017817372e-05, 'epoch': 56.26}\n",
      "{'loss': 1.1901, 'learning_rate': 4.370824053452116e-05, 'epoch': 56.29}\n",
      "{'loss': 1.2266, 'learning_rate': 4.36804008908686e-05, 'epoch': 56.32}\n",
      "{'loss': 1.1978, 'learning_rate': 4.365256124721604e-05, 'epoch': 56.35}\n",
      "{'loss': 1.1824, 'learning_rate': 4.362472160356348e-05, 'epoch': 56.38}\n",
      "{'loss': 1.1818, 'learning_rate': 4.359688195991092e-05, 'epoch': 56.4}\n",
      "{'loss': 1.1736, 'learning_rate': 4.356904231625835e-05, 'epoch': 56.43}\n",
      "{'loss': 1.1633, 'learning_rate': 4.354120267260579e-05, 'epoch': 56.46}\n",
      "{'loss': 1.1777, 'learning_rate': 4.351336302895323e-05, 'epoch': 56.49}\n",
      "{'loss': 1.2014, 'learning_rate': 4.348552338530067e-05, 'epoch': 56.51}\n",
      " 57% 101500/179600 [8:18:30<6:12:43,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 14:28:31,377 >> Saving model checkpoint to results/adapters/ag/checkpoint-101500\n",
      "[INFO|loading.py:59] 2021-08-02 14:28:31,378 >> Configuration saved in results/adapters/ag/checkpoint-101500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:28:31,389 >> Module weights saved in results/adapters/ag/checkpoint-101500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:28:31,389 >> Configuration saved in results/adapters/ag/checkpoint-101500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:28:31,681 >> Module weights saved in results/adapters/ag/checkpoint-101500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:28:31,681 >> Configuration saved in results/adapters/ag/checkpoint-101500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:28:32,006 >> Module weights saved in results/adapters/ag/checkpoint-101500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:28:32,006 >> tokenizer config file saved in results/adapters/ag/checkpoint-101500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:28:32,007 >> Special tokens file saved in results/adapters/ag/checkpoint-101500/special_tokens_map.json\n",
      "{'loss': 1.1742, 'learning_rate': 4.3457683741648106e-05, 'epoch': 56.54}\n",
      "{'loss': 1.2255, 'learning_rate': 4.342984409799555e-05, 'epoch': 56.57}\n",
      "{'loss': 1.2013, 'learning_rate': 4.3402004454342984e-05, 'epoch': 56.6}\n",
      "{'loss': 1.2097, 'learning_rate': 4.3374164810690426e-05, 'epoch': 56.63}\n",
      "{'loss': 1.1967, 'learning_rate': 4.334632516703787e-05, 'epoch': 56.65}\n",
      "{'loss': 1.2265, 'learning_rate': 4.3318485523385305e-05, 'epoch': 56.68}\n",
      "{'loss': 1.2213, 'learning_rate': 4.329064587973275e-05, 'epoch': 56.71}\n",
      "{'loss': 1.2017, 'learning_rate': 4.3262806236080176e-05, 'epoch': 56.74}\n",
      "{'loss': 1.1879, 'learning_rate': 4.323496659242762e-05, 'epoch': 56.76}\n",
      "{'loss': 1.2269, 'learning_rate': 4.3207126948775055e-05, 'epoch': 56.79}\n",
      " 57% 102000/179600 [8:20:56<6:10:38,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 14:30:57,091 >> Saving model checkpoint to results/adapters/ag/checkpoint-102000\n",
      "[INFO|loading.py:59] 2021-08-02 14:30:57,092 >> Configuration saved in results/adapters/ag/checkpoint-102000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:30:57,105 >> Module weights saved in results/adapters/ag/checkpoint-102000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:30:57,106 >> Configuration saved in results/adapters/ag/checkpoint-102000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:30:57,401 >> Module weights saved in results/adapters/ag/checkpoint-102000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:30:57,402 >> Configuration saved in results/adapters/ag/checkpoint-102000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:30:57,747 >> Module weights saved in results/adapters/ag/checkpoint-102000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:30:57,747 >> tokenizer config file saved in results/adapters/ag/checkpoint-102000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:30:57,748 >> Special tokens file saved in results/adapters/ag/checkpoint-102000/special_tokens_map.json\n",
      "{'loss': 1.1915, 'learning_rate': 4.31792873051225e-05, 'epoch': 56.82}\n",
      "{'loss': 1.1987, 'learning_rate': 4.315144766146993e-05, 'epoch': 56.85}\n",
      "{'loss': 1.1825, 'learning_rate': 4.3123608017817375e-05, 'epoch': 56.88}\n",
      "{'loss': 1.2056, 'learning_rate': 4.309576837416481e-05, 'epoch': 56.9}\n",
      "{'loss': 1.1796, 'learning_rate': 4.3067928730512254e-05, 'epoch': 56.93}\n",
      "{'loss': 1.167, 'learning_rate': 4.304008908685969e-05, 'epoch': 56.96}\n",
      "{'loss': 1.1771, 'learning_rate': 4.301224944320713e-05, 'epoch': 56.99}\n",
      "{'loss': 1.236, 'learning_rate': 4.298440979955457e-05, 'epoch': 57.02}\n",
      "{'loss': 1.1831, 'learning_rate': 4.2956570155902004e-05, 'epoch': 57.04}\n",
      "{'loss': 1.1842, 'learning_rate': 4.2928730512249446e-05, 'epoch': 57.07}\n",
      " 57% 102500/179600 [8:23:23<6:26:10,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 14:33:23,631 >> Saving model checkpoint to results/adapters/ag/checkpoint-102500\n",
      "[INFO|loading.py:59] 2021-08-02 14:33:23,632 >> Configuration saved in results/adapters/ag/checkpoint-102500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:33:23,643 >> Module weights saved in results/adapters/ag/checkpoint-102500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:33:23,644 >> Configuration saved in results/adapters/ag/checkpoint-102500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:33:23,953 >> Module weights saved in results/adapters/ag/checkpoint-102500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:33:23,954 >> Configuration saved in results/adapters/ag/checkpoint-102500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:33:24,295 >> Module weights saved in results/adapters/ag/checkpoint-102500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:33:24,295 >> tokenizer config file saved in results/adapters/ag/checkpoint-102500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:33:24,296 >> Special tokens file saved in results/adapters/ag/checkpoint-102500/special_tokens_map.json\n",
      "{'loss': 1.1968, 'learning_rate': 4.290089086859688e-05, 'epoch': 57.1}\n",
      "{'loss': 1.194, 'learning_rate': 4.2873051224944324e-05, 'epoch': 57.13}\n",
      "{'loss': 1.205, 'learning_rate': 4.284521158129176e-05, 'epoch': 57.15}\n",
      "{'loss': 1.1855, 'learning_rate': 4.28173719376392e-05, 'epoch': 57.18}\n",
      "{'loss': 1.2151, 'learning_rate': 4.278953229398664e-05, 'epoch': 57.21}\n",
      "{'loss': 1.1491, 'learning_rate': 4.276169265033408e-05, 'epoch': 57.24}\n",
      "{'loss': 1.1842, 'learning_rate': 4.273385300668152e-05, 'epoch': 57.27}\n",
      "{'loss': 1.1881, 'learning_rate': 4.270601336302896e-05, 'epoch': 57.29}\n",
      "{'loss': 1.1725, 'learning_rate': 4.2678173719376395e-05, 'epoch': 57.32}\n",
      "{'loss': 1.179, 'learning_rate': 4.265033407572383e-05, 'epoch': 57.35}\n",
      " 57% 103000/179600 [8:25:48<6:12:52,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 14:35:49,432 >> Saving model checkpoint to results/adapters/ag/checkpoint-103000\n",
      "[INFO|loading.py:59] 2021-08-02 14:35:49,432 >> Configuration saved in results/adapters/ag/checkpoint-103000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:35:49,443 >> Module weights saved in results/adapters/ag/checkpoint-103000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:35:49,444 >> Configuration saved in results/adapters/ag/checkpoint-103000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:35:49,735 >> Module weights saved in results/adapters/ag/checkpoint-103000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:35:49,735 >> Configuration saved in results/adapters/ag/checkpoint-103000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:35:50,065 >> Module weights saved in results/adapters/ag/checkpoint-103000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:35:50,065 >> tokenizer config file saved in results/adapters/ag/checkpoint-103000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:35:50,065 >> Special tokens file saved in results/adapters/ag/checkpoint-103000/special_tokens_map.json\n",
      "{'loss': 1.23, 'learning_rate': 4.262249443207127e-05, 'epoch': 57.38}\n",
      "{'loss': 1.1978, 'learning_rate': 4.259465478841871e-05, 'epoch': 57.41}\n",
      "{'loss': 1.2151, 'learning_rate': 4.2566815144766145e-05, 'epoch': 57.43}\n",
      "{'loss': 1.1971, 'learning_rate': 4.253897550111359e-05, 'epoch': 57.46}\n",
      "{'loss': 1.1464, 'learning_rate': 4.251113585746102e-05, 'epoch': 57.49}\n",
      "{'loss': 1.2002, 'learning_rate': 4.2483296213808466e-05, 'epoch': 57.52}\n",
      "{'loss': 1.2077, 'learning_rate': 4.24554565701559e-05, 'epoch': 57.54}\n",
      "{'loss': 1.1734, 'learning_rate': 4.2427616926503344e-05, 'epoch': 57.57}\n",
      "{'loss': 1.1721, 'learning_rate': 4.239977728285078e-05, 'epoch': 57.6}\n",
      "{'loss': 1.1662, 'learning_rate': 4.237193763919822e-05, 'epoch': 57.63}\n",
      " 58% 103500/179600 [8:28:15<6:06:38,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 14:38:15,567 >> Saving model checkpoint to results/adapters/ag/checkpoint-103500\n",
      "[INFO|loading.py:59] 2021-08-02 14:38:15,567 >> Configuration saved in results/adapters/ag/checkpoint-103500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:38:15,580 >> Module weights saved in results/adapters/ag/checkpoint-103500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:38:15,580 >> Configuration saved in results/adapters/ag/checkpoint-103500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:38:15,885 >> Module weights saved in results/adapters/ag/checkpoint-103500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:38:15,885 >> Configuration saved in results/adapters/ag/checkpoint-103500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:38:16,227 >> Module weights saved in results/adapters/ag/checkpoint-103500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:38:16,227 >> tokenizer config file saved in results/adapters/ag/checkpoint-103500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:38:16,228 >> Special tokens file saved in results/adapters/ag/checkpoint-103500/special_tokens_map.json\n",
      "{'loss': 1.1988, 'learning_rate': 4.2344097995545665e-05, 'epoch': 57.66}\n",
      "{'loss': 1.2205, 'learning_rate': 4.2316258351893094e-05, 'epoch': 57.68}\n",
      "{'loss': 1.1812, 'learning_rate': 4.2288418708240537e-05, 'epoch': 57.71}\n",
      "{'loss': 1.2154, 'learning_rate': 4.226057906458797e-05, 'epoch': 57.74}\n",
      "{'loss': 1.1855, 'learning_rate': 4.2232739420935415e-05, 'epoch': 57.77}\n",
      "{'loss': 1.174, 'learning_rate': 4.220489977728285e-05, 'epoch': 57.79}\n",
      "{'loss': 1.2064, 'learning_rate': 4.217706013363029e-05, 'epoch': 57.82}\n",
      "{'loss': 1.211, 'learning_rate': 4.214922048997773e-05, 'epoch': 57.85}\n",
      "{'loss': 1.1647, 'learning_rate': 4.212138084632517e-05, 'epoch': 57.88}\n",
      "{'loss': 1.2046, 'learning_rate': 4.209354120267261e-05, 'epoch': 57.91}\n",
      " 58% 104000/179600 [8:30:41<5:51:32,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 14:40:42,015 >> Saving model checkpoint to results/adapters/ag/checkpoint-104000\n",
      "[INFO|loading.py:59] 2021-08-02 14:40:42,015 >> Configuration saved in results/adapters/ag/checkpoint-104000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:40:42,026 >> Module weights saved in results/adapters/ag/checkpoint-104000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:40:42,027 >> Configuration saved in results/adapters/ag/checkpoint-104000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:40:42,322 >> Module weights saved in results/adapters/ag/checkpoint-104000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:40:42,323 >> Configuration saved in results/adapters/ag/checkpoint-104000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:40:42,652 >> Module weights saved in results/adapters/ag/checkpoint-104000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:40:42,653 >> tokenizer config file saved in results/adapters/ag/checkpoint-104000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:40:42,653 >> Special tokens file saved in results/adapters/ag/checkpoint-104000/special_tokens_map.json\n",
      "{'loss': 1.2277, 'learning_rate': 4.206570155902005e-05, 'epoch': 57.93}\n",
      "{'loss': 1.1942, 'learning_rate': 4.2037861915367486e-05, 'epoch': 57.96}\n",
      "{'loss': 1.2251, 'learning_rate': 4.201002227171492e-05, 'epoch': 57.99}\n",
      "{'loss': 1.1834, 'learning_rate': 4.1982182628062364e-05, 'epoch': 58.02}\n",
      "{'loss': 1.1831, 'learning_rate': 4.19543429844098e-05, 'epoch': 58.05}\n",
      "{'loss': 1.2081, 'learning_rate': 4.192650334075724e-05, 'epoch': 58.07}\n",
      "{'loss': 1.1668, 'learning_rate': 4.189866369710468e-05, 'epoch': 58.1}\n",
      "{'loss': 1.1919, 'learning_rate': 4.187082405345212e-05, 'epoch': 58.13}\n",
      "{'loss': 1.2069, 'learning_rate': 4.1842984409799556e-05, 'epoch': 58.16}\n",
      "{'loss': 1.1889, 'learning_rate': 4.1815144766147e-05, 'epoch': 58.18}\n",
      " 58% 104500/179600 [8:33:08<5:55:00,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 14:43:09,246 >> Saving model checkpoint to results/adapters/ag/checkpoint-104500\n",
      "[INFO|loading.py:59] 2021-08-02 14:43:09,246 >> Configuration saved in results/adapters/ag/checkpoint-104500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:43:09,258 >> Module weights saved in results/adapters/ag/checkpoint-104500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:43:09,259 >> Configuration saved in results/adapters/ag/checkpoint-104500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:43:09,558 >> Module weights saved in results/adapters/ag/checkpoint-104500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:43:09,558 >> Configuration saved in results/adapters/ag/checkpoint-104500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:43:09,896 >> Module weights saved in results/adapters/ag/checkpoint-104500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:43:09,897 >> tokenizer config file saved in results/adapters/ag/checkpoint-104500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:43:09,897 >> Special tokens file saved in results/adapters/ag/checkpoint-104500/special_tokens_map.json\n",
      "{'loss': 1.1577, 'learning_rate': 4.1787305122494435e-05, 'epoch': 58.21}\n",
      "{'loss': 1.1803, 'learning_rate': 4.175946547884188e-05, 'epoch': 58.24}\n",
      "{'loss': 1.1839, 'learning_rate': 4.173162583518931e-05, 'epoch': 58.27}\n",
      "{'loss': 1.2488, 'learning_rate': 4.170378619153675e-05, 'epoch': 58.3}\n",
      "{'loss': 1.1707, 'learning_rate': 4.1675946547884184e-05, 'epoch': 58.32}\n",
      "{'loss': 1.155, 'learning_rate': 4.164810690423163e-05, 'epoch': 58.35}\n",
      "{'loss': 1.2217, 'learning_rate': 4.162026726057906e-05, 'epoch': 58.38}\n",
      "{'loss': 1.2074, 'learning_rate': 4.1592427616926505e-05, 'epoch': 58.41}\n",
      "{'loss': 1.1982, 'learning_rate': 4.156458797327394e-05, 'epoch': 58.44}\n",
      "{'loss': 1.2101, 'learning_rate': 4.1536748329621384e-05, 'epoch': 58.46}\n",
      " 58% 105000/179600 [8:35:35<5:51:11,  3.54it/s][INFO|trainer.py:1989] 2021-08-02 14:45:36,254 >> Saving model checkpoint to results/adapters/ag/checkpoint-105000\n",
      "[INFO|loading.py:59] 2021-08-02 14:45:36,255 >> Configuration saved in results/adapters/ag/checkpoint-105000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:45:36,266 >> Module weights saved in results/adapters/ag/checkpoint-105000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:45:36,266 >> Configuration saved in results/adapters/ag/checkpoint-105000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:45:36,572 >> Module weights saved in results/adapters/ag/checkpoint-105000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:45:36,572 >> Configuration saved in results/adapters/ag/checkpoint-105000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:45:36,899 >> Module weights saved in results/adapters/ag/checkpoint-105000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:45:36,899 >> tokenizer config file saved in results/adapters/ag/checkpoint-105000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:45:36,899 >> Special tokens file saved in results/adapters/ag/checkpoint-105000/special_tokens_map.json\n",
      "{'loss': 1.2051, 'learning_rate': 4.150890868596882e-05, 'epoch': 58.49}\n",
      "{'loss': 1.2124, 'learning_rate': 4.148106904231626e-05, 'epoch': 58.52}\n",
      "{'loss': 1.2171, 'learning_rate': 4.14532293986637e-05, 'epoch': 58.55}\n",
      "{'loss': 1.1849, 'learning_rate': 4.142538975501114e-05, 'epoch': 58.57}\n",
      "{'loss': 1.2039, 'learning_rate': 4.1397550111358576e-05, 'epoch': 58.6}\n",
      "{'loss': 1.1971, 'learning_rate': 4.136971046770601e-05, 'epoch': 58.63}\n",
      "{'loss': 1.2047, 'learning_rate': 4.1341870824053454e-05, 'epoch': 58.66}\n",
      "{'loss': 1.2319, 'learning_rate': 4.131403118040089e-05, 'epoch': 58.69}\n",
      "{'loss': 1.2104, 'learning_rate': 4.128619153674833e-05, 'epoch': 58.71}\n",
      "{'loss': 1.186, 'learning_rate': 4.125835189309577e-05, 'epoch': 58.74}\n",
      " 59% 105500/179600 [8:38:01<5:56:33,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 14:48:02,020 >> Saving model checkpoint to results/adapters/ag/checkpoint-105500\n",
      "[INFO|loading.py:59] 2021-08-02 14:48:02,020 >> Configuration saved in results/adapters/ag/checkpoint-105500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:48:02,031 >> Module weights saved in results/adapters/ag/checkpoint-105500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:48:02,031 >> Configuration saved in results/adapters/ag/checkpoint-105500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:48:02,333 >> Module weights saved in results/adapters/ag/checkpoint-105500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:48:02,334 >> Configuration saved in results/adapters/ag/checkpoint-105500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:48:02,674 >> Module weights saved in results/adapters/ag/checkpoint-105500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:48:02,674 >> tokenizer config file saved in results/adapters/ag/checkpoint-105500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:48:02,675 >> Special tokens file saved in results/adapters/ag/checkpoint-105500/special_tokens_map.json\n",
      "{'loss': 1.1795, 'learning_rate': 4.123051224944321e-05, 'epoch': 58.77}\n",
      "{'loss': 1.1936, 'learning_rate': 4.120267260579065e-05, 'epoch': 58.8}\n",
      "{'loss': 1.1723, 'learning_rate': 4.117483296213809e-05, 'epoch': 58.82}\n",
      "{'loss': 1.2042, 'learning_rate': 4.1146993318485525e-05, 'epoch': 58.85}\n",
      "{'loss': 1.182, 'learning_rate': 4.111915367483297e-05, 'epoch': 58.88}\n",
      "{'loss': 1.1782, 'learning_rate': 4.10913140311804e-05, 'epoch': 58.91}\n",
      "{'loss': 1.2014, 'learning_rate': 4.106347438752784e-05, 'epoch': 58.94}\n",
      "{'loss': 1.1802, 'learning_rate': 4.103563474387528e-05, 'epoch': 58.96}\n",
      "{'loss': 1.2054, 'learning_rate': 4.100779510022272e-05, 'epoch': 58.99}\n",
      "{'loss': 1.202, 'learning_rate': 4.097995545657016e-05, 'epoch': 59.02}\n",
      " 59% 106000/179600 [8:40:28<5:42:01,  3.59it/s][INFO|trainer.py:1989] 2021-08-02 14:50:28,621 >> Saving model checkpoint to results/adapters/ag/checkpoint-106000\n",
      "[INFO|loading.py:59] 2021-08-02 14:50:28,622 >> Configuration saved in results/adapters/ag/checkpoint-106000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:50:28,633 >> Module weights saved in results/adapters/ag/checkpoint-106000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:50:28,633 >> Configuration saved in results/adapters/ag/checkpoint-106000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:50:28,937 >> Module weights saved in results/adapters/ag/checkpoint-106000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:50:28,937 >> Configuration saved in results/adapters/ag/checkpoint-106000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:50:29,267 >> Module weights saved in results/adapters/ag/checkpoint-106000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:50:29,267 >> tokenizer config file saved in results/adapters/ag/checkpoint-106000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:50:29,268 >> Special tokens file saved in results/adapters/ag/checkpoint-106000/special_tokens_map.json\n",
      "{'loss': 1.1852, 'learning_rate': 4.0952115812917596e-05, 'epoch': 59.05}\n",
      "{'loss': 1.1835, 'learning_rate': 4.092427616926504e-05, 'epoch': 59.08}\n",
      "{'loss': 1.1706, 'learning_rate': 4.0896436525612474e-05, 'epoch': 59.1}\n",
      "{'loss': 1.1481, 'learning_rate': 4.0868596881959917e-05, 'epoch': 59.13}\n",
      "{'loss': 1.1817, 'learning_rate': 4.084075723830735e-05, 'epoch': 59.16}\n",
      "{'loss': 1.1578, 'learning_rate': 4.0812917594654795e-05, 'epoch': 59.19}\n",
      "{'loss': 1.1795, 'learning_rate': 4.078507795100223e-05, 'epoch': 59.21}\n",
      "{'loss': 1.1932, 'learning_rate': 4.0757238307349666e-05, 'epoch': 59.24}\n",
      "{'loss': 1.228, 'learning_rate': 4.07293986636971e-05, 'epoch': 59.27}\n",
      "{'loss': 1.2069, 'learning_rate': 4.0701559020044545e-05, 'epoch': 59.3}\n",
      " 59% 106500/179600 [8:42:54<5:43:38,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 14:52:54,599 >> Saving model checkpoint to results/adapters/ag/checkpoint-106500\n",
      "[INFO|loading.py:59] 2021-08-02 14:52:54,599 >> Configuration saved in results/adapters/ag/checkpoint-106500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:52:54,610 >> Module weights saved in results/adapters/ag/checkpoint-106500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:52:54,611 >> Configuration saved in results/adapters/ag/checkpoint-106500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:52:54,904 >> Module weights saved in results/adapters/ag/checkpoint-106500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:52:54,904 >> Configuration saved in results/adapters/ag/checkpoint-106500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:52:55,237 >> Module weights saved in results/adapters/ag/checkpoint-106500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:52:55,238 >> tokenizer config file saved in results/adapters/ag/checkpoint-106500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:52:55,238 >> Special tokens file saved in results/adapters/ag/checkpoint-106500/special_tokens_map.json\n",
      "{'loss': 1.1737, 'learning_rate': 4.067371937639198e-05, 'epoch': 59.33}\n",
      "{'loss': 1.1877, 'learning_rate': 4.064587973273942e-05, 'epoch': 59.35}\n",
      "{'loss': 1.1765, 'learning_rate': 4.061804008908686e-05, 'epoch': 59.38}\n",
      "{'loss': 1.1924, 'learning_rate': 4.05902004454343e-05, 'epoch': 59.41}\n",
      "{'loss': 1.1731, 'learning_rate': 4.056236080178174e-05, 'epoch': 59.44}\n",
      "{'loss': 1.1948, 'learning_rate': 4.053452115812918e-05, 'epoch': 59.47}\n",
      "{'loss': 1.1963, 'learning_rate': 4.0506681514476615e-05, 'epoch': 59.49}\n",
      "{'loss': 1.1789, 'learning_rate': 4.047884187082406e-05, 'epoch': 59.52}\n",
      "{'loss': 1.2025, 'learning_rate': 4.0451002227171494e-05, 'epoch': 59.55}\n",
      "{'loss': 1.2095, 'learning_rate': 4.042316258351893e-05, 'epoch': 59.58}\n",
      " 60% 107000/179600 [8:45:20<5:49:22,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 14:55:21,324 >> Saving model checkpoint to results/adapters/ag/checkpoint-107000\n",
      "[INFO|loading.py:59] 2021-08-02 14:55:21,325 >> Configuration saved in results/adapters/ag/checkpoint-107000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:55:21,336 >> Module weights saved in results/adapters/ag/checkpoint-107000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:55:21,336 >> Configuration saved in results/adapters/ag/checkpoint-107000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:55:21,633 >> Module weights saved in results/adapters/ag/checkpoint-107000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:55:21,633 >> Configuration saved in results/adapters/ag/checkpoint-107000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:55:21,966 >> Module weights saved in results/adapters/ag/checkpoint-107000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:55:21,967 >> tokenizer config file saved in results/adapters/ag/checkpoint-107000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:55:21,967 >> Special tokens file saved in results/adapters/ag/checkpoint-107000/special_tokens_map.json\n",
      "{'loss': 1.1918, 'learning_rate': 4.039532293986637e-05, 'epoch': 59.6}\n",
      "{'loss': 1.2121, 'learning_rate': 4.036748329621381e-05, 'epoch': 59.63}\n",
      "{'loss': 1.2057, 'learning_rate': 4.033964365256125e-05, 'epoch': 59.66}\n",
      "{'loss': 1.209, 'learning_rate': 4.0311804008908686e-05, 'epoch': 59.69}\n",
      "{'loss': 1.1855, 'learning_rate': 4.028396436525613e-05, 'epoch': 59.72}\n",
      "{'loss': 1.1835, 'learning_rate': 4.0256124721603564e-05, 'epoch': 59.74}\n",
      "{'loss': 1.1976, 'learning_rate': 4.022828507795101e-05, 'epoch': 59.77}\n",
      "{'loss': 1.1789, 'learning_rate': 4.020044543429844e-05, 'epoch': 59.8}\n",
      "{'loss': 1.2079, 'learning_rate': 4.0172605790645885e-05, 'epoch': 59.83}\n",
      "{'loss': 1.2391, 'learning_rate': 4.014476614699332e-05, 'epoch': 59.85}\n",
      " 60% 107500/179600 [8:47:46<5:47:11,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 14:57:46,868 >> Saving model checkpoint to results/adapters/ag/checkpoint-107500\n",
      "[INFO|loading.py:59] 2021-08-02 14:57:46,868 >> Configuration saved in results/adapters/ag/checkpoint-107500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:57:46,879 >> Module weights saved in results/adapters/ag/checkpoint-107500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:57:46,880 >> Configuration saved in results/adapters/ag/checkpoint-107500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:57:47,175 >> Module weights saved in results/adapters/ag/checkpoint-107500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 14:57:47,175 >> Configuration saved in results/adapters/ag/checkpoint-107500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 14:57:47,498 >> Module weights saved in results/adapters/ag/checkpoint-107500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 14:57:47,499 >> tokenizer config file saved in results/adapters/ag/checkpoint-107500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 14:57:47,499 >> Special tokens file saved in results/adapters/ag/checkpoint-107500/special_tokens_map.json\n",
      "{'loss': 1.2038, 'learning_rate': 4.011692650334076e-05, 'epoch': 59.88}\n",
      "{'loss': 1.1786, 'learning_rate': 4.00890868596882e-05, 'epoch': 59.91}\n",
      "{'loss': 1.2292, 'learning_rate': 4.0061247216035635e-05, 'epoch': 59.94}\n",
      "{'loss': 1.2173, 'learning_rate': 4.003340757238308e-05, 'epoch': 59.97}\n",
      "{'loss': 1.1851, 'learning_rate': 4.0005567928730513e-05, 'epoch': 59.99}\n",
      "{'loss': 1.2257, 'learning_rate': 3.9977728285077956e-05, 'epoch': 60.02}\n",
      "{'loss': 1.1958, 'learning_rate': 3.994988864142539e-05, 'epoch': 60.05}\n",
      "{'loss': 1.1891, 'learning_rate': 3.9922048997772834e-05, 'epoch': 60.08}\n",
      "{'loss': 1.1988, 'learning_rate': 3.989420935412027e-05, 'epoch': 60.11}\n",
      "{'loss': 1.2168, 'learning_rate': 3.986636971046771e-05, 'epoch': 60.13}\n",
      " 60% 108000/179600 [8:50:14<5:47:36,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 15:00:14,555 >> Saving model checkpoint to results/adapters/ag/checkpoint-108000\n",
      "[INFO|loading.py:59] 2021-08-02 15:00:14,556 >> Configuration saved in results/adapters/ag/checkpoint-108000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:00:14,567 >> Module weights saved in results/adapters/ag/checkpoint-108000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:00:14,568 >> Configuration saved in results/adapters/ag/checkpoint-108000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:00:14,856 >> Module weights saved in results/adapters/ag/checkpoint-108000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:00:14,856 >> Configuration saved in results/adapters/ag/checkpoint-108000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:00:15,176 >> Module weights saved in results/adapters/ag/checkpoint-108000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:00:15,176 >> tokenizer config file saved in results/adapters/ag/checkpoint-108000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:00:15,177 >> Special tokens file saved in results/adapters/ag/checkpoint-108000/special_tokens_map.json\n",
      "{'loss': 1.2093, 'learning_rate': 3.983853006681514e-05, 'epoch': 60.16}\n",
      "{'loss': 1.1966, 'learning_rate': 3.9810690423162584e-05, 'epoch': 60.19}\n",
      "{'loss': 1.1759, 'learning_rate': 3.978285077951002e-05, 'epoch': 60.22}\n",
      "{'loss': 1.2281, 'learning_rate': 3.975501113585746e-05, 'epoch': 60.24}\n",
      "{'loss': 1.1715, 'learning_rate': 3.97271714922049e-05, 'epoch': 60.27}\n",
      "{'loss': 1.1992, 'learning_rate': 3.969933184855234e-05, 'epoch': 60.3}\n",
      "{'loss': 1.1622, 'learning_rate': 3.9671492204899776e-05, 'epoch': 60.33}\n",
      "{'loss': 1.2137, 'learning_rate': 3.964365256124722e-05, 'epoch': 60.36}\n",
      "{'loss': 1.2068, 'learning_rate': 3.9615812917594655e-05, 'epoch': 60.38}\n",
      "{'loss': 1.184, 'learning_rate': 3.95879732739421e-05, 'epoch': 60.41}\n",
      " 60% 108500/179600 [8:52:40<5:58:03,  3.31it/s][INFO|trainer.py:1989] 2021-08-02 15:02:41,312 >> Saving model checkpoint to results/adapters/ag/checkpoint-108500\n",
      "[INFO|loading.py:59] 2021-08-02 15:02:41,313 >> Configuration saved in results/adapters/ag/checkpoint-108500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:02:41,324 >> Module weights saved in results/adapters/ag/checkpoint-108500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:02:41,324 >> Configuration saved in results/adapters/ag/checkpoint-108500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:02:41,616 >> Module weights saved in results/adapters/ag/checkpoint-108500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:02:41,616 >> Configuration saved in results/adapters/ag/checkpoint-108500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:02:41,943 >> Module weights saved in results/adapters/ag/checkpoint-108500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:02:41,943 >> tokenizer config file saved in results/adapters/ag/checkpoint-108500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:02:41,944 >> Special tokens file saved in results/adapters/ag/checkpoint-108500/special_tokens_map.json\n",
      "{'loss': 1.1805, 'learning_rate': 3.956013363028953e-05, 'epoch': 60.44}\n",
      "{'loss': 1.1834, 'learning_rate': 3.9532293986636976e-05, 'epoch': 60.47}\n",
      "{'loss': 1.2204, 'learning_rate': 3.950445434298441e-05, 'epoch': 60.5}\n",
      "{'loss': 1.2054, 'learning_rate': 3.947661469933185e-05, 'epoch': 60.52}\n",
      "{'loss': 1.183, 'learning_rate': 3.944877505567929e-05, 'epoch': 60.55}\n",
      "{'loss': 1.1815, 'learning_rate': 3.9420935412026726e-05, 'epoch': 60.58}\n",
      "{'loss': 1.1821, 'learning_rate': 3.939309576837417e-05, 'epoch': 60.61}\n",
      "{'loss': 1.1774, 'learning_rate': 3.9365256124721604e-05, 'epoch': 60.63}\n",
      "{'loss': 1.1869, 'learning_rate': 3.9337416481069046e-05, 'epoch': 60.66}\n",
      "{'loss': 1.2065, 'learning_rate': 3.930957683741648e-05, 'epoch': 60.69}\n",
      " 61% 109000/179600 [8:55:07<5:45:44,  3.40it/s][INFO|trainer.py:1989] 2021-08-02 15:05:08,356 >> Saving model checkpoint to results/adapters/ag/checkpoint-109000\n",
      "[INFO|loading.py:59] 2021-08-02 15:05:08,357 >> Configuration saved in results/adapters/ag/checkpoint-109000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:05:08,368 >> Module weights saved in results/adapters/ag/checkpoint-109000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:05:08,368 >> Configuration saved in results/adapters/ag/checkpoint-109000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:05:08,670 >> Module weights saved in results/adapters/ag/checkpoint-109000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:05:08,671 >> Configuration saved in results/adapters/ag/checkpoint-109000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:05:09,002 >> Module weights saved in results/adapters/ag/checkpoint-109000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:05:09,003 >> tokenizer config file saved in results/adapters/ag/checkpoint-109000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:05:09,003 >> Special tokens file saved in results/adapters/ag/checkpoint-109000/special_tokens_map.json\n",
      "{'loss': 1.1951, 'learning_rate': 3.9281737193763925e-05, 'epoch': 60.72}\n",
      "{'loss': 1.195, 'learning_rate': 3.925389755011136e-05, 'epoch': 60.75}\n",
      "{'loss': 1.2025, 'learning_rate': 3.92260579064588e-05, 'epoch': 60.77}\n",
      "{'loss': 1.1395, 'learning_rate': 3.919821826280624e-05, 'epoch': 60.8}\n",
      "{'loss': 1.1657, 'learning_rate': 3.9170378619153675e-05, 'epoch': 60.83}\n",
      "{'loss': 1.1953, 'learning_rate': 3.914253897550112e-05, 'epoch': 60.86}\n",
      "{'loss': 1.1856, 'learning_rate': 3.911469933184855e-05, 'epoch': 60.88}\n",
      "{'loss': 1.1834, 'learning_rate': 3.9086859688195995e-05, 'epoch': 60.91}\n",
      "{'loss': 1.1984, 'learning_rate': 3.905902004454343e-05, 'epoch': 60.94}\n",
      "{'loss': 1.2254, 'learning_rate': 3.9031180400890874e-05, 'epoch': 60.97}\n",
      " 61% 109500/179600 [8:57:33<5:36:13,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 15:07:33,573 >> Saving model checkpoint to results/adapters/ag/checkpoint-109500\n",
      "[INFO|loading.py:59] 2021-08-02 15:07:33,574 >> Configuration saved in results/adapters/ag/checkpoint-109500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:07:33,585 >> Module weights saved in results/adapters/ag/checkpoint-109500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:07:33,586 >> Configuration saved in results/adapters/ag/checkpoint-109500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:07:33,883 >> Module weights saved in results/adapters/ag/checkpoint-109500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:07:33,883 >> Configuration saved in results/adapters/ag/checkpoint-109500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:07:34,205 >> Module weights saved in results/adapters/ag/checkpoint-109500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:07:34,205 >> tokenizer config file saved in results/adapters/ag/checkpoint-109500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:07:34,205 >> Special tokens file saved in results/adapters/ag/checkpoint-109500/special_tokens_map.json\n",
      "{'loss': 1.1812, 'learning_rate': 3.900334075723831e-05, 'epoch': 61.0}\n",
      "{'loss': 1.2064, 'learning_rate': 3.897550111358575e-05, 'epoch': 61.02}\n",
      "{'loss': 1.2086, 'learning_rate': 3.894766146993319e-05, 'epoch': 61.05}\n",
      "{'loss': 1.2276, 'learning_rate': 3.891982182628063e-05, 'epoch': 61.08}\n",
      "{'loss': 1.2007, 'learning_rate': 3.889198218262806e-05, 'epoch': 61.11}\n",
      "{'loss': 1.2039, 'learning_rate': 3.88641425389755e-05, 'epoch': 61.14}\n",
      "{'loss': 1.2068, 'learning_rate': 3.883630289532294e-05, 'epoch': 61.16}\n",
      "{'loss': 1.1659, 'learning_rate': 3.880846325167038e-05, 'epoch': 61.19}\n",
      "{'loss': 1.1934, 'learning_rate': 3.8780623608017816e-05, 'epoch': 61.22}\n",
      "{'loss': 1.197, 'learning_rate': 3.875278396436526e-05, 'epoch': 61.25}\n",
      " 61% 110000/179600 [8:59:59<5:26:17,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 15:09:59,504 >> Saving model checkpoint to results/adapters/ag/checkpoint-110000\n",
      "[INFO|loading.py:59] 2021-08-02 15:09:59,505 >> Configuration saved in results/adapters/ag/checkpoint-110000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:09:59,517 >> Module weights saved in results/adapters/ag/checkpoint-110000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:09:59,517 >> Configuration saved in results/adapters/ag/checkpoint-110000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:09:59,809 >> Module weights saved in results/adapters/ag/checkpoint-110000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:09:59,809 >> Configuration saved in results/adapters/ag/checkpoint-110000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:10:00,142 >> Module weights saved in results/adapters/ag/checkpoint-110000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:10:00,143 >> tokenizer config file saved in results/adapters/ag/checkpoint-110000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:10:00,143 >> Special tokens file saved in results/adapters/ag/checkpoint-110000/special_tokens_map.json\n",
      "{'loss': 1.1983, 'learning_rate': 3.8724944320712694e-05, 'epoch': 61.27}\n",
      "{'loss': 1.1809, 'learning_rate': 3.869710467706014e-05, 'epoch': 61.3}\n",
      "{'loss': 1.199, 'learning_rate': 3.866926503340757e-05, 'epoch': 61.33}\n",
      "{'loss': 1.203, 'learning_rate': 3.8641425389755015e-05, 'epoch': 61.36}\n",
      "{'loss': 1.1974, 'learning_rate': 3.861358574610245e-05, 'epoch': 61.39}\n",
      "{'loss': 1.1876, 'learning_rate': 3.8585746102449887e-05, 'epoch': 61.41}\n",
      "{'loss': 1.2003, 'learning_rate': 3.855790645879733e-05, 'epoch': 61.44}\n",
      "{'loss': 1.1411, 'learning_rate': 3.8530066815144765e-05, 'epoch': 61.47}\n",
      "{'loss': 1.1941, 'learning_rate': 3.850222717149221e-05, 'epoch': 61.5}\n",
      "{'loss': 1.1779, 'learning_rate': 3.847438752783964e-05, 'epoch': 61.53}\n",
      " 62% 110500/179600 [9:02:25<5:33:43,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 15:12:25,950 >> Saving model checkpoint to results/adapters/ag/checkpoint-110500\n",
      "[INFO|loading.py:59] 2021-08-02 15:12:25,950 >> Configuration saved in results/adapters/ag/checkpoint-110500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:12:25,962 >> Module weights saved in results/adapters/ag/checkpoint-110500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:12:25,962 >> Configuration saved in results/adapters/ag/checkpoint-110500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:12:26,269 >> Module weights saved in results/adapters/ag/checkpoint-110500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:12:26,269 >> Configuration saved in results/adapters/ag/checkpoint-110500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:12:26,604 >> Module weights saved in results/adapters/ag/checkpoint-110500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:12:26,605 >> tokenizer config file saved in results/adapters/ag/checkpoint-110500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:12:26,605 >> Special tokens file saved in results/adapters/ag/checkpoint-110500/special_tokens_map.json\n",
      "{'loss': 1.196, 'learning_rate': 3.8446547884187086e-05, 'epoch': 61.55}\n",
      "{'loss': 1.1723, 'learning_rate': 3.841870824053452e-05, 'epoch': 61.58}\n",
      "{'loss': 1.1845, 'learning_rate': 3.8390868596881964e-05, 'epoch': 61.61}\n",
      "{'loss': 1.2021, 'learning_rate': 3.83630289532294e-05, 'epoch': 61.64}\n",
      "{'loss': 1.2115, 'learning_rate': 3.833518930957684e-05, 'epoch': 61.66}\n",
      "{'loss': 1.208, 'learning_rate': 3.830734966592428e-05, 'epoch': 61.69}\n",
      "{'loss': 1.1745, 'learning_rate': 3.8279510022271714e-05, 'epoch': 61.72}\n",
      "{'loss': 1.1863, 'learning_rate': 3.8251670378619156e-05, 'epoch': 61.75}\n",
      "{'loss': 1.2008, 'learning_rate': 3.822383073496659e-05, 'epoch': 61.78}\n",
      "{'loss': 1.1841, 'learning_rate': 3.8195991091314035e-05, 'epoch': 61.8}\n",
      " 62% 111000/179600 [9:04:51<5:35:17,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 15:14:51,913 >> Saving model checkpoint to results/adapters/ag/checkpoint-111000\n",
      "[INFO|loading.py:59] 2021-08-02 15:14:51,913 >> Configuration saved in results/adapters/ag/checkpoint-111000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:14:51,926 >> Module weights saved in results/adapters/ag/checkpoint-111000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:14:51,926 >> Configuration saved in results/adapters/ag/checkpoint-111000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:14:52,218 >> Module weights saved in results/adapters/ag/checkpoint-111000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:14:52,218 >> Configuration saved in results/adapters/ag/checkpoint-111000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:14:52,549 >> Module weights saved in results/adapters/ag/checkpoint-111000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:14:52,549 >> tokenizer config file saved in results/adapters/ag/checkpoint-111000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:14:52,550 >> Special tokens file saved in results/adapters/ag/checkpoint-111000/special_tokens_map.json\n",
      "{'loss': 1.2089, 'learning_rate': 3.816815144766147e-05, 'epoch': 61.83}\n",
      "{'loss': 1.2092, 'learning_rate': 3.814031180400891e-05, 'epoch': 61.86}\n",
      "{'loss': 1.1629, 'learning_rate': 3.811247216035635e-05, 'epoch': 61.89}\n",
      "{'loss': 1.1626, 'learning_rate': 3.808463251670379e-05, 'epoch': 61.91}\n",
      "{'loss': 1.1863, 'learning_rate': 3.805679287305123e-05, 'epoch': 61.94}\n",
      "{'loss': 1.1829, 'learning_rate': 3.802895322939867e-05, 'epoch': 61.97}\n",
      "{'loss': 1.2076, 'learning_rate': 3.8001113585746105e-05, 'epoch': 62.0}\n",
      "{'loss': 1.2396, 'learning_rate': 3.797327394209355e-05, 'epoch': 62.03}\n",
      "{'loss': 1.1877, 'learning_rate': 3.794543429844098e-05, 'epoch': 62.05}\n",
      "{'loss': 1.1888, 'learning_rate': 3.791759465478842e-05, 'epoch': 62.08}\n",
      " 62% 111500/179600 [9:07:18<5:27:09,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 15:17:18,637 >> Saving model checkpoint to results/adapters/ag/checkpoint-111500\n",
      "[INFO|loading.py:59] 2021-08-02 15:17:18,638 >> Configuration saved in results/adapters/ag/checkpoint-111500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:17:18,649 >> Module weights saved in results/adapters/ag/checkpoint-111500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:17:18,650 >> Configuration saved in results/adapters/ag/checkpoint-111500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:17:18,944 >> Module weights saved in results/adapters/ag/checkpoint-111500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:17:18,944 >> Configuration saved in results/adapters/ag/checkpoint-111500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:17:19,276 >> Module weights saved in results/adapters/ag/checkpoint-111500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:17:19,276 >> tokenizer config file saved in results/adapters/ag/checkpoint-111500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:17:19,277 >> Special tokens file saved in results/adapters/ag/checkpoint-111500/special_tokens_map.json\n",
      "{'loss': 1.1965, 'learning_rate': 3.7889755011135855e-05, 'epoch': 62.11}\n",
      "{'loss': 1.1966, 'learning_rate': 3.78619153674833e-05, 'epoch': 62.14}\n",
      "{'loss': 1.16, 'learning_rate': 3.7834075723830734e-05, 'epoch': 62.17}\n",
      "{'loss': 1.181, 'learning_rate': 3.7806236080178176e-05, 'epoch': 62.19}\n",
      "{'loss': 1.1495, 'learning_rate': 3.777839643652561e-05, 'epoch': 62.22}\n",
      "{'loss': 1.2007, 'learning_rate': 3.7750556792873054e-05, 'epoch': 62.25}\n",
      "{'loss': 1.192, 'learning_rate': 3.772271714922049e-05, 'epoch': 62.28}\n",
      "{'loss': 1.1962, 'learning_rate': 3.769487750556793e-05, 'epoch': 62.3}\n",
      "{'loss': 1.1846, 'learning_rate': 3.766703786191537e-05, 'epoch': 62.33}\n",
      "{'loss': 1.1497, 'learning_rate': 3.7639198218262804e-05, 'epoch': 62.36}\n",
      " 62% 112000/179600 [9:09:44<5:28:50,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 15:19:44,738 >> Saving model checkpoint to results/adapters/ag/checkpoint-112000\n",
      "[INFO|loading.py:59] 2021-08-02 15:19:44,739 >> Configuration saved in results/adapters/ag/checkpoint-112000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:19:44,752 >> Module weights saved in results/adapters/ag/checkpoint-112000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:19:44,752 >> Configuration saved in results/adapters/ag/checkpoint-112000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:19:45,073 >> Module weights saved in results/adapters/ag/checkpoint-112000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:19:45,074 >> Configuration saved in results/adapters/ag/checkpoint-112000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:19:45,427 >> Module weights saved in results/adapters/ag/checkpoint-112000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:19:45,428 >> tokenizer config file saved in results/adapters/ag/checkpoint-112000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:19:45,428 >> Special tokens file saved in results/adapters/ag/checkpoint-112000/special_tokens_map.json\n",
      "{'loss': 1.1641, 'learning_rate': 3.761135857461025e-05, 'epoch': 62.39}\n",
      "{'loss': 1.2036, 'learning_rate': 3.758351893095768e-05, 'epoch': 62.42}\n",
      "{'loss': 1.1797, 'learning_rate': 3.7555679287305125e-05, 'epoch': 62.44}\n",
      "{'loss': 1.1969, 'learning_rate': 3.752783964365256e-05, 'epoch': 62.47}\n",
      "{'loss': 1.1921, 'learning_rate': 3.7500000000000003e-05, 'epoch': 62.5}\n",
      "{'loss': 1.1768, 'learning_rate': 3.747216035634744e-05, 'epoch': 62.53}\n",
      "{'loss': 1.2098, 'learning_rate': 3.744432071269488e-05, 'epoch': 62.56}\n",
      "{'loss': 1.1809, 'learning_rate': 3.741648106904232e-05, 'epoch': 62.58}\n",
      "{'loss': 1.1957, 'learning_rate': 3.738864142538976e-05, 'epoch': 62.61}\n",
      "{'loss': 1.1478, 'learning_rate': 3.7360801781737196e-05, 'epoch': 62.64}\n",
      " 63% 112500/179600 [9:12:11<5:16:40,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 15:22:11,638 >> Saving model checkpoint to results/adapters/ag/checkpoint-112500\n",
      "[INFO|loading.py:59] 2021-08-02 15:22:11,639 >> Configuration saved in results/adapters/ag/checkpoint-112500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:22:11,650 >> Module weights saved in results/adapters/ag/checkpoint-112500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:22:11,650 >> Configuration saved in results/adapters/ag/checkpoint-112500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:22:11,949 >> Module weights saved in results/adapters/ag/checkpoint-112500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:22:11,950 >> Configuration saved in results/adapters/ag/checkpoint-112500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:22:12,281 >> Module weights saved in results/adapters/ag/checkpoint-112500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:22:12,282 >> tokenizer config file saved in results/adapters/ag/checkpoint-112500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:22:12,282 >> Special tokens file saved in results/adapters/ag/checkpoint-112500/special_tokens_map.json\n",
      "{'loss': 1.2034, 'learning_rate': 3.733296213808463e-05, 'epoch': 62.67}\n",
      "{'loss': 1.2024, 'learning_rate': 3.7305122494432074e-05, 'epoch': 62.69}\n",
      "{'loss': 1.2191, 'learning_rate': 3.727728285077951e-05, 'epoch': 62.72}\n",
      "{'loss': 1.2024, 'learning_rate': 3.724944320712695e-05, 'epoch': 62.75}\n",
      "{'loss': 1.1964, 'learning_rate': 3.722160356347439e-05, 'epoch': 62.78}\n",
      "{'loss': 1.1967, 'learning_rate': 3.719376391982183e-05, 'epoch': 62.81}\n",
      "{'loss': 1.1622, 'learning_rate': 3.7165924276169267e-05, 'epoch': 62.83}\n",
      "{'loss': 1.2117, 'learning_rate': 3.713808463251671e-05, 'epoch': 62.86}\n",
      "{'loss': 1.1995, 'learning_rate': 3.7110244988864145e-05, 'epoch': 62.89}\n",
      "{'loss': 1.1988, 'learning_rate': 3.708240534521159e-05, 'epoch': 62.92}\n",
      " 63% 113000/179600 [9:14:37<5:25:31,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 15:24:37,817 >> Saving model checkpoint to results/adapters/ag/checkpoint-113000\n",
      "[INFO|loading.py:59] 2021-08-02 15:24:37,818 >> Configuration saved in results/adapters/ag/checkpoint-113000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:24:37,829 >> Module weights saved in results/adapters/ag/checkpoint-113000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:24:37,829 >> Configuration saved in results/adapters/ag/checkpoint-113000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:24:38,135 >> Module weights saved in results/adapters/ag/checkpoint-113000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:24:38,135 >> Configuration saved in results/adapters/ag/checkpoint-113000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:24:38,479 >> Module weights saved in results/adapters/ag/checkpoint-113000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:24:38,479 >> tokenizer config file saved in results/adapters/ag/checkpoint-113000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:24:38,480 >> Special tokens file saved in results/adapters/ag/checkpoint-113000/special_tokens_map.json\n",
      "{'loss': 1.1918, 'learning_rate': 3.705456570155902e-05, 'epoch': 62.94}\n",
      "{'loss': 1.2163, 'learning_rate': 3.702672605790646e-05, 'epoch': 62.97}\n",
      "{'loss': 1.2052, 'learning_rate': 3.6998886414253895e-05, 'epoch': 63.0}\n",
      "{'loss': 1.1905, 'learning_rate': 3.697104677060134e-05, 'epoch': 63.03}\n",
      "{'loss': 1.1924, 'learning_rate': 3.694320712694877e-05, 'epoch': 63.06}\n",
      "{'loss': 1.1586, 'learning_rate': 3.6915367483296216e-05, 'epoch': 63.08}\n",
      "{'loss': 1.233, 'learning_rate': 3.688752783964365e-05, 'epoch': 63.11}\n",
      "{'loss': 1.1637, 'learning_rate': 3.6859688195991094e-05, 'epoch': 63.14}\n",
      "{'loss': 1.1933, 'learning_rate': 3.683184855233853e-05, 'epoch': 63.17}\n",
      "{'loss': 1.2106, 'learning_rate': 3.680400890868597e-05, 'epoch': 63.2}\n",
      " 63% 113500/179600 [9:17:03<5:21:56,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 15:27:04,419 >> Saving model checkpoint to results/adapters/ag/checkpoint-113500\n",
      "[INFO|loading.py:59] 2021-08-02 15:27:04,420 >> Configuration saved in results/adapters/ag/checkpoint-113500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:27:04,433 >> Module weights saved in results/adapters/ag/checkpoint-113500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:27:04,434 >> Configuration saved in results/adapters/ag/checkpoint-113500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:27:04,744 >> Module weights saved in results/adapters/ag/checkpoint-113500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:27:04,745 >> Configuration saved in results/adapters/ag/checkpoint-113500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:27:05,086 >> Module weights saved in results/adapters/ag/checkpoint-113500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:27:05,086 >> tokenizer config file saved in results/adapters/ag/checkpoint-113500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:27:05,086 >> Special tokens file saved in results/adapters/ag/checkpoint-113500/special_tokens_map.json\n",
      "{'loss': 1.1739, 'learning_rate': 3.677616926503341e-05, 'epoch': 63.22}\n",
      "{'loss': 1.1752, 'learning_rate': 3.674832962138085e-05, 'epoch': 63.25}\n",
      "{'loss': 1.1823, 'learning_rate': 3.6720489977728286e-05, 'epoch': 63.28}\n",
      "{'loss': 1.2138, 'learning_rate': 3.669265033407572e-05, 'epoch': 63.31}\n",
      "{'loss': 1.159, 'learning_rate': 3.6664810690423165e-05, 'epoch': 63.34}\n",
      "{'loss': 1.1801, 'learning_rate': 3.66369710467706e-05, 'epoch': 63.36}\n",
      "{'loss': 1.1954, 'learning_rate': 3.660913140311804e-05, 'epoch': 63.39}\n",
      "{'loss': 1.1865, 'learning_rate': 3.658129175946548e-05, 'epoch': 63.42}\n",
      "{'loss': 1.1765, 'learning_rate': 3.655345211581292e-05, 'epoch': 63.45}\n",
      "{'loss': 1.1766, 'learning_rate': 3.652561247216036e-05, 'epoch': 63.47}\n",
      " 63% 114000/179600 [9:19:29<5:17:52,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 15:29:30,075 >> Saving model checkpoint to results/adapters/ag/checkpoint-114000\n",
      "[INFO|loading.py:59] 2021-08-02 15:29:30,076 >> Configuration saved in results/adapters/ag/checkpoint-114000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:29:30,089 >> Module weights saved in results/adapters/ag/checkpoint-114000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:29:30,090 >> Configuration saved in results/adapters/ag/checkpoint-114000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:29:30,392 >> Module weights saved in results/adapters/ag/checkpoint-114000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:29:30,392 >> Configuration saved in results/adapters/ag/checkpoint-114000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:29:30,728 >> Module weights saved in results/adapters/ag/checkpoint-114000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:29:30,729 >> tokenizer config file saved in results/adapters/ag/checkpoint-114000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:29:30,729 >> Special tokens file saved in results/adapters/ag/checkpoint-114000/special_tokens_map.json\n",
      "{'loss': 1.1812, 'learning_rate': 3.64977728285078e-05, 'epoch': 63.5}\n",
      "{'loss': 1.1659, 'learning_rate': 3.6469933184855235e-05, 'epoch': 63.53}\n",
      "{'loss': 1.1615, 'learning_rate': 3.644209354120268e-05, 'epoch': 63.56}\n",
      "{'loss': 1.2177, 'learning_rate': 3.6414253897550114e-05, 'epoch': 63.59}\n",
      "{'loss': 1.1876, 'learning_rate': 3.638641425389755e-05, 'epoch': 63.61}\n",
      "{'loss': 1.1403, 'learning_rate': 3.635857461024499e-05, 'epoch': 63.64}\n",
      "{'loss': 1.1871, 'learning_rate': 3.633073496659243e-05, 'epoch': 63.67}\n",
      "{'loss': 1.1645, 'learning_rate': 3.630289532293987e-05, 'epoch': 63.7}\n",
      "{'loss': 1.1765, 'learning_rate': 3.6275055679287306e-05, 'epoch': 63.72}\n",
      "{'loss': 1.162, 'learning_rate': 3.624721603563475e-05, 'epoch': 63.75}\n",
      " 64% 114500/179600 [9:21:56<5:12:41,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 15:31:56,815 >> Saving model checkpoint to results/adapters/ag/checkpoint-114500\n",
      "[INFO|loading.py:59] 2021-08-02 15:31:56,815 >> Configuration saved in results/adapters/ag/checkpoint-114500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:31:56,827 >> Module weights saved in results/adapters/ag/checkpoint-114500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:31:56,827 >> Configuration saved in results/adapters/ag/checkpoint-114500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:31:57,129 >> Module weights saved in results/adapters/ag/checkpoint-114500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:31:57,129 >> Configuration saved in results/adapters/ag/checkpoint-114500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:31:57,458 >> Module weights saved in results/adapters/ag/checkpoint-114500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:31:57,459 >> tokenizer config file saved in results/adapters/ag/checkpoint-114500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:31:57,459 >> Special tokens file saved in results/adapters/ag/checkpoint-114500/special_tokens_map.json\n",
      "{'loss': 1.1829, 'learning_rate': 3.6219376391982184e-05, 'epoch': 63.78}\n",
      "{'loss': 1.1509, 'learning_rate': 3.619153674832963e-05, 'epoch': 63.81}\n",
      "{'loss': 1.1928, 'learning_rate': 3.616369710467706e-05, 'epoch': 63.84}\n",
      "{'loss': 1.2108, 'learning_rate': 3.6135857461024505e-05, 'epoch': 63.86}\n",
      "{'loss': 1.2005, 'learning_rate': 3.610801781737194e-05, 'epoch': 63.89}\n",
      "{'loss': 1.1998, 'learning_rate': 3.608017817371938e-05, 'epoch': 63.92}\n",
      "{'loss': 1.2156, 'learning_rate': 3.605233853006681e-05, 'epoch': 63.95}\n",
      "{'loss': 1.1763, 'learning_rate': 3.6024498886414255e-05, 'epoch': 63.98}\n",
      "{'loss': 1.1946, 'learning_rate': 3.599665924276169e-05, 'epoch': 64.0}\n",
      "{'loss': 1.1631, 'learning_rate': 3.596881959910913e-05, 'epoch': 64.03}\n",
      " 64% 115000/179600 [9:24:22<5:14:14,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 15:34:22,856 >> Saving model checkpoint to results/adapters/ag/checkpoint-115000\n",
      "[INFO|loading.py:59] 2021-08-02 15:34:22,856 >> Configuration saved in results/adapters/ag/checkpoint-115000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:34:22,869 >> Module weights saved in results/adapters/ag/checkpoint-115000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:34:22,869 >> Configuration saved in results/adapters/ag/checkpoint-115000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:34:23,166 >> Module weights saved in results/adapters/ag/checkpoint-115000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:34:23,166 >> Configuration saved in results/adapters/ag/checkpoint-115000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:34:23,502 >> Module weights saved in results/adapters/ag/checkpoint-115000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:34:23,503 >> tokenizer config file saved in results/adapters/ag/checkpoint-115000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:34:23,503 >> Special tokens file saved in results/adapters/ag/checkpoint-115000/special_tokens_map.json\n",
      "{'loss': 1.2056, 'learning_rate': 3.594097995545657e-05, 'epoch': 64.06}\n",
      "{'loss': 1.1998, 'learning_rate': 3.591314031180401e-05, 'epoch': 64.09}\n",
      "{'loss': 1.1838, 'learning_rate': 3.588530066815145e-05, 'epoch': 64.11}\n",
      "{'loss': 1.1664, 'learning_rate': 3.585746102449889e-05, 'epoch': 64.14}\n",
      "{'loss': 1.1996, 'learning_rate': 3.5829621380846326e-05, 'epoch': 64.17}\n",
      "{'loss': 1.1649, 'learning_rate': 3.580178173719377e-05, 'epoch': 64.2}\n",
      "{'loss': 1.1947, 'learning_rate': 3.5773942093541204e-05, 'epoch': 64.23}\n",
      "{'loss': 1.1829, 'learning_rate': 3.574610244988864e-05, 'epoch': 64.25}\n",
      "{'loss': 1.1483, 'learning_rate': 3.571826280623608e-05, 'epoch': 64.28}\n",
      "{'loss': 1.1903, 'learning_rate': 3.569042316258352e-05, 'epoch': 64.31}\n",
      " 64% 115500/179600 [9:26:47<5:12:16,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 15:36:48,180 >> Saving model checkpoint to results/adapters/ag/checkpoint-115500\n",
      "[INFO|loading.py:59] 2021-08-02 15:36:48,181 >> Configuration saved in results/adapters/ag/checkpoint-115500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:36:48,192 >> Module weights saved in results/adapters/ag/checkpoint-115500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:36:48,192 >> Configuration saved in results/adapters/ag/checkpoint-115500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:36:48,487 >> Module weights saved in results/adapters/ag/checkpoint-115500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:36:48,487 >> Configuration saved in results/adapters/ag/checkpoint-115500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:36:48,827 >> Module weights saved in results/adapters/ag/checkpoint-115500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:36:48,828 >> tokenizer config file saved in results/adapters/ag/checkpoint-115500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:36:48,828 >> Special tokens file saved in results/adapters/ag/checkpoint-115500/special_tokens_map.json\n",
      "{'loss': 1.2033, 'learning_rate': 3.566258351893096e-05, 'epoch': 64.34}\n",
      "{'loss': 1.1713, 'learning_rate': 3.5634743875278396e-05, 'epoch': 64.37}\n",
      "{'loss': 1.1949, 'learning_rate': 3.560690423162584e-05, 'epoch': 64.39}\n",
      "{'loss': 1.1807, 'learning_rate': 3.5579064587973275e-05, 'epoch': 64.42}\n",
      "{'loss': 1.1555, 'learning_rate': 3.555122494432072e-05, 'epoch': 64.45}\n",
      "{'loss': 1.1917, 'learning_rate': 3.552338530066815e-05, 'epoch': 64.48}\n",
      "{'loss': 1.2316, 'learning_rate': 3.5495545657015596e-05, 'epoch': 64.5}\n",
      "{'loss': 1.1609, 'learning_rate': 3.5467706013363025e-05, 'epoch': 64.53}\n",
      "{'loss': 1.2215, 'learning_rate': 3.543986636971047e-05, 'epoch': 64.56}\n",
      "{'loss': 1.1791, 'learning_rate': 3.541202672605791e-05, 'epoch': 64.59}\n",
      " 65% 116000/179600 [9:29:15<5:00:16,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 15:39:16,118 >> Saving model checkpoint to results/adapters/ag/checkpoint-116000\n",
      "[INFO|loading.py:59] 2021-08-02 15:39:16,119 >> Configuration saved in results/adapters/ag/checkpoint-116000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:39:16,133 >> Module weights saved in results/adapters/ag/checkpoint-116000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:39:16,134 >> Configuration saved in results/adapters/ag/checkpoint-116000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:39:16,436 >> Module weights saved in results/adapters/ag/checkpoint-116000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:39:16,436 >> Configuration saved in results/adapters/ag/checkpoint-116000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:39:16,771 >> Module weights saved in results/adapters/ag/checkpoint-116000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:39:16,771 >> tokenizer config file saved in results/adapters/ag/checkpoint-116000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:39:16,772 >> Special tokens file saved in results/adapters/ag/checkpoint-116000/special_tokens_map.json\n",
      "{'loss': 1.1952, 'learning_rate': 3.5384187082405345e-05, 'epoch': 64.62}\n",
      "{'loss': 1.1721, 'learning_rate': 3.535634743875279e-05, 'epoch': 64.64}\n",
      "{'loss': 1.1872, 'learning_rate': 3.5328507795100224e-05, 'epoch': 64.67}\n",
      "{'loss': 1.2043, 'learning_rate': 3.5300668151447666e-05, 'epoch': 64.7}\n",
      "{'loss': 1.2107, 'learning_rate': 3.52728285077951e-05, 'epoch': 64.73}\n",
      "{'loss': 1.1797, 'learning_rate': 3.5244988864142545e-05, 'epoch': 64.75}\n",
      "{'loss': 1.1647, 'learning_rate': 3.521714922048998e-05, 'epoch': 64.78}\n",
      "{'loss': 1.1495, 'learning_rate': 3.518930957683742e-05, 'epoch': 64.81}\n",
      "{'loss': 1.1913, 'learning_rate': 3.516146993318486e-05, 'epoch': 64.84}\n",
      "{'loss': 1.1881, 'learning_rate': 3.5133630289532294e-05, 'epoch': 64.87}\n",
      " 65% 116500/179600 [9:31:38<4:51:34,  3.61it/s][INFO|trainer.py:1989] 2021-08-02 15:41:39,305 >> Saving model checkpoint to results/adapters/ag/checkpoint-116500\n",
      "[INFO|loading.py:59] 2021-08-02 15:41:39,306 >> Configuration saved in results/adapters/ag/checkpoint-116500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:41:39,317 >> Module weights saved in results/adapters/ag/checkpoint-116500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:41:39,318 >> Configuration saved in results/adapters/ag/checkpoint-116500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:41:39,620 >> Module weights saved in results/adapters/ag/checkpoint-116500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:41:39,620 >> Configuration saved in results/adapters/ag/checkpoint-116500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:41:39,948 >> Module weights saved in results/adapters/ag/checkpoint-116500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:41:39,948 >> tokenizer config file saved in results/adapters/ag/checkpoint-116500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:41:39,948 >> Special tokens file saved in results/adapters/ag/checkpoint-116500/special_tokens_map.json\n",
      "{'loss': 1.1594, 'learning_rate': 3.510579064587973e-05, 'epoch': 64.89}\n",
      "{'loss': 1.2039, 'learning_rate': 3.507795100222717e-05, 'epoch': 64.92}\n",
      "{'loss': 1.1599, 'learning_rate': 3.505011135857461e-05, 'epoch': 64.95}\n",
      "{'loss': 1.2165, 'learning_rate': 3.502227171492205e-05, 'epoch': 64.98}\n",
      "{'loss': 1.1822, 'learning_rate': 3.499443207126949e-05, 'epoch': 65.01}\n",
      "{'loss': 1.1936, 'learning_rate': 3.496659242761693e-05, 'epoch': 65.03}\n",
      "{'loss': 1.2024, 'learning_rate': 3.4938752783964365e-05, 'epoch': 65.06}\n",
      "{'loss': 1.2044, 'learning_rate': 3.491091314031181e-05, 'epoch': 65.09}\n",
      "{'loss': 1.2107, 'learning_rate': 3.4883073496659243e-05, 'epoch': 65.12}\n",
      "{'loss': 1.1897, 'learning_rate': 3.4855233853006686e-05, 'epoch': 65.14}\n",
      " 65% 117000/179600 [9:34:02<4:59:24,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 15:44:02,475 >> Saving model checkpoint to results/adapters/ag/checkpoint-117000\n",
      "[INFO|loading.py:59] 2021-08-02 15:44:02,476 >> Configuration saved in results/adapters/ag/checkpoint-117000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:44:02,488 >> Module weights saved in results/adapters/ag/checkpoint-117000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:44:02,488 >> Configuration saved in results/adapters/ag/checkpoint-117000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:44:02,788 >> Module weights saved in results/adapters/ag/checkpoint-117000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:44:02,789 >> Configuration saved in results/adapters/ag/checkpoint-117000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:44:03,120 >> Module weights saved in results/adapters/ag/checkpoint-117000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:44:03,121 >> tokenizer config file saved in results/adapters/ag/checkpoint-117000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:44:03,121 >> Special tokens file saved in results/adapters/ag/checkpoint-117000/special_tokens_map.json\n",
      "{'loss': 1.198, 'learning_rate': 3.482739420935412e-05, 'epoch': 65.17}\n",
      "{'loss': 1.1959, 'learning_rate': 3.479955456570156e-05, 'epoch': 65.2}\n",
      "{'loss': 1.1621, 'learning_rate': 3.4771714922049e-05, 'epoch': 65.23}\n",
      "{'loss': 1.1893, 'learning_rate': 3.4743875278396436e-05, 'epoch': 65.26}\n",
      "{'loss': 1.1662, 'learning_rate': 3.471603563474388e-05, 'epoch': 65.28}\n",
      "{'loss': 1.1858, 'learning_rate': 3.4688195991091314e-05, 'epoch': 65.31}\n",
      "{'loss': 1.1887, 'learning_rate': 3.466035634743876e-05, 'epoch': 65.34}\n",
      "{'loss': 1.171, 'learning_rate': 3.463251670378619e-05, 'epoch': 65.37}\n",
      "{'loss': 1.1775, 'learning_rate': 3.4604677060133635e-05, 'epoch': 65.4}\n",
      "{'loss': 1.2013, 'learning_rate': 3.457683741648107e-05, 'epoch': 65.42}\n",
      " 65% 117500/179600 [9:36:24<4:48:42,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 15:46:24,550 >> Saving model checkpoint to results/adapters/ag/checkpoint-117500\n",
      "[INFO|loading.py:59] 2021-08-02 15:46:24,551 >> Configuration saved in results/adapters/ag/checkpoint-117500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:46:24,562 >> Module weights saved in results/adapters/ag/checkpoint-117500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:46:24,562 >> Configuration saved in results/adapters/ag/checkpoint-117500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:46:24,860 >> Module weights saved in results/adapters/ag/checkpoint-117500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:46:24,861 >> Configuration saved in results/adapters/ag/checkpoint-117500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:46:25,192 >> Module weights saved in results/adapters/ag/checkpoint-117500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:46:25,193 >> tokenizer config file saved in results/adapters/ag/checkpoint-117500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:46:25,193 >> Special tokens file saved in results/adapters/ag/checkpoint-117500/special_tokens_map.json\n",
      "{'loss': 1.1955, 'learning_rate': 3.454899777282851e-05, 'epoch': 65.45}\n",
      "{'loss': 1.1788, 'learning_rate': 3.452115812917594e-05, 'epoch': 65.48}\n",
      "{'loss': 1.1951, 'learning_rate': 3.4493318485523385e-05, 'epoch': 65.51}\n",
      "{'loss': 1.1799, 'learning_rate': 3.446547884187083e-05, 'epoch': 65.53}\n",
      "{'loss': 1.1875, 'learning_rate': 3.443763919821826e-05, 'epoch': 65.56}\n",
      "{'loss': 1.1942, 'learning_rate': 3.4409799554565706e-05, 'epoch': 65.59}\n",
      "{'loss': 1.1996, 'learning_rate': 3.438195991091314e-05, 'epoch': 65.62}\n",
      "{'loss': 1.1998, 'learning_rate': 3.4354120267260584e-05, 'epoch': 65.65}\n",
      "{'loss': 1.1925, 'learning_rate': 3.432628062360802e-05, 'epoch': 65.67}\n",
      "{'loss': 1.193, 'learning_rate': 3.429844097995546e-05, 'epoch': 65.7}\n",
      " 66% 118000/179600 [9:38:47<4:55:50,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 15:48:47,519 >> Saving model checkpoint to results/adapters/ag/checkpoint-118000\n",
      "[INFO|loading.py:59] 2021-08-02 15:48:47,519 >> Configuration saved in results/adapters/ag/checkpoint-118000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:48:47,532 >> Module weights saved in results/adapters/ag/checkpoint-118000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:48:47,532 >> Configuration saved in results/adapters/ag/checkpoint-118000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:48:47,837 >> Module weights saved in results/adapters/ag/checkpoint-118000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:48:47,838 >> Configuration saved in results/adapters/ag/checkpoint-118000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:48:48,178 >> Module weights saved in results/adapters/ag/checkpoint-118000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:48:48,179 >> tokenizer config file saved in results/adapters/ag/checkpoint-118000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:48:48,179 >> Special tokens file saved in results/adapters/ag/checkpoint-118000/special_tokens_map.json\n",
      "{'loss': 1.1996, 'learning_rate': 3.42706013363029e-05, 'epoch': 65.73}\n",
      "{'loss': 1.1824, 'learning_rate': 3.424276169265034e-05, 'epoch': 65.76}\n",
      "{'loss': 1.1627, 'learning_rate': 3.421492204899777e-05, 'epoch': 65.78}\n",
      "{'loss': 1.1701, 'learning_rate': 3.418708240534521e-05, 'epoch': 65.81}\n",
      "{'loss': 1.1878, 'learning_rate': 3.415924276169265e-05, 'epoch': 65.84}\n",
      "{'loss': 1.1756, 'learning_rate': 3.413140311804009e-05, 'epoch': 65.87}\n",
      "{'loss': 1.1995, 'learning_rate': 3.4103563474387526e-05, 'epoch': 65.9}\n",
      "{'loss': 1.2022, 'learning_rate': 3.407572383073497e-05, 'epoch': 65.92}\n",
      "{'loss': 1.2104, 'learning_rate': 3.4047884187082405e-05, 'epoch': 65.95}\n",
      "{'loss': 1.1914, 'learning_rate': 3.402004454342985e-05, 'epoch': 65.98}\n",
      " 66% 118500/179600 [9:41:11<4:54:33,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 15:51:11,750 >> Saving model checkpoint to results/adapters/ag/checkpoint-118500\n",
      "[INFO|loading.py:59] 2021-08-02 15:51:11,751 >> Configuration saved in results/adapters/ag/checkpoint-118500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:51:11,763 >> Module weights saved in results/adapters/ag/checkpoint-118500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:51:11,764 >> Configuration saved in results/adapters/ag/checkpoint-118500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:51:12,067 >> Module weights saved in results/adapters/ag/checkpoint-118500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:51:12,067 >> Configuration saved in results/adapters/ag/checkpoint-118500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:51:12,399 >> Module weights saved in results/adapters/ag/checkpoint-118500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:51:12,399 >> tokenizer config file saved in results/adapters/ag/checkpoint-118500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:51:12,399 >> Special tokens file saved in results/adapters/ag/checkpoint-118500/special_tokens_map.json\n",
      "{'loss': 1.1883, 'learning_rate': 3.399220489977728e-05, 'epoch': 66.01}\n",
      "{'loss': 1.2114, 'learning_rate': 3.3964365256124725e-05, 'epoch': 66.04}\n",
      "{'loss': 1.1758, 'learning_rate': 3.393652561247216e-05, 'epoch': 66.06}\n",
      "{'loss': 1.1982, 'learning_rate': 3.3908685968819604e-05, 'epoch': 66.09}\n",
      "{'loss': 1.184, 'learning_rate': 3.388084632516704e-05, 'epoch': 66.12}\n",
      "{'loss': 1.2074, 'learning_rate': 3.3853006681514475e-05, 'epoch': 66.15}\n",
      "{'loss': 1.1762, 'learning_rate': 3.382516703786192e-05, 'epoch': 66.17}\n",
      "{'loss': 1.1833, 'learning_rate': 3.3797327394209354e-05, 'epoch': 66.2}\n",
      "{'loss': 1.1488, 'learning_rate': 3.3769487750556796e-05, 'epoch': 66.23}\n",
      "{'loss': 1.1898, 'learning_rate': 3.374164810690423e-05, 'epoch': 66.26}\n",
      " 66% 119000/179600 [9:43:37<4:54:27,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 15:53:38,199 >> Saving model checkpoint to results/adapters/ag/checkpoint-119000\n",
      "[INFO|loading.py:59] 2021-08-02 15:53:38,200 >> Configuration saved in results/adapters/ag/checkpoint-119000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:53:38,212 >> Module weights saved in results/adapters/ag/checkpoint-119000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:53:38,212 >> Configuration saved in results/adapters/ag/checkpoint-119000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:53:38,526 >> Module weights saved in results/adapters/ag/checkpoint-119000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:53:38,527 >> Configuration saved in results/adapters/ag/checkpoint-119000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:53:38,864 >> Module weights saved in results/adapters/ag/checkpoint-119000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:53:38,865 >> tokenizer config file saved in results/adapters/ag/checkpoint-119000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:53:38,865 >> Special tokens file saved in results/adapters/ag/checkpoint-119000/special_tokens_map.json\n",
      "{'loss': 1.1936, 'learning_rate': 3.3713808463251674e-05, 'epoch': 66.29}\n",
      "{'loss': 1.2013, 'learning_rate': 3.368596881959911e-05, 'epoch': 66.31}\n",
      "{'loss': 1.1897, 'learning_rate': 3.365812917594655e-05, 'epoch': 66.34}\n",
      "{'loss': 1.1956, 'learning_rate': 3.363028953229399e-05, 'epoch': 66.37}\n",
      "{'loss': 1.1873, 'learning_rate': 3.360244988864143e-05, 'epoch': 66.4}\n",
      "{'loss': 1.2017, 'learning_rate': 3.357461024498886e-05, 'epoch': 66.43}\n",
      "{'loss': 1.1709, 'learning_rate': 3.35467706013363e-05, 'epoch': 66.45}\n",
      "{'loss': 1.2063, 'learning_rate': 3.3518930957683745e-05, 'epoch': 66.48}\n",
      "{'loss': 1.2376, 'learning_rate': 3.349109131403118e-05, 'epoch': 66.51}\n",
      "{'loss': 1.1841, 'learning_rate': 3.3463251670378623e-05, 'epoch': 66.54}\n",
      " 67% 119500/179600 [9:46:03<4:44:32,  3.52it/s][INFO|trainer.py:1989] 2021-08-02 15:56:03,957 >> Saving model checkpoint to results/adapters/ag/checkpoint-119500\n",
      "[INFO|loading.py:59] 2021-08-02 15:56:03,958 >> Configuration saved in results/adapters/ag/checkpoint-119500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:56:03,971 >> Module weights saved in results/adapters/ag/checkpoint-119500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:56:03,971 >> Configuration saved in results/adapters/ag/checkpoint-119500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:56:04,275 >> Module weights saved in results/adapters/ag/checkpoint-119500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:56:04,276 >> Configuration saved in results/adapters/ag/checkpoint-119500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:56:04,614 >> Module weights saved in results/adapters/ag/checkpoint-119500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:56:04,615 >> tokenizer config file saved in results/adapters/ag/checkpoint-119500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:56:04,615 >> Special tokens file saved in results/adapters/ag/checkpoint-119500/special_tokens_map.json\n",
      "{'loss': 1.1556, 'learning_rate': 3.343541202672606e-05, 'epoch': 66.56}\n",
      "{'loss': 1.1724, 'learning_rate': 3.34075723830735e-05, 'epoch': 66.59}\n",
      "{'loss': 1.2023, 'learning_rate': 3.337973273942094e-05, 'epoch': 66.62}\n",
      "{'loss': 1.1733, 'learning_rate': 3.335189309576838e-05, 'epoch': 66.65}\n",
      "{'loss': 1.17, 'learning_rate': 3.3324053452115816e-05, 'epoch': 66.68}\n",
      "{'loss': 1.1631, 'learning_rate': 3.329621380846326e-05, 'epoch': 66.7}\n",
      "{'loss': 1.1829, 'learning_rate': 3.326837416481069e-05, 'epoch': 66.73}\n",
      "{'loss': 1.1705, 'learning_rate': 3.324053452115813e-05, 'epoch': 66.76}\n",
      "{'loss': 1.1927, 'learning_rate': 3.3212694877505566e-05, 'epoch': 66.79}\n",
      "{'loss': 1.1734, 'learning_rate': 3.318485523385301e-05, 'epoch': 66.81}\n",
      " 67% 120000/179600 [9:48:29<4:46:16,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 15:58:30,080 >> Saving model checkpoint to results/adapters/ag/checkpoint-120000\n",
      "[INFO|loading.py:59] 2021-08-02 15:58:30,080 >> Configuration saved in results/adapters/ag/checkpoint-120000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:58:30,093 >> Module weights saved in results/adapters/ag/checkpoint-120000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:58:30,093 >> Configuration saved in results/adapters/ag/checkpoint-120000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:58:30,473 >> Module weights saved in results/adapters/ag/checkpoint-120000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:58:30,473 >> Configuration saved in results/adapters/ag/checkpoint-120000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:58:30,806 >> Module weights saved in results/adapters/ag/checkpoint-120000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:58:30,807 >> tokenizer config file saved in results/adapters/ag/checkpoint-120000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:58:30,807 >> Special tokens file saved in results/adapters/ag/checkpoint-120000/special_tokens_map.json\n",
      "{'loss': 1.1703, 'learning_rate': 3.3157015590200444e-05, 'epoch': 66.84}\n",
      "{'loss': 1.1702, 'learning_rate': 3.3129175946547886e-05, 'epoch': 66.87}\n",
      "{'loss': 1.2167, 'learning_rate': 3.310133630289532e-05, 'epoch': 66.9}\n",
      "{'loss': 1.201, 'learning_rate': 3.3073496659242765e-05, 'epoch': 66.93}\n",
      "{'loss': 1.2262, 'learning_rate': 3.30456570155902e-05, 'epoch': 66.95}\n",
      "{'loss': 1.1665, 'learning_rate': 3.301781737193764e-05, 'epoch': 66.98}\n",
      "{'loss': 1.2209, 'learning_rate': 3.298997772828508e-05, 'epoch': 67.01}\n",
      "{'loss': 1.187, 'learning_rate': 3.2962138084632515e-05, 'epoch': 67.04}\n",
      "{'loss': 1.1994, 'learning_rate': 3.293429844097996e-05, 'epoch': 67.07}\n",
      "{'loss': 1.1846, 'learning_rate': 3.290645879732739e-05, 'epoch': 67.09}\n",
      " 67% 120500/179600 [9:50:56<4:48:48,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 16:00:57,337 >> Saving model checkpoint to results/adapters/ag/checkpoint-120500\n",
      "[INFO|loading.py:59] 2021-08-02 16:00:57,338 >> Configuration saved in results/adapters/ag/checkpoint-120500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:00:57,351 >> Module weights saved in results/adapters/ag/checkpoint-120500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:00:57,352 >> Configuration saved in results/adapters/ag/checkpoint-120500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:00:57,657 >> Module weights saved in results/adapters/ag/checkpoint-120500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:00:57,657 >> Configuration saved in results/adapters/ag/checkpoint-120500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:00:57,998 >> Module weights saved in results/adapters/ag/checkpoint-120500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:00:57,998 >> tokenizer config file saved in results/adapters/ag/checkpoint-120500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:00:57,999 >> Special tokens file saved in results/adapters/ag/checkpoint-120500/special_tokens_map.json\n",
      "{'loss': 1.2111, 'learning_rate': 3.2878619153674835e-05, 'epoch': 67.12}\n",
      "{'loss': 1.1741, 'learning_rate': 3.285077951002227e-05, 'epoch': 67.15}\n",
      "{'loss': 1.1861, 'learning_rate': 3.2822939866369714e-05, 'epoch': 67.18}\n",
      "{'loss': 1.183, 'learning_rate': 3.279510022271715e-05, 'epoch': 67.2}\n",
      "{'loss': 1.1594, 'learning_rate': 3.276726057906459e-05, 'epoch': 67.23}\n",
      "{'loss': 1.1636, 'learning_rate': 3.273942093541203e-05, 'epoch': 67.26}\n",
      "{'loss': 1.224, 'learning_rate': 3.271158129175947e-05, 'epoch': 67.29}\n",
      "{'loss': 1.1659, 'learning_rate': 3.2683741648106906e-05, 'epoch': 67.32}\n",
      "{'loss': 1.1792, 'learning_rate': 3.265590200445434e-05, 'epoch': 67.34}\n",
      "{'loss': 1.1981, 'learning_rate': 3.262806236080178e-05, 'epoch': 67.37}\n",
      " 67% 121000/179600 [9:53:23<4:39:45,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 16:03:23,559 >> Saving model checkpoint to results/adapters/ag/checkpoint-121000\n",
      "[INFO|loading.py:59] 2021-08-02 16:03:23,560 >> Configuration saved in results/adapters/ag/checkpoint-121000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:03:23,571 >> Module weights saved in results/adapters/ag/checkpoint-121000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:03:23,571 >> Configuration saved in results/adapters/ag/checkpoint-121000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:03:23,872 >> Module weights saved in results/adapters/ag/checkpoint-121000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:03:23,873 >> Configuration saved in results/adapters/ag/checkpoint-121000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:03:24,204 >> Module weights saved in results/adapters/ag/checkpoint-121000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:03:24,204 >> tokenizer config file saved in results/adapters/ag/checkpoint-121000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:03:24,204 >> Special tokens file saved in results/adapters/ag/checkpoint-121000/special_tokens_map.json\n",
      "{'loss': 1.2194, 'learning_rate': 3.260022271714922e-05, 'epoch': 67.4}\n",
      "{'loss': 1.2095, 'learning_rate': 3.257238307349666e-05, 'epoch': 67.43}\n",
      "{'loss': 1.1886, 'learning_rate': 3.25445434298441e-05, 'epoch': 67.46}\n",
      "{'loss': 1.1941, 'learning_rate': 3.251670378619154e-05, 'epoch': 67.48}\n",
      "{'loss': 1.1638, 'learning_rate': 3.248886414253898e-05, 'epoch': 67.51}\n",
      "{'loss': 1.2116, 'learning_rate': 3.246102449888642e-05, 'epoch': 67.54}\n",
      "{'loss': 1.1624, 'learning_rate': 3.2433184855233855e-05, 'epoch': 67.57}\n",
      "{'loss': 1.1974, 'learning_rate': 3.24053452115813e-05, 'epoch': 67.59}\n",
      "{'loss': 1.2065, 'learning_rate': 3.2377505567928734e-05, 'epoch': 67.62}\n",
      "{'loss': 1.2023, 'learning_rate': 3.2349665924276176e-05, 'epoch': 67.65}\n",
      " 68% 121500/179600 [9:55:49<4:43:38,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 16:05:49,757 >> Saving model checkpoint to results/adapters/ag/checkpoint-121500\n",
      "[INFO|loading.py:59] 2021-08-02 16:05:49,758 >> Configuration saved in results/adapters/ag/checkpoint-121500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:05:49,769 >> Module weights saved in results/adapters/ag/checkpoint-121500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:05:49,769 >> Configuration saved in results/adapters/ag/checkpoint-121500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:05:50,076 >> Module weights saved in results/adapters/ag/checkpoint-121500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:05:50,077 >> Configuration saved in results/adapters/ag/checkpoint-121500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:05:50,425 >> Module weights saved in results/adapters/ag/checkpoint-121500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:05:50,426 >> tokenizer config file saved in results/adapters/ag/checkpoint-121500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:05:50,426 >> Special tokens file saved in results/adapters/ag/checkpoint-121500/special_tokens_map.json\n",
      "{'loss': 1.1918, 'learning_rate': 3.2321826280623605e-05, 'epoch': 67.68}\n",
      "{'loss': 1.1963, 'learning_rate': 3.229398663697105e-05, 'epoch': 67.71}\n",
      "{'loss': 1.2002, 'learning_rate': 3.226614699331848e-05, 'epoch': 67.73}\n",
      "{'loss': 1.1565, 'learning_rate': 3.2238307349665926e-05, 'epoch': 67.76}\n",
      "{'loss': 1.2138, 'learning_rate': 3.221046770601336e-05, 'epoch': 67.79}\n",
      "{'loss': 1.195, 'learning_rate': 3.2182628062360804e-05, 'epoch': 67.82}\n",
      "{'loss': 1.181, 'learning_rate': 3.215478841870824e-05, 'epoch': 67.84}\n",
      "{'loss': 1.1823, 'learning_rate': 3.212694877505568e-05, 'epoch': 67.87}\n",
      "{'loss': 1.1691, 'learning_rate': 3.209910913140312e-05, 'epoch': 67.9}\n",
      "{'loss': 1.172, 'learning_rate': 3.207126948775056e-05, 'epoch': 67.93}\n",
      " 68% 122000/179600 [9:58:15<4:40:01,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 16:08:16,287 >> Saving model checkpoint to results/adapters/ag/checkpoint-122000\n",
      "[INFO|loading.py:59] 2021-08-02 16:08:16,287 >> Configuration saved in results/adapters/ag/checkpoint-122000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:08:16,298 >> Module weights saved in results/adapters/ag/checkpoint-122000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:08:16,298 >> Configuration saved in results/adapters/ag/checkpoint-122000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:08:16,600 >> Module weights saved in results/adapters/ag/checkpoint-122000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:08:16,601 >> Configuration saved in results/adapters/ag/checkpoint-122000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:08:16,931 >> Module weights saved in results/adapters/ag/checkpoint-122000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:08:16,932 >> tokenizer config file saved in results/adapters/ag/checkpoint-122000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:08:16,932 >> Special tokens file saved in results/adapters/ag/checkpoint-122000/special_tokens_map.json\n",
      "{'loss': 1.2174, 'learning_rate': 3.2043429844097997e-05, 'epoch': 67.96}\n",
      "{'loss': 1.2204, 'learning_rate': 3.201559020044543e-05, 'epoch': 67.98}\n",
      "{'loss': 1.2444, 'learning_rate': 3.1987750556792875e-05, 'epoch': 68.01}\n",
      "{'loss': 1.1865, 'learning_rate': 3.195991091314031e-05, 'epoch': 68.04}\n",
      "{'loss': 1.2112, 'learning_rate': 3.193207126948775e-05, 'epoch': 68.07}\n",
      "{'loss': 1.1457, 'learning_rate': 3.190423162583519e-05, 'epoch': 68.1}\n",
      "{'loss': 1.2085, 'learning_rate': 3.187639198218263e-05, 'epoch': 68.12}\n",
      "{'loss': 1.1871, 'learning_rate': 3.184855233853007e-05, 'epoch': 68.15}\n",
      "{'loss': 1.1489, 'learning_rate': 3.182071269487751e-05, 'epoch': 68.18}\n",
      "{'loss': 1.1723, 'learning_rate': 3.1792873051224946e-05, 'epoch': 68.21}\n",
      " 68% 122500/179600 [10:00:42<4:32:40,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 16:10:43,179 >> Saving model checkpoint to results/adapters/ag/checkpoint-122500\n",
      "[INFO|loading.py:59] 2021-08-02 16:10:43,180 >> Configuration saved in results/adapters/ag/checkpoint-122500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:10:43,192 >> Module weights saved in results/adapters/ag/checkpoint-122500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:10:43,192 >> Configuration saved in results/adapters/ag/checkpoint-122500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:10:43,496 >> Module weights saved in results/adapters/ag/checkpoint-122500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:10:43,496 >> Configuration saved in results/adapters/ag/checkpoint-122500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:10:43,842 >> Module weights saved in results/adapters/ag/checkpoint-122500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:10:43,843 >> tokenizer config file saved in results/adapters/ag/checkpoint-122500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:10:43,843 >> Special tokens file saved in results/adapters/ag/checkpoint-122500/special_tokens_map.json\n",
      "{'loss': 1.1959, 'learning_rate': 3.176503340757239e-05, 'epoch': 68.23}\n",
      "{'loss': 1.1746, 'learning_rate': 3.1737193763919824e-05, 'epoch': 68.26}\n",
      "{'loss': 1.2143, 'learning_rate': 3.170935412026726e-05, 'epoch': 68.29}\n",
      "{'loss': 1.1981, 'learning_rate': 3.1681514476614695e-05, 'epoch': 68.32}\n",
      "{'loss': 1.2, 'learning_rate': 3.165367483296214e-05, 'epoch': 68.35}\n",
      "{'loss': 1.1863, 'learning_rate': 3.162583518930958e-05, 'epoch': 68.37}\n",
      "{'loss': 1.1976, 'learning_rate': 3.1597995545657016e-05, 'epoch': 68.4}\n",
      "{'loss': 1.2143, 'learning_rate': 3.157015590200446e-05, 'epoch': 68.43}\n",
      "{'loss': 1.1657, 'learning_rate': 3.1542316258351895e-05, 'epoch': 68.46}\n",
      "{'loss': 1.1974, 'learning_rate': 3.151447661469934e-05, 'epoch': 68.49}\n",
      " 68% 123000/179600 [10:03:08<4:38:26,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 16:13:09,236 >> Saving model checkpoint to results/adapters/ag/checkpoint-123000\n",
      "[INFO|loading.py:59] 2021-08-02 16:13:09,237 >> Configuration saved in results/adapters/ag/checkpoint-123000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:13:09,249 >> Module weights saved in results/adapters/ag/checkpoint-123000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:13:09,249 >> Configuration saved in results/adapters/ag/checkpoint-123000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:13:09,563 >> Module weights saved in results/adapters/ag/checkpoint-123000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:13:09,564 >> Configuration saved in results/adapters/ag/checkpoint-123000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:13:09,911 >> Module weights saved in results/adapters/ag/checkpoint-123000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:13:09,911 >> tokenizer config file saved in results/adapters/ag/checkpoint-123000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:13:09,912 >> Special tokens file saved in results/adapters/ag/checkpoint-123000/special_tokens_map.json\n",
      "{'loss': 1.1803, 'learning_rate': 3.148663697104677e-05, 'epoch': 68.51}\n",
      "{'loss': 1.2004, 'learning_rate': 3.1458797327394215e-05, 'epoch': 68.54}\n",
      "{'loss': 1.2223, 'learning_rate': 3.143095768374165e-05, 'epoch': 68.57}\n",
      "{'loss': 1.2023, 'learning_rate': 3.140311804008909e-05, 'epoch': 68.6}\n",
      "{'loss': 1.1592, 'learning_rate': 3.137527839643652e-05, 'epoch': 68.62}\n",
      "{'loss': 1.2073, 'learning_rate': 3.1347438752783965e-05, 'epoch': 68.65}\n",
      "{'loss': 1.1649, 'learning_rate': 3.13195991091314e-05, 'epoch': 68.68}\n",
      "{'loss': 1.1733, 'learning_rate': 3.1291759465478844e-05, 'epoch': 68.71}\n",
      "{'loss': 1.1579, 'learning_rate': 3.126391982182628e-05, 'epoch': 68.74}\n",
      "{'loss': 1.1795, 'learning_rate': 3.123608017817372e-05, 'epoch': 68.76}\n",
      " 69% 123500/179600 [10:05:34<4:32:10,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 16:15:35,126 >> Saving model checkpoint to results/adapters/ag/checkpoint-123500\n",
      "[INFO|loading.py:59] 2021-08-02 16:15:35,127 >> Configuration saved in results/adapters/ag/checkpoint-123500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:15:35,140 >> Module weights saved in results/adapters/ag/checkpoint-123500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:15:35,140 >> Configuration saved in results/adapters/ag/checkpoint-123500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:15:35,449 >> Module weights saved in results/adapters/ag/checkpoint-123500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:15:35,449 >> Configuration saved in results/adapters/ag/checkpoint-123500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:15:35,799 >> Module weights saved in results/adapters/ag/checkpoint-123500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:15:35,800 >> tokenizer config file saved in results/adapters/ag/checkpoint-123500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:15:35,800 >> Special tokens file saved in results/adapters/ag/checkpoint-123500/special_tokens_map.json\n",
      "{'loss': 1.1444, 'learning_rate': 3.120824053452116e-05, 'epoch': 68.79}\n",
      "{'loss': 1.1799, 'learning_rate': 3.11804008908686e-05, 'epoch': 68.82}\n",
      "{'loss': 1.1923, 'learning_rate': 3.1152561247216036e-05, 'epoch': 68.85}\n",
      "{'loss': 1.1683, 'learning_rate': 3.112472160356348e-05, 'epoch': 68.87}\n",
      "{'loss': 1.1848, 'learning_rate': 3.1096881959910914e-05, 'epoch': 68.9}\n",
      "{'loss': 1.1669, 'learning_rate': 3.106904231625835e-05, 'epoch': 68.93}\n",
      "{'loss': 1.2043, 'learning_rate': 3.104120267260579e-05, 'epoch': 68.96}\n",
      "{'loss': 1.208, 'learning_rate': 3.101336302895323e-05, 'epoch': 68.99}\n",
      "{'loss': 1.1678, 'learning_rate': 3.098552338530067e-05, 'epoch': 69.01}\n",
      "{'loss': 1.1641, 'learning_rate': 3.095768374164811e-05, 'epoch': 69.04}\n",
      " 69% 124000/179600 [10:08:01<4:23:54,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 16:18:01,464 >> Saving model checkpoint to results/adapters/ag/checkpoint-124000\n",
      "[INFO|loading.py:59] 2021-08-02 16:18:01,464 >> Configuration saved in results/adapters/ag/checkpoint-124000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:18:01,480 >> Module weights saved in results/adapters/ag/checkpoint-124000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:18:01,480 >> Configuration saved in results/adapters/ag/checkpoint-124000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:18:01,796 >> Module weights saved in results/adapters/ag/checkpoint-124000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:18:01,796 >> Configuration saved in results/adapters/ag/checkpoint-124000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:18:02,137 >> Module weights saved in results/adapters/ag/checkpoint-124000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:18:02,137 >> tokenizer config file saved in results/adapters/ag/checkpoint-124000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:18:02,137 >> Special tokens file saved in results/adapters/ag/checkpoint-124000/special_tokens_map.json\n",
      "{'loss': 1.1458, 'learning_rate': 3.092984409799555e-05, 'epoch': 69.07}\n",
      "{'loss': 1.197, 'learning_rate': 3.0902004454342985e-05, 'epoch': 69.1}\n",
      "{'loss': 1.1626, 'learning_rate': 3.087416481069043e-05, 'epoch': 69.13}\n",
      "{'loss': 1.1943, 'learning_rate': 3.084632516703786e-05, 'epoch': 69.15}\n",
      "{'loss': 1.1881, 'learning_rate': 3.0818485523385306e-05, 'epoch': 69.18}\n",
      "{'loss': 1.1813, 'learning_rate': 3.079064587973274e-05, 'epoch': 69.21}\n",
      "{'loss': 1.1604, 'learning_rate': 3.076280623608018e-05, 'epoch': 69.24}\n",
      "{'loss': 1.1896, 'learning_rate': 3.073496659242761e-05, 'epoch': 69.26}\n",
      "{'loss': 1.1587, 'learning_rate': 3.0707126948775056e-05, 'epoch': 69.29}\n",
      "{'loss': 1.1864, 'learning_rate': 3.06792873051225e-05, 'epoch': 69.32}\n",
      " 69% 124500/179600 [10:10:27<4:21:07,  3.52it/s][INFO|trainer.py:1989] 2021-08-02 16:20:27,618 >> Saving model checkpoint to results/adapters/ag/checkpoint-124500\n",
      "[INFO|loading.py:59] 2021-08-02 16:20:27,619 >> Configuration saved in results/adapters/ag/checkpoint-124500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:20:27,630 >> Module weights saved in results/adapters/ag/checkpoint-124500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:20:27,631 >> Configuration saved in results/adapters/ag/checkpoint-124500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:20:27,924 >> Module weights saved in results/adapters/ag/checkpoint-124500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:20:27,924 >> Configuration saved in results/adapters/ag/checkpoint-124500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:20:28,253 >> Module weights saved in results/adapters/ag/checkpoint-124500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:20:28,254 >> tokenizer config file saved in results/adapters/ag/checkpoint-124500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:20:28,254 >> Special tokens file saved in results/adapters/ag/checkpoint-124500/special_tokens_map.json\n",
      "{'loss': 1.1755, 'learning_rate': 3.0651447661469934e-05, 'epoch': 69.35}\n",
      "{'loss': 1.1693, 'learning_rate': 3.0623608017817377e-05, 'epoch': 69.38}\n",
      "{'loss': 1.1995, 'learning_rate': 3.059576837416481e-05, 'epoch': 69.4}\n",
      "{'loss': 1.1598, 'learning_rate': 3.0567928730512255e-05, 'epoch': 69.43}\n",
      "{'loss': 1.1715, 'learning_rate': 3.054008908685969e-05, 'epoch': 69.46}\n",
      "{'loss': 1.1655, 'learning_rate': 3.051224944320713e-05, 'epoch': 69.49}\n",
      "{'loss': 1.2396, 'learning_rate': 3.048440979955457e-05, 'epoch': 69.52}\n",
      "{'loss': 1.1834, 'learning_rate': 3.0456570155902005e-05, 'epoch': 69.54}\n",
      "{'loss': 1.1791, 'learning_rate': 3.0428730512249444e-05, 'epoch': 69.57}\n",
      "{'loss': 1.1773, 'learning_rate': 3.0400890868596883e-05, 'epoch': 69.6}\n",
      " 70% 125000/179600 [10:12:53<4:29:37,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 16:22:53,643 >> Saving model checkpoint to results/adapters/ag/checkpoint-125000\n",
      "[INFO|loading.py:59] 2021-08-02 16:22:53,644 >> Configuration saved in results/adapters/ag/checkpoint-125000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:22:53,657 >> Module weights saved in results/adapters/ag/checkpoint-125000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:22:53,657 >> Configuration saved in results/adapters/ag/checkpoint-125000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:22:53,976 >> Module weights saved in results/adapters/ag/checkpoint-125000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:22:53,977 >> Configuration saved in results/adapters/ag/checkpoint-125000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:22:54,326 >> Module weights saved in results/adapters/ag/checkpoint-125000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:22:54,327 >> tokenizer config file saved in results/adapters/ag/checkpoint-125000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:22:54,327 >> Special tokens file saved in results/adapters/ag/checkpoint-125000/special_tokens_map.json\n",
      "{'loss': 1.1791, 'learning_rate': 3.0373051224944322e-05, 'epoch': 69.63}\n",
      "{'loss': 1.2041, 'learning_rate': 3.034521158129176e-05, 'epoch': 69.65}\n",
      "{'loss': 1.2437, 'learning_rate': 3.03173719376392e-05, 'epoch': 69.68}\n",
      "{'loss': 1.1596, 'learning_rate': 3.028953229398664e-05, 'epoch': 69.71}\n",
      "{'loss': 1.1518, 'learning_rate': 3.026169265033408e-05, 'epoch': 69.74}\n",
      "{'loss': 1.2044, 'learning_rate': 3.0233853006681518e-05, 'epoch': 69.77}\n",
      "{'loss': 1.1931, 'learning_rate': 3.0206013363028957e-05, 'epoch': 69.79}\n",
      "{'loss': 1.2008, 'learning_rate': 3.0178173719376396e-05, 'epoch': 69.82}\n",
      "{'loss': 1.1662, 'learning_rate': 3.015033407572383e-05, 'epoch': 69.85}\n",
      "{'loss': 1.1823, 'learning_rate': 3.0122494432071268e-05, 'epoch': 69.88}\n",
      " 70% 125500/179600 [10:15:19<4:26:03,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 16:25:19,840 >> Saving model checkpoint to results/adapters/ag/checkpoint-125500\n",
      "[INFO|loading.py:59] 2021-08-02 16:25:19,841 >> Configuration saved in results/adapters/ag/checkpoint-125500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:25:19,854 >> Module weights saved in results/adapters/ag/checkpoint-125500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:25:19,854 >> Configuration saved in results/adapters/ag/checkpoint-125500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:25:20,169 >> Module weights saved in results/adapters/ag/checkpoint-125500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:25:20,170 >> Configuration saved in results/adapters/ag/checkpoint-125500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:25:20,498 >> Module weights saved in results/adapters/ag/checkpoint-125500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:25:20,499 >> tokenizer config file saved in results/adapters/ag/checkpoint-125500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:25:20,499 >> Special tokens file saved in results/adapters/ag/checkpoint-125500/special_tokens_map.json\n",
      "{'loss': 1.2134, 'learning_rate': 3.0094654788418707e-05, 'epoch': 69.9}\n",
      "{'loss': 1.1847, 'learning_rate': 3.0066815144766146e-05, 'epoch': 69.93}\n",
      "{'loss': 1.1715, 'learning_rate': 3.0038975501113585e-05, 'epoch': 69.96}\n",
      "{'loss': 1.1886, 'learning_rate': 3.0011135857461024e-05, 'epoch': 69.99}\n",
      "{'loss': 1.2053, 'learning_rate': 2.9983296213808464e-05, 'epoch': 70.02}\n",
      "{'loss': 1.1958, 'learning_rate': 2.9955456570155903e-05, 'epoch': 70.04}\n",
      "{'loss': 1.1663, 'learning_rate': 2.9927616926503342e-05, 'epoch': 70.07}\n",
      "{'loss': 1.1848, 'learning_rate': 2.989977728285078e-05, 'epoch': 70.1}\n",
      "{'loss': 1.1853, 'learning_rate': 2.9871937639198224e-05, 'epoch': 70.13}\n",
      "{'loss': 1.1785, 'learning_rate': 2.9844097995545663e-05, 'epoch': 70.16}\n",
      " 70% 126000/179600 [10:17:47<4:19:29,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 16:27:47,491 >> Saving model checkpoint to results/adapters/ag/checkpoint-126000\n",
      "[INFO|loading.py:59] 2021-08-02 16:27:47,492 >> Configuration saved in results/adapters/ag/checkpoint-126000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:27:47,503 >> Module weights saved in results/adapters/ag/checkpoint-126000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:27:47,504 >> Configuration saved in results/adapters/ag/checkpoint-126000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:27:47,811 >> Module weights saved in results/adapters/ag/checkpoint-126000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:27:47,812 >> Configuration saved in results/adapters/ag/checkpoint-126000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:27:48,158 >> Module weights saved in results/adapters/ag/checkpoint-126000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:27:48,158 >> tokenizer config file saved in results/adapters/ag/checkpoint-126000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:27:48,159 >> Special tokens file saved in results/adapters/ag/checkpoint-126000/special_tokens_map.json\n",
      "{'loss': 1.2101, 'learning_rate': 2.9816258351893095e-05, 'epoch': 70.18}\n",
      "{'loss': 1.1627, 'learning_rate': 2.9788418708240534e-05, 'epoch': 70.21}\n",
      "{'loss': 1.1861, 'learning_rate': 2.9760579064587973e-05, 'epoch': 70.24}\n",
      "{'loss': 1.1852, 'learning_rate': 2.9732739420935413e-05, 'epoch': 70.27}\n",
      "{'loss': 1.1907, 'learning_rate': 2.9704899777282852e-05, 'epoch': 70.29}\n",
      "{'loss': 1.1538, 'learning_rate': 2.967706013363029e-05, 'epoch': 70.32}\n",
      "{'loss': 1.1787, 'learning_rate': 2.964922048997773e-05, 'epoch': 70.35}\n",
      "{'loss': 1.1717, 'learning_rate': 2.962138084632517e-05, 'epoch': 70.38}\n",
      "{'loss': 1.1845, 'learning_rate': 2.959354120267261e-05, 'epoch': 70.41}\n",
      "{'loss': 1.2283, 'learning_rate': 2.9565701559020048e-05, 'epoch': 70.43}\n",
      " 70% 126500/179600 [10:20:12<4:16:47,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 16:30:12,630 >> Saving model checkpoint to results/adapters/ag/checkpoint-126500\n",
      "[INFO|loading.py:59] 2021-08-02 16:30:12,631 >> Configuration saved in results/adapters/ag/checkpoint-126500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:30:12,643 >> Module weights saved in results/adapters/ag/checkpoint-126500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:30:12,644 >> Configuration saved in results/adapters/ag/checkpoint-126500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:30:12,956 >> Module weights saved in results/adapters/ag/checkpoint-126500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:30:12,957 >> Configuration saved in results/adapters/ag/checkpoint-126500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:30:13,302 >> Module weights saved in results/adapters/ag/checkpoint-126500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:30:13,303 >> tokenizer config file saved in results/adapters/ag/checkpoint-126500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:30:13,303 >> Special tokens file saved in results/adapters/ag/checkpoint-126500/special_tokens_map.json\n",
      "{'loss': 1.1822, 'learning_rate': 2.9537861915367487e-05, 'epoch': 70.46}\n",
      "{'loss': 1.1749, 'learning_rate': 2.9510022271714922e-05, 'epoch': 70.49}\n",
      "{'loss': 1.1576, 'learning_rate': 2.948218262806236e-05, 'epoch': 70.52}\n",
      "{'loss': 1.1988, 'learning_rate': 2.94543429844098e-05, 'epoch': 70.55}\n",
      "{'loss': 1.1502, 'learning_rate': 2.942650334075724e-05, 'epoch': 70.57}\n",
      "{'loss': 1.2111, 'learning_rate': 2.939866369710468e-05, 'epoch': 70.6}\n",
      "{'loss': 1.2024, 'learning_rate': 2.9370824053452118e-05, 'epoch': 70.63}\n",
      "{'loss': 1.2062, 'learning_rate': 2.9342984409799557e-05, 'epoch': 70.66}\n",
      "{'loss': 1.1853, 'learning_rate': 2.9315144766146997e-05, 'epoch': 70.68}\n",
      "{'loss': 1.1648, 'learning_rate': 2.9287305122494436e-05, 'epoch': 70.71}\n",
      " 71% 127000/179600 [10:22:37<4:13:01,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 16:32:38,015 >> Saving model checkpoint to results/adapters/ag/checkpoint-127000\n",
      "[INFO|loading.py:59] 2021-08-02 16:32:38,016 >> Configuration saved in results/adapters/ag/checkpoint-127000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:32:38,028 >> Module weights saved in results/adapters/ag/checkpoint-127000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:32:38,028 >> Configuration saved in results/adapters/ag/checkpoint-127000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:32:38,345 >> Module weights saved in results/adapters/ag/checkpoint-127000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:32:38,345 >> Configuration saved in results/adapters/ag/checkpoint-127000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:32:38,693 >> Module weights saved in results/adapters/ag/checkpoint-127000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:32:38,693 >> tokenizer config file saved in results/adapters/ag/checkpoint-127000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:32:38,693 >> Special tokens file saved in results/adapters/ag/checkpoint-127000/special_tokens_map.json\n",
      "{'loss': 1.212, 'learning_rate': 2.9259465478841875e-05, 'epoch': 70.74}\n",
      "{'loss': 1.1797, 'learning_rate': 2.9231625835189314e-05, 'epoch': 70.77}\n",
      "{'loss': 1.1984, 'learning_rate': 2.9203786191536746e-05, 'epoch': 70.8}\n",
      "{'loss': 1.1656, 'learning_rate': 2.9175946547884186e-05, 'epoch': 70.82}\n",
      "{'loss': 1.1765, 'learning_rate': 2.9148106904231625e-05, 'epoch': 70.85}\n",
      "{'loss': 1.1931, 'learning_rate': 2.9120267260579064e-05, 'epoch': 70.88}\n",
      "{'loss': 1.2193, 'learning_rate': 2.9092427616926503e-05, 'epoch': 70.91}\n",
      "{'loss': 1.1797, 'learning_rate': 2.9064587973273942e-05, 'epoch': 70.93}\n",
      "{'loss': 1.1915, 'learning_rate': 2.903674832962138e-05, 'epoch': 70.96}\n",
      "{'loss': 1.1786, 'learning_rate': 2.900890868596882e-05, 'epoch': 70.99}\n",
      " 71% 127500/179600 [10:25:03<4:26:03,  3.26it/s][INFO|trainer.py:1989] 2021-08-02 16:35:04,118 >> Saving model checkpoint to results/adapters/ag/checkpoint-127500\n",
      "[INFO|loading.py:59] 2021-08-02 16:35:04,119 >> Configuration saved in results/adapters/ag/checkpoint-127500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:35:04,132 >> Module weights saved in results/adapters/ag/checkpoint-127500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:35:04,132 >> Configuration saved in results/adapters/ag/checkpoint-127500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:35:04,438 >> Module weights saved in results/adapters/ag/checkpoint-127500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:35:04,439 >> Configuration saved in results/adapters/ag/checkpoint-127500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:35:04,782 >> Module weights saved in results/adapters/ag/checkpoint-127500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:35:04,783 >> tokenizer config file saved in results/adapters/ag/checkpoint-127500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:35:04,783 >> Special tokens file saved in results/adapters/ag/checkpoint-127500/special_tokens_map.json\n",
      "{'loss': 1.2418, 'learning_rate': 2.898106904231626e-05, 'epoch': 71.02}\n",
      "{'loss': 1.1993, 'learning_rate': 2.89532293986637e-05, 'epoch': 71.05}\n",
      "{'loss': 1.2191, 'learning_rate': 2.892538975501114e-05, 'epoch': 71.07}\n",
      "{'loss': 1.1971, 'learning_rate': 2.8897550111358574e-05, 'epoch': 71.1}\n",
      "{'loss': 1.186, 'learning_rate': 2.8869710467706013e-05, 'epoch': 71.13}\n",
      "{'loss': 1.1628, 'learning_rate': 2.8841870824053452e-05, 'epoch': 71.16}\n",
      "{'loss': 1.2006, 'learning_rate': 2.881403118040089e-05, 'epoch': 71.19}\n",
      "{'loss': 1.1619, 'learning_rate': 2.878619153674833e-05, 'epoch': 71.21}\n",
      "{'loss': 1.1707, 'learning_rate': 2.875835189309577e-05, 'epoch': 71.24}\n",
      "{'loss': 1.2013, 'learning_rate': 2.873051224944321e-05, 'epoch': 71.27}\n",
      " 71% 128000/179600 [10:27:29<4:04:50,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 16:37:30,053 >> Saving model checkpoint to results/adapters/ag/checkpoint-128000\n",
      "[INFO|loading.py:59] 2021-08-02 16:37:30,054 >> Configuration saved in results/adapters/ag/checkpoint-128000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:37:30,064 >> Module weights saved in results/adapters/ag/checkpoint-128000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:37:30,065 >> Configuration saved in results/adapters/ag/checkpoint-128000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:37:30,358 >> Module weights saved in results/adapters/ag/checkpoint-128000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:37:30,359 >> Configuration saved in results/adapters/ag/checkpoint-128000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:37:30,685 >> Module weights saved in results/adapters/ag/checkpoint-128000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:37:30,685 >> tokenizer config file saved in results/adapters/ag/checkpoint-128000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:37:30,685 >> Special tokens file saved in results/adapters/ag/checkpoint-128000/special_tokens_map.json\n",
      "{'loss': 1.2134, 'learning_rate': 2.8702672605790648e-05, 'epoch': 71.3}\n",
      "{'loss': 1.1825, 'learning_rate': 2.8674832962138087e-05, 'epoch': 71.33}\n",
      "{'loss': 1.1956, 'learning_rate': 2.8646993318485526e-05, 'epoch': 71.35}\n",
      "{'loss': 1.178, 'learning_rate': 2.8619153674832965e-05, 'epoch': 71.38}\n",
      "{'loss': 1.1985, 'learning_rate': 2.85913140311804e-05, 'epoch': 71.41}\n",
      "{'loss': 1.182, 'learning_rate': 2.856347438752784e-05, 'epoch': 71.44}\n",
      "{'loss': 1.1878, 'learning_rate': 2.853563474387528e-05, 'epoch': 71.46}\n",
      "{'loss': 1.2096, 'learning_rate': 2.850779510022272e-05, 'epoch': 71.49}\n",
      "{'loss': 1.1795, 'learning_rate': 2.8479955456570158e-05, 'epoch': 71.52}\n",
      "{'loss': 1.1671, 'learning_rate': 2.8452115812917597e-05, 'epoch': 71.55}\n",
      " 72% 128500/179600 [10:29:55<4:05:32,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 16:39:55,818 >> Saving model checkpoint to results/adapters/ag/checkpoint-128500\n",
      "[INFO|loading.py:59] 2021-08-02 16:39:55,819 >> Configuration saved in results/adapters/ag/checkpoint-128500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:39:55,831 >> Module weights saved in results/adapters/ag/checkpoint-128500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:39:55,832 >> Configuration saved in results/adapters/ag/checkpoint-128500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:39:56,139 >> Module weights saved in results/adapters/ag/checkpoint-128500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:39:56,139 >> Configuration saved in results/adapters/ag/checkpoint-128500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:39:56,479 >> Module weights saved in results/adapters/ag/checkpoint-128500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:39:56,480 >> tokenizer config file saved in results/adapters/ag/checkpoint-128500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:39:56,480 >> Special tokens file saved in results/adapters/ag/checkpoint-128500/special_tokens_map.json\n",
      "{'loss': 1.1982, 'learning_rate': 2.8424276169265036e-05, 'epoch': 71.58}\n",
      "{'loss': 1.1728, 'learning_rate': 2.8396436525612475e-05, 'epoch': 71.6}\n",
      "{'loss': 1.1936, 'learning_rate': 2.8368596881959914e-05, 'epoch': 71.63}\n",
      "{'loss': 1.1845, 'learning_rate': 2.8340757238307353e-05, 'epoch': 71.66}\n",
      "{'loss': 1.1727, 'learning_rate': 2.8312917594654793e-05, 'epoch': 71.69}\n",
      "{'loss': 1.172, 'learning_rate': 2.8285077951002232e-05, 'epoch': 71.71}\n",
      "{'loss': 1.1778, 'learning_rate': 2.8257238307349664e-05, 'epoch': 71.74}\n",
      "{'loss': 1.2002, 'learning_rate': 2.8229398663697103e-05, 'epoch': 71.77}\n",
      "{'loss': 1.1946, 'learning_rate': 2.8201559020044542e-05, 'epoch': 71.8}\n",
      "{'loss': 1.1581, 'learning_rate': 2.817371937639198e-05, 'epoch': 71.83}\n",
      " 72% 129000/179600 [10:32:21<4:11:15,  3.36it/s][INFO|trainer.py:1989] 2021-08-02 16:42:21,973 >> Saving model checkpoint to results/adapters/ag/checkpoint-129000\n",
      "[INFO|loading.py:59] 2021-08-02 16:42:21,973 >> Configuration saved in results/adapters/ag/checkpoint-129000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:42:21,985 >> Module weights saved in results/adapters/ag/checkpoint-129000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:42:21,986 >> Configuration saved in results/adapters/ag/checkpoint-129000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:42:22,298 >> Module weights saved in results/adapters/ag/checkpoint-129000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:42:22,303 >> Configuration saved in results/adapters/ag/checkpoint-129000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:42:22,656 >> Module weights saved in results/adapters/ag/checkpoint-129000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:42:22,656 >> tokenizer config file saved in results/adapters/ag/checkpoint-129000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:42:22,657 >> Special tokens file saved in results/adapters/ag/checkpoint-129000/special_tokens_map.json\n",
      "{'loss': 1.1757, 'learning_rate': 2.814587973273942e-05, 'epoch': 71.85}\n",
      "{'loss': 1.1676, 'learning_rate': 2.811804008908686e-05, 'epoch': 71.88}\n",
      "{'loss': 1.1734, 'learning_rate': 2.80902004454343e-05, 'epoch': 71.91}\n",
      "{'loss': 1.172, 'learning_rate': 2.8062360801781738e-05, 'epoch': 71.94}\n",
      "{'loss': 1.1856, 'learning_rate': 2.8034521158129177e-05, 'epoch': 71.97}\n",
      "{'loss': 1.1754, 'learning_rate': 2.8006681514476616e-05, 'epoch': 71.99}\n",
      "{'loss': 1.2163, 'learning_rate': 2.797884187082406e-05, 'epoch': 72.02}\n",
      "{'loss': 1.1779, 'learning_rate': 2.795100222717149e-05, 'epoch': 72.05}\n",
      "{'loss': 1.205, 'learning_rate': 2.792316258351893e-05, 'epoch': 72.08}\n",
      "{'loss': 1.2098, 'learning_rate': 2.789532293986637e-05, 'epoch': 72.1}\n",
      " 72% 129500/179600 [10:34:47<4:10:50,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 16:44:48,148 >> Saving model checkpoint to results/adapters/ag/checkpoint-129500\n",
      "[INFO|loading.py:59] 2021-08-02 16:44:48,149 >> Configuration saved in results/adapters/ag/checkpoint-129500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:44:48,160 >> Module weights saved in results/adapters/ag/checkpoint-129500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:44:48,161 >> Configuration saved in results/adapters/ag/checkpoint-129500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:44:48,474 >> Module weights saved in results/adapters/ag/checkpoint-129500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:44:48,474 >> Configuration saved in results/adapters/ag/checkpoint-129500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:44:48,821 >> Module weights saved in results/adapters/ag/checkpoint-129500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:44:48,822 >> tokenizer config file saved in results/adapters/ag/checkpoint-129500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:44:48,822 >> Special tokens file saved in results/adapters/ag/checkpoint-129500/special_tokens_map.json\n",
      "{'loss': 1.1921, 'learning_rate': 2.786748329621381e-05, 'epoch': 72.13}\n",
      "{'loss': 1.1816, 'learning_rate': 2.7839643652561248e-05, 'epoch': 72.16}\n",
      "{'loss': 1.1659, 'learning_rate': 2.7811804008908687e-05, 'epoch': 72.19}\n",
      "{'loss': 1.1812, 'learning_rate': 2.7783964365256126e-05, 'epoch': 72.22}\n",
      "{'loss': 1.1831, 'learning_rate': 2.7756124721603565e-05, 'epoch': 72.24}\n",
      "{'loss': 1.1647, 'learning_rate': 2.7728285077951005e-05, 'epoch': 72.27}\n",
      "{'loss': 1.1918, 'learning_rate': 2.7700445434298444e-05, 'epoch': 72.3}\n",
      "{'loss': 1.1616, 'learning_rate': 2.7672605790645883e-05, 'epoch': 72.33}\n",
      "{'loss': 1.1832, 'learning_rate': 2.764476614699332e-05, 'epoch': 72.36}\n",
      "{'loss': 1.1624, 'learning_rate': 2.7616926503340758e-05, 'epoch': 72.38}\n",
      " 72% 130000/179600 [10:37:13<4:08:34,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 16:47:13,677 >> Saving model checkpoint to results/adapters/ag/checkpoint-130000\n",
      "[INFO|loading.py:59] 2021-08-02 16:47:13,677 >> Configuration saved in results/adapters/ag/checkpoint-130000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:47:13,689 >> Module weights saved in results/adapters/ag/checkpoint-130000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:47:13,690 >> Configuration saved in results/adapters/ag/checkpoint-130000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:47:14,029 >> Module weights saved in results/adapters/ag/checkpoint-130000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:47:14,030 >> Configuration saved in results/adapters/ag/checkpoint-130000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:47:14,381 >> Module weights saved in results/adapters/ag/checkpoint-130000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:47:14,382 >> tokenizer config file saved in results/adapters/ag/checkpoint-130000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:47:14,382 >> Special tokens file saved in results/adapters/ag/checkpoint-130000/special_tokens_map.json\n",
      "{'loss': 1.1917, 'learning_rate': 2.7589086859688197e-05, 'epoch': 72.41}\n",
      "{'loss': 1.152, 'learning_rate': 2.7561247216035636e-05, 'epoch': 72.44}\n",
      "{'loss': 1.1849, 'learning_rate': 2.7533407572383075e-05, 'epoch': 72.47}\n",
      "{'loss': 1.198, 'learning_rate': 2.7505567928730515e-05, 'epoch': 72.49}\n",
      "{'loss': 1.1842, 'learning_rate': 2.7477728285077954e-05, 'epoch': 72.52}\n",
      "{'loss': 1.2119, 'learning_rate': 2.7449888641425393e-05, 'epoch': 72.55}\n",
      "{'loss': 1.1733, 'learning_rate': 2.7422048997772832e-05, 'epoch': 72.58}\n",
      "{'loss': 1.1579, 'learning_rate': 2.739420935412027e-05, 'epoch': 72.61}\n",
      "{'loss': 1.2102, 'learning_rate': 2.736636971046771e-05, 'epoch': 72.63}\n",
      "{'loss': 1.2031, 'learning_rate': 2.7338530066815143e-05, 'epoch': 72.66}\n",
      " 73% 130500/179600 [10:39:38<3:56:38,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 16:49:38,942 >> Saving model checkpoint to results/adapters/ag/checkpoint-130500\n",
      "[INFO|loading.py:59] 2021-08-02 16:49:38,943 >> Configuration saved in results/adapters/ag/checkpoint-130500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:49:38,955 >> Module weights saved in results/adapters/ag/checkpoint-130500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:49:38,955 >> Configuration saved in results/adapters/ag/checkpoint-130500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:49:39,245 >> Module weights saved in results/adapters/ag/checkpoint-130500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:49:39,246 >> Configuration saved in results/adapters/ag/checkpoint-130500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:49:39,568 >> Module weights saved in results/adapters/ag/checkpoint-130500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:49:39,569 >> tokenizer config file saved in results/adapters/ag/checkpoint-130500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:49:39,569 >> Special tokens file saved in results/adapters/ag/checkpoint-130500/special_tokens_map.json\n",
      "{'loss': 1.1642, 'learning_rate': 2.7310690423162582e-05, 'epoch': 72.69}\n",
      "{'loss': 1.1821, 'learning_rate': 2.728285077951002e-05, 'epoch': 72.72}\n",
      "{'loss': 1.1901, 'learning_rate': 2.725501113585746e-05, 'epoch': 72.74}\n",
      "{'loss': 1.1822, 'learning_rate': 2.72271714922049e-05, 'epoch': 72.77}\n",
      "{'loss': 1.1945, 'learning_rate': 2.719933184855234e-05, 'epoch': 72.8}\n",
      "{'loss': 1.2181, 'learning_rate': 2.7171492204899778e-05, 'epoch': 72.83}\n",
      "{'loss': 1.2007, 'learning_rate': 2.7143652561247217e-05, 'epoch': 72.86}\n",
      "{'loss': 1.1512, 'learning_rate': 2.7115812917594656e-05, 'epoch': 72.88}\n",
      "{'loss': 1.1836, 'learning_rate': 2.7087973273942095e-05, 'epoch': 72.91}\n",
      "{'loss': 1.208, 'learning_rate': 2.7060133630289534e-05, 'epoch': 72.94}\n",
      " 73% 131000/179600 [10:42:01<3:46:28,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 16:52:02,102 >> Saving model checkpoint to results/adapters/ag/checkpoint-131000\n",
      "[INFO|loading.py:59] 2021-08-02 16:52:02,103 >> Configuration saved in results/adapters/ag/checkpoint-131000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:52:02,116 >> Module weights saved in results/adapters/ag/checkpoint-131000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:52:02,116 >> Configuration saved in results/adapters/ag/checkpoint-131000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:52:02,423 >> Module weights saved in results/adapters/ag/checkpoint-131000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:52:02,424 >> Configuration saved in results/adapters/ag/checkpoint-131000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:52:02,768 >> Module weights saved in results/adapters/ag/checkpoint-131000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:52:02,768 >> tokenizer config file saved in results/adapters/ag/checkpoint-131000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:52:02,768 >> Special tokens file saved in results/adapters/ag/checkpoint-131000/special_tokens_map.json\n",
      "{'loss': 1.1515, 'learning_rate': 2.7032293986636977e-05, 'epoch': 72.97}\n",
      "{'loss': 1.2012, 'learning_rate': 2.700445434298441e-05, 'epoch': 73.0}\n",
      "{'loss': 1.1461, 'learning_rate': 2.6976614699331848e-05, 'epoch': 73.02}\n",
      "{'loss': 1.1718, 'learning_rate': 2.6948775055679287e-05, 'epoch': 73.05}\n",
      "{'loss': 1.1818, 'learning_rate': 2.6920935412026727e-05, 'epoch': 73.08}\n",
      "{'loss': 1.1747, 'learning_rate': 2.6893095768374166e-05, 'epoch': 73.11}\n",
      "{'loss': 1.1616, 'learning_rate': 2.6865256124721605e-05, 'epoch': 73.13}\n",
      "{'loss': 1.1995, 'learning_rate': 2.6837416481069044e-05, 'epoch': 73.16}\n",
      "{'loss': 1.1811, 'learning_rate': 2.6809576837416483e-05, 'epoch': 73.19}\n",
      "{'loss': 1.1804, 'learning_rate': 2.6781737193763922e-05, 'epoch': 73.22}\n",
      " 73% 131500/179600 [10:44:24<3:43:38,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 16:54:25,063 >> Saving model checkpoint to results/adapters/ag/checkpoint-131500\n",
      "[INFO|loading.py:59] 2021-08-02 16:54:25,064 >> Configuration saved in results/adapters/ag/checkpoint-131500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:54:25,074 >> Module weights saved in results/adapters/ag/checkpoint-131500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:54:25,075 >> Configuration saved in results/adapters/ag/checkpoint-131500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:54:25,364 >> Module weights saved in results/adapters/ag/checkpoint-131500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:54:25,365 >> Configuration saved in results/adapters/ag/checkpoint-131500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:54:25,691 >> Module weights saved in results/adapters/ag/checkpoint-131500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:54:25,691 >> tokenizer config file saved in results/adapters/ag/checkpoint-131500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:54:25,691 >> Special tokens file saved in results/adapters/ag/checkpoint-131500/special_tokens_map.json\n",
      "{'loss': 1.1847, 'learning_rate': 2.675389755011136e-05, 'epoch': 73.25}\n",
      "{'loss': 1.1676, 'learning_rate': 2.67260579064588e-05, 'epoch': 73.27}\n",
      "{'loss': 1.1769, 'learning_rate': 2.6698218262806236e-05, 'epoch': 73.3}\n",
      "{'loss': 1.1627, 'learning_rate': 2.6670378619153676e-05, 'epoch': 73.33}\n",
      "{'loss': 1.1725, 'learning_rate': 2.6642538975501115e-05, 'epoch': 73.36}\n",
      "{'loss': 1.1711, 'learning_rate': 2.6614699331848554e-05, 'epoch': 73.39}\n",
      "{'loss': 1.1605, 'learning_rate': 2.6586859688195993e-05, 'epoch': 73.41}\n",
      "{'loss': 1.1833, 'learning_rate': 2.6559020044543432e-05, 'epoch': 73.44}\n",
      "{'loss': 1.1699, 'learning_rate': 2.653118040089087e-05, 'epoch': 73.47}\n",
      "{'loss': 1.2178, 'learning_rate': 2.650334075723831e-05, 'epoch': 73.5}\n",
      " 73% 132000/179600 [10:46:46<3:40:02,  3.61it/s][INFO|trainer.py:1989] 2021-08-02 16:56:46,591 >> Saving model checkpoint to results/adapters/ag/checkpoint-132000\n",
      "[INFO|loading.py:59] 2021-08-02 16:56:46,591 >> Configuration saved in results/adapters/ag/checkpoint-132000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:56:46,603 >> Module weights saved in results/adapters/ag/checkpoint-132000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:56:46,604 >> Configuration saved in results/adapters/ag/checkpoint-132000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:56:46,892 >> Module weights saved in results/adapters/ag/checkpoint-132000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:56:46,893 >> Configuration saved in results/adapters/ag/checkpoint-132000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:56:47,216 >> Module weights saved in results/adapters/ag/checkpoint-132000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:56:47,217 >> tokenizer config file saved in results/adapters/ag/checkpoint-132000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:56:47,217 >> Special tokens file saved in results/adapters/ag/checkpoint-132000/special_tokens_map.json\n",
      "{'loss': 1.1783, 'learning_rate': 2.647550111358575e-05, 'epoch': 73.52}\n",
      "{'loss': 1.1874, 'learning_rate': 2.644766146993319e-05, 'epoch': 73.55}\n",
      "{'loss': 1.2064, 'learning_rate': 2.6419821826280628e-05, 'epoch': 73.58}\n",
      "{'loss': 1.1749, 'learning_rate': 2.639198218262806e-05, 'epoch': 73.61}\n",
      "{'loss': 1.1838, 'learning_rate': 2.63641425389755e-05, 'epoch': 73.64}\n",
      "{'loss': 1.1709, 'learning_rate': 2.633630289532294e-05, 'epoch': 73.66}\n",
      "{'loss': 1.1718, 'learning_rate': 2.6308463251670378e-05, 'epoch': 73.69}\n",
      "{'loss': 1.2032, 'learning_rate': 2.6280623608017817e-05, 'epoch': 73.72}\n",
      "{'loss': 1.1756, 'learning_rate': 2.6252783964365256e-05, 'epoch': 73.75}\n",
      "{'loss': 1.2047, 'learning_rate': 2.6224944320712695e-05, 'epoch': 73.77}\n",
      " 74% 132500/179600 [10:49:10<3:43:20,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 16:59:11,261 >> Saving model checkpoint to results/adapters/ag/checkpoint-132500\n",
      "[INFO|loading.py:59] 2021-08-02 16:59:11,262 >> Configuration saved in results/adapters/ag/checkpoint-132500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:59:11,276 >> Module weights saved in results/adapters/ag/checkpoint-132500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:59:11,276 >> Configuration saved in results/adapters/ag/checkpoint-132500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:59:11,580 >> Module weights saved in results/adapters/ag/checkpoint-132500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 16:59:11,580 >> Configuration saved in results/adapters/ag/checkpoint-132500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 16:59:11,927 >> Module weights saved in results/adapters/ag/checkpoint-132500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 16:59:11,927 >> tokenizer config file saved in results/adapters/ag/checkpoint-132500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 16:59:11,928 >> Special tokens file saved in results/adapters/ag/checkpoint-132500/special_tokens_map.json\n",
      "{'loss': 1.1616, 'learning_rate': 2.6197104677060134e-05, 'epoch': 73.8}\n",
      "{'loss': 1.1648, 'learning_rate': 2.6169265033407574e-05, 'epoch': 73.83}\n",
      "{'loss': 1.1753, 'learning_rate': 2.6141425389755013e-05, 'epoch': 73.86}\n",
      "{'loss': 1.1879, 'learning_rate': 2.6113585746102452e-05, 'epoch': 73.89}\n",
      "{'loss': 1.2047, 'learning_rate': 2.6085746102449888e-05, 'epoch': 73.91}\n",
      "{'loss': 1.1639, 'learning_rate': 2.6057906458797327e-05, 'epoch': 73.94}\n",
      "{'loss': 1.1293, 'learning_rate': 2.6030066815144766e-05, 'epoch': 73.97}\n",
      "{'loss': 1.1777, 'learning_rate': 2.6002227171492205e-05, 'epoch': 74.0}\n",
      "{'loss': 1.2101, 'learning_rate': 2.5974387527839644e-05, 'epoch': 74.03}\n",
      "{'loss': 1.1816, 'learning_rate': 2.5946547884187083e-05, 'epoch': 74.05}\n",
      " 74% 133000/179600 [10:51:34<3:41:41,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 17:01:35,061 >> Saving model checkpoint to results/adapters/ag/checkpoint-133000\n",
      "[INFO|loading.py:59] 2021-08-02 17:01:35,061 >> Configuration saved in results/adapters/ag/checkpoint-133000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:01:35,072 >> Module weights saved in results/adapters/ag/checkpoint-133000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:01:35,073 >> Configuration saved in results/adapters/ag/checkpoint-133000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:01:35,371 >> Module weights saved in results/adapters/ag/checkpoint-133000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:01:35,371 >> Configuration saved in results/adapters/ag/checkpoint-133000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:01:35,701 >> Module weights saved in results/adapters/ag/checkpoint-133000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:01:35,701 >> tokenizer config file saved in results/adapters/ag/checkpoint-133000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:01:35,702 >> Special tokens file saved in results/adapters/ag/checkpoint-133000/special_tokens_map.json\n",
      "{'loss': 1.1664, 'learning_rate': 2.5918708240534523e-05, 'epoch': 74.08}\n",
      "{'loss': 1.1724, 'learning_rate': 2.5890868596881962e-05, 'epoch': 74.11}\n",
      "{'loss': 1.1935, 'learning_rate': 2.58630289532294e-05, 'epoch': 74.14}\n",
      "{'loss': 1.1606, 'learning_rate': 2.583518930957684e-05, 'epoch': 74.16}\n",
      "{'loss': 1.1728, 'learning_rate': 2.580734966592428e-05, 'epoch': 74.19}\n",
      "{'loss': 1.1757, 'learning_rate': 2.5779510022271715e-05, 'epoch': 74.22}\n",
      "{'loss': 1.1831, 'learning_rate': 2.5751670378619154e-05, 'epoch': 74.25}\n",
      "{'loss': 1.1907, 'learning_rate': 2.5723830734966593e-05, 'epoch': 74.28}\n",
      "{'loss': 1.1664, 'learning_rate': 2.5695991091314032e-05, 'epoch': 74.3}\n",
      "{'loss': 1.1922, 'learning_rate': 2.566815144766147e-05, 'epoch': 74.33}\n",
      " 74% 133500/179600 [10:53:57<3:32:02,  3.62it/s][INFO|trainer.py:1989] 2021-08-02 17:03:58,352 >> Saving model checkpoint to results/adapters/ag/checkpoint-133500\n",
      "[INFO|loading.py:59] 2021-08-02 17:03:58,352 >> Configuration saved in results/adapters/ag/checkpoint-133500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:03:58,363 >> Module weights saved in results/adapters/ag/checkpoint-133500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:03:58,363 >> Configuration saved in results/adapters/ag/checkpoint-133500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:03:58,658 >> Module weights saved in results/adapters/ag/checkpoint-133500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:03:58,658 >> Configuration saved in results/adapters/ag/checkpoint-133500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:03:58,987 >> Module weights saved in results/adapters/ag/checkpoint-133500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:03:58,987 >> tokenizer config file saved in results/adapters/ag/checkpoint-133500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:03:58,987 >> Special tokens file saved in results/adapters/ag/checkpoint-133500/special_tokens_map.json\n",
      "{'loss': 1.2187, 'learning_rate': 2.564031180400891e-05, 'epoch': 74.36}\n",
      "{'loss': 1.1977, 'learning_rate': 2.561247216035635e-05, 'epoch': 74.39}\n",
      "{'loss': 1.1725, 'learning_rate': 2.558463251670379e-05, 'epoch': 74.42}\n",
      "{'loss': 1.1729, 'learning_rate': 2.5556792873051228e-05, 'epoch': 74.44}\n",
      "{'loss': 1.1894, 'learning_rate': 2.5528953229398667e-05, 'epoch': 74.47}\n",
      "{'loss': 1.2029, 'learning_rate': 2.5501113585746107e-05, 'epoch': 74.5}\n",
      "{'loss': 1.1576, 'learning_rate': 2.5473273942093546e-05, 'epoch': 74.53}\n",
      "{'loss': 1.1476, 'learning_rate': 2.5445434298440978e-05, 'epoch': 74.55}\n",
      "{'loss': 1.1805, 'learning_rate': 2.5417594654788417e-05, 'epoch': 74.58}\n",
      "{'loss': 1.1909, 'learning_rate': 2.5389755011135856e-05, 'epoch': 74.61}\n",
      " 75% 134000/179600 [10:56:21<3:37:04,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 17:06:21,868 >> Saving model checkpoint to results/adapters/ag/checkpoint-134000\n",
      "[INFO|loading.py:59] 2021-08-02 17:06:21,868 >> Configuration saved in results/adapters/ag/checkpoint-134000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:06:21,881 >> Module weights saved in results/adapters/ag/checkpoint-134000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:06:21,882 >> Configuration saved in results/adapters/ag/checkpoint-134000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:06:22,174 >> Module weights saved in results/adapters/ag/checkpoint-134000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:06:22,174 >> Configuration saved in results/adapters/ag/checkpoint-134000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:06:22,499 >> Module weights saved in results/adapters/ag/checkpoint-134000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:06:22,500 >> tokenizer config file saved in results/adapters/ag/checkpoint-134000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:06:22,500 >> Special tokens file saved in results/adapters/ag/checkpoint-134000/special_tokens_map.json\n",
      "{'loss': 1.1854, 'learning_rate': 2.5361915367483296e-05, 'epoch': 74.64}\n",
      "{'loss': 1.1823, 'learning_rate': 2.5334075723830735e-05, 'epoch': 74.67}\n",
      "{'loss': 1.1752, 'learning_rate': 2.5306236080178174e-05, 'epoch': 74.69}\n",
      "{'loss': 1.1673, 'learning_rate': 2.5278396436525613e-05, 'epoch': 74.72}\n",
      "{'loss': 1.1514, 'learning_rate': 2.5250556792873052e-05, 'epoch': 74.75}\n",
      "{'loss': 1.1843, 'learning_rate': 2.522271714922049e-05, 'epoch': 74.78}\n",
      "{'loss': 1.1894, 'learning_rate': 2.519487750556793e-05, 'epoch': 74.8}\n",
      "{'loss': 1.1826, 'learning_rate': 2.516703786191537e-05, 'epoch': 74.83}\n",
      "{'loss': 1.1819, 'learning_rate': 2.5139198218262805e-05, 'epoch': 74.86}\n",
      "{'loss': 1.1647, 'learning_rate': 2.5111358574610245e-05, 'epoch': 74.89}\n",
      " 75% 134500/179600 [10:58:45<3:30:29,  3.57it/s][INFO|trainer.py:1989] 2021-08-02 17:08:45,497 >> Saving model checkpoint to results/adapters/ag/checkpoint-134500\n",
      "[INFO|loading.py:59] 2021-08-02 17:08:45,498 >> Configuration saved in results/adapters/ag/checkpoint-134500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:08:45,509 >> Module weights saved in results/adapters/ag/checkpoint-134500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:08:45,510 >> Configuration saved in results/adapters/ag/checkpoint-134500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:08:45,813 >> Module weights saved in results/adapters/ag/checkpoint-134500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:08:45,814 >> Configuration saved in results/adapters/ag/checkpoint-134500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:08:46,158 >> Module weights saved in results/adapters/ag/checkpoint-134500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:08:46,158 >> tokenizer config file saved in results/adapters/ag/checkpoint-134500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:08:46,159 >> Special tokens file saved in results/adapters/ag/checkpoint-134500/special_tokens_map.json\n",
      "{'loss': 1.2084, 'learning_rate': 2.5083518930957684e-05, 'epoch': 74.92}\n",
      "{'loss': 1.1725, 'learning_rate': 2.5055679287305123e-05, 'epoch': 74.94}\n",
      "{'loss': 1.1677, 'learning_rate': 2.5027839643652562e-05, 'epoch': 74.97}\n",
      "{'loss': 1.1692, 'learning_rate': 2.5e-05, 'epoch': 75.0}\n",
      "{'loss': 1.1985, 'learning_rate': 2.497216035634744e-05, 'epoch': 75.03}\n",
      "{'loss': 1.2045, 'learning_rate': 2.494432071269488e-05, 'epoch': 75.06}\n",
      "{'loss': 1.1985, 'learning_rate': 2.4916481069042315e-05, 'epoch': 75.08}\n",
      "{'loss': 1.2106, 'learning_rate': 2.4888641425389754e-05, 'epoch': 75.11}\n",
      "{'loss': 1.2026, 'learning_rate': 2.4860801781737194e-05, 'epoch': 75.14}\n",
      "{'loss': 1.1697, 'learning_rate': 2.4832962138084633e-05, 'epoch': 75.17}\n",
      " 75% 135000/179600 [11:01:10<3:27:31,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 17:11:10,581 >> Saving model checkpoint to results/adapters/ag/checkpoint-135000\n",
      "[INFO|loading.py:59] 2021-08-02 17:11:10,582 >> Configuration saved in results/adapters/ag/checkpoint-135000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:11:10,593 >> Module weights saved in results/adapters/ag/checkpoint-135000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:11:10,593 >> Configuration saved in results/adapters/ag/checkpoint-135000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:11:10,925 >> Module weights saved in results/adapters/ag/checkpoint-135000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:11:10,925 >> Configuration saved in results/adapters/ag/checkpoint-135000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:11:11,267 >> Module weights saved in results/adapters/ag/checkpoint-135000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:11:11,267 >> tokenizer config file saved in results/adapters/ag/checkpoint-135000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:11:11,268 >> Special tokens file saved in results/adapters/ag/checkpoint-135000/special_tokens_map.json\n",
      "{'loss': 1.1679, 'learning_rate': 2.4805122494432072e-05, 'epoch': 75.19}\n",
      "{'loss': 1.1982, 'learning_rate': 2.477728285077951e-05, 'epoch': 75.22}\n",
      "{'loss': 1.1807, 'learning_rate': 2.474944320712695e-05, 'epoch': 75.25}\n",
      "{'loss': 1.2153, 'learning_rate': 2.472160356347439e-05, 'epoch': 75.28}\n",
      "{'loss': 1.1874, 'learning_rate': 2.469376391982183e-05, 'epoch': 75.31}\n",
      "{'loss': 1.1845, 'learning_rate': 2.4665924276169268e-05, 'epoch': 75.33}\n",
      "{'loss': 1.2142, 'learning_rate': 2.4638084632516707e-05, 'epoch': 75.36}\n",
      "{'loss': 1.1951, 'learning_rate': 2.4610244988864146e-05, 'epoch': 75.39}\n",
      "{'loss': 1.1992, 'learning_rate': 2.4582405345211582e-05, 'epoch': 75.42}\n",
      "{'loss': 1.1786, 'learning_rate': 2.455456570155902e-05, 'epoch': 75.45}\n",
      " 75% 135500/179600 [11:03:33<3:32:47,  3.45it/s][INFO|trainer.py:1989] 2021-08-02 17:13:33,442 >> Saving model checkpoint to results/adapters/ag/checkpoint-135500\n",
      "[INFO|loading.py:59] 2021-08-02 17:13:33,442 >> Configuration saved in results/adapters/ag/checkpoint-135500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:13:33,453 >> Module weights saved in results/adapters/ag/checkpoint-135500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:13:33,453 >> Configuration saved in results/adapters/ag/checkpoint-135500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:13:33,751 >> Module weights saved in results/adapters/ag/checkpoint-135500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:13:33,751 >> Configuration saved in results/adapters/ag/checkpoint-135500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:13:34,083 >> Module weights saved in results/adapters/ag/checkpoint-135500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:13:34,083 >> tokenizer config file saved in results/adapters/ag/checkpoint-135500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:13:34,083 >> Special tokens file saved in results/adapters/ag/checkpoint-135500/special_tokens_map.json\n",
      "{'loss': 1.1884, 'learning_rate': 2.452672605790646e-05, 'epoch': 75.47}\n",
      "{'loss': 1.1735, 'learning_rate': 2.44988864142539e-05, 'epoch': 75.5}\n",
      "{'loss': 1.2254, 'learning_rate': 2.447104677060134e-05, 'epoch': 75.53}\n",
      "{'loss': 1.2111, 'learning_rate': 2.4443207126948774e-05, 'epoch': 75.56}\n",
      "{'loss': 1.1532, 'learning_rate': 2.4415367483296213e-05, 'epoch': 75.58}\n",
      "{'loss': 1.1772, 'learning_rate': 2.4387527839643652e-05, 'epoch': 75.61}\n",
      "{'loss': 1.17, 'learning_rate': 2.435968819599109e-05, 'epoch': 75.64}\n",
      "{'loss': 1.183, 'learning_rate': 2.433184855233853e-05, 'epoch': 75.67}\n",
      "{'loss': 1.1656, 'learning_rate': 2.430400890868597e-05, 'epoch': 75.7}\n",
      "{'loss': 1.1623, 'learning_rate': 2.427616926503341e-05, 'epoch': 75.72}\n",
      " 76% 136000/179600 [11:05:57<3:33:24,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 17:15:57,905 >> Saving model checkpoint to results/adapters/ag/checkpoint-136000\n",
      "[INFO|loading.py:59] 2021-08-02 17:15:57,905 >> Configuration saved in results/adapters/ag/checkpoint-136000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:15:57,917 >> Module weights saved in results/adapters/ag/checkpoint-136000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:15:57,917 >> Configuration saved in results/adapters/ag/checkpoint-136000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:15:58,224 >> Module weights saved in results/adapters/ag/checkpoint-136000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:15:58,225 >> Configuration saved in results/adapters/ag/checkpoint-136000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:15:58,575 >> Module weights saved in results/adapters/ag/checkpoint-136000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:15:58,576 >> tokenizer config file saved in results/adapters/ag/checkpoint-136000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:15:58,576 >> Special tokens file saved in results/adapters/ag/checkpoint-136000/special_tokens_map.json\n",
      "{'loss': 1.1757, 'learning_rate': 2.4248329621380848e-05, 'epoch': 75.75}\n",
      "{'loss': 1.1764, 'learning_rate': 2.4220489977728287e-05, 'epoch': 75.78}\n",
      "{'loss': 1.1813, 'learning_rate': 2.4192650334075727e-05, 'epoch': 75.81}\n",
      "{'loss': 1.1726, 'learning_rate': 2.4164810690423166e-05, 'epoch': 75.83}\n",
      "{'loss': 1.1772, 'learning_rate': 2.41369710467706e-05, 'epoch': 75.86}\n",
      "{'loss': 1.2216, 'learning_rate': 2.410913140311804e-05, 'epoch': 75.89}\n",
      "{'loss': 1.167, 'learning_rate': 2.408129175946548e-05, 'epoch': 75.92}\n",
      "{'loss': 1.1878, 'learning_rate': 2.405345211581292e-05, 'epoch': 75.95}\n",
      "{'loss': 1.2041, 'learning_rate': 2.4025612472160358e-05, 'epoch': 75.97}\n",
      "{'loss': 1.1928, 'learning_rate': 2.3997772828507797e-05, 'epoch': 76.0}\n",
      " 76% 136500/179600 [11:08:22<3:51:30,  3.10it/s][INFO|trainer.py:1989] 2021-08-02 17:18:22,681 >> Saving model checkpoint to results/adapters/ag/checkpoint-136500\n",
      "[INFO|loading.py:59] 2021-08-02 17:18:22,682 >> Configuration saved in results/adapters/ag/checkpoint-136500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:18:22,693 >> Module weights saved in results/adapters/ag/checkpoint-136500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:18:22,694 >> Configuration saved in results/adapters/ag/checkpoint-136500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:18:22,990 >> Module weights saved in results/adapters/ag/checkpoint-136500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:18:22,990 >> Configuration saved in results/adapters/ag/checkpoint-136500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:18:23,330 >> Module weights saved in results/adapters/ag/checkpoint-136500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:18:23,331 >> tokenizer config file saved in results/adapters/ag/checkpoint-136500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:18:23,331 >> Special tokens file saved in results/adapters/ag/checkpoint-136500/special_tokens_map.json\n",
      "{'loss': 1.205, 'learning_rate': 2.3969933184855233e-05, 'epoch': 76.03}\n",
      "{'loss': 1.1793, 'learning_rate': 2.3942093541202672e-05, 'epoch': 76.06}\n",
      "{'loss': 1.1796, 'learning_rate': 2.391425389755011e-05, 'epoch': 76.09}\n",
      "{'loss': 1.19, 'learning_rate': 2.388641425389755e-05, 'epoch': 76.11}\n",
      "{'loss': 1.199, 'learning_rate': 2.385857461024499e-05, 'epoch': 76.14}\n",
      "{'loss': 1.1934, 'learning_rate': 2.383073496659243e-05, 'epoch': 76.17}\n",
      "{'loss': 1.1988, 'learning_rate': 2.3802895322939868e-05, 'epoch': 76.2}\n",
      "{'loss': 1.1937, 'learning_rate': 2.3775055679287307e-05, 'epoch': 76.22}\n",
      "{'loss': 1.1505, 'learning_rate': 2.3747216035634746e-05, 'epoch': 76.25}\n",
      "{'loss': 1.1936, 'learning_rate': 2.3719376391982185e-05, 'epoch': 76.28}\n",
      " 76% 137000/179600 [11:10:46<3:34:14,  3.31it/s][INFO|trainer.py:1989] 2021-08-02 17:20:47,129 >> Saving model checkpoint to results/adapters/ag/checkpoint-137000\n",
      "[INFO|loading.py:59] 2021-08-02 17:20:47,129 >> Configuration saved in results/adapters/ag/checkpoint-137000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:20:47,142 >> Module weights saved in results/adapters/ag/checkpoint-137000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:20:47,142 >> Configuration saved in results/adapters/ag/checkpoint-137000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:20:47,449 >> Module weights saved in results/adapters/ag/checkpoint-137000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:20:47,450 >> Configuration saved in results/adapters/ag/checkpoint-137000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:20:47,794 >> Module weights saved in results/adapters/ag/checkpoint-137000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:20:47,795 >> tokenizer config file saved in results/adapters/ag/checkpoint-137000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:20:47,795 >> Special tokens file saved in results/adapters/ag/checkpoint-137000/special_tokens_map.json\n",
      "{'loss': 1.1435, 'learning_rate': 2.3691536748329625e-05, 'epoch': 76.31}\n",
      "{'loss': 1.1687, 'learning_rate': 2.366369710467706e-05, 'epoch': 76.34}\n",
      "{'loss': 1.1822, 'learning_rate': 2.36358574610245e-05, 'epoch': 76.36}\n",
      "{'loss': 1.1675, 'learning_rate': 2.360801781737194e-05, 'epoch': 76.39}\n",
      "{'loss': 1.1491, 'learning_rate': 2.3580178173719378e-05, 'epoch': 76.42}\n",
      "{'loss': 1.1789, 'learning_rate': 2.3552338530066817e-05, 'epoch': 76.45}\n",
      "{'loss': 1.1591, 'learning_rate': 2.3524498886414256e-05, 'epoch': 76.48}\n",
      "{'loss': 1.1793, 'learning_rate': 2.3496659242761692e-05, 'epoch': 76.5}\n",
      "{'loss': 1.1885, 'learning_rate': 2.346881959910913e-05, 'epoch': 76.53}\n",
      "{'loss': 1.1736, 'learning_rate': 2.344097995545657e-05, 'epoch': 76.56}\n",
      " 77% 137500/179600 [11:13:10<3:14:10,  3.61it/s][INFO|trainer.py:1989] 2021-08-02 17:23:11,273 >> Saving model checkpoint to results/adapters/ag/checkpoint-137500\n",
      "[INFO|loading.py:59] 2021-08-02 17:23:11,274 >> Configuration saved in results/adapters/ag/checkpoint-137500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:23:11,287 >> Module weights saved in results/adapters/ag/checkpoint-137500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:23:11,287 >> Configuration saved in results/adapters/ag/checkpoint-137500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:23:11,617 >> Module weights saved in results/adapters/ag/checkpoint-137500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:23:11,619 >> Configuration saved in results/adapters/ag/checkpoint-137500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:23:11,972 >> Module weights saved in results/adapters/ag/checkpoint-137500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:23:11,973 >> tokenizer config file saved in results/adapters/ag/checkpoint-137500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:23:11,973 >> Special tokens file saved in results/adapters/ag/checkpoint-137500/special_tokens_map.json\n",
      "{'loss': 1.1824, 'learning_rate': 2.341314031180401e-05, 'epoch': 76.59}\n",
      "{'loss': 1.2408, 'learning_rate': 2.338530066815145e-05, 'epoch': 76.61}\n",
      "{'loss': 1.164, 'learning_rate': 2.3357461024498888e-05, 'epoch': 76.64}\n",
      "{'loss': 1.1755, 'learning_rate': 2.3329621380846327e-05, 'epoch': 76.67}\n",
      "{'loss': 1.2062, 'learning_rate': 2.3301781737193766e-05, 'epoch': 76.7}\n",
      "{'loss': 1.1815, 'learning_rate': 2.3273942093541205e-05, 'epoch': 76.73}\n",
      "{'loss': 1.1623, 'learning_rate': 2.3246102449888644e-05, 'epoch': 76.75}\n",
      "{'loss': 1.1595, 'learning_rate': 2.3218262806236083e-05, 'epoch': 76.78}\n",
      "{'loss': 1.1944, 'learning_rate': 2.319042316258352e-05, 'epoch': 76.81}\n",
      "{'loss': 1.2017, 'learning_rate': 2.316258351893096e-05, 'epoch': 76.84}\n",
      " 77% 138000/179600 [11:15:34<3:10:45,  3.63it/s][INFO|trainer.py:1989] 2021-08-02 17:25:34,847 >> Saving model checkpoint to results/adapters/ag/checkpoint-138000\n",
      "[INFO|loading.py:59] 2021-08-02 17:25:34,848 >> Configuration saved in results/adapters/ag/checkpoint-138000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:25:34,859 >> Module weights saved in results/adapters/ag/checkpoint-138000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:25:34,859 >> Configuration saved in results/adapters/ag/checkpoint-138000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:25:35,155 >> Module weights saved in results/adapters/ag/checkpoint-138000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:25:35,155 >> Configuration saved in results/adapters/ag/checkpoint-138000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:25:35,484 >> Module weights saved in results/adapters/ag/checkpoint-138000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:25:35,484 >> tokenizer config file saved in results/adapters/ag/checkpoint-138000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:25:35,484 >> Special tokens file saved in results/adapters/ag/checkpoint-138000/special_tokens_map.json\n",
      "{'loss': 1.2036, 'learning_rate': 2.3134743875278397e-05, 'epoch': 76.86}\n",
      "{'loss': 1.2092, 'learning_rate': 2.3106904231625837e-05, 'epoch': 76.89}\n",
      "{'loss': 1.158, 'learning_rate': 2.3079064587973276e-05, 'epoch': 76.92}\n",
      "{'loss': 1.2061, 'learning_rate': 2.3051224944320715e-05, 'epoch': 76.95}\n",
      "{'loss': 1.2005, 'learning_rate': 2.302338530066815e-05, 'epoch': 76.98}\n",
      "{'loss': 1.2355, 'learning_rate': 2.299554565701559e-05, 'epoch': 77.0}\n",
      "{'loss': 1.1676, 'learning_rate': 2.296770601336303e-05, 'epoch': 77.03}\n",
      "{'loss': 1.1898, 'learning_rate': 2.2939866369710468e-05, 'epoch': 77.06}\n",
      "{'loss': 1.148, 'learning_rate': 2.2912026726057907e-05, 'epoch': 77.09}\n",
      "{'loss': 1.1857, 'learning_rate': 2.2884187082405346e-05, 'epoch': 77.12}\n",
      " 77% 138500/179600 [11:17:59<3:21:00,  3.41it/s][INFO|trainer.py:1989] 2021-08-02 17:27:59,504 >> Saving model checkpoint to results/adapters/ag/checkpoint-138500\n",
      "[INFO|loading.py:59] 2021-08-02 17:27:59,505 >> Configuration saved in results/adapters/ag/checkpoint-138500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:27:59,517 >> Module weights saved in results/adapters/ag/checkpoint-138500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:27:59,517 >> Configuration saved in results/adapters/ag/checkpoint-138500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:27:59,835 >> Module weights saved in results/adapters/ag/checkpoint-138500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:27:59,835 >> Configuration saved in results/adapters/ag/checkpoint-138500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:28:00,184 >> Module weights saved in results/adapters/ag/checkpoint-138500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:28:00,184 >> tokenizer config file saved in results/adapters/ag/checkpoint-138500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:28:00,184 >> Special tokens file saved in results/adapters/ag/checkpoint-138500/special_tokens_map.json\n",
      "{'loss': 1.1727, 'learning_rate': 2.2856347438752786e-05, 'epoch': 77.14}\n",
      "{'loss': 1.1768, 'learning_rate': 2.2828507795100225e-05, 'epoch': 77.17}\n",
      "{'loss': 1.1899, 'learning_rate': 2.2800668151447664e-05, 'epoch': 77.2}\n",
      "{'loss': 1.1635, 'learning_rate': 2.2772828507795103e-05, 'epoch': 77.23}\n",
      "{'loss': 1.185, 'learning_rate': 2.2744988864142542e-05, 'epoch': 77.25}\n",
      "{'loss': 1.2094, 'learning_rate': 2.2717149220489978e-05, 'epoch': 77.28}\n",
      "{'loss': 1.1859, 'learning_rate': 2.2689309576837417e-05, 'epoch': 77.31}\n",
      "{'loss': 1.1461, 'learning_rate': 2.2661469933184856e-05, 'epoch': 77.34}\n",
      "{'loss': 1.1642, 'learning_rate': 2.2633630289532296e-05, 'epoch': 77.37}\n",
      "{'loss': 1.2145, 'learning_rate': 2.2605790645879735e-05, 'epoch': 77.39}\n",
      " 77% 139000/179600 [11:20:22<3:14:52,  3.47it/s][INFO|trainer.py:1989] 2021-08-02 17:30:23,351 >> Saving model checkpoint to results/adapters/ag/checkpoint-139000\n",
      "[INFO|loading.py:59] 2021-08-02 17:30:23,352 >> Configuration saved in results/adapters/ag/checkpoint-139000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:30:23,364 >> Module weights saved in results/adapters/ag/checkpoint-139000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:30:23,364 >> Configuration saved in results/adapters/ag/checkpoint-139000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:30:23,672 >> Module weights saved in results/adapters/ag/checkpoint-139000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:30:23,672 >> Configuration saved in results/adapters/ag/checkpoint-139000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:30:24,001 >> Module weights saved in results/adapters/ag/checkpoint-139000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:30:24,002 >> tokenizer config file saved in results/adapters/ag/checkpoint-139000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:30:24,002 >> Special tokens file saved in results/adapters/ag/checkpoint-139000/special_tokens_map.json\n",
      "{'loss': 1.1306, 'learning_rate': 2.257795100222717e-05, 'epoch': 77.42}\n",
      "{'loss': 1.1584, 'learning_rate': 2.255011135857461e-05, 'epoch': 77.45}\n",
      "{'loss': 1.1836, 'learning_rate': 2.252227171492205e-05, 'epoch': 77.48}\n",
      "{'loss': 1.1512, 'learning_rate': 2.2494432071269488e-05, 'epoch': 77.51}\n",
      "{'loss': 1.2294, 'learning_rate': 2.2466592427616927e-05, 'epoch': 77.53}\n",
      "{'loss': 1.1916, 'learning_rate': 2.2438752783964366e-05, 'epoch': 77.56}\n",
      "{'loss': 1.2025, 'learning_rate': 2.2410913140311805e-05, 'epoch': 77.59}\n",
      "{'loss': 1.1732, 'learning_rate': 2.2383073496659245e-05, 'epoch': 77.62}\n",
      "{'loss': 1.1942, 'learning_rate': 2.2355233853006684e-05, 'epoch': 77.64}\n",
      "{'loss': 1.1983, 'learning_rate': 2.2327394209354123e-05, 'epoch': 77.67}\n",
      " 78% 139500/179600 [11:22:46<3:10:51,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 17:32:47,322 >> Saving model checkpoint to results/adapters/ag/checkpoint-139500\n",
      "[INFO|loading.py:59] 2021-08-02 17:32:47,323 >> Configuration saved in results/adapters/ag/checkpoint-139500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:32:47,336 >> Module weights saved in results/adapters/ag/checkpoint-139500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:32:47,336 >> Configuration saved in results/adapters/ag/checkpoint-139500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:32:47,643 >> Module weights saved in results/adapters/ag/checkpoint-139500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:32:47,643 >> Configuration saved in results/adapters/ag/checkpoint-139500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:32:47,995 >> Module weights saved in results/adapters/ag/checkpoint-139500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:32:47,995 >> tokenizer config file saved in results/adapters/ag/checkpoint-139500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:32:47,996 >> Special tokens file saved in results/adapters/ag/checkpoint-139500/special_tokens_map.json\n",
      "{'loss': 1.1795, 'learning_rate': 2.2299554565701562e-05, 'epoch': 77.7}\n",
      "{'loss': 1.1893, 'learning_rate': 2.2271714922049e-05, 'epoch': 77.73}\n",
      "{'loss': 1.1776, 'learning_rate': 2.2243875278396437e-05, 'epoch': 77.76}\n",
      "{'loss': 1.2052, 'learning_rate': 2.2216035634743876e-05, 'epoch': 77.78}\n",
      "{'loss': 1.1927, 'learning_rate': 2.2188195991091315e-05, 'epoch': 77.81}\n",
      "{'loss': 1.1774, 'learning_rate': 2.2160356347438754e-05, 'epoch': 77.84}\n",
      "{'loss': 1.1554, 'learning_rate': 2.2132516703786194e-05, 'epoch': 77.87}\n",
      "{'loss': 1.1589, 'learning_rate': 2.210467706013363e-05, 'epoch': 77.89}\n",
      "{'loss': 1.1682, 'learning_rate': 2.207683741648107e-05, 'epoch': 77.92}\n",
      "{'loss': 1.1628, 'learning_rate': 2.2048997772828508e-05, 'epoch': 77.95}\n",
      " 78% 140000/179600 [11:25:11<3:08:21,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 17:35:11,998 >> Saving model checkpoint to results/adapters/ag/checkpoint-140000\n",
      "[INFO|loading.py:59] 2021-08-02 17:35:11,999 >> Configuration saved in results/adapters/ag/checkpoint-140000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:35:12,011 >> Module weights saved in results/adapters/ag/checkpoint-140000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:35:12,011 >> Configuration saved in results/adapters/ag/checkpoint-140000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:35:12,318 >> Module weights saved in results/adapters/ag/checkpoint-140000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:35:12,318 >> Configuration saved in results/adapters/ag/checkpoint-140000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:35:12,666 >> Module weights saved in results/adapters/ag/checkpoint-140000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:35:12,667 >> tokenizer config file saved in results/adapters/ag/checkpoint-140000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:35:12,667 >> Special tokens file saved in results/adapters/ag/checkpoint-140000/special_tokens_map.json\n",
      "{'loss': 1.1779, 'learning_rate': 2.2021158129175947e-05, 'epoch': 77.98}\n",
      "{'loss': 1.2043, 'learning_rate': 2.1993318485523386e-05, 'epoch': 78.01}\n",
      "{'loss': 1.1889, 'learning_rate': 2.1965478841870825e-05, 'epoch': 78.03}\n",
      "{'loss': 1.1713, 'learning_rate': 2.1937639198218264e-05, 'epoch': 78.06}\n",
      "{'loss': 1.1735, 'learning_rate': 2.1909799554565703e-05, 'epoch': 78.09}\n",
      "{'loss': 1.1779, 'learning_rate': 2.1881959910913143e-05, 'epoch': 78.12}\n",
      "{'loss': 1.1761, 'learning_rate': 2.185412026726058e-05, 'epoch': 78.15}\n",
      "{'loss': 1.1672, 'learning_rate': 2.182628062360802e-05, 'epoch': 78.17}\n",
      "{'loss': 1.1393, 'learning_rate': 2.179844097995546e-05, 'epoch': 78.2}\n",
      "{'loss': 1.1707, 'learning_rate': 2.1770601336302896e-05, 'epoch': 78.23}\n",
      " 78% 140500/179600 [11:27:36<3:07:14,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 17:37:37,100 >> Saving model checkpoint to results/adapters/ag/checkpoint-140500\n",
      "[INFO|loading.py:59] 2021-08-02 17:37:37,101 >> Configuration saved in results/adapters/ag/checkpoint-140500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:37:37,113 >> Module weights saved in results/adapters/ag/checkpoint-140500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:37:37,113 >> Configuration saved in results/adapters/ag/checkpoint-140500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:37:37,424 >> Module weights saved in results/adapters/ag/checkpoint-140500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:37:37,424 >> Configuration saved in results/adapters/ag/checkpoint-140500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:37:37,768 >> Module weights saved in results/adapters/ag/checkpoint-140500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:37:37,768 >> tokenizer config file saved in results/adapters/ag/checkpoint-140500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:37:37,769 >> Special tokens file saved in results/adapters/ag/checkpoint-140500/special_tokens_map.json\n",
      "{'loss': 1.1892, 'learning_rate': 2.1742761692650335e-05, 'epoch': 78.26}\n",
      "{'loss': 1.1779, 'learning_rate': 2.1714922048997774e-05, 'epoch': 78.28}\n",
      "{'loss': 1.1702, 'learning_rate': 2.1687082405345213e-05, 'epoch': 78.31}\n",
      "{'loss': 1.1655, 'learning_rate': 2.1659242761692652e-05, 'epoch': 78.34}\n",
      "{'loss': 1.1869, 'learning_rate': 2.1631403118040088e-05, 'epoch': 78.37}\n",
      "{'loss': 1.1652, 'learning_rate': 2.1603563474387527e-05, 'epoch': 78.4}\n",
      "{'loss': 1.1951, 'learning_rate': 2.1575723830734966e-05, 'epoch': 78.42}\n",
      "{'loss': 1.1884, 'learning_rate': 2.1547884187082406e-05, 'epoch': 78.45}\n",
      "{'loss': 1.1829, 'learning_rate': 2.1520044543429845e-05, 'epoch': 78.48}\n",
      "{'loss': 1.2013, 'learning_rate': 2.1492204899777284e-05, 'epoch': 78.51}\n",
      " 79% 141000/179600 [11:30:00<3:04:03,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 17:40:01,295 >> Saving model checkpoint to results/adapters/ag/checkpoint-141000\n",
      "[INFO|loading.py:59] 2021-08-02 17:40:01,295 >> Configuration saved in results/adapters/ag/checkpoint-141000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:40:01,306 >> Module weights saved in results/adapters/ag/checkpoint-141000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:40:01,307 >> Configuration saved in results/adapters/ag/checkpoint-141000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:40:01,610 >> Module weights saved in results/adapters/ag/checkpoint-141000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:40:01,611 >> Configuration saved in results/adapters/ag/checkpoint-141000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:40:01,959 >> Module weights saved in results/adapters/ag/checkpoint-141000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:40:01,960 >> tokenizer config file saved in results/adapters/ag/checkpoint-141000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:40:01,960 >> Special tokens file saved in results/adapters/ag/checkpoint-141000/special_tokens_map.json\n",
      "{'loss': 1.179, 'learning_rate': 2.1464365256124723e-05, 'epoch': 78.54}\n",
      "{'loss': 1.2144, 'learning_rate': 2.1436525612472162e-05, 'epoch': 78.56}\n",
      "{'loss': 1.1664, 'learning_rate': 2.14086859688196e-05, 'epoch': 78.59}\n",
      "{'loss': 1.1689, 'learning_rate': 2.138084632516704e-05, 'epoch': 78.62}\n",
      "{'loss': 1.1901, 'learning_rate': 2.135300668151448e-05, 'epoch': 78.65}\n",
      "{'loss': 1.1721, 'learning_rate': 2.1325167037861915e-05, 'epoch': 78.67}\n",
      "{'loss': 1.1912, 'learning_rate': 2.1297327394209355e-05, 'epoch': 78.7}\n",
      "{'loss': 1.146, 'learning_rate': 2.1269487750556794e-05, 'epoch': 78.73}\n",
      "{'loss': 1.202, 'learning_rate': 2.1241648106904233e-05, 'epoch': 78.76}\n",
      "{'loss': 1.2119, 'learning_rate': 2.1213808463251672e-05, 'epoch': 78.79}\n",
      " 79% 141500/179600 [11:32:27<2:58:06,  3.57it/s][INFO|trainer.py:1989] 2021-08-02 17:42:27,772 >> Saving model checkpoint to results/adapters/ag/checkpoint-141500\n",
      "[INFO|loading.py:59] 2021-08-02 17:42:27,773 >> Configuration saved in results/adapters/ag/checkpoint-141500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:42:27,784 >> Module weights saved in results/adapters/ag/checkpoint-141500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:42:27,785 >> Configuration saved in results/adapters/ag/checkpoint-141500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:42:28,083 >> Module weights saved in results/adapters/ag/checkpoint-141500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:42:28,083 >> Configuration saved in results/adapters/ag/checkpoint-141500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:42:28,422 >> Module weights saved in results/adapters/ag/checkpoint-141500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:42:28,423 >> tokenizer config file saved in results/adapters/ag/checkpoint-141500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:42:28,423 >> Special tokens file saved in results/adapters/ag/checkpoint-141500/special_tokens_map.json\n",
      "{'loss': 1.1732, 'learning_rate': 2.118596881959911e-05, 'epoch': 78.81}\n",
      "{'loss': 1.2097, 'learning_rate': 2.1158129175946547e-05, 'epoch': 78.84}\n",
      "{'loss': 1.1462, 'learning_rate': 2.1130289532293986e-05, 'epoch': 78.87}\n",
      "{'loss': 1.1743, 'learning_rate': 2.1102449888641425e-05, 'epoch': 78.9}\n",
      "{'loss': 1.181, 'learning_rate': 2.1074610244988864e-05, 'epoch': 78.92}\n",
      "{'loss': 1.1573, 'learning_rate': 2.1046770601336304e-05, 'epoch': 78.95}\n",
      "{'loss': 1.1706, 'learning_rate': 2.1018930957683743e-05, 'epoch': 78.98}\n",
      "{'loss': 1.2411, 'learning_rate': 2.0991091314031182e-05, 'epoch': 79.01}\n",
      "{'loss': 1.1598, 'learning_rate': 2.096325167037862e-05, 'epoch': 79.04}\n",
      "{'loss': 1.1909, 'learning_rate': 2.093541202672606e-05, 'epoch': 79.06}\n",
      " 79% 142000/179600 [11:34:51<3:08:22,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 17:44:52,287 >> Saving model checkpoint to results/adapters/ag/checkpoint-142000\n",
      "[INFO|loading.py:59] 2021-08-02 17:44:52,288 >> Configuration saved in results/adapters/ag/checkpoint-142000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:44:52,303 >> Module weights saved in results/adapters/ag/checkpoint-142000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:44:52,304 >> Configuration saved in results/adapters/ag/checkpoint-142000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:44:52,612 >> Module weights saved in results/adapters/ag/checkpoint-142000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:44:52,612 >> Configuration saved in results/adapters/ag/checkpoint-142000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:44:52,956 >> Module weights saved in results/adapters/ag/checkpoint-142000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:44:52,956 >> tokenizer config file saved in results/adapters/ag/checkpoint-142000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:44:52,957 >> Special tokens file saved in results/adapters/ag/checkpoint-142000/special_tokens_map.json\n",
      "{'loss': 1.232, 'learning_rate': 2.09075723830735e-05, 'epoch': 79.09}\n",
      "{'loss': 1.1737, 'learning_rate': 2.087973273942094e-05, 'epoch': 79.12}\n",
      "{'loss': 1.1826, 'learning_rate': 2.0851893095768374e-05, 'epoch': 79.15}\n",
      "{'loss': 1.1853, 'learning_rate': 2.0824053452115813e-05, 'epoch': 79.18}\n",
      "{'loss': 1.1551, 'learning_rate': 2.0796213808463253e-05, 'epoch': 79.2}\n",
      "{'loss': 1.2021, 'learning_rate': 2.0768374164810692e-05, 'epoch': 79.23}\n",
      "{'loss': 1.1595, 'learning_rate': 2.074053452115813e-05, 'epoch': 79.26}\n",
      "{'loss': 1.1692, 'learning_rate': 2.071269487750557e-05, 'epoch': 79.29}\n",
      "{'loss': 1.1772, 'learning_rate': 2.0684855233853006e-05, 'epoch': 79.31}\n",
      "{'loss': 1.1627, 'learning_rate': 2.0657015590200445e-05, 'epoch': 79.34}\n",
      " 79% 142500/179600 [11:37:16<2:55:33,  3.52it/s][INFO|trainer.py:1989] 2021-08-02 17:47:17,192 >> Saving model checkpoint to results/adapters/ag/checkpoint-142500\n",
      "[INFO|loading.py:59] 2021-08-02 17:47:17,193 >> Configuration saved in results/adapters/ag/checkpoint-142500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:47:17,205 >> Module weights saved in results/adapters/ag/checkpoint-142500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:47:17,206 >> Configuration saved in results/adapters/ag/checkpoint-142500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:47:17,507 >> Module weights saved in results/adapters/ag/checkpoint-142500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:47:17,508 >> Configuration saved in results/adapters/ag/checkpoint-142500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:47:17,857 >> Module weights saved in results/adapters/ag/checkpoint-142500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:47:17,858 >> tokenizer config file saved in results/adapters/ag/checkpoint-142500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:47:17,858 >> Special tokens file saved in results/adapters/ag/checkpoint-142500/special_tokens_map.json\n",
      "{'loss': 1.1534, 'learning_rate': 2.0629175946547884e-05, 'epoch': 79.37}\n",
      "{'loss': 1.1985, 'learning_rate': 2.0601336302895323e-05, 'epoch': 79.4}\n",
      "{'loss': 1.1693, 'learning_rate': 2.0573496659242762e-05, 'epoch': 79.43}\n",
      "{'loss': 1.1666, 'learning_rate': 2.05456570155902e-05, 'epoch': 79.45}\n",
      "{'loss': 1.1691, 'learning_rate': 2.051781737193764e-05, 'epoch': 79.48}\n",
      "{'loss': 1.2047, 'learning_rate': 2.048997772828508e-05, 'epoch': 79.51}\n",
      "{'loss': 1.167, 'learning_rate': 2.046213808463252e-05, 'epoch': 79.54}\n",
      "{'loss': 1.1652, 'learning_rate': 2.0434298440979958e-05, 'epoch': 79.57}\n",
      "{'loss': 1.2011, 'learning_rate': 2.0406458797327397e-05, 'epoch': 79.59}\n",
      "{'loss': 1.202, 'learning_rate': 2.0378619153674833e-05, 'epoch': 79.62}\n",
      " 80% 143000/179600 [11:39:40<2:49:48,  3.59it/s][INFO|trainer.py:1989] 2021-08-02 17:49:40,985 >> Saving model checkpoint to results/adapters/ag/checkpoint-143000\n",
      "[INFO|loading.py:59] 2021-08-02 17:49:40,986 >> Configuration saved in results/adapters/ag/checkpoint-143000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:49:40,998 >> Module weights saved in results/adapters/ag/checkpoint-143000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:49:40,998 >> Configuration saved in results/adapters/ag/checkpoint-143000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:49:41,307 >> Module weights saved in results/adapters/ag/checkpoint-143000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:49:41,307 >> Configuration saved in results/adapters/ag/checkpoint-143000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:49:41,634 >> Module weights saved in results/adapters/ag/checkpoint-143000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:49:41,635 >> tokenizer config file saved in results/adapters/ag/checkpoint-143000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:49:41,635 >> Special tokens file saved in results/adapters/ag/checkpoint-143000/special_tokens_map.json\n",
      "{'loss': 1.185, 'learning_rate': 2.0350779510022272e-05, 'epoch': 79.65}\n",
      "{'loss': 1.1702, 'learning_rate': 2.032293986636971e-05, 'epoch': 79.68}\n",
      "{'loss': 1.1839, 'learning_rate': 2.029510022271715e-05, 'epoch': 79.7}\n",
      "{'loss': 1.1781, 'learning_rate': 2.026726057906459e-05, 'epoch': 79.73}\n",
      "{'loss': 1.1723, 'learning_rate': 2.023942093541203e-05, 'epoch': 79.76}\n",
      "{'loss': 1.1987, 'learning_rate': 2.0211581291759465e-05, 'epoch': 79.79}\n",
      "{'loss': 1.165, 'learning_rate': 2.0183741648106904e-05, 'epoch': 79.82}\n",
      "{'loss': 1.192, 'learning_rate': 2.0155902004454343e-05, 'epoch': 79.84}\n",
      "{'loss': 1.169, 'learning_rate': 2.0128062360801782e-05, 'epoch': 79.87}\n",
      "{'loss': 1.2149, 'learning_rate': 2.010022271714922e-05, 'epoch': 79.9}\n",
      " 80% 143500/179600 [11:42:04<2:55:17,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 17:52:04,910 >> Saving model checkpoint to results/adapters/ag/checkpoint-143500\n",
      "[INFO|loading.py:59] 2021-08-02 17:52:04,911 >> Configuration saved in results/adapters/ag/checkpoint-143500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:52:04,923 >> Module weights saved in results/adapters/ag/checkpoint-143500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:52:04,924 >> Configuration saved in results/adapters/ag/checkpoint-143500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:52:05,242 >> Module weights saved in results/adapters/ag/checkpoint-143500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:52:05,243 >> Configuration saved in results/adapters/ag/checkpoint-143500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:52:05,587 >> Module weights saved in results/adapters/ag/checkpoint-143500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:52:05,587 >> tokenizer config file saved in results/adapters/ag/checkpoint-143500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:52:05,587 >> Special tokens file saved in results/adapters/ag/checkpoint-143500/special_tokens_map.json\n",
      "{'loss': 1.1977, 'learning_rate': 2.007238307349666e-05, 'epoch': 79.93}\n",
      "{'loss': 1.1446, 'learning_rate': 2.00445434298441e-05, 'epoch': 79.95}\n",
      "{'loss': 1.1801, 'learning_rate': 2.001670378619154e-05, 'epoch': 79.98}\n",
      "{'loss': 1.2269, 'learning_rate': 1.9988864142538978e-05, 'epoch': 80.01}\n",
      "{'loss': 1.1473, 'learning_rate': 1.9961024498886417e-05, 'epoch': 80.04}\n",
      "{'loss': 1.1905, 'learning_rate': 1.9933184855233856e-05, 'epoch': 80.07}\n",
      "{'loss': 1.1964, 'learning_rate': 1.9905345211581292e-05, 'epoch': 80.09}\n",
      "{'loss': 1.1755, 'learning_rate': 1.987750556792873e-05, 'epoch': 80.12}\n",
      "{'loss': 1.1941, 'learning_rate': 1.984966592427617e-05, 'epoch': 80.15}\n",
      "{'loss': 1.1758, 'learning_rate': 1.982182628062361e-05, 'epoch': 80.18}\n",
      " 80% 144000/179600 [11:44:29<2:49:51,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 17:54:29,644 >> Saving model checkpoint to results/adapters/ag/checkpoint-144000\n",
      "[INFO|loading.py:59] 2021-08-02 17:54:29,644 >> Configuration saved in results/adapters/ag/checkpoint-144000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:54:29,655 >> Module weights saved in results/adapters/ag/checkpoint-144000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:54:29,656 >> Configuration saved in results/adapters/ag/checkpoint-144000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:54:29,960 >> Module weights saved in results/adapters/ag/checkpoint-144000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:54:29,960 >> Configuration saved in results/adapters/ag/checkpoint-144000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:54:30,304 >> Module weights saved in results/adapters/ag/checkpoint-144000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:54:30,304 >> tokenizer config file saved in results/adapters/ag/checkpoint-144000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:54:30,305 >> Special tokens file saved in results/adapters/ag/checkpoint-144000/special_tokens_map.json\n",
      "{'loss': 1.1887, 'learning_rate': 1.979398663697105e-05, 'epoch': 80.21}\n",
      "{'loss': 1.1842, 'learning_rate': 1.9766146993318488e-05, 'epoch': 80.23}\n",
      "{'loss': 1.1565, 'learning_rate': 1.9738307349665924e-05, 'epoch': 80.26}\n",
      "{'loss': 1.1974, 'learning_rate': 1.9710467706013363e-05, 'epoch': 80.29}\n",
      "{'loss': 1.1959, 'learning_rate': 1.9682628062360802e-05, 'epoch': 80.32}\n",
      "{'loss': 1.196, 'learning_rate': 1.965478841870824e-05, 'epoch': 80.35}\n",
      "{'loss': 1.1873, 'learning_rate': 1.962694877505568e-05, 'epoch': 80.37}\n",
      "{'loss': 1.1677, 'learning_rate': 1.959910913140312e-05, 'epoch': 80.4}\n",
      "{'loss': 1.1672, 'learning_rate': 1.957126948775056e-05, 'epoch': 80.43}\n",
      "{'loss': 1.184, 'learning_rate': 1.9543429844097998e-05, 'epoch': 80.46}\n",
      " 80% 144500/179600 [11:46:52<2:48:50,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 17:56:53,336 >> Saving model checkpoint to results/adapters/ag/checkpoint-144500\n",
      "[INFO|loading.py:59] 2021-08-02 17:56:53,336 >> Configuration saved in results/adapters/ag/checkpoint-144500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:56:53,349 >> Module weights saved in results/adapters/ag/checkpoint-144500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:56:53,349 >> Configuration saved in results/adapters/ag/checkpoint-144500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:56:53,661 >> Module weights saved in results/adapters/ag/checkpoint-144500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:56:53,662 >> Configuration saved in results/adapters/ag/checkpoint-144500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:56:54,005 >> Module weights saved in results/adapters/ag/checkpoint-144500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:56:54,006 >> tokenizer config file saved in results/adapters/ag/checkpoint-144500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:56:54,006 >> Special tokens file saved in results/adapters/ag/checkpoint-144500/special_tokens_map.json\n",
      "{'loss': 1.1529, 'learning_rate': 1.9515590200445437e-05, 'epoch': 80.48}\n",
      "{'loss': 1.1966, 'learning_rate': 1.9487750556792876e-05, 'epoch': 80.51}\n",
      "{'loss': 1.1642, 'learning_rate': 1.9459910913140315e-05, 'epoch': 80.54}\n",
      "{'loss': 1.1789, 'learning_rate': 1.943207126948775e-05, 'epoch': 80.57}\n",
      "{'loss': 1.1805, 'learning_rate': 1.940423162583519e-05, 'epoch': 80.6}\n",
      "{'loss': 1.1861, 'learning_rate': 1.937639198218263e-05, 'epoch': 80.62}\n",
      "{'loss': 1.1661, 'learning_rate': 1.934855233853007e-05, 'epoch': 80.65}\n",
      "{'loss': 1.164, 'learning_rate': 1.9320712694877508e-05, 'epoch': 80.68}\n",
      "{'loss': 1.203, 'learning_rate': 1.9292873051224943e-05, 'epoch': 80.71}\n",
      "{'loss': 1.1669, 'learning_rate': 1.9265033407572382e-05, 'epoch': 80.73}\n",
      " 81% 145000/179600 [11:49:16<2:44:26,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 17:59:16,768 >> Saving model checkpoint to results/adapters/ag/checkpoint-145000\n",
      "[INFO|loading.py:59] 2021-08-02 17:59:16,768 >> Configuration saved in results/adapters/ag/checkpoint-145000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:59:16,780 >> Module weights saved in results/adapters/ag/checkpoint-145000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:59:16,780 >> Configuration saved in results/adapters/ag/checkpoint-145000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:59:17,092 >> Module weights saved in results/adapters/ag/checkpoint-145000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 17:59:17,093 >> Configuration saved in results/adapters/ag/checkpoint-145000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 17:59:17,437 >> Module weights saved in results/adapters/ag/checkpoint-145000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:59:17,437 >> tokenizer config file saved in results/adapters/ag/checkpoint-145000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:59:17,438 >> Special tokens file saved in results/adapters/ag/checkpoint-145000/special_tokens_map.json\n",
      "{'loss': 1.161, 'learning_rate': 1.923719376391982e-05, 'epoch': 80.76}\n",
      "{'loss': 1.1832, 'learning_rate': 1.920935412026726e-05, 'epoch': 80.79}\n",
      "{'loss': 1.1735, 'learning_rate': 1.91815144766147e-05, 'epoch': 80.82}\n",
      "{'loss': 1.1881, 'learning_rate': 1.915367483296214e-05, 'epoch': 80.85}\n",
      "{'loss': 1.174, 'learning_rate': 1.9125835189309578e-05, 'epoch': 80.87}\n",
      "{'loss': 1.1924, 'learning_rate': 1.9097995545657017e-05, 'epoch': 80.9}\n",
      "{'loss': 1.205, 'learning_rate': 1.9070155902004457e-05, 'epoch': 80.93}\n",
      "{'loss': 1.1735, 'learning_rate': 1.9042316258351896e-05, 'epoch': 80.96}\n",
      "{'loss': 1.1805, 'learning_rate': 1.9014476614699335e-05, 'epoch': 80.99}\n",
      "{'loss': 1.1884, 'learning_rate': 1.8986636971046774e-05, 'epoch': 81.01}\n",
      " 81% 145500/179600 [11:51:39<2:35:43,  3.65it/s][INFO|trainer.py:1989] 2021-08-02 18:01:40,394 >> Saving model checkpoint to results/adapters/ag/checkpoint-145500\n",
      "[INFO|loading.py:59] 2021-08-02 18:01:40,395 >> Configuration saved in results/adapters/ag/checkpoint-145500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:01:40,409 >> Module weights saved in results/adapters/ag/checkpoint-145500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:01:40,410 >> Configuration saved in results/adapters/ag/checkpoint-145500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:01:40,721 >> Module weights saved in results/adapters/ag/checkpoint-145500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:01:40,722 >> Configuration saved in results/adapters/ag/checkpoint-145500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:01:41,068 >> Module weights saved in results/adapters/ag/checkpoint-145500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:01:41,069 >> tokenizer config file saved in results/adapters/ag/checkpoint-145500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:01:41,069 >> Special tokens file saved in results/adapters/ag/checkpoint-145500/special_tokens_map.json\n",
      "{'loss': 1.173, 'learning_rate': 1.895879732739421e-05, 'epoch': 81.04}\n",
      "{'loss': 1.1862, 'learning_rate': 1.893095768374165e-05, 'epoch': 81.07}\n",
      "{'loss': 1.1577, 'learning_rate': 1.8903118040089088e-05, 'epoch': 81.1}\n",
      "{'loss': 1.1658, 'learning_rate': 1.8875278396436527e-05, 'epoch': 81.12}\n",
      "{'loss': 1.166, 'learning_rate': 1.8847438752783966e-05, 'epoch': 81.15}\n",
      "{'loss': 1.2093, 'learning_rate': 1.8819599109131402e-05, 'epoch': 81.18}\n",
      "{'loss': 1.1683, 'learning_rate': 1.879175946547884e-05, 'epoch': 81.21}\n",
      "{'loss': 1.1708, 'learning_rate': 1.876391982182628e-05, 'epoch': 81.24}\n",
      "{'loss': 1.1394, 'learning_rate': 1.873608017817372e-05, 'epoch': 81.26}\n",
      "{'loss': 1.1568, 'learning_rate': 1.870824053452116e-05, 'epoch': 81.29}\n",
      " 81% 146000/179600 [11:54:02<2:41:06,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 18:04:03,113 >> Saving model checkpoint to results/adapters/ag/checkpoint-146000\n",
      "[INFO|loading.py:59] 2021-08-02 18:04:03,114 >> Configuration saved in results/adapters/ag/checkpoint-146000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:04:03,128 >> Module weights saved in results/adapters/ag/checkpoint-146000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:04:03,128 >> Configuration saved in results/adapters/ag/checkpoint-146000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:04:03,452 >> Module weights saved in results/adapters/ag/checkpoint-146000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:04:03,452 >> Configuration saved in results/adapters/ag/checkpoint-146000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:04:03,795 >> Module weights saved in results/adapters/ag/checkpoint-146000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:04:03,796 >> tokenizer config file saved in results/adapters/ag/checkpoint-146000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:04:03,796 >> Special tokens file saved in results/adapters/ag/checkpoint-146000/special_tokens_map.json\n",
      "{'loss': 1.1745, 'learning_rate': 1.8680400890868598e-05, 'epoch': 81.32}\n",
      "{'loss': 1.1499, 'learning_rate': 1.8652561247216037e-05, 'epoch': 81.35}\n",
      "{'loss': 1.1643, 'learning_rate': 1.8624721603563476e-05, 'epoch': 81.38}\n",
      "{'loss': 1.1743, 'learning_rate': 1.8596881959910915e-05, 'epoch': 81.4}\n",
      "{'loss': 1.2445, 'learning_rate': 1.8569042316258355e-05, 'epoch': 81.43}\n",
      "{'loss': 1.1804, 'learning_rate': 1.8541202672605794e-05, 'epoch': 81.46}\n",
      "{'loss': 1.1538, 'learning_rate': 1.851336302895323e-05, 'epoch': 81.49}\n",
      "{'loss': 1.1777, 'learning_rate': 1.848552338530067e-05, 'epoch': 81.51}\n",
      "{'loss': 1.1603, 'learning_rate': 1.8457683741648108e-05, 'epoch': 81.54}\n",
      "{'loss': 1.1952, 'learning_rate': 1.8429844097995547e-05, 'epoch': 81.57}\n",
      " 82% 146500/179600 [11:56:24<2:36:23,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 18:06:24,890 >> Saving model checkpoint to results/adapters/ag/checkpoint-146500\n",
      "[INFO|loading.py:59] 2021-08-02 18:06:24,890 >> Configuration saved in results/adapters/ag/checkpoint-146500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:06:24,901 >> Module weights saved in results/adapters/ag/checkpoint-146500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:06:24,901 >> Configuration saved in results/adapters/ag/checkpoint-146500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:06:25,193 >> Module weights saved in results/adapters/ag/checkpoint-146500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:06:25,194 >> Configuration saved in results/adapters/ag/checkpoint-146500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:06:25,521 >> Module weights saved in results/adapters/ag/checkpoint-146500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:06:25,522 >> tokenizer config file saved in results/adapters/ag/checkpoint-146500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:06:25,522 >> Special tokens file saved in results/adapters/ag/checkpoint-146500/special_tokens_map.json\n",
      "{'loss': 1.2083, 'learning_rate': 1.8402004454342986e-05, 'epoch': 81.6}\n",
      "{'loss': 1.223, 'learning_rate': 1.8374164810690425e-05, 'epoch': 81.63}\n",
      "{'loss': 1.1797, 'learning_rate': 1.834632516703786e-05, 'epoch': 81.65}\n",
      "{'loss': 1.1953, 'learning_rate': 1.83184855233853e-05, 'epoch': 81.68}\n",
      "{'loss': 1.1723, 'learning_rate': 1.829064587973274e-05, 'epoch': 81.71}\n",
      "{'loss': 1.1489, 'learning_rate': 1.826280623608018e-05, 'epoch': 81.74}\n",
      "{'loss': 1.1694, 'learning_rate': 1.8234966592427618e-05, 'epoch': 81.76}\n",
      "{'loss': 1.1748, 'learning_rate': 1.8207126948775057e-05, 'epoch': 81.79}\n",
      "{'loss': 1.1977, 'learning_rate': 1.8179287305122496e-05, 'epoch': 81.82}\n",
      "{'loss': 1.1977, 'learning_rate': 1.8151447661469935e-05, 'epoch': 81.85}\n",
      " 82% 147000/179600 [11:58:47<2:29:00,  3.65it/s][INFO|trainer.py:1989] 2021-08-02 18:08:47,845 >> Saving model checkpoint to results/adapters/ag/checkpoint-147000\n",
      "[INFO|loading.py:59] 2021-08-02 18:08:47,845 >> Configuration saved in results/adapters/ag/checkpoint-147000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:08:47,857 >> Module weights saved in results/adapters/ag/checkpoint-147000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:08:47,857 >> Configuration saved in results/adapters/ag/checkpoint-147000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:08:48,161 >> Module weights saved in results/adapters/ag/checkpoint-147000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:08:48,162 >> Configuration saved in results/adapters/ag/checkpoint-147000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:08:48,488 >> Module weights saved in results/adapters/ag/checkpoint-147000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:08:48,489 >> tokenizer config file saved in results/adapters/ag/checkpoint-147000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:08:48,489 >> Special tokens file saved in results/adapters/ag/checkpoint-147000/special_tokens_map.json\n",
      "{'loss': 1.2257, 'learning_rate': 1.8123608017817374e-05, 'epoch': 81.88}\n",
      "{'loss': 1.1649, 'learning_rate': 1.8095768374164813e-05, 'epoch': 81.9}\n",
      "{'loss': 1.1728, 'learning_rate': 1.8067928730512253e-05, 'epoch': 81.93}\n",
      "{'loss': 1.1595, 'learning_rate': 1.804008908685969e-05, 'epoch': 81.96}\n",
      "{'loss': 1.1598, 'learning_rate': 1.8012249443207127e-05, 'epoch': 81.99}\n",
      "{'loss': 1.1781, 'learning_rate': 1.7984409799554567e-05, 'epoch': 82.02}\n",
      "{'loss': 1.191, 'learning_rate': 1.7956570155902006e-05, 'epoch': 82.04}\n",
      "{'loss': 1.1692, 'learning_rate': 1.7928730512249445e-05, 'epoch': 82.07}\n",
      "{'loss': 1.1595, 'learning_rate': 1.7900890868596884e-05, 'epoch': 82.1}\n",
      "{'loss': 1.1934, 'learning_rate': 1.787305122494432e-05, 'epoch': 82.13}\n",
      " 82% 147500/179600 [12:01:11<2:27:02,  3.64it/s][INFO|trainer.py:1989] 2021-08-02 18:11:11,585 >> Saving model checkpoint to results/adapters/ag/checkpoint-147500\n",
      "[INFO|loading.py:59] 2021-08-02 18:11:11,586 >> Configuration saved in results/adapters/ag/checkpoint-147500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:11:11,596 >> Module weights saved in results/adapters/ag/checkpoint-147500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:11:11,597 >> Configuration saved in results/adapters/ag/checkpoint-147500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:11:11,889 >> Module weights saved in results/adapters/ag/checkpoint-147500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:11:11,890 >> Configuration saved in results/adapters/ag/checkpoint-147500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:11:12,216 >> Module weights saved in results/adapters/ag/checkpoint-147500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:11:12,217 >> tokenizer config file saved in results/adapters/ag/checkpoint-147500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:11:12,217 >> Special tokens file saved in results/adapters/ag/checkpoint-147500/special_tokens_map.json\n",
      "{'loss': 1.1924, 'learning_rate': 1.784521158129176e-05, 'epoch': 82.15}\n",
      "{'loss': 1.1839, 'learning_rate': 1.7817371937639198e-05, 'epoch': 82.18}\n",
      "{'loss': 1.1688, 'learning_rate': 1.7789532293986637e-05, 'epoch': 82.21}\n",
      "{'loss': 1.1578, 'learning_rate': 1.7761692650334077e-05, 'epoch': 82.24}\n",
      "{'loss': 1.2032, 'learning_rate': 1.7733853006681512e-05, 'epoch': 82.27}\n",
      "{'loss': 1.196, 'learning_rate': 1.7706013363028955e-05, 'epoch': 82.29}\n",
      "{'loss': 1.2038, 'learning_rate': 1.7678173719376394e-05, 'epoch': 82.32}\n",
      "{'loss': 1.2057, 'learning_rate': 1.7650334075723833e-05, 'epoch': 82.35}\n",
      "{'loss': 1.206, 'learning_rate': 1.7622494432071272e-05, 'epoch': 82.38}\n",
      "{'loss': 1.2418, 'learning_rate': 1.759465478841871e-05, 'epoch': 82.41}\n",
      " 82% 148000/179600 [12:03:32<2:25:16,  3.63it/s][INFO|trainer.py:1989] 2021-08-02 18:13:32,781 >> Saving model checkpoint to results/adapters/ag/checkpoint-148000\n",
      "[INFO|loading.py:59] 2021-08-02 18:13:32,782 >> Configuration saved in results/adapters/ag/checkpoint-148000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:13:32,794 >> Module weights saved in results/adapters/ag/checkpoint-148000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:13:32,795 >> Configuration saved in results/adapters/ag/checkpoint-148000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:13:33,093 >> Module weights saved in results/adapters/ag/checkpoint-148000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:13:33,093 >> Configuration saved in results/adapters/ag/checkpoint-148000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:13:33,417 >> Module weights saved in results/adapters/ag/checkpoint-148000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:13:33,417 >> tokenizer config file saved in results/adapters/ag/checkpoint-148000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:13:33,418 >> Special tokens file saved in results/adapters/ag/checkpoint-148000/special_tokens_map.json\n",
      "{'loss': 1.1744, 'learning_rate': 1.7566815144766147e-05, 'epoch': 82.43}\n",
      "{'loss': 1.1694, 'learning_rate': 1.7538975501113586e-05, 'epoch': 82.46}\n",
      "{'loss': 1.1934, 'learning_rate': 1.7511135857461026e-05, 'epoch': 82.49}\n",
      "{'loss': 1.1595, 'learning_rate': 1.7483296213808465e-05, 'epoch': 82.52}\n",
      "{'loss': 1.199, 'learning_rate': 1.7455456570155904e-05, 'epoch': 82.54}\n",
      "{'loss': 1.1895, 'learning_rate': 1.7427616926503343e-05, 'epoch': 82.57}\n",
      "{'loss': 1.1608, 'learning_rate': 1.739977728285078e-05, 'epoch': 82.6}\n",
      "{'loss': 1.1521, 'learning_rate': 1.7371937639198218e-05, 'epoch': 82.63}\n",
      "{'loss': 1.194, 'learning_rate': 1.7344097995545657e-05, 'epoch': 82.66}\n",
      "{'loss': 1.1718, 'learning_rate': 1.7316258351893096e-05, 'epoch': 82.68}\n",
      " 83% 148500/179600 [12:05:54<2:24:55,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 18:15:55,267 >> Saving model checkpoint to results/adapters/ag/checkpoint-148500\n",
      "[INFO|loading.py:59] 2021-08-02 18:15:55,267 >> Configuration saved in results/adapters/ag/checkpoint-148500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:15:55,280 >> Module weights saved in results/adapters/ag/checkpoint-148500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:15:55,281 >> Configuration saved in results/adapters/ag/checkpoint-148500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:15:55,582 >> Module weights saved in results/adapters/ag/checkpoint-148500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:15:55,583 >> Configuration saved in results/adapters/ag/checkpoint-148500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:15:55,925 >> Module weights saved in results/adapters/ag/checkpoint-148500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:15:55,926 >> tokenizer config file saved in results/adapters/ag/checkpoint-148500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:15:55,926 >> Special tokens file saved in results/adapters/ag/checkpoint-148500/special_tokens_map.json\n",
      "{'loss': 1.1477, 'learning_rate': 1.7288418708240535e-05, 'epoch': 82.71}\n",
      "{'loss': 1.1497, 'learning_rate': 1.726057906458797e-05, 'epoch': 82.74}\n",
      "{'loss': 1.1556, 'learning_rate': 1.7232739420935414e-05, 'epoch': 82.77}\n",
      "{'loss': 1.2106, 'learning_rate': 1.7204899777282853e-05, 'epoch': 82.79}\n",
      "{'loss': 1.1767, 'learning_rate': 1.7177060133630292e-05, 'epoch': 82.82}\n",
      "{'loss': 1.1716, 'learning_rate': 1.714922048997773e-05, 'epoch': 82.85}\n",
      "{'loss': 1.1993, 'learning_rate': 1.712138084632517e-05, 'epoch': 82.88}\n",
      "{'loss': 1.1446, 'learning_rate': 1.7093541202672606e-05, 'epoch': 82.91}\n",
      "{'loss': 1.1847, 'learning_rate': 1.7065701559020045e-05, 'epoch': 82.93}\n",
      "{'loss': 1.1486, 'learning_rate': 1.7037861915367484e-05, 'epoch': 82.96}\n",
      " 83% 149000/179600 [12:08:16<2:20:14,  3.64it/s][INFO|trainer.py:1989] 2021-08-02 18:18:16,969 >> Saving model checkpoint to results/adapters/ag/checkpoint-149000\n",
      "[INFO|loading.py:59] 2021-08-02 18:18:16,970 >> Configuration saved in results/adapters/ag/checkpoint-149000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:18:16,981 >> Module weights saved in results/adapters/ag/checkpoint-149000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:18:16,981 >> Configuration saved in results/adapters/ag/checkpoint-149000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:18:17,285 >> Module weights saved in results/adapters/ag/checkpoint-149000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:18:17,286 >> Configuration saved in results/adapters/ag/checkpoint-149000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:18:17,609 >> Module weights saved in results/adapters/ag/checkpoint-149000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:18:17,610 >> tokenizer config file saved in results/adapters/ag/checkpoint-149000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:18:17,610 >> Special tokens file saved in results/adapters/ag/checkpoint-149000/special_tokens_map.json\n",
      "{'loss': 1.1756, 'learning_rate': 1.7010022271714924e-05, 'epoch': 82.99}\n",
      "{'loss': 1.222, 'learning_rate': 1.6982182628062363e-05, 'epoch': 83.02}\n",
      "{'loss': 1.1927, 'learning_rate': 1.6954342984409802e-05, 'epoch': 83.05}\n",
      "{'loss': 1.1581, 'learning_rate': 1.6926503340757238e-05, 'epoch': 83.07}\n",
      "{'loss': 1.1923, 'learning_rate': 1.6898663697104677e-05, 'epoch': 83.1}\n",
      "{'loss': 1.1734, 'learning_rate': 1.6870824053452116e-05, 'epoch': 83.13}\n",
      "{'loss': 1.1662, 'learning_rate': 1.6842984409799555e-05, 'epoch': 83.16}\n",
      "{'loss': 1.1514, 'learning_rate': 1.6815144766146994e-05, 'epoch': 83.18}\n",
      "{'loss': 1.1581, 'learning_rate': 1.678730512249443e-05, 'epoch': 83.21}\n",
      "{'loss': 1.1788, 'learning_rate': 1.6759465478841873e-05, 'epoch': 83.24}\n",
      " 83% 149500/179600 [12:10:38<2:23:43,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 18:20:38,963 >> Saving model checkpoint to results/adapters/ag/checkpoint-149500\n",
      "[INFO|loading.py:59] 2021-08-02 18:20:38,963 >> Configuration saved in results/adapters/ag/checkpoint-149500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:20:38,977 >> Module weights saved in results/adapters/ag/checkpoint-149500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:20:38,978 >> Configuration saved in results/adapters/ag/checkpoint-149500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:20:39,278 >> Module weights saved in results/adapters/ag/checkpoint-149500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:20:39,278 >> Configuration saved in results/adapters/ag/checkpoint-149500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:20:39,622 >> Module weights saved in results/adapters/ag/checkpoint-149500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:20:39,623 >> tokenizer config file saved in results/adapters/ag/checkpoint-149500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:20:39,623 >> Special tokens file saved in results/adapters/ag/checkpoint-149500/special_tokens_map.json\n",
      "{'loss': 1.1878, 'learning_rate': 1.6731625835189312e-05, 'epoch': 83.27}\n",
      "{'loss': 1.1969, 'learning_rate': 1.670378619153675e-05, 'epoch': 83.3}\n",
      "{'loss': 1.1903, 'learning_rate': 1.667594654788419e-05, 'epoch': 83.32}\n",
      "{'loss': 1.1727, 'learning_rate': 1.664810690423163e-05, 'epoch': 83.35}\n",
      "{'loss': 1.1744, 'learning_rate': 1.6620267260579065e-05, 'epoch': 83.38}\n",
      "{'loss': 1.1818, 'learning_rate': 1.6592427616926504e-05, 'epoch': 83.41}\n",
      "{'loss': 1.176, 'learning_rate': 1.6564587973273943e-05, 'epoch': 83.44}\n",
      "{'loss': 1.1721, 'learning_rate': 1.6536748329621382e-05, 'epoch': 83.46}\n",
      "{'loss': 1.1853, 'learning_rate': 1.650890868596882e-05, 'epoch': 83.49}\n",
      "{'loss': 1.1835, 'learning_rate': 1.6481069042316257e-05, 'epoch': 83.52}\n",
      " 84% 150000/179600 [12:13:00<2:21:50,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 18:23:01,072 >> Saving model checkpoint to results/adapters/ag/checkpoint-150000\n",
      "[INFO|loading.py:59] 2021-08-02 18:23:01,072 >> Configuration saved in results/adapters/ag/checkpoint-150000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:23:01,084 >> Module weights saved in results/adapters/ag/checkpoint-150000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:23:01,084 >> Configuration saved in results/adapters/ag/checkpoint-150000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:23:01,449 >> Module weights saved in results/adapters/ag/checkpoint-150000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:23:01,450 >> Configuration saved in results/adapters/ag/checkpoint-150000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:23:01,789 >> Module weights saved in results/adapters/ag/checkpoint-150000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:23:01,789 >> tokenizer config file saved in results/adapters/ag/checkpoint-150000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:23:01,789 >> Special tokens file saved in results/adapters/ag/checkpoint-150000/special_tokens_map.json\n",
      "{'loss': 1.146, 'learning_rate': 1.6453229398663696e-05, 'epoch': 83.55}\n",
      "{'loss': 1.1564, 'learning_rate': 1.6425389755011136e-05, 'epoch': 83.57}\n",
      "{'loss': 1.2154, 'learning_rate': 1.6397550111358575e-05, 'epoch': 83.6}\n",
      "{'loss': 1.2061, 'learning_rate': 1.6369710467706014e-05, 'epoch': 83.63}\n",
      "{'loss': 1.1709, 'learning_rate': 1.6341870824053453e-05, 'epoch': 83.66}\n",
      "{'loss': 1.1892, 'learning_rate': 1.631403118040089e-05, 'epoch': 83.69}\n",
      "{'loss': 1.161, 'learning_rate': 1.628619153674833e-05, 'epoch': 83.71}\n",
      "{'loss': 1.1806, 'learning_rate': 1.625835189309577e-05, 'epoch': 83.74}\n",
      "{'loss': 1.1371, 'learning_rate': 1.623051224944321e-05, 'epoch': 83.77}\n",
      "{'loss': 1.1666, 'learning_rate': 1.620267260579065e-05, 'epoch': 83.8}\n",
      " 84% 150500/179600 [12:15:23<2:26:18,  3.31it/s][INFO|trainer.py:1989] 2021-08-02 18:25:24,211 >> Saving model checkpoint to results/adapters/ag/checkpoint-150500\n",
      "[INFO|loading.py:59] 2021-08-02 18:25:24,211 >> Configuration saved in results/adapters/ag/checkpoint-150500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:25:24,227 >> Module weights saved in results/adapters/ag/checkpoint-150500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:25:24,227 >> Configuration saved in results/adapters/ag/checkpoint-150500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:25:24,538 >> Module weights saved in results/adapters/ag/checkpoint-150500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:25:24,539 >> Configuration saved in results/adapters/ag/checkpoint-150500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:25:24,891 >> Module weights saved in results/adapters/ag/checkpoint-150500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:25:24,892 >> tokenizer config file saved in results/adapters/ag/checkpoint-150500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:25:24,892 >> Special tokens file saved in results/adapters/ag/checkpoint-150500/special_tokens_map.json\n",
      "{'loss': 1.1916, 'learning_rate': 1.6174832962138088e-05, 'epoch': 83.82}\n",
      "{'loss': 1.2049, 'learning_rate': 1.6146993318485524e-05, 'epoch': 83.85}\n",
      "{'loss': 1.1503, 'learning_rate': 1.6119153674832963e-05, 'epoch': 83.88}\n",
      "{'loss': 1.1801, 'learning_rate': 1.6091314031180402e-05, 'epoch': 83.91}\n",
      "{'loss': 1.1686, 'learning_rate': 1.606347438752784e-05, 'epoch': 83.94}\n",
      "{'loss': 1.1988, 'learning_rate': 1.603563474387528e-05, 'epoch': 83.96}\n",
      "{'loss': 1.1795, 'learning_rate': 1.6007795100222716e-05, 'epoch': 83.99}\n",
      "{'loss': 1.2044, 'learning_rate': 1.5979955456570155e-05, 'epoch': 84.02}\n",
      "{'loss': 1.1972, 'learning_rate': 1.5952115812917594e-05, 'epoch': 84.05}\n",
      "{'loss': 1.1914, 'learning_rate': 1.5924276169265034e-05, 'epoch': 84.08}\n",
      " 84% 151000/179600 [12:17:48<2:15:54,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 18:27:48,844 >> Saving model checkpoint to results/adapters/ag/checkpoint-151000\n",
      "[INFO|loading.py:59] 2021-08-02 18:27:48,845 >> Configuration saved in results/adapters/ag/checkpoint-151000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:27:48,858 >> Module weights saved in results/adapters/ag/checkpoint-151000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:27:48,858 >> Configuration saved in results/adapters/ag/checkpoint-151000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:27:49,175 >> Module weights saved in results/adapters/ag/checkpoint-151000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:27:49,175 >> Configuration saved in results/adapters/ag/checkpoint-151000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:27:49,523 >> Module weights saved in results/adapters/ag/checkpoint-151000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:27:49,523 >> tokenizer config file saved in results/adapters/ag/checkpoint-151000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:27:49,524 >> Special tokens file saved in results/adapters/ag/checkpoint-151000/special_tokens_map.json\n",
      "{'loss': 1.2046, 'learning_rate': 1.5896436525612473e-05, 'epoch': 84.1}\n",
      "{'loss': 1.1792, 'learning_rate': 1.5868596881959912e-05, 'epoch': 84.13}\n",
      "{'loss': 1.1749, 'learning_rate': 1.5840757238307348e-05, 'epoch': 84.16}\n",
      "{'loss': 1.17, 'learning_rate': 1.581291759465479e-05, 'epoch': 84.19}\n",
      "{'loss': 1.1545, 'learning_rate': 1.578507795100223e-05, 'epoch': 84.21}\n",
      "{'loss': 1.1832, 'learning_rate': 1.575723830734967e-05, 'epoch': 84.24}\n",
      "{'loss': 1.1555, 'learning_rate': 1.5729398663697108e-05, 'epoch': 84.27}\n",
      "{'loss': 1.1512, 'learning_rate': 1.5701559020044543e-05, 'epoch': 84.3}\n",
      "{'loss': 1.1882, 'learning_rate': 1.5673719376391983e-05, 'epoch': 84.33}\n",
      "{'loss': 1.1826, 'learning_rate': 1.5645879732739422e-05, 'epoch': 84.35}\n",
      " 84% 151500/179600 [12:20:12<2:23:38,  3.26it/s][INFO|trainer.py:1989] 2021-08-02 18:30:13,093 >> Saving model checkpoint to results/adapters/ag/checkpoint-151500\n",
      "[INFO|loading.py:59] 2021-08-02 18:30:13,094 >> Configuration saved in results/adapters/ag/checkpoint-151500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:30:13,106 >> Module weights saved in results/adapters/ag/checkpoint-151500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:30:13,106 >> Configuration saved in results/adapters/ag/checkpoint-151500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:30:13,414 >> Module weights saved in results/adapters/ag/checkpoint-151500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:30:13,414 >> Configuration saved in results/adapters/ag/checkpoint-151500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:30:13,757 >> Module weights saved in results/adapters/ag/checkpoint-151500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:30:13,757 >> tokenizer config file saved in results/adapters/ag/checkpoint-151500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:30:13,758 >> Special tokens file saved in results/adapters/ag/checkpoint-151500/special_tokens_map.json\n",
      "{'loss': 1.1782, 'learning_rate': 1.561804008908686e-05, 'epoch': 84.38}\n",
      "{'loss': 1.1643, 'learning_rate': 1.55902004454343e-05, 'epoch': 84.41}\n",
      "{'loss': 1.1724, 'learning_rate': 1.556236080178174e-05, 'epoch': 84.44}\n",
      "{'loss': 1.1763, 'learning_rate': 1.5534521158129175e-05, 'epoch': 84.47}\n",
      "{'loss': 1.1829, 'learning_rate': 1.5506681514476614e-05, 'epoch': 84.49}\n",
      "{'loss': 1.2049, 'learning_rate': 1.5478841870824053e-05, 'epoch': 84.52}\n",
      "{'loss': 1.1634, 'learning_rate': 1.5451002227171493e-05, 'epoch': 84.55}\n",
      "{'loss': 1.1879, 'learning_rate': 1.542316258351893e-05, 'epoch': 84.58}\n",
      "{'loss': 1.1734, 'learning_rate': 1.539532293986637e-05, 'epoch': 84.6}\n",
      "{'loss': 1.1893, 'learning_rate': 1.5367483296213807e-05, 'epoch': 84.63}\n",
      " 85% 152000/179600 [12:22:35<2:14:12,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 18:32:35,934 >> Saving model checkpoint to results/adapters/ag/checkpoint-152000\n",
      "[INFO|loading.py:59] 2021-08-02 18:32:35,935 >> Configuration saved in results/adapters/ag/checkpoint-152000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:32:35,950 >> Module weights saved in results/adapters/ag/checkpoint-152000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:32:35,950 >> Configuration saved in results/adapters/ag/checkpoint-152000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:32:36,250 >> Module weights saved in results/adapters/ag/checkpoint-152000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:32:36,251 >> Configuration saved in results/adapters/ag/checkpoint-152000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:32:36,578 >> Module weights saved in results/adapters/ag/checkpoint-152000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:32:36,578 >> tokenizer config file saved in results/adapters/ag/checkpoint-152000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:32:36,579 >> Special tokens file saved in results/adapters/ag/checkpoint-152000/special_tokens_map.json\n",
      "{'loss': 1.1404, 'learning_rate': 1.533964365256125e-05, 'epoch': 84.66}\n",
      "{'loss': 1.1795, 'learning_rate': 1.5311804008908688e-05, 'epoch': 84.69}\n",
      "{'loss': 1.1802, 'learning_rate': 1.5283964365256127e-05, 'epoch': 84.72}\n",
      "{'loss': 1.1516, 'learning_rate': 1.5256124721603565e-05, 'epoch': 84.74}\n",
      "{'loss': 1.1732, 'learning_rate': 1.5228285077951002e-05, 'epoch': 84.77}\n",
      "{'loss': 1.1321, 'learning_rate': 1.5200445434298442e-05, 'epoch': 84.8}\n",
      "{'loss': 1.1782, 'learning_rate': 1.517260579064588e-05, 'epoch': 84.83}\n",
      "{'loss': 1.1665, 'learning_rate': 1.514476614699332e-05, 'epoch': 84.85}\n",
      "{'loss': 1.1929, 'learning_rate': 1.5116926503340759e-05, 'epoch': 84.88}\n",
      "{'loss': 1.1612, 'learning_rate': 1.5089086859688198e-05, 'epoch': 84.91}\n",
      " 85% 152500/179600 [12:24:59<2:10:24,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 18:34:59,717 >> Saving model checkpoint to results/adapters/ag/checkpoint-152500\n",
      "[INFO|loading.py:59] 2021-08-02 18:34:59,718 >> Configuration saved in results/adapters/ag/checkpoint-152500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:34:59,729 >> Module weights saved in results/adapters/ag/checkpoint-152500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:34:59,730 >> Configuration saved in results/adapters/ag/checkpoint-152500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:35:00,032 >> Module weights saved in results/adapters/ag/checkpoint-152500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:35:00,033 >> Configuration saved in results/adapters/ag/checkpoint-152500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:35:00,387 >> Module weights saved in results/adapters/ag/checkpoint-152500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:35:00,387 >> tokenizer config file saved in results/adapters/ag/checkpoint-152500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:35:00,388 >> Special tokens file saved in results/adapters/ag/checkpoint-152500/special_tokens_map.json\n",
      "{'loss': 1.1495, 'learning_rate': 1.5061247216035634e-05, 'epoch': 84.94}\n",
      "{'loss': 1.1858, 'learning_rate': 1.5033407572383073e-05, 'epoch': 84.97}\n",
      "{'loss': 1.1382, 'learning_rate': 1.5005567928730512e-05, 'epoch': 84.99}\n",
      "{'loss': 1.1852, 'learning_rate': 1.4977728285077951e-05, 'epoch': 85.02}\n",
      "{'loss': 1.1837, 'learning_rate': 1.494988864142539e-05, 'epoch': 85.05}\n",
      "{'loss': 1.1555, 'learning_rate': 1.4922048997772831e-05, 'epoch': 85.08}\n",
      "{'loss': 1.1637, 'learning_rate': 1.4894209354120267e-05, 'epoch': 85.11}\n",
      "{'loss': 1.1685, 'learning_rate': 1.4866369710467706e-05, 'epoch': 85.13}\n",
      "{'loss': 1.1732, 'learning_rate': 1.4838530066815145e-05, 'epoch': 85.16}\n",
      "{'loss': 1.1541, 'learning_rate': 1.4810690423162585e-05, 'epoch': 85.19}\n",
      " 85% 153000/179600 [12:27:26<2:03:39,  3.58it/s][INFO|trainer.py:1989] 2021-08-02 18:37:26,953 >> Saving model checkpoint to results/adapters/ag/checkpoint-153000\n",
      "[INFO|loading.py:59] 2021-08-02 18:37:26,954 >> Configuration saved in results/adapters/ag/checkpoint-153000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:37:26,966 >> Module weights saved in results/adapters/ag/checkpoint-153000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:37:26,966 >> Configuration saved in results/adapters/ag/checkpoint-153000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:37:27,260 >> Module weights saved in results/adapters/ag/checkpoint-153000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:37:27,260 >> Configuration saved in results/adapters/ag/checkpoint-153000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:37:27,592 >> Module weights saved in results/adapters/ag/checkpoint-153000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:37:27,592 >> tokenizer config file saved in results/adapters/ag/checkpoint-153000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:37:27,593 >> Special tokens file saved in results/adapters/ag/checkpoint-153000/special_tokens_map.json\n",
      "{'loss': 1.2177, 'learning_rate': 1.4782850779510024e-05, 'epoch': 85.22}\n",
      "{'loss': 1.1759, 'learning_rate': 1.4755011135857461e-05, 'epoch': 85.24}\n",
      "{'loss': 1.168, 'learning_rate': 1.47271714922049e-05, 'epoch': 85.27}\n",
      "{'loss': 1.1919, 'learning_rate': 1.469933184855234e-05, 'epoch': 85.3}\n",
      "{'loss': 1.1892, 'learning_rate': 1.4671492204899779e-05, 'epoch': 85.33}\n",
      "{'loss': 1.1525, 'learning_rate': 1.4643652561247218e-05, 'epoch': 85.36}\n",
      "{'loss': 1.169, 'learning_rate': 1.4615812917594657e-05, 'epoch': 85.38}\n",
      "{'loss': 1.1726, 'learning_rate': 1.4587973273942093e-05, 'epoch': 85.41}\n",
      "{'loss': 1.1825, 'learning_rate': 1.4560133630289532e-05, 'epoch': 85.44}\n",
      "{'loss': 1.1863, 'learning_rate': 1.4532293986636971e-05, 'epoch': 85.47}\n",
      " 85% 153500/179600 [12:29:50<2:04:14,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 18:39:50,456 >> Saving model checkpoint to results/adapters/ag/checkpoint-153500\n",
      "[INFO|loading.py:59] 2021-08-02 18:39:50,456 >> Configuration saved in results/adapters/ag/checkpoint-153500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:39:50,468 >> Module weights saved in results/adapters/ag/checkpoint-153500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:39:50,469 >> Configuration saved in results/adapters/ag/checkpoint-153500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:39:50,776 >> Module weights saved in results/adapters/ag/checkpoint-153500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:39:50,776 >> Configuration saved in results/adapters/ag/checkpoint-153500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:39:51,132 >> Module weights saved in results/adapters/ag/checkpoint-153500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:39:51,133 >> tokenizer config file saved in results/adapters/ag/checkpoint-153500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:39:51,133 >> Special tokens file saved in results/adapters/ag/checkpoint-153500/special_tokens_map.json\n",
      "{'loss': 1.1439, 'learning_rate': 1.450445434298441e-05, 'epoch': 85.5}\n",
      "{'loss': 1.2011, 'learning_rate': 1.447661469933185e-05, 'epoch': 85.52}\n",
      "{'loss': 1.1431, 'learning_rate': 1.4448775055679287e-05, 'epoch': 85.55}\n",
      "{'loss': 1.1912, 'learning_rate': 1.4420935412026726e-05, 'epoch': 85.58}\n",
      "{'loss': 1.1783, 'learning_rate': 1.4393095768374165e-05, 'epoch': 85.61}\n",
      "{'loss': 1.176, 'learning_rate': 1.4365256124721604e-05, 'epoch': 85.63}\n",
      "{'loss': 1.1492, 'learning_rate': 1.4337416481069043e-05, 'epoch': 85.66}\n",
      "{'loss': 1.1827, 'learning_rate': 1.4309576837416483e-05, 'epoch': 85.69}\n",
      "{'loss': 1.1877, 'learning_rate': 1.428173719376392e-05, 'epoch': 85.72}\n",
      "{'loss': 1.1584, 'learning_rate': 1.425389755011136e-05, 'epoch': 85.75}\n",
      " 86% 154000/179600 [12:32:16<2:09:09,  3.30it/s][INFO|trainer.py:1989] 2021-08-02 18:42:16,570 >> Saving model checkpoint to results/adapters/ag/checkpoint-154000\n",
      "[INFO|loading.py:59] 2021-08-02 18:42:16,571 >> Configuration saved in results/adapters/ag/checkpoint-154000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:42:16,584 >> Module weights saved in results/adapters/ag/checkpoint-154000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:42:16,584 >> Configuration saved in results/adapters/ag/checkpoint-154000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:42:16,902 >> Module weights saved in results/adapters/ag/checkpoint-154000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:42:16,902 >> Configuration saved in results/adapters/ag/checkpoint-154000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:42:17,261 >> Module weights saved in results/adapters/ag/checkpoint-154000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:42:17,262 >> tokenizer config file saved in results/adapters/ag/checkpoint-154000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:42:17,262 >> Special tokens file saved in results/adapters/ag/checkpoint-154000/special_tokens_map.json\n",
      "{'loss': 1.1953, 'learning_rate': 1.4226057906458798e-05, 'epoch': 85.77}\n",
      "{'loss': 1.1854, 'learning_rate': 1.4198218262806238e-05, 'epoch': 85.8}\n",
      "{'loss': 1.1546, 'learning_rate': 1.4170378619153677e-05, 'epoch': 85.83}\n",
      "{'loss': 1.1628, 'learning_rate': 1.4142538975501116e-05, 'epoch': 85.86}\n",
      "{'loss': 1.1935, 'learning_rate': 1.4114699331848552e-05, 'epoch': 85.88}\n",
      "{'loss': 1.1812, 'learning_rate': 1.408685968819599e-05, 'epoch': 85.91}\n",
      "{'loss': 1.1884, 'learning_rate': 1.405902004454343e-05, 'epoch': 85.94}\n",
      "{'loss': 1.1589, 'learning_rate': 1.4031180400890869e-05, 'epoch': 85.97}\n",
      "{'loss': 1.1733, 'learning_rate': 1.4003340757238308e-05, 'epoch': 86.0}\n",
      "{'loss': 1.1707, 'learning_rate': 1.3975501113585746e-05, 'epoch': 86.02}\n",
      " 86% 154500/179600 [12:34:41<1:56:22,  3.59it/s][INFO|trainer.py:1989] 2021-08-02 18:44:42,222 >> Saving model checkpoint to results/adapters/ag/checkpoint-154500\n",
      "[INFO|loading.py:59] 2021-08-02 18:44:42,223 >> Configuration saved in results/adapters/ag/checkpoint-154500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:44:42,238 >> Module weights saved in results/adapters/ag/checkpoint-154500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:44:42,238 >> Configuration saved in results/adapters/ag/checkpoint-154500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:44:42,533 >> Module weights saved in results/adapters/ag/checkpoint-154500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:44:42,533 >> Configuration saved in results/adapters/ag/checkpoint-154500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:44:42,863 >> Module weights saved in results/adapters/ag/checkpoint-154500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:44:42,864 >> tokenizer config file saved in results/adapters/ag/checkpoint-154500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:44:42,864 >> Special tokens file saved in results/adapters/ag/checkpoint-154500/special_tokens_map.json\n",
      "{'loss': 1.1875, 'learning_rate': 1.3947661469933185e-05, 'epoch': 86.05}\n",
      "{'loss': 1.1941, 'learning_rate': 1.3919821826280624e-05, 'epoch': 86.08}\n",
      "{'loss': 1.1794, 'learning_rate': 1.3891982182628063e-05, 'epoch': 86.11}\n",
      "{'loss': 1.1687, 'learning_rate': 1.3864142538975502e-05, 'epoch': 86.14}\n",
      "{'loss': 1.1684, 'learning_rate': 1.3836302895322941e-05, 'epoch': 86.16}\n",
      "{'loss': 1.1524, 'learning_rate': 1.3808463251670379e-05, 'epoch': 86.19}\n",
      "{'loss': 1.1948, 'learning_rate': 1.3780623608017818e-05, 'epoch': 86.22}\n",
      "{'loss': 1.1776, 'learning_rate': 1.3752783964365257e-05, 'epoch': 86.25}\n",
      "{'loss': 1.198, 'learning_rate': 1.3724944320712696e-05, 'epoch': 86.27}\n",
      "{'loss': 1.1845, 'learning_rate': 1.3697104677060136e-05, 'epoch': 86.3}\n",
      " 86% 155000/179600 [12:37:06<2:00:24,  3.40it/s][INFO|trainer.py:1989] 2021-08-02 18:47:06,902 >> Saving model checkpoint to results/adapters/ag/checkpoint-155000\n",
      "[INFO|loading.py:59] 2021-08-02 18:47:06,902 >> Configuration saved in results/adapters/ag/checkpoint-155000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:47:06,914 >> Module weights saved in results/adapters/ag/checkpoint-155000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:47:06,915 >> Configuration saved in results/adapters/ag/checkpoint-155000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:47:07,230 >> Module weights saved in results/adapters/ag/checkpoint-155000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:47:07,231 >> Configuration saved in results/adapters/ag/checkpoint-155000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:47:07,580 >> Module weights saved in results/adapters/ag/checkpoint-155000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:47:07,581 >> tokenizer config file saved in results/adapters/ag/checkpoint-155000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:47:07,581 >> Special tokens file saved in results/adapters/ag/checkpoint-155000/special_tokens_map.json\n",
      "{'loss': 1.195, 'learning_rate': 1.3669265033407571e-05, 'epoch': 86.33}\n",
      "{'loss': 1.1449, 'learning_rate': 1.364142538975501e-05, 'epoch': 86.36}\n",
      "{'loss': 1.1807, 'learning_rate': 1.361358574610245e-05, 'epoch': 86.39}\n",
      "{'loss': 1.1906, 'learning_rate': 1.3585746102449889e-05, 'epoch': 86.41}\n",
      "{'loss': 1.1711, 'learning_rate': 1.3557906458797328e-05, 'epoch': 86.44}\n",
      "{'loss': 1.1424, 'learning_rate': 1.3530066815144767e-05, 'epoch': 86.47}\n",
      "{'loss': 1.1922, 'learning_rate': 1.3502227171492205e-05, 'epoch': 86.5}\n",
      "{'loss': 1.1864, 'learning_rate': 1.3474387527839644e-05, 'epoch': 86.53}\n",
      "{'loss': 1.1637, 'learning_rate': 1.3446547884187083e-05, 'epoch': 86.55}\n",
      "{'loss': 1.1774, 'learning_rate': 1.3418708240534522e-05, 'epoch': 86.58}\n",
      " 87% 155500/179600 [12:39:31<1:47:29,  3.74it/s][INFO|trainer.py:1989] 2021-08-02 18:49:31,741 >> Saving model checkpoint to results/adapters/ag/checkpoint-155500\n",
      "[INFO|loading.py:59] 2021-08-02 18:49:31,741 >> Configuration saved in results/adapters/ag/checkpoint-155500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:49:31,752 >> Module weights saved in results/adapters/ag/checkpoint-155500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:49:31,752 >> Configuration saved in results/adapters/ag/checkpoint-155500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:49:32,043 >> Module weights saved in results/adapters/ag/checkpoint-155500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:49:32,044 >> Configuration saved in results/adapters/ag/checkpoint-155500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:49:32,374 >> Module weights saved in results/adapters/ag/checkpoint-155500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:49:32,375 >> tokenizer config file saved in results/adapters/ag/checkpoint-155500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:49:32,375 >> Special tokens file saved in results/adapters/ag/checkpoint-155500/special_tokens_map.json\n",
      "{'loss': 1.1701, 'learning_rate': 1.3390868596881961e-05, 'epoch': 86.61}\n",
      "{'loss': 1.16, 'learning_rate': 1.33630289532294e-05, 'epoch': 86.64}\n",
      "{'loss': 1.1982, 'learning_rate': 1.3335189309576838e-05, 'epoch': 86.66}\n",
      "{'loss': 1.2032, 'learning_rate': 1.3307349665924277e-05, 'epoch': 86.69}\n",
      "{'loss': 1.1614, 'learning_rate': 1.3279510022271716e-05, 'epoch': 86.72}\n",
      "{'loss': 1.167, 'learning_rate': 1.3251670378619155e-05, 'epoch': 86.75}\n",
      "{'loss': 1.1874, 'learning_rate': 1.3223830734966594e-05, 'epoch': 86.78}\n",
      "{'loss': 1.1308, 'learning_rate': 1.319599109131403e-05, 'epoch': 86.8}\n",
      "{'loss': 1.1559, 'learning_rate': 1.316815144766147e-05, 'epoch': 86.83}\n",
      "{'loss': 1.185, 'learning_rate': 1.3140311804008909e-05, 'epoch': 86.86}\n",
      " 87% 156000/179600 [12:41:56<1:53:06,  3.48it/s][INFO|trainer.py:1989] 2021-08-02 18:51:56,622 >> Saving model checkpoint to results/adapters/ag/checkpoint-156000\n",
      "[INFO|loading.py:59] 2021-08-02 18:51:56,623 >> Configuration saved in results/adapters/ag/checkpoint-156000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:51:56,638 >> Module weights saved in results/adapters/ag/checkpoint-156000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:51:56,638 >> Configuration saved in results/adapters/ag/checkpoint-156000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:51:56,943 >> Module weights saved in results/adapters/ag/checkpoint-156000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:51:56,944 >> Configuration saved in results/adapters/ag/checkpoint-156000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:51:57,286 >> Module weights saved in results/adapters/ag/checkpoint-156000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:51:57,286 >> tokenizer config file saved in results/adapters/ag/checkpoint-156000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:51:57,287 >> Special tokens file saved in results/adapters/ag/checkpoint-156000/special_tokens_map.json\n",
      "{'loss': 1.195, 'learning_rate': 1.3112472160356348e-05, 'epoch': 86.89}\n",
      "{'loss': 1.193, 'learning_rate': 1.3084632516703787e-05, 'epoch': 86.91}\n",
      "{'loss': 1.1393, 'learning_rate': 1.3056792873051226e-05, 'epoch': 86.94}\n",
      "{'loss': 1.1729, 'learning_rate': 1.3028953229398663e-05, 'epoch': 86.97}\n",
      "{'loss': 1.1952, 'learning_rate': 1.3001113585746103e-05, 'epoch': 87.0}\n",
      "{'loss': 1.1756, 'learning_rate': 1.2973273942093542e-05, 'epoch': 87.03}\n",
      "{'loss': 1.1539, 'learning_rate': 1.2945434298440981e-05, 'epoch': 87.05}\n",
      "{'loss': 1.1708, 'learning_rate': 1.291759465478842e-05, 'epoch': 87.08}\n",
      "{'loss': 1.164, 'learning_rate': 1.2889755011135858e-05, 'epoch': 87.11}\n",
      "{'loss': 1.1819, 'learning_rate': 1.2861915367483297e-05, 'epoch': 87.14}\n",
      " 87% 156500/179600 [12:44:22<1:48:23,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 18:54:22,679 >> Saving model checkpoint to results/adapters/ag/checkpoint-156500\n",
      "[INFO|loading.py:59] 2021-08-02 18:54:22,679 >> Configuration saved in results/adapters/ag/checkpoint-156500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:54:22,693 >> Module weights saved in results/adapters/ag/checkpoint-156500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:54:22,693 >> Configuration saved in results/adapters/ag/checkpoint-156500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:54:23,009 >> Module weights saved in results/adapters/ag/checkpoint-156500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:54:23,010 >> Configuration saved in results/adapters/ag/checkpoint-156500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:54:23,343 >> Module weights saved in results/adapters/ag/checkpoint-156500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:54:23,343 >> tokenizer config file saved in results/adapters/ag/checkpoint-156500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:54:23,344 >> Special tokens file saved in results/adapters/ag/checkpoint-156500/special_tokens_map.json\n",
      "{'loss': 1.1878, 'learning_rate': 1.2834075723830736e-05, 'epoch': 87.17}\n",
      "{'loss': 1.1849, 'learning_rate': 1.2806236080178175e-05, 'epoch': 87.19}\n",
      "{'loss': 1.1723, 'learning_rate': 1.2778396436525614e-05, 'epoch': 87.22}\n",
      "{'loss': 1.2012, 'learning_rate': 1.2750556792873053e-05, 'epoch': 87.25}\n",
      "{'loss': 1.163, 'learning_rate': 1.2722717149220489e-05, 'epoch': 87.28}\n",
      "{'loss': 1.1682, 'learning_rate': 1.2694877505567928e-05, 'epoch': 87.3}\n",
      "{'loss': 1.1822, 'learning_rate': 1.2667037861915367e-05, 'epoch': 87.33}\n",
      "{'loss': 1.1575, 'learning_rate': 1.2639198218262807e-05, 'epoch': 87.36}\n",
      "{'loss': 1.1836, 'learning_rate': 1.2611358574610246e-05, 'epoch': 87.39}\n",
      "{'loss': 1.202, 'learning_rate': 1.2583518930957685e-05, 'epoch': 87.42}\n",
      " 87% 157000/179600 [12:46:48<1:46:42,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 18:56:48,914 >> Saving model checkpoint to results/adapters/ag/checkpoint-157000\n",
      "[INFO|loading.py:59] 2021-08-02 18:56:48,915 >> Configuration saved in results/adapters/ag/checkpoint-157000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:56:48,927 >> Module weights saved in results/adapters/ag/checkpoint-157000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:56:48,927 >> Configuration saved in results/adapters/ag/checkpoint-157000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:56:49,224 >> Module weights saved in results/adapters/ag/checkpoint-157000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:56:49,224 >> Configuration saved in results/adapters/ag/checkpoint-157000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:56:49,559 >> Module weights saved in results/adapters/ag/checkpoint-157000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:56:49,559 >> tokenizer config file saved in results/adapters/ag/checkpoint-157000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:56:49,560 >> Special tokens file saved in results/adapters/ag/checkpoint-157000/special_tokens_map.json\n",
      "{'loss': 1.1544, 'learning_rate': 1.2555679287305122e-05, 'epoch': 87.44}\n",
      "{'loss': 1.1814, 'learning_rate': 1.2527839643652561e-05, 'epoch': 87.47}\n",
      "{'loss': 1.1708, 'learning_rate': 1.25e-05, 'epoch': 87.5}\n",
      "{'loss': 1.1728, 'learning_rate': 1.247216035634744e-05, 'epoch': 87.53}\n",
      "{'loss': 1.1812, 'learning_rate': 1.2444320712694877e-05, 'epoch': 87.56}\n",
      "{'loss': 1.1799, 'learning_rate': 1.2416481069042316e-05, 'epoch': 87.58}\n",
      "{'loss': 1.1603, 'learning_rate': 1.2388641425389756e-05, 'epoch': 87.61}\n",
      "{'loss': 1.1755, 'learning_rate': 1.2360801781737195e-05, 'epoch': 87.64}\n",
      "{'loss': 1.1991, 'learning_rate': 1.2332962138084634e-05, 'epoch': 87.67}\n",
      "{'loss': 1.2073, 'learning_rate': 1.2305122494432073e-05, 'epoch': 87.69}\n",
      " 88% 157500/179600 [12:49:15<1:51:29,  3.30it/s][INFO|trainer.py:1989] 2021-08-02 18:59:15,754 >> Saving model checkpoint to results/adapters/ag/checkpoint-157500\n",
      "[INFO|loading.py:59] 2021-08-02 18:59:15,754 >> Configuration saved in results/adapters/ag/checkpoint-157500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:59:15,768 >> Module weights saved in results/adapters/ag/checkpoint-157500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:59:15,768 >> Configuration saved in results/adapters/ag/checkpoint-157500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:59:16,089 >> Module weights saved in results/adapters/ag/checkpoint-157500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 18:59:16,089 >> Configuration saved in results/adapters/ag/checkpoint-157500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 18:59:16,432 >> Module weights saved in results/adapters/ag/checkpoint-157500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 18:59:16,433 >> tokenizer config file saved in results/adapters/ag/checkpoint-157500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 18:59:16,433 >> Special tokens file saved in results/adapters/ag/checkpoint-157500/special_tokens_map.json\n",
      "{'loss': 1.1955, 'learning_rate': 1.227728285077951e-05, 'epoch': 87.72}\n",
      "{'loss': 1.1721, 'learning_rate': 1.224944320712695e-05, 'epoch': 87.75}\n",
      "{'loss': 1.1587, 'learning_rate': 1.2221603563474387e-05, 'epoch': 87.78}\n",
      "{'loss': 1.1858, 'learning_rate': 1.2193763919821826e-05, 'epoch': 87.81}\n",
      "{'loss': 1.1908, 'learning_rate': 1.2165924276169265e-05, 'epoch': 87.83}\n",
      "{'loss': 1.1903, 'learning_rate': 1.2138084632516705e-05, 'epoch': 87.86}\n",
      "{'loss': 1.1822, 'learning_rate': 1.2110244988864144e-05, 'epoch': 87.89}\n",
      "{'loss': 1.1495, 'learning_rate': 1.2082405345211583e-05, 'epoch': 87.92}\n",
      "{'loss': 1.1638, 'learning_rate': 1.205456570155902e-05, 'epoch': 87.94}\n",
      "{'loss': 1.1761, 'learning_rate': 1.202672605790646e-05, 'epoch': 87.97}\n",
      " 88% 158000/179600 [12:51:43<1:41:25,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 19:01:43,810 >> Saving model checkpoint to results/adapters/ag/checkpoint-158000\n",
      "[INFO|loading.py:59] 2021-08-02 19:01:43,811 >> Configuration saved in results/adapters/ag/checkpoint-158000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:01:43,825 >> Module weights saved in results/adapters/ag/checkpoint-158000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:01:43,825 >> Configuration saved in results/adapters/ag/checkpoint-158000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:01:44,124 >> Module weights saved in results/adapters/ag/checkpoint-158000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:01:44,124 >> Configuration saved in results/adapters/ag/checkpoint-158000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:01:44,454 >> Module weights saved in results/adapters/ag/checkpoint-158000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:01:44,455 >> tokenizer config file saved in results/adapters/ag/checkpoint-158000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:01:44,455 >> Special tokens file saved in results/adapters/ag/checkpoint-158000/special_tokens_map.json\n",
      "{'loss': 1.1891, 'learning_rate': 1.1998886414253899e-05, 'epoch': 88.0}\n",
      "{'loss': 1.1742, 'learning_rate': 1.1971046770601336e-05, 'epoch': 88.03}\n",
      "{'loss': 1.188, 'learning_rate': 1.1943207126948775e-05, 'epoch': 88.06}\n",
      "{'loss': 1.1623, 'learning_rate': 1.1915367483296214e-05, 'epoch': 88.08}\n",
      "{'loss': 1.1875, 'learning_rate': 1.1887527839643654e-05, 'epoch': 88.11}\n",
      "{'loss': 1.1905, 'learning_rate': 1.1859688195991093e-05, 'epoch': 88.14}\n",
      "{'loss': 1.161, 'learning_rate': 1.183184855233853e-05, 'epoch': 88.17}\n",
      "{'loss': 1.1514, 'learning_rate': 1.180400890868597e-05, 'epoch': 88.2}\n",
      "{'loss': 1.188, 'learning_rate': 1.1776169265033408e-05, 'epoch': 88.22}\n",
      "{'loss': 1.2009, 'learning_rate': 1.1748329621380846e-05, 'epoch': 88.25}\n",
      " 88% 158500/179600 [12:54:11<1:43:26,  3.40it/s][INFO|trainer.py:1989] 2021-08-02 19:04:11,806 >> Saving model checkpoint to results/adapters/ag/checkpoint-158500\n",
      "[INFO|loading.py:59] 2021-08-02 19:04:11,807 >> Configuration saved in results/adapters/ag/checkpoint-158500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:04:11,823 >> Module weights saved in results/adapters/ag/checkpoint-158500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:04:11,824 >> Configuration saved in results/adapters/ag/checkpoint-158500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:04:12,143 >> Module weights saved in results/adapters/ag/checkpoint-158500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:04:12,143 >> Configuration saved in results/adapters/ag/checkpoint-158500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:04:12,494 >> Module weights saved in results/adapters/ag/checkpoint-158500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:04:12,495 >> tokenizer config file saved in results/adapters/ag/checkpoint-158500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:04:12,495 >> Special tokens file saved in results/adapters/ag/checkpoint-158500/special_tokens_map.json\n",
      "{'loss': 1.1623, 'learning_rate': 1.1720489977728285e-05, 'epoch': 88.28}\n",
      "{'loss': 1.2011, 'learning_rate': 1.1692650334075724e-05, 'epoch': 88.31}\n",
      "{'loss': 1.1698, 'learning_rate': 1.1664810690423163e-05, 'epoch': 88.34}\n",
      "{'loss': 1.2031, 'learning_rate': 1.1636971046770603e-05, 'epoch': 88.36}\n",
      "{'loss': 1.2056, 'learning_rate': 1.1609131403118042e-05, 'epoch': 88.39}\n",
      "{'loss': 1.1604, 'learning_rate': 1.158129175946548e-05, 'epoch': 88.42}\n",
      "{'loss': 1.1423, 'learning_rate': 1.1553452115812918e-05, 'epoch': 88.45}\n",
      "{'loss': 1.1805, 'learning_rate': 1.1525612472160357e-05, 'epoch': 88.47}\n",
      "{'loss': 1.1912, 'learning_rate': 1.1497772828507795e-05, 'epoch': 88.5}\n",
      "{'loss': 1.1667, 'learning_rate': 1.1469933184855234e-05, 'epoch': 88.53}\n",
      " 89% 159000/179600 [12:56:39<1:39:06,  3.46it/s][INFO|trainer.py:1989] 2021-08-02 19:06:40,034 >> Saving model checkpoint to results/adapters/ag/checkpoint-159000\n",
      "[INFO|loading.py:59] 2021-08-02 19:06:40,035 >> Configuration saved in results/adapters/ag/checkpoint-159000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:06:40,048 >> Module weights saved in results/adapters/ag/checkpoint-159000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:06:40,049 >> Configuration saved in results/adapters/ag/checkpoint-159000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:06:40,347 >> Module weights saved in results/adapters/ag/checkpoint-159000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:06:40,348 >> Configuration saved in results/adapters/ag/checkpoint-159000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:06:40,684 >> Module weights saved in results/adapters/ag/checkpoint-159000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:06:40,685 >> tokenizer config file saved in results/adapters/ag/checkpoint-159000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:06:40,685 >> Special tokens file saved in results/adapters/ag/checkpoint-159000/special_tokens_map.json\n",
      "{'loss': 1.1667, 'learning_rate': 1.1442093541202673e-05, 'epoch': 88.56}\n",
      "{'loss': 1.1665, 'learning_rate': 1.1414253897550112e-05, 'epoch': 88.59}\n",
      "{'loss': 1.1541, 'learning_rate': 1.1386414253897552e-05, 'epoch': 88.61}\n",
      "{'loss': 1.1666, 'learning_rate': 1.1358574610244989e-05, 'epoch': 88.64}\n",
      "{'loss': 1.1722, 'learning_rate': 1.1330734966592428e-05, 'epoch': 88.67}\n",
      "{'loss': 1.2052, 'learning_rate': 1.1302895322939867e-05, 'epoch': 88.7}\n",
      "{'loss': 1.1532, 'learning_rate': 1.1275055679287305e-05, 'epoch': 88.72}\n",
      "{'loss': 1.1737, 'learning_rate': 1.1247216035634744e-05, 'epoch': 88.75}\n",
      "{'loss': 1.2182, 'learning_rate': 1.1219376391982183e-05, 'epoch': 88.78}\n",
      "{'loss': 1.1635, 'learning_rate': 1.1191536748329622e-05, 'epoch': 88.81}\n",
      " 89% 159500/179600 [12:59:07<1:40:10,  3.34it/s][INFO|trainer.py:1989] 2021-08-02 19:09:07,680 >> Saving model checkpoint to results/adapters/ag/checkpoint-159500\n",
      "[INFO|loading.py:59] 2021-08-02 19:09:07,681 >> Configuration saved in results/adapters/ag/checkpoint-159500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:09:07,693 >> Module weights saved in results/adapters/ag/checkpoint-159500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:09:07,693 >> Configuration saved in results/adapters/ag/checkpoint-159500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:09:08,007 >> Module weights saved in results/adapters/ag/checkpoint-159500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:09:08,007 >> Configuration saved in results/adapters/ag/checkpoint-159500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:09:08,352 >> Module weights saved in results/adapters/ag/checkpoint-159500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:09:08,352 >> tokenizer config file saved in results/adapters/ag/checkpoint-159500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:09:08,353 >> Special tokens file saved in results/adapters/ag/checkpoint-159500/special_tokens_map.json\n",
      "{'loss': 1.1911, 'learning_rate': 1.1163697104677061e-05, 'epoch': 88.84}\n",
      "{'loss': 1.1575, 'learning_rate': 1.11358574610245e-05, 'epoch': 88.86}\n",
      "{'loss': 1.1881, 'learning_rate': 1.1108017817371938e-05, 'epoch': 88.89}\n",
      "{'loss': 1.1607, 'learning_rate': 1.1080178173719377e-05, 'epoch': 88.92}\n",
      "{'loss': 1.1483, 'learning_rate': 1.1052338530066815e-05, 'epoch': 88.95}\n",
      "{'loss': 1.2136, 'learning_rate': 1.1024498886414254e-05, 'epoch': 88.98}\n",
      "{'loss': 1.2117, 'learning_rate': 1.0996659242761693e-05, 'epoch': 89.0}\n",
      "{'loss': 1.1839, 'learning_rate': 1.0968819599109132e-05, 'epoch': 89.03}\n",
      "{'loss': 1.1842, 'learning_rate': 1.0940979955456571e-05, 'epoch': 89.06}\n",
      "{'loss': 1.1658, 'learning_rate': 1.091314031180401e-05, 'epoch': 89.09}\n",
      " 89% 160000/179600 [13:01:35<1:35:22,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 19:11:35,742 >> Saving model checkpoint to results/adapters/ag/checkpoint-160000\n",
      "[INFO|loading.py:59] 2021-08-02 19:11:35,743 >> Configuration saved in results/adapters/ag/checkpoint-160000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:11:35,758 >> Module weights saved in results/adapters/ag/checkpoint-160000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:11:35,759 >> Configuration saved in results/adapters/ag/checkpoint-160000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:11:36,060 >> Module weights saved in results/adapters/ag/checkpoint-160000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:11:36,061 >> Configuration saved in results/adapters/ag/checkpoint-160000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:11:36,398 >> Module weights saved in results/adapters/ag/checkpoint-160000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:11:36,398 >> tokenizer config file saved in results/adapters/ag/checkpoint-160000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:11:36,399 >> Special tokens file saved in results/adapters/ag/checkpoint-160000/special_tokens_map.json\n",
      "{'loss': 1.1912, 'learning_rate': 1.0885300668151448e-05, 'epoch': 89.11}\n",
      "{'loss': 1.171, 'learning_rate': 1.0857461024498887e-05, 'epoch': 89.14}\n",
      "{'loss': 1.1677, 'learning_rate': 1.0829621380846326e-05, 'epoch': 89.17}\n",
      "{'loss': 1.1573, 'learning_rate': 1.0801781737193764e-05, 'epoch': 89.2}\n",
      "{'loss': 1.1685, 'learning_rate': 1.0773942093541203e-05, 'epoch': 89.23}\n",
      "{'loss': 1.1866, 'learning_rate': 1.0746102449888642e-05, 'epoch': 89.25}\n",
      "{'loss': 1.1822, 'learning_rate': 1.0718262806236081e-05, 'epoch': 89.28}\n",
      "{'loss': 1.1467, 'learning_rate': 1.069042316258352e-05, 'epoch': 89.31}\n",
      "{'loss': 1.1714, 'learning_rate': 1.0662583518930958e-05, 'epoch': 89.34}\n",
      "{'loss': 1.1813, 'learning_rate': 1.0634743875278397e-05, 'epoch': 89.37}\n",
      " 89% 160500/179600 [13:04:03<1:35:07,  3.35it/s][INFO|trainer.py:1989] 2021-08-02 19:14:04,165 >> Saving model checkpoint to results/adapters/ag/checkpoint-160500\n",
      "[INFO|loading.py:59] 2021-08-02 19:14:04,166 >> Configuration saved in results/adapters/ag/checkpoint-160500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:14:04,176 >> Module weights saved in results/adapters/ag/checkpoint-160500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:14:04,177 >> Configuration saved in results/adapters/ag/checkpoint-160500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:14:04,484 >> Module weights saved in results/adapters/ag/checkpoint-160500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:14:04,484 >> Configuration saved in results/adapters/ag/checkpoint-160500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:14:04,821 >> Module weights saved in results/adapters/ag/checkpoint-160500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:14:04,822 >> tokenizer config file saved in results/adapters/ag/checkpoint-160500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:14:04,822 >> Special tokens file saved in results/adapters/ag/checkpoint-160500/special_tokens_map.json\n",
      "{'loss': 1.165, 'learning_rate': 1.0606904231625836e-05, 'epoch': 89.39}\n",
      "{'loss': 1.1706, 'learning_rate': 1.0579064587973274e-05, 'epoch': 89.42}\n",
      "{'loss': 1.165, 'learning_rate': 1.0551224944320713e-05, 'epoch': 89.45}\n",
      "{'loss': 1.1299, 'learning_rate': 1.0523385300668152e-05, 'epoch': 89.48}\n",
      "{'loss': 1.1799, 'learning_rate': 1.0495545657015591e-05, 'epoch': 89.5}\n",
      "{'loss': 1.1485, 'learning_rate': 1.046770601336303e-05, 'epoch': 89.53}\n",
      "{'loss': 1.1552, 'learning_rate': 1.043986636971047e-05, 'epoch': 89.56}\n",
      "{'loss': 1.1488, 'learning_rate': 1.0412026726057907e-05, 'epoch': 89.59}\n",
      "{'loss': 1.1891, 'learning_rate': 1.0384187082405346e-05, 'epoch': 89.62}\n",
      "{'loss': 1.171, 'learning_rate': 1.0356347438752785e-05, 'epoch': 89.64}\n",
      " 90% 161000/179600 [13:06:29<1:27:01,  3.56it/s][INFO|trainer.py:1989] 2021-08-02 19:16:29,457 >> Saving model checkpoint to results/adapters/ag/checkpoint-161000\n",
      "[INFO|loading.py:59] 2021-08-02 19:16:29,458 >> Configuration saved in results/adapters/ag/checkpoint-161000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:16:29,469 >> Module weights saved in results/adapters/ag/checkpoint-161000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:16:29,470 >> Configuration saved in results/adapters/ag/checkpoint-161000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:16:29,773 >> Module weights saved in results/adapters/ag/checkpoint-161000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:16:29,774 >> Configuration saved in results/adapters/ag/checkpoint-161000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:16:30,104 >> Module weights saved in results/adapters/ag/checkpoint-161000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:16:30,105 >> tokenizer config file saved in results/adapters/ag/checkpoint-161000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:16:30,105 >> Special tokens file saved in results/adapters/ag/checkpoint-161000/special_tokens_map.json\n",
      "{'loss': 1.1578, 'learning_rate': 1.0328507795100223e-05, 'epoch': 89.67}\n",
      "{'loss': 1.172, 'learning_rate': 1.0300668151447662e-05, 'epoch': 89.7}\n",
      "{'loss': 1.1297, 'learning_rate': 1.02728285077951e-05, 'epoch': 89.73}\n",
      "{'loss': 1.2028, 'learning_rate': 1.024498886414254e-05, 'epoch': 89.75}\n",
      "{'loss': 1.1624, 'learning_rate': 1.0217149220489979e-05, 'epoch': 89.78}\n",
      "{'loss': 1.1614, 'learning_rate': 1.0189309576837417e-05, 'epoch': 89.81}\n",
      "{'loss': 1.1855, 'learning_rate': 1.0161469933184856e-05, 'epoch': 89.84}\n",
      "{'loss': 1.1539, 'learning_rate': 1.0133630289532295e-05, 'epoch': 89.87}\n",
      "{'loss': 1.1698, 'learning_rate': 1.0105790645879732e-05, 'epoch': 89.89}\n",
      "{'loss': 1.1539, 'learning_rate': 1.0077951002227172e-05, 'epoch': 89.92}\n",
      " 90% 161500/179600 [13:08:53<1:25:58,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 19:18:53,682 >> Saving model checkpoint to results/adapters/ag/checkpoint-161500\n",
      "[INFO|loading.py:59] 2021-08-02 19:18:53,683 >> Configuration saved in results/adapters/ag/checkpoint-161500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:18:53,694 >> Module weights saved in results/adapters/ag/checkpoint-161500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:18:53,694 >> Configuration saved in results/adapters/ag/checkpoint-161500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:18:54,002 >> Module weights saved in results/adapters/ag/checkpoint-161500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:18:54,002 >> Configuration saved in results/adapters/ag/checkpoint-161500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:18:54,347 >> Module weights saved in results/adapters/ag/checkpoint-161500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:18:54,348 >> tokenizer config file saved in results/adapters/ag/checkpoint-161500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:18:54,348 >> Special tokens file saved in results/adapters/ag/checkpoint-161500/special_tokens_map.json\n",
      "{'loss': 1.1991, 'learning_rate': 1.005011135857461e-05, 'epoch': 89.95}\n",
      "{'loss': 1.1828, 'learning_rate': 1.002227171492205e-05, 'epoch': 89.98}\n",
      "{'loss': 1.187, 'learning_rate': 9.994432071269489e-06, 'epoch': 90.01}\n",
      "{'loss': 1.1656, 'learning_rate': 9.966592427616928e-06, 'epoch': 90.03}\n",
      "{'loss': 1.2097, 'learning_rate': 9.938752783964366e-06, 'epoch': 90.06}\n",
      "{'loss': 1.1799, 'learning_rate': 9.910913140311805e-06, 'epoch': 90.09}\n",
      "{'loss': 1.1646, 'learning_rate': 9.883073496659244e-06, 'epoch': 90.12}\n",
      "{'loss': 1.2045, 'learning_rate': 9.855233853006681e-06, 'epoch': 90.14}\n",
      "{'loss': 1.153, 'learning_rate': 9.82739420935412e-06, 'epoch': 90.17}\n",
      "{'loss': 1.2193, 'learning_rate': 9.79955456570156e-06, 'epoch': 90.2}\n",
      " 90% 162000/179600 [13:11:18<1:23:34,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 19:21:19,375 >> Saving model checkpoint to results/adapters/ag/checkpoint-162000\n",
      "[INFO|loading.py:59] 2021-08-02 19:21:19,376 >> Configuration saved in results/adapters/ag/checkpoint-162000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:21:19,387 >> Module weights saved in results/adapters/ag/checkpoint-162000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:21:19,388 >> Configuration saved in results/adapters/ag/checkpoint-162000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:21:19,693 >> Module weights saved in results/adapters/ag/checkpoint-162000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:21:19,694 >> Configuration saved in results/adapters/ag/checkpoint-162000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:21:20,024 >> Module weights saved in results/adapters/ag/checkpoint-162000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:21:20,024 >> tokenizer config file saved in results/adapters/ag/checkpoint-162000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:21:20,025 >> Special tokens file saved in results/adapters/ag/checkpoint-162000/special_tokens_map.json\n",
      "{'loss': 1.1713, 'learning_rate': 9.771714922048999e-06, 'epoch': 90.23}\n",
      "{'loss': 1.1905, 'learning_rate': 9.743875278396438e-06, 'epoch': 90.26}\n",
      "{'loss': 1.1508, 'learning_rate': 9.716035634743875e-06, 'epoch': 90.28}\n",
      "{'loss': 1.1711, 'learning_rate': 9.688195991091315e-06, 'epoch': 90.31}\n",
      "{'loss': 1.1796, 'learning_rate': 9.660356347438754e-06, 'epoch': 90.34}\n",
      "{'loss': 1.1556, 'learning_rate': 9.632516703786191e-06, 'epoch': 90.37}\n",
      "{'loss': 1.1835, 'learning_rate': 9.60467706013363e-06, 'epoch': 90.4}\n",
      "{'loss': 1.1788, 'learning_rate': 9.57683741648107e-06, 'epoch': 90.42}\n",
      "{'loss': 1.1443, 'learning_rate': 9.548997772828509e-06, 'epoch': 90.45}\n",
      "{'loss': 1.1353, 'learning_rate': 9.521158129175948e-06, 'epoch': 90.48}\n",
      " 90% 162500/179600 [13:13:45<1:24:08,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 19:23:46,177 >> Saving model checkpoint to results/adapters/ag/checkpoint-162500\n",
      "[INFO|loading.py:59] 2021-08-02 19:23:46,177 >> Configuration saved in results/adapters/ag/checkpoint-162500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:23:46,189 >> Module weights saved in results/adapters/ag/checkpoint-162500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:23:46,189 >> Configuration saved in results/adapters/ag/checkpoint-162500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:23:46,483 >> Module weights saved in results/adapters/ag/checkpoint-162500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:23:46,483 >> Configuration saved in results/adapters/ag/checkpoint-162500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:23:46,829 >> Module weights saved in results/adapters/ag/checkpoint-162500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:23:46,829 >> tokenizer config file saved in results/adapters/ag/checkpoint-162500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:23:46,830 >> Special tokens file saved in results/adapters/ag/checkpoint-162500/special_tokens_map.json\n",
      "{'loss': 1.1911, 'learning_rate': 9.493318485523387e-06, 'epoch': 90.51}\n",
      "{'loss': 1.1839, 'learning_rate': 9.465478841870824e-06, 'epoch': 90.53}\n",
      "{'loss': 1.2021, 'learning_rate': 9.437639198218264e-06, 'epoch': 90.56}\n",
      "{'loss': 1.1714, 'learning_rate': 9.409799554565701e-06, 'epoch': 90.59}\n",
      "{'loss': 1.1645, 'learning_rate': 9.38195991091314e-06, 'epoch': 90.62}\n",
      "{'loss': 1.1767, 'learning_rate': 9.35412026726058e-06, 'epoch': 90.65}\n",
      "{'loss': 1.1636, 'learning_rate': 9.326280623608019e-06, 'epoch': 90.67}\n",
      "{'loss': 1.1577, 'learning_rate': 9.298440979955458e-06, 'epoch': 90.7}\n",
      "{'loss': 1.1622, 'learning_rate': 9.270601336302897e-06, 'epoch': 90.73}\n",
      "{'loss': 1.1422, 'learning_rate': 9.242761692650334e-06, 'epoch': 90.76}\n",
      " 91% 163000/179600 [13:16:12<1:24:15,  3.28it/s][INFO|trainer.py:1989] 2021-08-02 19:26:13,129 >> Saving model checkpoint to results/adapters/ag/checkpoint-163000\n",
      "[INFO|loading.py:59] 2021-08-02 19:26:13,130 >> Configuration saved in results/adapters/ag/checkpoint-163000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:26:13,142 >> Module weights saved in results/adapters/ag/checkpoint-163000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:26:13,142 >> Configuration saved in results/adapters/ag/checkpoint-163000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:26:13,462 >> Module weights saved in results/adapters/ag/checkpoint-163000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:26:13,463 >> Configuration saved in results/adapters/ag/checkpoint-163000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:26:13,805 >> Module weights saved in results/adapters/ag/checkpoint-163000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:26:13,806 >> tokenizer config file saved in results/adapters/ag/checkpoint-163000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:26:13,806 >> Special tokens file saved in results/adapters/ag/checkpoint-163000/special_tokens_map.json\n",
      "{'loss': 1.172, 'learning_rate': 9.214922048997773e-06, 'epoch': 90.78}\n",
      "{'loss': 1.1882, 'learning_rate': 9.187082405345213e-06, 'epoch': 90.81}\n",
      "{'loss': 1.176, 'learning_rate': 9.15924276169265e-06, 'epoch': 90.84}\n",
      "{'loss': 1.182, 'learning_rate': 9.13140311804009e-06, 'epoch': 90.87}\n",
      "{'loss': 1.1706, 'learning_rate': 9.103563474387528e-06, 'epoch': 90.9}\n",
      "{'loss': 1.1858, 'learning_rate': 9.075723830734968e-06, 'epoch': 90.92}\n",
      "{'loss': 1.1852, 'learning_rate': 9.047884187082407e-06, 'epoch': 90.95}\n",
      "{'loss': 1.1746, 'learning_rate': 9.020044543429844e-06, 'epoch': 90.98}\n",
      "{'loss': 1.2032, 'learning_rate': 8.992204899777283e-06, 'epoch': 91.01}\n",
      "{'loss': 1.1754, 'learning_rate': 8.964365256124722e-06, 'epoch': 91.04}\n",
      " 91% 163500/179600 [13:18:39<1:16:06,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 19:28:40,217 >> Saving model checkpoint to results/adapters/ag/checkpoint-163500\n",
      "[INFO|loading.py:59] 2021-08-02 19:28:40,217 >> Configuration saved in results/adapters/ag/checkpoint-163500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:28:40,232 >> Module weights saved in results/adapters/ag/checkpoint-163500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:28:40,233 >> Configuration saved in results/adapters/ag/checkpoint-163500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:28:40,531 >> Module weights saved in results/adapters/ag/checkpoint-163500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:28:40,532 >> Configuration saved in results/adapters/ag/checkpoint-163500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:28:40,867 >> Module weights saved in results/adapters/ag/checkpoint-163500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:28:40,868 >> tokenizer config file saved in results/adapters/ag/checkpoint-163500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:28:40,868 >> Special tokens file saved in results/adapters/ag/checkpoint-163500/special_tokens_map.json\n",
      "{'loss': 1.1957, 'learning_rate': 8.93652561247216e-06, 'epoch': 91.06}\n",
      "{'loss': 1.1608, 'learning_rate': 8.908685968819599e-06, 'epoch': 91.09}\n",
      "{'loss': 1.1834, 'learning_rate': 8.880846325167038e-06, 'epoch': 91.12}\n",
      "{'loss': 1.171, 'learning_rate': 8.853006681514477e-06, 'epoch': 91.15}\n",
      "{'loss': 1.2147, 'learning_rate': 8.825167037861917e-06, 'epoch': 91.17}\n",
      "{'loss': 1.1549, 'learning_rate': 8.797327394209356e-06, 'epoch': 91.2}\n",
      "{'loss': 1.1233, 'learning_rate': 8.769487750556793e-06, 'epoch': 91.23}\n",
      "{'loss': 1.1623, 'learning_rate': 8.741648106904232e-06, 'epoch': 91.26}\n",
      "{'loss': 1.171, 'learning_rate': 8.713808463251671e-06, 'epoch': 91.29}\n",
      "{'loss': 1.2018, 'learning_rate': 8.685968819599109e-06, 'epoch': 91.31}\n",
      " 91% 164000/179600 [13:21:06<1:15:46,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 19:31:06,988 >> Saving model checkpoint to results/adapters/ag/checkpoint-164000\n",
      "[INFO|loading.py:59] 2021-08-02 19:31:06,989 >> Configuration saved in results/adapters/ag/checkpoint-164000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:31:07,004 >> Module weights saved in results/adapters/ag/checkpoint-164000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:31:07,004 >> Configuration saved in results/adapters/ag/checkpoint-164000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:31:07,310 >> Module weights saved in results/adapters/ag/checkpoint-164000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:31:07,311 >> Configuration saved in results/adapters/ag/checkpoint-164000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:31:07,656 >> Module weights saved in results/adapters/ag/checkpoint-164000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:31:07,656 >> tokenizer config file saved in results/adapters/ag/checkpoint-164000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:31:07,657 >> Special tokens file saved in results/adapters/ag/checkpoint-164000/special_tokens_map.json\n",
      "{'loss': 1.1904, 'learning_rate': 8.658129175946548e-06, 'epoch': 91.34}\n",
      "{'loss': 1.158, 'learning_rate': 8.630289532293986e-06, 'epoch': 91.37}\n",
      "{'loss': 1.1461, 'learning_rate': 8.602449888641426e-06, 'epoch': 91.4}\n",
      "{'loss': 1.1728, 'learning_rate': 8.574610244988866e-06, 'epoch': 91.43}\n",
      "{'loss': 1.1537, 'learning_rate': 8.546770601336303e-06, 'epoch': 91.45}\n",
      "{'loss': 1.1996, 'learning_rate': 8.518930957683742e-06, 'epoch': 91.48}\n",
      "{'loss': 1.1607, 'learning_rate': 8.491091314031181e-06, 'epoch': 91.51}\n",
      "{'loss': 1.1836, 'learning_rate': 8.463251670378619e-06, 'epoch': 91.54}\n",
      "{'loss': 1.1808, 'learning_rate': 8.435412026726058e-06, 'epoch': 91.56}\n",
      "{'loss': 1.194, 'learning_rate': 8.407572383073497e-06, 'epoch': 91.59}\n",
      " 92% 164500/179600 [13:23:32<1:14:31,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 19:33:33,274 >> Saving model checkpoint to results/adapters/ag/checkpoint-164500\n",
      "[INFO|loading.py:59] 2021-08-02 19:33:33,275 >> Configuration saved in results/adapters/ag/checkpoint-164500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:33:33,286 >> Module weights saved in results/adapters/ag/checkpoint-164500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:33:33,287 >> Configuration saved in results/adapters/ag/checkpoint-164500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:33:33,583 >> Module weights saved in results/adapters/ag/checkpoint-164500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:33:33,584 >> Configuration saved in results/adapters/ag/checkpoint-164500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:33:33,918 >> Module weights saved in results/adapters/ag/checkpoint-164500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:33:33,918 >> tokenizer config file saved in results/adapters/ag/checkpoint-164500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:33:33,919 >> Special tokens file saved in results/adapters/ag/checkpoint-164500/special_tokens_map.json\n",
      "{'loss': 1.1807, 'learning_rate': 8.379732739420936e-06, 'epoch': 91.62}\n",
      "{'loss': 1.15, 'learning_rate': 8.351893095768375e-06, 'epoch': 91.65}\n",
      "{'loss': 1.1958, 'learning_rate': 8.324053452115815e-06, 'epoch': 91.68}\n",
      "{'loss': 1.1569, 'learning_rate': 8.296213808463252e-06, 'epoch': 91.7}\n",
      "{'loss': 1.1824, 'learning_rate': 8.268374164810691e-06, 'epoch': 91.73}\n",
      "{'loss': 1.1778, 'learning_rate': 8.240534521158129e-06, 'epoch': 91.76}\n",
      "{'loss': 1.1137, 'learning_rate': 8.212694877505568e-06, 'epoch': 91.79}\n",
      "{'loss': 1.1528, 'learning_rate': 8.184855233853007e-06, 'epoch': 91.81}\n",
      "{'loss': 1.1909, 'learning_rate': 8.157015590200444e-06, 'epoch': 91.84}\n",
      "{'loss': 1.1545, 'learning_rate': 8.129175946547885e-06, 'epoch': 91.87}\n",
      " 92% 165000/179600 [13:25:58<1:14:07,  3.28it/s][INFO|trainer.py:1989] 2021-08-02 19:35:58,581 >> Saving model checkpoint to results/adapters/ag/checkpoint-165000\n",
      "[INFO|loading.py:59] 2021-08-02 19:35:58,582 >> Configuration saved in results/adapters/ag/checkpoint-165000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:35:58,594 >> Module weights saved in results/adapters/ag/checkpoint-165000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:35:58,594 >> Configuration saved in results/adapters/ag/checkpoint-165000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:35:58,912 >> Module weights saved in results/adapters/ag/checkpoint-165000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:35:58,912 >> Configuration saved in results/adapters/ag/checkpoint-165000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:35:59,262 >> Module weights saved in results/adapters/ag/checkpoint-165000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:35:59,263 >> tokenizer config file saved in results/adapters/ag/checkpoint-165000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:35:59,263 >> Special tokens file saved in results/adapters/ag/checkpoint-165000/special_tokens_map.json\n",
      "{'loss': 1.1939, 'learning_rate': 8.101336302895324e-06, 'epoch': 91.9}\n",
      "{'loss': 1.1329, 'learning_rate': 8.073496659242762e-06, 'epoch': 91.93}\n",
      "{'loss': 1.1997, 'learning_rate': 8.045657015590201e-06, 'epoch': 91.95}\n",
      "{'loss': 1.1732, 'learning_rate': 8.01781737193764e-06, 'epoch': 91.98}\n",
      "{'loss': 1.2142, 'learning_rate': 7.989977728285078e-06, 'epoch': 92.01}\n",
      "{'loss': 1.1963, 'learning_rate': 7.962138084632517e-06, 'epoch': 92.04}\n",
      "{'loss': 1.1969, 'learning_rate': 7.934298440979956e-06, 'epoch': 92.07}\n",
      "{'loss': 1.1834, 'learning_rate': 7.906458797327395e-06, 'epoch': 92.09}\n",
      "{'loss': 1.1758, 'learning_rate': 7.878619153674834e-06, 'epoch': 92.12}\n",
      "{'loss': 1.165, 'learning_rate': 7.850779510022272e-06, 'epoch': 92.15}\n",
      " 92% 165500/179600 [13:28:25<1:06:50,  3.52it/s][INFO|trainer.py:1989] 2021-08-02 19:38:26,022 >> Saving model checkpoint to results/adapters/ag/checkpoint-165500\n",
      "[INFO|loading.py:59] 2021-08-02 19:38:26,023 >> Configuration saved in results/adapters/ag/checkpoint-165500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:38:26,037 >> Module weights saved in results/adapters/ag/checkpoint-165500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:38:26,037 >> Configuration saved in results/adapters/ag/checkpoint-165500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:38:26,332 >> Module weights saved in results/adapters/ag/checkpoint-165500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:38:26,332 >> Configuration saved in results/adapters/ag/checkpoint-165500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:38:26,664 >> Module weights saved in results/adapters/ag/checkpoint-165500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:38:26,665 >> tokenizer config file saved in results/adapters/ag/checkpoint-165500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:38:26,665 >> Special tokens file saved in results/adapters/ag/checkpoint-165500/special_tokens_map.json\n",
      "{'loss': 1.1577, 'learning_rate': 7.822939866369711e-06, 'epoch': 92.18}\n",
      "{'loss': 1.1698, 'learning_rate': 7.79510022271715e-06, 'epoch': 92.2}\n",
      "{'loss': 1.1977, 'learning_rate': 7.767260579064588e-06, 'epoch': 92.23}\n",
      "{'loss': 1.1769, 'learning_rate': 7.739420935412027e-06, 'epoch': 92.26}\n",
      "{'loss': 1.1795, 'learning_rate': 7.711581291759466e-06, 'epoch': 92.29}\n",
      "{'loss': 1.1655, 'learning_rate': 7.683741648106903e-06, 'epoch': 92.32}\n",
      "{'loss': 1.1747, 'learning_rate': 7.655902004454344e-06, 'epoch': 92.34}\n",
      "{'loss': 1.1737, 'learning_rate': 7.6280623608017824e-06, 'epoch': 92.37}\n",
      "{'loss': 1.1736, 'learning_rate': 7.600222717149221e-06, 'epoch': 92.4}\n",
      "{'loss': 1.2008, 'learning_rate': 7.57238307349666e-06, 'epoch': 92.43}\n",
      " 92% 166000/179600 [13:30:52<1:06:01,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 19:40:53,045 >> Saving model checkpoint to results/adapters/ag/checkpoint-166000\n",
      "[INFO|loading.py:59] 2021-08-02 19:40:53,045 >> Configuration saved in results/adapters/ag/checkpoint-166000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:40:53,058 >> Module weights saved in results/adapters/ag/checkpoint-166000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:40:53,058 >> Configuration saved in results/adapters/ag/checkpoint-166000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:40:53,360 >> Module weights saved in results/adapters/ag/checkpoint-166000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:40:53,361 >> Configuration saved in results/adapters/ag/checkpoint-166000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:40:53,700 >> Module weights saved in results/adapters/ag/checkpoint-166000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:40:53,700 >> tokenizer config file saved in results/adapters/ag/checkpoint-166000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:40:53,701 >> Special tokens file saved in results/adapters/ag/checkpoint-166000/special_tokens_map.json\n",
      "{'loss': 1.1878, 'learning_rate': 7.544543429844099e-06, 'epoch': 92.46}\n",
      "{'loss': 1.2035, 'learning_rate': 7.5167037861915365e-06, 'epoch': 92.48}\n",
      "{'loss': 1.1801, 'learning_rate': 7.488864142538976e-06, 'epoch': 92.51}\n",
      "{'loss': 1.1722, 'learning_rate': 7.461024498886416e-06, 'epoch': 92.54}\n",
      "{'loss': 1.1684, 'learning_rate': 7.433184855233853e-06, 'epoch': 92.57}\n",
      "{'loss': 1.1661, 'learning_rate': 7.405345211581292e-06, 'epoch': 92.59}\n",
      "{'loss': 1.1717, 'learning_rate': 7.377505567928731e-06, 'epoch': 92.62}\n",
      "{'loss': 1.1414, 'learning_rate': 7.34966592427617e-06, 'epoch': 92.65}\n",
      "{'loss': 1.2162, 'learning_rate': 7.321826280623609e-06, 'epoch': 92.68}\n",
      "{'loss': 1.1664, 'learning_rate': 7.293986636971046e-06, 'epoch': 92.71}\n",
      " 93% 166500/179600 [13:33:20<1:01:48,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 19:43:20,848 >> Saving model checkpoint to results/adapters/ag/checkpoint-166500\n",
      "[INFO|loading.py:59] 2021-08-02 19:43:20,849 >> Configuration saved in results/adapters/ag/checkpoint-166500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:43:20,862 >> Module weights saved in results/adapters/ag/checkpoint-166500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:43:20,862 >> Configuration saved in results/adapters/ag/checkpoint-166500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:43:21,156 >> Module weights saved in results/adapters/ag/checkpoint-166500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:43:21,157 >> Configuration saved in results/adapters/ag/checkpoint-166500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:43:21,490 >> Module weights saved in results/adapters/ag/checkpoint-166500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:43:21,490 >> tokenizer config file saved in results/adapters/ag/checkpoint-166500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:43:21,491 >> Special tokens file saved in results/adapters/ag/checkpoint-166500/special_tokens_map.json\n",
      "{'loss': 1.1909, 'learning_rate': 7.2661469933184855e-06, 'epoch': 92.73}\n",
      "{'loss': 1.1868, 'learning_rate': 7.238307349665925e-06, 'epoch': 92.76}\n",
      "{'loss': 1.1789, 'learning_rate': 7.210467706013363e-06, 'epoch': 92.79}\n",
      "{'loss': 1.1705, 'learning_rate': 7.182628062360802e-06, 'epoch': 92.82}\n",
      "{'loss': 1.1704, 'learning_rate': 7.154788418708241e-06, 'epoch': 92.84}\n",
      "{'loss': 1.1709, 'learning_rate': 7.12694877505568e-06, 'epoch': 92.87}\n",
      "{'loss': 1.1701, 'learning_rate': 7.099109131403119e-06, 'epoch': 92.9}\n",
      "{'loss': 1.1658, 'learning_rate': 7.071269487750558e-06, 'epoch': 92.93}\n",
      "{'loss': 1.143, 'learning_rate': 7.043429844097995e-06, 'epoch': 92.96}\n",
      "{'loss': 1.1706, 'learning_rate': 7.0155902004454345e-06, 'epoch': 92.98}\n",
      " 93% 167000/179600 [13:35:46<1:02:03,  3.38it/s][INFO|trainer.py:1989] 2021-08-02 19:45:47,084 >> Saving model checkpoint to results/adapters/ag/checkpoint-167000\n",
      "[INFO|loading.py:59] 2021-08-02 19:45:47,085 >> Configuration saved in results/adapters/ag/checkpoint-167000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:45:47,096 >> Module weights saved in results/adapters/ag/checkpoint-167000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:45:47,097 >> Configuration saved in results/adapters/ag/checkpoint-167000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:45:47,397 >> Module weights saved in results/adapters/ag/checkpoint-167000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:45:47,397 >> Configuration saved in results/adapters/ag/checkpoint-167000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:45:47,729 >> Module weights saved in results/adapters/ag/checkpoint-167000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:45:47,729 >> tokenizer config file saved in results/adapters/ag/checkpoint-167000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:45:47,729 >> Special tokens file saved in results/adapters/ag/checkpoint-167000/special_tokens_map.json\n",
      "{'loss': 1.1702, 'learning_rate': 6.987750556792873e-06, 'epoch': 93.01}\n",
      "{'loss': 1.1863, 'learning_rate': 6.959910913140312e-06, 'epoch': 93.04}\n",
      "{'loss': 1.1791, 'learning_rate': 6.932071269487751e-06, 'epoch': 93.07}\n",
      "{'loss': 1.1977, 'learning_rate': 6.9042316258351895e-06, 'epoch': 93.1}\n",
      "{'loss': 1.1674, 'learning_rate': 6.876391982182629e-06, 'epoch': 93.12}\n",
      "{'loss': 1.171, 'learning_rate': 6.848552338530068e-06, 'epoch': 93.15}\n",
      "{'loss': 1.1647, 'learning_rate': 6.820712694877505e-06, 'epoch': 93.18}\n",
      "{'loss': 1.1727, 'learning_rate': 6.792873051224944e-06, 'epoch': 93.21}\n",
      "{'loss': 1.1461, 'learning_rate': 6.7650334075723836e-06, 'epoch': 93.23}\n",
      "{'loss': 1.1816, 'learning_rate': 6.737193763919822e-06, 'epoch': 93.26}\n",
      " 93% 167500/179600 [13:38:11<58:39,  3.44it/s][INFO|trainer.py:1989] 2021-08-02 19:48:11,985 >> Saving model checkpoint to results/adapters/ag/checkpoint-167500\n",
      "[INFO|loading.py:59] 2021-08-02 19:48:11,985 >> Configuration saved in results/adapters/ag/checkpoint-167500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:48:12,000 >> Module weights saved in results/adapters/ag/checkpoint-167500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:48:12,000 >> Configuration saved in results/adapters/ag/checkpoint-167500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:48:12,295 >> Module weights saved in results/adapters/ag/checkpoint-167500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:48:12,296 >> Configuration saved in results/adapters/ag/checkpoint-167500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:48:12,638 >> Module weights saved in results/adapters/ag/checkpoint-167500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:48:12,639 >> tokenizer config file saved in results/adapters/ag/checkpoint-167500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:48:12,639 >> Special tokens file saved in results/adapters/ag/checkpoint-167500/special_tokens_map.json\n",
      "{'loss': 1.1463, 'learning_rate': 6.709354120267261e-06, 'epoch': 93.29}\n",
      "{'loss': 1.1544, 'learning_rate': 6.6815144766147e-06, 'epoch': 93.32}\n",
      "{'loss': 1.1733, 'learning_rate': 6.6536748329621385e-06, 'epoch': 93.35}\n",
      "{'loss': 1.1859, 'learning_rate': 6.625835189309578e-06, 'epoch': 93.37}\n",
      "{'loss': 1.1801, 'learning_rate': 6.597995545657015e-06, 'epoch': 93.4}\n",
      "{'loss': 1.2027, 'learning_rate': 6.570155902004454e-06, 'epoch': 93.43}\n",
      "{'loss': 1.1675, 'learning_rate': 6.542316258351893e-06, 'epoch': 93.46}\n",
      "{'loss': 1.1869, 'learning_rate': 6.514476614699332e-06, 'epoch': 93.49}\n",
      "{'loss': 1.1658, 'learning_rate': 6.486636971046771e-06, 'epoch': 93.51}\n",
      "{'loss': 1.1777, 'learning_rate': 6.45879732739421e-06, 'epoch': 93.54}\n",
      " 94% 168000/179600 [13:40:40<55:25,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 19:50:40,645 >> Saving model checkpoint to results/adapters/ag/checkpoint-168000\n",
      "[INFO|loading.py:59] 2021-08-02 19:50:40,646 >> Configuration saved in results/adapters/ag/checkpoint-168000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:50:40,658 >> Module weights saved in results/adapters/ag/checkpoint-168000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:50:40,658 >> Configuration saved in results/adapters/ag/checkpoint-168000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:50:40,956 >> Module weights saved in results/adapters/ag/checkpoint-168000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:50:40,957 >> Configuration saved in results/adapters/ag/checkpoint-168000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:50:41,296 >> Module weights saved in results/adapters/ag/checkpoint-168000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:50:41,297 >> tokenizer config file saved in results/adapters/ag/checkpoint-168000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:50:41,297 >> Special tokens file saved in results/adapters/ag/checkpoint-168000/special_tokens_map.json\n",
      "{'loss': 1.1821, 'learning_rate': 6.430957683741648e-06, 'epoch': 93.57}\n",
      "{'loss': 1.1611, 'learning_rate': 6.4031180400890875e-06, 'epoch': 93.6}\n",
      "{'loss': 1.1452, 'learning_rate': 6.375278396436527e-06, 'epoch': 93.62}\n",
      "{'loss': 1.1708, 'learning_rate': 6.347438752783964e-06, 'epoch': 93.65}\n",
      "{'loss': 1.1754, 'learning_rate': 6.319599109131403e-06, 'epoch': 93.68}\n",
      "{'loss': 1.1942, 'learning_rate': 6.291759465478842e-06, 'epoch': 93.71}\n",
      "{'loss': 1.1538, 'learning_rate': 6.263919821826281e-06, 'epoch': 93.74}\n",
      "{'loss': 1.2026, 'learning_rate': 6.23608017817372e-06, 'epoch': 93.76}\n",
      "{'loss': 1.1622, 'learning_rate': 6.208240534521158e-06, 'epoch': 93.79}\n",
      "{'loss': 1.1995, 'learning_rate': 6.180400890868597e-06, 'epoch': 93.82}\n",
      " 94% 168500/179600 [13:43:06<52:44,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 19:53:07,294 >> Saving model checkpoint to results/adapters/ag/checkpoint-168500\n",
      "[INFO|loading.py:59] 2021-08-02 19:53:07,294 >> Configuration saved in results/adapters/ag/checkpoint-168500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:53:07,308 >> Module weights saved in results/adapters/ag/checkpoint-168500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:53:07,309 >> Configuration saved in results/adapters/ag/checkpoint-168500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:53:07,616 >> Module weights saved in results/adapters/ag/checkpoint-168500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:53:07,616 >> Configuration saved in results/adapters/ag/checkpoint-168500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:53:07,965 >> Module weights saved in results/adapters/ag/checkpoint-168500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:53:07,965 >> tokenizer config file saved in results/adapters/ag/checkpoint-168500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:53:07,966 >> Special tokens file saved in results/adapters/ag/checkpoint-168500/special_tokens_map.json\n",
      "{'loss': 1.162, 'learning_rate': 6.1525612472160365e-06, 'epoch': 93.85}\n",
      "{'loss': 1.1476, 'learning_rate': 6.124721603563475e-06, 'epoch': 93.87}\n",
      "{'loss': 1.2061, 'learning_rate': 6.096881959910913e-06, 'epoch': 93.9}\n",
      "{'loss': 1.1562, 'learning_rate': 6.069042316258352e-06, 'epoch': 93.93}\n",
      "{'loss': 1.1821, 'learning_rate': 6.041202672605791e-06, 'epoch': 93.96}\n",
      "{'loss': 1.1558, 'learning_rate': 6.01336302895323e-06, 'epoch': 93.99}\n",
      "{'loss': 1.2029, 'learning_rate': 5.985523385300668e-06, 'epoch': 94.01}\n",
      "{'loss': 1.1768, 'learning_rate': 5.957683741648107e-06, 'epoch': 94.04}\n",
      "{'loss': 1.1998, 'learning_rate': 5.929844097995546e-06, 'epoch': 94.07}\n",
      "{'loss': 1.1732, 'learning_rate': 5.902004454342985e-06, 'epoch': 94.1}\n",
      " 94% 169000/179600 [13:45:33<50:00,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 19:55:34,383 >> Saving model checkpoint to results/adapters/ag/checkpoint-169000\n",
      "[INFO|loading.py:59] 2021-08-02 19:55:34,383 >> Configuration saved in results/adapters/ag/checkpoint-169000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:55:34,396 >> Module weights saved in results/adapters/ag/checkpoint-169000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:55:34,396 >> Configuration saved in results/adapters/ag/checkpoint-169000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:55:34,704 >> Module weights saved in results/adapters/ag/checkpoint-169000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:55:34,705 >> Configuration saved in results/adapters/ag/checkpoint-169000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:55:35,051 >> Module weights saved in results/adapters/ag/checkpoint-169000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:55:35,051 >> tokenizer config file saved in results/adapters/ag/checkpoint-169000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:55:35,051 >> Special tokens file saved in results/adapters/ag/checkpoint-169000/special_tokens_map.json\n",
      "{'loss': 1.1713, 'learning_rate': 5.874164810690423e-06, 'epoch': 94.13}\n",
      "{'loss': 1.1805, 'learning_rate': 5.846325167037862e-06, 'epoch': 94.15}\n",
      "{'loss': 1.1517, 'learning_rate': 5.818485523385301e-06, 'epoch': 94.18}\n",
      "{'loss': 1.1901, 'learning_rate': 5.79064587973274e-06, 'epoch': 94.21}\n",
      "{'loss': 1.1774, 'learning_rate': 5.762806236080179e-06, 'epoch': 94.24}\n",
      "{'loss': 1.1809, 'learning_rate': 5.734966592427617e-06, 'epoch': 94.26}\n",
      "{'loss': 1.1721, 'learning_rate': 5.707126948775056e-06, 'epoch': 94.29}\n",
      "{'loss': 1.1734, 'learning_rate': 5.6792873051224945e-06, 'epoch': 94.32}\n",
      "{'loss': 1.1847, 'learning_rate': 5.651447661469934e-06, 'epoch': 94.35}\n",
      "{'loss': 1.1833, 'learning_rate': 5.623608017817372e-06, 'epoch': 94.38}\n",
      " 94% 169500/179600 [13:48:02<49:01,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 19:58:02,619 >> Saving model checkpoint to results/adapters/ag/checkpoint-169500\n",
      "[INFO|loading.py:59] 2021-08-02 19:58:02,620 >> Configuration saved in results/adapters/ag/checkpoint-169500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:58:02,632 >> Module weights saved in results/adapters/ag/checkpoint-169500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:58:02,632 >> Configuration saved in results/adapters/ag/checkpoint-169500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:58:02,945 >> Module weights saved in results/adapters/ag/checkpoint-169500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 19:58:02,946 >> Configuration saved in results/adapters/ag/checkpoint-169500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 19:58:03,288 >> Module weights saved in results/adapters/ag/checkpoint-169500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 19:58:03,289 >> tokenizer config file saved in results/adapters/ag/checkpoint-169500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 19:58:03,289 >> Special tokens file saved in results/adapters/ag/checkpoint-169500/special_tokens_map.json\n",
      "{'loss': 1.1586, 'learning_rate': 5.595768374164811e-06, 'epoch': 94.4}\n",
      "{'loss': 1.195, 'learning_rate': 5.56792873051225e-06, 'epoch': 94.43}\n",
      "{'loss': 1.1797, 'learning_rate': 5.540089086859689e-06, 'epoch': 94.46}\n",
      "{'loss': 1.1762, 'learning_rate': 5.512249443207127e-06, 'epoch': 94.49}\n",
      "{'loss': 1.2016, 'learning_rate': 5.484409799554566e-06, 'epoch': 94.52}\n",
      "{'loss': 1.1934, 'learning_rate': 5.456570155902005e-06, 'epoch': 94.54}\n",
      "{'loss': 1.1776, 'learning_rate': 5.4287305122494435e-06, 'epoch': 94.57}\n",
      "{'loss': 1.1597, 'learning_rate': 5.400890868596882e-06, 'epoch': 94.6}\n",
      "{'loss': 1.171, 'learning_rate': 5.373051224944321e-06, 'epoch': 94.63}\n",
      "{'loss': 1.1553, 'learning_rate': 5.34521158129176e-06, 'epoch': 94.65}\n",
      " 95% 170000/179600 [13:50:27<44:27,  3.60it/s][INFO|trainer.py:1989] 2021-08-02 20:00:27,620 >> Saving model checkpoint to results/adapters/ag/checkpoint-170000\n",
      "[INFO|loading.py:59] 2021-08-02 20:00:27,621 >> Configuration saved in results/adapters/ag/checkpoint-170000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:00:27,635 >> Module weights saved in results/adapters/ag/checkpoint-170000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:00:27,635 >> Configuration saved in results/adapters/ag/checkpoint-170000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:00:27,939 >> Module weights saved in results/adapters/ag/checkpoint-170000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:00:27,940 >> Configuration saved in results/adapters/ag/checkpoint-170000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:00:28,264 >> Module weights saved in results/adapters/ag/checkpoint-170000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:00:28,265 >> tokenizer config file saved in results/adapters/ag/checkpoint-170000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:00:28,265 >> Special tokens file saved in results/adapters/ag/checkpoint-170000/special_tokens_map.json\n",
      "{'loss': 1.1519, 'learning_rate': 5.3173719376391984e-06, 'epoch': 94.68}\n",
      "{'loss': 1.1713, 'learning_rate': 5.289532293986637e-06, 'epoch': 94.71}\n",
      "{'loss': 1.176, 'learning_rate': 5.261692650334076e-06, 'epoch': 94.74}\n",
      "{'loss': 1.1398, 'learning_rate': 5.233853006681515e-06, 'epoch': 94.77}\n",
      "{'loss': 1.1767, 'learning_rate': 5.206013363028953e-06, 'epoch': 94.79}\n",
      "{'loss': 1.1483, 'learning_rate': 5.1781737193763925e-06, 'epoch': 94.82}\n",
      "{'loss': 1.1518, 'learning_rate': 5.150334075723831e-06, 'epoch': 94.85}\n",
      "{'loss': 1.1645, 'learning_rate': 5.12249443207127e-06, 'epoch': 94.88}\n",
      "{'loss': 1.1923, 'learning_rate': 5.094654788418708e-06, 'epoch': 94.9}\n",
      "{'loss': 1.154, 'learning_rate': 5.0668151447661475e-06, 'epoch': 94.93}\n",
      " 95% 170500/179600 [13:52:50<44:18,  3.42it/s][INFO|trainer.py:1989] 2021-08-02 20:02:51,054 >> Saving model checkpoint to results/adapters/ag/checkpoint-170500\n",
      "[INFO|loading.py:59] 2021-08-02 20:02:51,055 >> Configuration saved in results/adapters/ag/checkpoint-170500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:02:51,066 >> Module weights saved in results/adapters/ag/checkpoint-170500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:02:51,066 >> Configuration saved in results/adapters/ag/checkpoint-170500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:02:51,363 >> Module weights saved in results/adapters/ag/checkpoint-170500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:02:51,363 >> Configuration saved in results/adapters/ag/checkpoint-170500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:02:51,693 >> Module weights saved in results/adapters/ag/checkpoint-170500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:02:51,694 >> tokenizer config file saved in results/adapters/ag/checkpoint-170500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:02:51,694 >> Special tokens file saved in results/adapters/ag/checkpoint-170500/special_tokens_map.json\n",
      "{'loss': 1.1714, 'learning_rate': 5.038975501113586e-06, 'epoch': 94.96}\n",
      "{'loss': 1.1836, 'learning_rate': 5.011135857461025e-06, 'epoch': 94.99}\n",
      "{'loss': 1.192, 'learning_rate': 4.983296213808464e-06, 'epoch': 95.02}\n",
      "{'loss': 1.2029, 'learning_rate': 4.955456570155902e-06, 'epoch': 95.04}\n",
      "{'loss': 1.1795, 'learning_rate': 4.927616926503341e-06, 'epoch': 95.07}\n",
      "{'loss': 1.1724, 'learning_rate': 4.89977728285078e-06, 'epoch': 95.1}\n",
      "{'loss': 1.142, 'learning_rate': 4.871937639198219e-06, 'epoch': 95.13}\n",
      "{'loss': 1.164, 'learning_rate': 4.844097995545657e-06, 'epoch': 95.16}\n",
      "{'loss': 1.1751, 'learning_rate': 4.816258351893096e-06, 'epoch': 95.18}\n",
      "{'loss': 1.1789, 'learning_rate': 4.788418708240535e-06, 'epoch': 95.21}\n",
      " 95% 171000/179600 [13:55:16<41:47,  3.43it/s][INFO|trainer.py:1989] 2021-08-02 20:05:17,388 >> Saving model checkpoint to results/adapters/ag/checkpoint-171000\n",
      "[INFO|loading.py:59] 2021-08-02 20:05:17,389 >> Configuration saved in results/adapters/ag/checkpoint-171000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:05:17,400 >> Module weights saved in results/adapters/ag/checkpoint-171000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:05:17,400 >> Configuration saved in results/adapters/ag/checkpoint-171000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:05:17,695 >> Module weights saved in results/adapters/ag/checkpoint-171000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:05:17,695 >> Configuration saved in results/adapters/ag/checkpoint-171000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:05:18,036 >> Module weights saved in results/adapters/ag/checkpoint-171000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:05:18,037 >> tokenizer config file saved in results/adapters/ag/checkpoint-171000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:05:18,037 >> Special tokens file saved in results/adapters/ag/checkpoint-171000/special_tokens_map.json\n",
      "{'loss': 1.214, 'learning_rate': 4.760579064587974e-06, 'epoch': 95.24}\n",
      "{'loss': 1.1849, 'learning_rate': 4.732739420935412e-06, 'epoch': 95.27}\n",
      "{'loss': 1.1886, 'learning_rate': 4.7048997772828505e-06, 'epoch': 95.29}\n",
      "{'loss': 1.1523, 'learning_rate': 4.67706013363029e-06, 'epoch': 95.32}\n",
      "{'loss': 1.1719, 'learning_rate': 4.649220489977729e-06, 'epoch': 95.35}\n",
      "{'loss': 1.1789, 'learning_rate': 4.621380846325167e-06, 'epoch': 95.38}\n",
      "{'loss': 1.1543, 'learning_rate': 4.593541202672606e-06, 'epoch': 95.41}\n",
      "{'loss': 1.2171, 'learning_rate': 4.565701559020045e-06, 'epoch': 95.43}\n",
      "{'loss': 1.1753, 'learning_rate': 4.537861915367484e-06, 'epoch': 95.46}\n",
      "{'loss': 1.1627, 'learning_rate': 4.510022271714922e-06, 'epoch': 95.49}\n",
      " 95% 171500/179600 [13:57:41<37:35,  3.59it/s][INFO|trainer.py:1989] 2021-08-02 20:07:42,119 >> Saving model checkpoint to results/adapters/ag/checkpoint-171500\n",
      "[INFO|loading.py:59] 2021-08-02 20:07:42,119 >> Configuration saved in results/adapters/ag/checkpoint-171500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:07:42,133 >> Module weights saved in results/adapters/ag/checkpoint-171500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:07:42,133 >> Configuration saved in results/adapters/ag/checkpoint-171500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:07:42,432 >> Module weights saved in results/adapters/ag/checkpoint-171500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:07:42,433 >> Configuration saved in results/adapters/ag/checkpoint-171500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:07:42,770 >> Module weights saved in results/adapters/ag/checkpoint-171500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:07:42,771 >> tokenizer config file saved in results/adapters/ag/checkpoint-171500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:07:42,771 >> Special tokens file saved in results/adapters/ag/checkpoint-171500/special_tokens_map.json\n",
      "{'loss': 1.1836, 'learning_rate': 4.482182628062361e-06, 'epoch': 95.52}\n",
      "{'loss': 1.1504, 'learning_rate': 4.4543429844097995e-06, 'epoch': 95.55}\n",
      "{'loss': 1.1649, 'learning_rate': 4.426503340757239e-06, 'epoch': 95.57}\n",
      "{'loss': 1.157, 'learning_rate': 4.398663697104678e-06, 'epoch': 95.6}\n",
      "{'loss': 1.197, 'learning_rate': 4.370824053452116e-06, 'epoch': 95.63}\n",
      "{'loss': 1.1537, 'learning_rate': 4.3429844097995545e-06, 'epoch': 95.66}\n",
      "{'loss': 1.1616, 'learning_rate': 4.315144766146993e-06, 'epoch': 95.68}\n",
      "{'loss': 1.1771, 'learning_rate': 4.287305122494433e-06, 'epoch': 95.71}\n",
      "{'loss': 1.1589, 'learning_rate': 4.259465478841871e-06, 'epoch': 95.74}\n",
      "{'loss': 1.1674, 'learning_rate': 4.231625835189309e-06, 'epoch': 95.77}\n",
      " 96% 172000/179600 [14:00:06<36:00,  3.52it/s][INFO|trainer.py:1989] 2021-08-02 20:10:07,177 >> Saving model checkpoint to results/adapters/ag/checkpoint-172000\n",
      "[INFO|loading.py:59] 2021-08-02 20:10:07,178 >> Configuration saved in results/adapters/ag/checkpoint-172000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:10:07,188 >> Module weights saved in results/adapters/ag/checkpoint-172000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:10:07,189 >> Configuration saved in results/adapters/ag/checkpoint-172000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:10:07,489 >> Module weights saved in results/adapters/ag/checkpoint-172000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:10:07,490 >> Configuration saved in results/adapters/ag/checkpoint-172000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:10:07,819 >> Module weights saved in results/adapters/ag/checkpoint-172000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:10:07,819 >> tokenizer config file saved in results/adapters/ag/checkpoint-172000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:10:07,820 >> Special tokens file saved in results/adapters/ag/checkpoint-172000/special_tokens_map.json\n",
      "{'loss': 1.1604, 'learning_rate': 4.2037861915367486e-06, 'epoch': 95.8}\n",
      "{'loss': 1.1568, 'learning_rate': 4.175946547884188e-06, 'epoch': 95.82}\n",
      "{'loss': 1.1575, 'learning_rate': 4.148106904231626e-06, 'epoch': 95.85}\n",
      "{'loss': 1.1661, 'learning_rate': 4.120267260579064e-06, 'epoch': 95.88}\n",
      "{'loss': 1.1382, 'learning_rate': 4.0924276169265035e-06, 'epoch': 95.91}\n",
      "{'loss': 1.1744, 'learning_rate': 4.064587973273943e-06, 'epoch': 95.93}\n",
      "{'loss': 1.1289, 'learning_rate': 4.036748329621381e-06, 'epoch': 95.96}\n",
      "{'loss': 1.1923, 'learning_rate': 4.00890868596882e-06, 'epoch': 95.99}\n",
      "{'loss': 1.2055, 'learning_rate': 3.981069042316258e-06, 'epoch': 96.02}\n",
      "{'loss': 1.1521, 'learning_rate': 3.9532293986636976e-06, 'epoch': 96.05}\n",
      " 96% 172500/179600 [14:02:31<32:51,  3.60it/s][INFO|trainer.py:1989] 2021-08-02 20:12:31,919 >> Saving model checkpoint to results/adapters/ag/checkpoint-172500\n",
      "[INFO|loading.py:59] 2021-08-02 20:12:31,920 >> Configuration saved in results/adapters/ag/checkpoint-172500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:12:31,931 >> Module weights saved in results/adapters/ag/checkpoint-172500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:12:31,931 >> Configuration saved in results/adapters/ag/checkpoint-172500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:12:32,224 >> Module weights saved in results/adapters/ag/checkpoint-172500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:12:32,225 >> Configuration saved in results/adapters/ag/checkpoint-172500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:12:32,551 >> Module weights saved in results/adapters/ag/checkpoint-172500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:12:32,552 >> tokenizer config file saved in results/adapters/ag/checkpoint-172500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:12:32,552 >> Special tokens file saved in results/adapters/ag/checkpoint-172500/special_tokens_map.json\n",
      "{'loss': 1.1518, 'learning_rate': 3.925389755011136e-06, 'epoch': 96.07}\n",
      "{'loss': 1.1764, 'learning_rate': 3.897550111358575e-06, 'epoch': 96.1}\n",
      "{'loss': 1.1704, 'learning_rate': 3.869710467706013e-06, 'epoch': 96.13}\n",
      "{'loss': 1.1645, 'learning_rate': 3.841870824053452e-06, 'epoch': 96.16}\n",
      "{'loss': 1.1812, 'learning_rate': 3.8140311804008912e-06, 'epoch': 96.19}\n",
      "{'loss': 1.1888, 'learning_rate': 3.78619153674833e-06, 'epoch': 96.21}\n",
      "{'loss': 1.1953, 'learning_rate': 3.7583518930957683e-06, 'epoch': 96.24}\n",
      "{'loss': 1.1996, 'learning_rate': 3.730512249443208e-06, 'epoch': 96.27}\n",
      "{'loss': 1.1848, 'learning_rate': 3.702672605790646e-06, 'epoch': 96.3}\n",
      "{'loss': 1.1774, 'learning_rate': 3.674832962138085e-06, 'epoch': 96.33}\n",
      " 96% 173000/179600 [14:04:55<31:28,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 20:14:55,842 >> Saving model checkpoint to results/adapters/ag/checkpoint-173000\n",
      "[INFO|loading.py:59] 2021-08-02 20:14:55,843 >> Configuration saved in results/adapters/ag/checkpoint-173000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:14:55,855 >> Module weights saved in results/adapters/ag/checkpoint-173000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:14:55,855 >> Configuration saved in results/adapters/ag/checkpoint-173000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:14:56,155 >> Module weights saved in results/adapters/ag/checkpoint-173000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:14:56,156 >> Configuration saved in results/adapters/ag/checkpoint-173000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:14:56,483 >> Module weights saved in results/adapters/ag/checkpoint-173000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:14:56,484 >> tokenizer config file saved in results/adapters/ag/checkpoint-173000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:14:56,484 >> Special tokens file saved in results/adapters/ag/checkpoint-173000/special_tokens_map.json\n",
      "{'loss': 1.1657, 'learning_rate': 3.646993318485523e-06, 'epoch': 96.35}\n",
      "{'loss': 1.1523, 'learning_rate': 3.6191536748329623e-06, 'epoch': 96.38}\n",
      "{'loss': 1.181, 'learning_rate': 3.591314031180401e-06, 'epoch': 96.41}\n",
      "{'loss': 1.1746, 'learning_rate': 3.56347438752784e-06, 'epoch': 96.44}\n",
      "{'loss': 1.1795, 'learning_rate': 3.535634743875279e-06, 'epoch': 96.46}\n",
      "{'loss': 1.1846, 'learning_rate': 3.5077951002227173e-06, 'epoch': 96.49}\n",
      "{'loss': 1.1825, 'learning_rate': 3.479955456570156e-06, 'epoch': 96.52}\n",
      "{'loss': 1.1556, 'learning_rate': 3.4521158129175947e-06, 'epoch': 96.55}\n",
      "{'loss': 1.1681, 'learning_rate': 3.424276169265034e-06, 'epoch': 96.58}\n",
      "{'loss': 1.1831, 'learning_rate': 3.396436525612472e-06, 'epoch': 96.6}\n",
      " 97% 173500/179600 [14:07:19<29:59,  3.39it/s][INFO|trainer.py:1989] 2021-08-02 20:17:20,348 >> Saving model checkpoint to results/adapters/ag/checkpoint-173500\n",
      "[INFO|loading.py:59] 2021-08-02 20:17:20,348 >> Configuration saved in results/adapters/ag/checkpoint-173500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:17:20,360 >> Module weights saved in results/adapters/ag/checkpoint-173500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:17:20,360 >> Configuration saved in results/adapters/ag/checkpoint-173500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:17:20,658 >> Module weights saved in results/adapters/ag/checkpoint-173500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:17:20,658 >> Configuration saved in results/adapters/ag/checkpoint-173500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:17:20,987 >> Module weights saved in results/adapters/ag/checkpoint-173500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:17:20,988 >> tokenizer config file saved in results/adapters/ag/checkpoint-173500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:17:20,988 >> Special tokens file saved in results/adapters/ag/checkpoint-173500/special_tokens_map.json\n",
      "{'loss': 1.1757, 'learning_rate': 3.368596881959911e-06, 'epoch': 96.63}\n",
      "{'loss': 1.17, 'learning_rate': 3.34075723830735e-06, 'epoch': 96.66}\n",
      "{'loss': 1.1634, 'learning_rate': 3.312917594654789e-06, 'epoch': 96.69}\n",
      "{'loss': 1.173, 'learning_rate': 3.285077951002227e-06, 'epoch': 96.71}\n",
      "{'loss': 1.1599, 'learning_rate': 3.257238307349666e-06, 'epoch': 96.74}\n",
      "{'loss': 1.1701, 'learning_rate': 3.229398663697105e-06, 'epoch': 96.77}\n",
      "{'loss': 1.1809, 'learning_rate': 3.2015590200445437e-06, 'epoch': 96.8}\n",
      "{'loss': 1.1822, 'learning_rate': 3.173719376391982e-06, 'epoch': 96.83}\n",
      "{'loss': 1.1198, 'learning_rate': 3.145879732739421e-06, 'epoch': 96.85}\n",
      "{'loss': 1.1788, 'learning_rate': 3.11804008908686e-06, 'epoch': 96.88}\n",
      " 97% 174000/179600 [14:09:44<26:37,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 20:19:44,940 >> Saving model checkpoint to results/adapters/ag/checkpoint-174000\n",
      "[INFO|loading.py:59] 2021-08-02 20:19:44,941 >> Configuration saved in results/adapters/ag/checkpoint-174000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:19:44,952 >> Module weights saved in results/adapters/ag/checkpoint-174000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:19:44,953 >> Configuration saved in results/adapters/ag/checkpoint-174000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:19:45,264 >> Module weights saved in results/adapters/ag/checkpoint-174000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:19:45,264 >> Configuration saved in results/adapters/ag/checkpoint-174000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:19:45,596 >> Module weights saved in results/adapters/ag/checkpoint-174000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:19:45,597 >> tokenizer config file saved in results/adapters/ag/checkpoint-174000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:19:45,597 >> Special tokens file saved in results/adapters/ag/checkpoint-174000/special_tokens_map.json\n",
      "{'loss': 1.1804, 'learning_rate': 3.0902004454342987e-06, 'epoch': 96.91}\n",
      "{'loss': 1.1642, 'learning_rate': 3.0623608017817374e-06, 'epoch': 96.94}\n",
      "{'loss': 1.174, 'learning_rate': 3.034521158129176e-06, 'epoch': 96.97}\n",
      "{'loss': 1.1958, 'learning_rate': 3.006681514476615e-06, 'epoch': 96.99}\n",
      "{'loss': 1.1658, 'learning_rate': 2.9788418708240536e-06, 'epoch': 97.02}\n",
      "{'loss': 1.1748, 'learning_rate': 2.9510022271714923e-06, 'epoch': 97.05}\n",
      "{'loss': 1.1422, 'learning_rate': 2.923162583518931e-06, 'epoch': 97.08}\n",
      "{'loss': 1.1733, 'learning_rate': 2.89532293986637e-06, 'epoch': 97.1}\n",
      "{'loss': 1.1787, 'learning_rate': 2.8674832962138085e-06, 'epoch': 97.13}\n",
      "{'loss': 1.1672, 'learning_rate': 2.8396436525612473e-06, 'epoch': 97.16}\n",
      " 97% 174500/179600 [14:12:09<25:30,  3.33it/s][INFO|trainer.py:1989] 2021-08-02 20:22:10,233 >> Saving model checkpoint to results/adapters/ag/checkpoint-174500\n",
      "[INFO|loading.py:59] 2021-08-02 20:22:10,234 >> Configuration saved in results/adapters/ag/checkpoint-174500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:22:10,246 >> Module weights saved in results/adapters/ag/checkpoint-174500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:22:10,246 >> Configuration saved in results/adapters/ag/checkpoint-174500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:22:10,540 >> Module weights saved in results/adapters/ag/checkpoint-174500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:22:10,541 >> Configuration saved in results/adapters/ag/checkpoint-174500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:22:10,876 >> Module weights saved in results/adapters/ag/checkpoint-174500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:22:10,877 >> tokenizer config file saved in results/adapters/ag/checkpoint-174500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:22:10,877 >> Special tokens file saved in results/adapters/ag/checkpoint-174500/special_tokens_map.json\n",
      "{'loss': 1.1606, 'learning_rate': 2.811804008908686e-06, 'epoch': 97.19}\n",
      "{'loss': 1.1431, 'learning_rate': 2.783964365256125e-06, 'epoch': 97.22}\n",
      "{'loss': 1.1765, 'learning_rate': 2.7561247216035634e-06, 'epoch': 97.24}\n",
      "{'loss': 1.1506, 'learning_rate': 2.7282850779510026e-06, 'epoch': 97.27}\n",
      "{'loss': 1.1547, 'learning_rate': 2.700445434298441e-06, 'epoch': 97.3}\n",
      "{'loss': 1.1788, 'learning_rate': 2.67260579064588e-06, 'epoch': 97.33}\n",
      "{'loss': 1.1573, 'learning_rate': 2.6447661469933184e-06, 'epoch': 97.36}\n",
      "{'loss': 1.1746, 'learning_rate': 2.6169265033407575e-06, 'epoch': 97.38}\n",
      "{'loss': 1.1786, 'learning_rate': 2.5890868596881963e-06, 'epoch': 97.41}\n",
      "{'loss': 1.1667, 'learning_rate': 2.561247216035635e-06, 'epoch': 97.44}\n",
      " 97% 175000/179600 [14:14:35<21:35,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 20:24:35,663 >> Saving model checkpoint to results/adapters/ag/checkpoint-175000\n",
      "[INFO|loading.py:59] 2021-08-02 20:24:35,663 >> Configuration saved in results/adapters/ag/checkpoint-175000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:24:35,675 >> Module weights saved in results/adapters/ag/checkpoint-175000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:24:35,675 >> Configuration saved in results/adapters/ag/checkpoint-175000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:24:35,976 >> Module weights saved in results/adapters/ag/checkpoint-175000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:24:35,977 >> Configuration saved in results/adapters/ag/checkpoint-175000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:24:36,318 >> Module weights saved in results/adapters/ag/checkpoint-175000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:24:36,319 >> tokenizer config file saved in results/adapters/ag/checkpoint-175000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:24:36,319 >> Special tokens file saved in results/adapters/ag/checkpoint-175000/special_tokens_map.json\n",
      "{'loss': 1.1346, 'learning_rate': 2.5334075723830737e-06, 'epoch': 97.47}\n",
      "{'loss': 1.1739, 'learning_rate': 2.5055679287305125e-06, 'epoch': 97.49}\n",
      "{'loss': 1.1823, 'learning_rate': 2.477728285077951e-06, 'epoch': 97.52}\n",
      "{'loss': 1.2079, 'learning_rate': 2.44988864142539e-06, 'epoch': 97.55}\n",
      "{'loss': 1.1771, 'learning_rate': 2.4220489977728287e-06, 'epoch': 97.58}\n",
      "{'loss': 1.162, 'learning_rate': 2.3942093541202674e-06, 'epoch': 97.61}\n",
      "{'loss': 1.1746, 'learning_rate': 2.366369710467706e-06, 'epoch': 97.63}\n",
      "{'loss': 1.1597, 'learning_rate': 2.338530066815145e-06, 'epoch': 97.66}\n",
      "{'loss': 1.1909, 'learning_rate': 2.3106904231625836e-06, 'epoch': 97.69}\n",
      "{'loss': 1.1447, 'learning_rate': 2.2828507795100223e-06, 'epoch': 97.72}\n",
      " 98% 175500/179600 [14:17:00<19:18,  3.54it/s][INFO|trainer.py:1989] 2021-08-02 20:27:00,597 >> Saving model checkpoint to results/adapters/ag/checkpoint-175500\n",
      "[INFO|loading.py:59] 2021-08-02 20:27:00,598 >> Configuration saved in results/adapters/ag/checkpoint-175500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:27:00,610 >> Module weights saved in results/adapters/ag/checkpoint-175500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:27:00,610 >> Configuration saved in results/adapters/ag/checkpoint-175500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:27:00,906 >> Module weights saved in results/adapters/ag/checkpoint-175500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:27:00,906 >> Configuration saved in results/adapters/ag/checkpoint-175500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:27:01,240 >> Module weights saved in results/adapters/ag/checkpoint-175500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:27:01,241 >> tokenizer config file saved in results/adapters/ag/checkpoint-175500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:27:01,241 >> Special tokens file saved in results/adapters/ag/checkpoint-175500/special_tokens_map.json\n",
      "{'loss': 1.1792, 'learning_rate': 2.255011135857461e-06, 'epoch': 97.74}\n",
      "{'loss': 1.1737, 'learning_rate': 2.2271714922048998e-06, 'epoch': 97.77}\n",
      "{'loss': 1.1811, 'learning_rate': 2.199331848552339e-06, 'epoch': 97.8}\n",
      "{'loss': 1.1632, 'learning_rate': 2.1714922048997772e-06, 'epoch': 97.83}\n",
      "{'loss': 1.1378, 'learning_rate': 2.1436525612472164e-06, 'epoch': 97.86}\n",
      "{'loss': 1.165, 'learning_rate': 2.1158129175946547e-06, 'epoch': 97.88}\n",
      "{'loss': 1.1632, 'learning_rate': 2.087973273942094e-06, 'epoch': 97.91}\n",
      "{'loss': 1.1579, 'learning_rate': 2.060133630289532e-06, 'epoch': 97.94}\n",
      "{'loss': 1.1901, 'learning_rate': 2.0322939866369713e-06, 'epoch': 97.97}\n",
      "{'loss': 1.177, 'learning_rate': 2.00445434298441e-06, 'epoch': 98.0}\n",
      " 98% 176000/179600 [14:19:24<16:53,  3.55it/s][INFO|trainer.py:1989] 2021-08-02 20:29:25,075 >> Saving model checkpoint to results/adapters/ag/checkpoint-176000\n",
      "[INFO|loading.py:59] 2021-08-02 20:29:25,076 >> Configuration saved in results/adapters/ag/checkpoint-176000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:29:25,088 >> Module weights saved in results/adapters/ag/checkpoint-176000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:29:25,089 >> Configuration saved in results/adapters/ag/checkpoint-176000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:29:25,387 >> Module weights saved in results/adapters/ag/checkpoint-176000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:29:25,388 >> Configuration saved in results/adapters/ag/checkpoint-176000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:29:25,728 >> Module weights saved in results/adapters/ag/checkpoint-176000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:29:25,729 >> tokenizer config file saved in results/adapters/ag/checkpoint-176000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:29:25,729 >> Special tokens file saved in results/adapters/ag/checkpoint-176000/special_tokens_map.json\n",
      "{'loss': 1.2052, 'learning_rate': 1.9766146993318488e-06, 'epoch': 98.02}\n",
      "{'loss': 1.1766, 'learning_rate': 1.9487750556792875e-06, 'epoch': 98.05}\n",
      "{'loss': 1.161, 'learning_rate': 1.920935412026726e-06, 'epoch': 98.08}\n",
      "{'loss': 1.1894, 'learning_rate': 1.893095768374165e-06, 'epoch': 98.11}\n",
      "{'loss': 1.1927, 'learning_rate': 1.865256124721604e-06, 'epoch': 98.13}\n",
      "{'loss': 1.1547, 'learning_rate': 1.8374164810690424e-06, 'epoch': 98.16}\n",
      "{'loss': 1.1918, 'learning_rate': 1.8095768374164812e-06, 'epoch': 98.19}\n",
      "{'loss': 1.1166, 'learning_rate': 1.78173719376392e-06, 'epoch': 98.22}\n",
      "{'loss': 1.1827, 'learning_rate': 1.7538975501113586e-06, 'epoch': 98.25}\n",
      "{'loss': 1.1658, 'learning_rate': 1.7260579064587974e-06, 'epoch': 98.27}\n",
      " 98% 176500/179600 [14:21:49<14:45,  3.50it/s][INFO|trainer.py:1989] 2021-08-02 20:31:49,888 >> Saving model checkpoint to results/adapters/ag/checkpoint-176500\n",
      "[INFO|loading.py:59] 2021-08-02 20:31:49,889 >> Configuration saved in results/adapters/ag/checkpoint-176500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:31:49,901 >> Module weights saved in results/adapters/ag/checkpoint-176500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:31:49,901 >> Configuration saved in results/adapters/ag/checkpoint-176500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:31:50,212 >> Module weights saved in results/adapters/ag/checkpoint-176500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:31:50,212 >> Configuration saved in results/adapters/ag/checkpoint-176500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:31:50,557 >> Module weights saved in results/adapters/ag/checkpoint-176500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:31:50,558 >> tokenizer config file saved in results/adapters/ag/checkpoint-176500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:31:50,558 >> Special tokens file saved in results/adapters/ag/checkpoint-176500/special_tokens_map.json\n",
      "{'loss': 1.1957, 'learning_rate': 1.698218262806236e-06, 'epoch': 98.3}\n",
      "{'loss': 1.1501, 'learning_rate': 1.670378619153675e-06, 'epoch': 98.33}\n",
      "{'loss': 1.1796, 'learning_rate': 1.6425389755011136e-06, 'epoch': 98.36}\n",
      "{'loss': 1.181, 'learning_rate': 1.6146993318485525e-06, 'epoch': 98.39}\n",
      "{'loss': 1.1531, 'learning_rate': 1.586859688195991e-06, 'epoch': 98.41}\n",
      "{'loss': 1.1565, 'learning_rate': 1.55902004454343e-06, 'epoch': 98.44}\n",
      "{'loss': 1.1564, 'learning_rate': 1.5311804008908687e-06, 'epoch': 98.47}\n",
      "{'loss': 1.1686, 'learning_rate': 1.5033407572383074e-06, 'epoch': 98.5}\n",
      "{'loss': 1.1676, 'learning_rate': 1.4755011135857462e-06, 'epoch': 98.52}\n",
      "{'loss': 1.1372, 'learning_rate': 1.447661469933185e-06, 'epoch': 98.55}\n",
      " 99% 177000/179600 [14:24:13<12:07,  3.57it/s][INFO|trainer.py:1989] 2021-08-02 20:34:13,900 >> Saving model checkpoint to results/adapters/ag/checkpoint-177000\n",
      "[INFO|loading.py:59] 2021-08-02 20:34:13,900 >> Configuration saved in results/adapters/ag/checkpoint-177000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:34:13,912 >> Module weights saved in results/adapters/ag/checkpoint-177000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:34:13,912 >> Configuration saved in results/adapters/ag/checkpoint-177000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:34:14,209 >> Module weights saved in results/adapters/ag/checkpoint-177000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:34:14,209 >> Configuration saved in results/adapters/ag/checkpoint-177000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:34:14,543 >> Module weights saved in results/adapters/ag/checkpoint-177000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:34:14,544 >> tokenizer config file saved in results/adapters/ag/checkpoint-177000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:34:14,544 >> Special tokens file saved in results/adapters/ag/checkpoint-177000/special_tokens_map.json\n",
      "{'loss': 1.1785, 'learning_rate': 1.4198218262806236e-06, 'epoch': 98.58}\n",
      "{'loss': 1.1508, 'learning_rate': 1.3919821826280626e-06, 'epoch': 98.61}\n",
      "{'loss': 1.1734, 'learning_rate': 1.3641425389755013e-06, 'epoch': 98.64}\n",
      "{'loss': 1.1755, 'learning_rate': 1.33630289532294e-06, 'epoch': 98.66}\n",
      "{'loss': 1.1822, 'learning_rate': 1.3084632516703788e-06, 'epoch': 98.69}\n",
      "{'loss': 1.1876, 'learning_rate': 1.2806236080178175e-06, 'epoch': 98.72}\n",
      "{'loss': 1.1774, 'learning_rate': 1.2527839643652562e-06, 'epoch': 98.75}\n",
      "{'loss': 1.2021, 'learning_rate': 1.224944320712695e-06, 'epoch': 98.77}\n",
      "{'loss': 1.1758, 'learning_rate': 1.1971046770601337e-06, 'epoch': 98.8}\n",
      "{'loss': 1.2078, 'learning_rate': 1.1692650334075724e-06, 'epoch': 98.83}\n",
      " 99% 177500/179600 [14:26:37<09:55,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 20:36:38,088 >> Saving model checkpoint to results/adapters/ag/checkpoint-177500\n",
      "[INFO|loading.py:59] 2021-08-02 20:36:38,089 >> Configuration saved in results/adapters/ag/checkpoint-177500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:36:38,103 >> Module weights saved in results/adapters/ag/checkpoint-177500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:36:38,104 >> Configuration saved in results/adapters/ag/checkpoint-177500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:36:38,399 >> Module weights saved in results/adapters/ag/checkpoint-177500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:36:38,399 >> Configuration saved in results/adapters/ag/checkpoint-177500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:36:38,727 >> Module weights saved in results/adapters/ag/checkpoint-177500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:36:38,728 >> tokenizer config file saved in results/adapters/ag/checkpoint-177500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:36:38,728 >> Special tokens file saved in results/adapters/ag/checkpoint-177500/special_tokens_map.json\n",
      "{'loss': 1.1518, 'learning_rate': 1.1414253897550112e-06, 'epoch': 98.86}\n",
      "{'loss': 1.1486, 'learning_rate': 1.1135857461024499e-06, 'epoch': 98.89}\n",
      "{'loss': 1.1774, 'learning_rate': 1.0857461024498886e-06, 'epoch': 98.91}\n",
      "{'loss': 1.1597, 'learning_rate': 1.0579064587973274e-06, 'epoch': 98.94}\n",
      "{'loss': 1.1641, 'learning_rate': 1.030066815144766e-06, 'epoch': 98.97}\n",
      "{'loss': 1.1649, 'learning_rate': 1.002227171492205e-06, 'epoch': 99.0}\n",
      "{'loss': 1.183, 'learning_rate': 9.743875278396438e-07, 'epoch': 99.03}\n",
      "{'loss': 1.1655, 'learning_rate': 9.465478841870825e-07, 'epoch': 99.05}\n",
      "{'loss': 1.1733, 'learning_rate': 9.187082405345212e-07, 'epoch': 99.08}\n",
      "{'loss': 1.1662, 'learning_rate': 8.9086859688196e-07, 'epoch': 99.11}\n",
      " 99% 178000/179600 [14:29:01<08:03,  3.31it/s][INFO|trainer.py:1989] 2021-08-02 20:39:02,323 >> Saving model checkpoint to results/adapters/ag/checkpoint-178000\n",
      "[INFO|loading.py:59] 2021-08-02 20:39:02,323 >> Configuration saved in results/adapters/ag/checkpoint-178000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:39:02,335 >> Module weights saved in results/adapters/ag/checkpoint-178000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:39:02,335 >> Configuration saved in results/adapters/ag/checkpoint-178000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:39:02,639 >> Module weights saved in results/adapters/ag/checkpoint-178000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:39:02,640 >> Configuration saved in results/adapters/ag/checkpoint-178000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:39:02,976 >> Module weights saved in results/adapters/ag/checkpoint-178000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:39:02,976 >> tokenizer config file saved in results/adapters/ag/checkpoint-178000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:39:02,977 >> Special tokens file saved in results/adapters/ag/checkpoint-178000/special_tokens_map.json\n",
      "{'loss': 1.1755, 'learning_rate': 8.630289532293987e-07, 'epoch': 99.14}\n",
      "{'loss': 1.1583, 'learning_rate': 8.351893095768375e-07, 'epoch': 99.16}\n",
      "{'loss': 1.1747, 'learning_rate': 8.073496659242763e-07, 'epoch': 99.19}\n",
      "{'loss': 1.1634, 'learning_rate': 7.79510022271715e-07, 'epoch': 99.22}\n",
      "{'loss': 1.1983, 'learning_rate': 7.516703786191537e-07, 'epoch': 99.25}\n",
      "{'loss': 1.2052, 'learning_rate': 7.238307349665924e-07, 'epoch': 99.28}\n",
      "{'loss': 1.1754, 'learning_rate': 6.959910913140313e-07, 'epoch': 99.3}\n",
      "{'loss': 1.1637, 'learning_rate': 6.6815144766147e-07, 'epoch': 99.33}\n",
      "{'loss': 1.1773, 'learning_rate': 6.403118040089087e-07, 'epoch': 99.36}\n",
      "{'loss': 1.1627, 'learning_rate': 6.124721603563475e-07, 'epoch': 99.39}\n",
      " 99% 178500/179600 [14:31:24<05:11,  3.53it/s][INFO|trainer.py:1989] 2021-08-02 20:41:24,462 >> Saving model checkpoint to results/adapters/ag/checkpoint-178500\n",
      "[INFO|loading.py:59] 2021-08-02 20:41:24,463 >> Configuration saved in results/adapters/ag/checkpoint-178500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:41:24,474 >> Module weights saved in results/adapters/ag/checkpoint-178500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:41:24,474 >> Configuration saved in results/adapters/ag/checkpoint-178500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:41:24,765 >> Module weights saved in results/adapters/ag/checkpoint-178500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:41:24,765 >> Configuration saved in results/adapters/ag/checkpoint-178500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:41:25,089 >> Module weights saved in results/adapters/ag/checkpoint-178500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:41:25,090 >> tokenizer config file saved in results/adapters/ag/checkpoint-178500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:41:25,090 >> Special tokens file saved in results/adapters/ag/checkpoint-178500/special_tokens_map.json\n",
      "{'loss': 1.1693, 'learning_rate': 5.846325167037862e-07, 'epoch': 99.42}\n",
      "{'loss': 1.207, 'learning_rate': 5.567928730512249e-07, 'epoch': 99.44}\n",
      "{'loss': 1.1621, 'learning_rate': 5.289532293986637e-07, 'epoch': 99.47}\n",
      "{'loss': 1.1819, 'learning_rate': 5.011135857461025e-07, 'epoch': 99.5}\n",
      "{'loss': 1.1905, 'learning_rate': 4.7327394209354124e-07, 'epoch': 99.53}\n",
      "{'loss': 1.1837, 'learning_rate': 4.4543429844098e-07, 'epoch': 99.55}\n",
      "{'loss': 1.1568, 'learning_rate': 4.1759465478841876e-07, 'epoch': 99.58}\n",
      "{'loss': 1.187, 'learning_rate': 3.897550111358575e-07, 'epoch': 99.61}\n",
      "{'loss': 1.1658, 'learning_rate': 3.619153674832962e-07, 'epoch': 99.64}\n",
      "{'loss': 1.1784, 'learning_rate': 3.34075723830735e-07, 'epoch': 99.67}\n",
      "100% 179000/179600 [14:33:46<02:50,  3.51it/s][INFO|trainer.py:1989] 2021-08-02 20:43:47,224 >> Saving model checkpoint to results/adapters/ag/checkpoint-179000\n",
      "[INFO|loading.py:59] 2021-08-02 20:43:47,224 >> Configuration saved in results/adapters/ag/checkpoint-179000/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:43:47,235 >> Module weights saved in results/adapters/ag/checkpoint-179000/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:43:47,235 >> Configuration saved in results/adapters/ag/checkpoint-179000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:43:47,536 >> Module weights saved in results/adapters/ag/checkpoint-179000/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:43:47,537 >> Configuration saved in results/adapters/ag/checkpoint-179000/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:43:47,874 >> Module weights saved in results/adapters/ag/checkpoint-179000/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:43:47,874 >> tokenizer config file saved in results/adapters/ag/checkpoint-179000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:43:47,874 >> Special tokens file saved in results/adapters/ag/checkpoint-179000/special_tokens_map.json\n",
      "{'loss': 1.1695, 'learning_rate': 3.0623608017817374e-07, 'epoch': 99.69}\n",
      "{'loss': 1.1755, 'learning_rate': 2.7839643652561247e-07, 'epoch': 99.72}\n",
      "{'loss': 1.1848, 'learning_rate': 2.5055679287305126e-07, 'epoch': 99.75}\n",
      "{'loss': 1.1791, 'learning_rate': 2.2271714922049e-07, 'epoch': 99.78}\n",
      "{'loss': 1.1597, 'learning_rate': 1.9487750556792875e-07, 'epoch': 99.8}\n",
      "{'loss': 1.1697, 'learning_rate': 1.670378619153675e-07, 'epoch': 99.83}\n",
      "{'loss': 1.1397, 'learning_rate': 1.3919821826280624e-07, 'epoch': 99.86}\n",
      "{'loss': 1.2076, 'learning_rate': 1.11358574610245e-07, 'epoch': 99.89}\n",
      "{'loss': 1.1631, 'learning_rate': 8.351893095768375e-08, 'epoch': 99.92}\n",
      "{'loss': 1.1641, 'learning_rate': 5.56792873051225e-08, 'epoch': 99.94}\n",
      "100% 179500/179600 [14:36:08<00:28,  3.49it/s][INFO|trainer.py:1989] 2021-08-02 20:46:09,104 >> Saving model checkpoint to results/adapters/ag/checkpoint-179500\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:09,105 >> Configuration saved in results/adapters/ag/checkpoint-179500/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:09,117 >> Module weights saved in results/adapters/ag/checkpoint-179500/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:09,117 >> Configuration saved in results/adapters/ag/checkpoint-179500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:09,417 >> Module weights saved in results/adapters/ag/checkpoint-179500/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:09,417 >> Configuration saved in results/adapters/ag/checkpoint-179500/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:09,761 >> Module weights saved in results/adapters/ag/checkpoint-179500/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:46:09,761 >> tokenizer config file saved in results/adapters/ag/checkpoint-179500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:46:09,761 >> Special tokens file saved in results/adapters/ag/checkpoint-179500/special_tokens_map.json\n",
      "{'loss': 1.1826, 'learning_rate': 2.783964365256125e-08, 'epoch': 99.97}\n",
      "{'loss': 1.1688, 'learning_rate': 0.0, 'epoch': 100.0}\n",
      "100% 179600/179600 [14:36:37<00:00,  3.63it/s][INFO|trainer.py:1403] 2021-08-02 20:46:38,027 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 52597.6012, 'train_samples_per_second': 218.639, 'train_steps_per_second': 3.415, 'train_loss': 1.222830727817221, 'epoch': 100.0}\n",
      "100% 179600/179600 [14:36:37<00:00,  3.41it/s]\n",
      "[INFO|trainer.py:1989] 2021-08-02 20:46:38,036 >> Saving model checkpoint to results/adapters/ag\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:38,037 >> Configuration saved in results/adapters/ag/mlm/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:38,051 >> Module weights saved in results/adapters/ag/mlm/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:38,051 >> Configuration saved in results/adapters/ag/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:38,398 >> Module weights saved in results/adapters/ag/mlm/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 20:46:38,399 >> Configuration saved in results/adapters/ag/mlm/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 20:46:38,721 >> Module weights saved in results/adapters/ag/mlm/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 20:46:38,721 >> tokenizer config file saved in results/adapters/ag/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 20:46:38,721 >> Special tokens file saved in results/adapters/ag/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       100.0\n",
      "  train_loss               =      1.2228\n",
      "  train_runtime            = 14:36:37.60\n",
      "  train_samples            =      114999\n",
      "  train_samples_per_second =     218.639\n",
      "  train_steps_per_second   =       3.415\n",
      "08/02/2021 20:46:38 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:547] 2021-08-02 20:46:38,927 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:774] 2021-08-02 20:46:38,971 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "[INFO|trainer.py:2239] 2021-08-02 20:46:38,971 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-08-02 20:46:38,971 >>   Num examples = 5000\n",
      "[INFO|trainer.py:2244] 2021-08-02 20:46:38,971 >>   Batch size = 8\n",
      "100% 622/625 [00:10<00:00, 59.98it/s][WARNING|training_args.py:774] 2021-08-02 20:46:49,601 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "100% 625/625 [00:10<00:00, 58.78it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =      100.0\n",
      "  eval_loss               =     1.1089\n",
      "  eval_runtime            = 0:00:10.63\n",
      "  eval_samples            =       5000\n",
      "  eval_samples_per_second =    470.367\n",
      "  eval_steps_per_second   =     58.796\n",
      "  perplexity              =     3.0311\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2RlvoapGIlY"
   },
   "source": [
    "3. Tuning with newly created adapter \"ag\" for hyperpartisan news dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJbIXAeCt-8d",
    "outputId": "67904d56-f1b5-45ef-8473-7f5a485b46d9"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 20 \\\n",
    "--output_dir results/ag-new-adapter1/ \\\n",
    "--task_name mlm \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--overwrite_output_dir \\"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 10:18:22.989507: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 10:18:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2580\n",
      " 12% 306/2580 [01:46<13:08,  2.88it/s]Traceback (most recent call last):\n",
      "  File \"run_multiple_choice.py\", line 355, in <module>\n",
      "    main()\n",
      "  File \"run_multiple_choice.py\", line 310, in main\n",
      "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1332, in train\n",
      "    fusion_reg_loss = self.model.base_model.get_fusion_regularization_loss()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/models/bert.py\", line 143, in get_fusion_regularization_loss\n",
      "    target = torch.zeros((self.config.hidden_size, self.config.hidden_size)).fill_diagonal_(1.0).to(self.device)\n",
      "KeyboardInterrupt\n",
      " 12% 306/2580 [01:46<13:14,  2.86it/s]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTiagO02uTlZ",
    "outputId": "338c19b8-5097-4254-acca-87c34ec09391"
   },
   "source": [
    "%cd \"/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\"\n",
    "!ls"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\n",
      "data\t     results\t\t     tapos_test.ipynb\n",
      "experiments  run_mlm.py\t\t     tapos-training\n",
      "README.md    run_multiple_choice.py  utils_multiple_choice.py\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xO4NDk7vvsY3",
    "outputId": "eb07a4b6-bb58-4336-95eb-fc80498531e2"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/ag-new-adapter2/ \\\n",
    "--task_name mlm \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--overwrite_output_dir \\"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 10:20:23.038820: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 10:20:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 1373, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 319, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 280, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.7/ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"run_multiple_choice.py\", line 355, in <module>\n",
      "    main()\n",
      "  File \"run_multiple_choice.py\", line 235, in main\n",
      "    cache_dir=model_args.cache_dir,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\", line 446, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 506, in get_config_dict\n",
      "    user_agent=user_agent,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1337, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1499, in get_from_cache\n",
      "    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 104, in head\n",
      "    return request('head', url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 1389, in getresponse\n",
      "    response.close()\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 423, in close\n",
      "    super().close() # set \"closed\" flag\n",
      "  File \"/usr/lib/python3.7/http/client.py\", line 434, in flush\n",
      "    super().flush()\n",
      "KeyboardInterrupt\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK390RMk5hVD"
   },
   "source": [
    "3. Tuning with hyperpartisan news dataset with adapterhub adapters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPRyzwxo5gwj",
    "outputId": "24cb9f00-5889-48c9-e2bd-0b1c950ee4f2"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters1/ \\\n",
    "--task_name mlm \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 22:06:02.684076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 22:06:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/02/2021 22:06:10 - INFO - filelock - Lock 140558622509328 acquired on ~/.cache/torch/adapters/a611e0492e49205d61214b60dbcb972ba138b2e6d1e44c13f4e0e97f837a2083.7ca3961adb44294d595d2673e6496d1c1a7202623166fe9893be364c7159405b.lock\n",
      "Downloading: 7.12kB [00:00, 5.69MB/s]     \n",
      "08/02/2021 22:06:10 - INFO - filelock - Lock 140558622509328 released on ~/.cache/torch/adapters/a611e0492e49205d61214b60dbcb972ba138b2e6d1e44c13f4e0e97f837a2083.7ca3961adb44294d595d2673e6496d1c1a7202623166fe9893be364c7159405b.lock\n",
      "08/02/2021 22:06:10 - INFO - filelock - Lock 140558622508880 acquired on ~/.cache/torch/adapters/a4b20be37c13e64cda7f46716bdda616abb4b396b90a61c7044244d99f5e275d.76b733dc2b30f0f82e23270b627d8237231cd7a8a90c43e2aaa6c9b6881ff1ba.lock\n",
      "Downloading: 100% 3.32M/3.32M [00:00<00:00, 15.1MB/s]\n",
      "08/02/2021 22:06:11 - INFO - filelock - Lock 140558622508880 released on ~/.cache/torch/adapters/a4b20be37c13e64cda7f46716bdda616abb4b396b90a61c7044244d99f5e275d.76b733dc2b30f0f82e23270b627d8237231cd7a8a90c43e2aaa6c9b6881ff1ba.lock\n",
      "08/02/2021 22:06:11 - INFO - filelock - Lock 140558621792400 acquired on ~/.cache/torch/adapters/a4b20be37c13e64cda7f46716bdda616abb4b396b90a61c7044244d99f5e275d.76b733dc2b30f0f82e23270b627d8237231cd7a8a90c43e2aaa6c9b6881ff1ba.lock\n",
      "08/02/2021 22:06:11 - INFO - filelock - Lock 140558621792400 released on ~/.cache/torch/adapters/a4b20be37c13e64cda7f46716bdda616abb4b396b90a61c7044244d99f5e275d.76b733dc2b30f0f82e23270b627d8237231cd7a8a90c43e2aaa6c9b6881ff1ba.lock\n",
      "08/02/2021 22:06:11 - INFO - filelock - Lock 140558621791888 acquired on ~/.cache/torch/adapters/cc572bf7d9898961547ebfe30e6d501b04c32e6a1e60ec59aaae75a18b3a3590.50448f00b5bcda4df943da413ec313f7a77c0ed75146a1a38189cd6dfffac6ca.lock\n",
      "Downloading: 100% 3.33M/3.33M [00:00<00:00, 9.71MB/s]\n",
      "08/02/2021 22:06:11 - INFO - filelock - Lock 140558621791888 released on ~/.cache/torch/adapters/cc572bf7d9898961547ebfe30e6d501b04c32e6a1e60ec59aaae75a18b3a3590.50448f00b5bcda4df943da413ec313f7a77c0ed75146a1a38189cd6dfffac6ca.lock\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558621791184 acquired on ~/.cache/torch/adapters/cc572bf7d9898961547ebfe30e6d501b04c32e6a1e60ec59aaae75a18b3a3590.50448f00b5bcda4df943da413ec313f7a77c0ed75146a1a38189cd6dfffac6ca.lock\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558621791184 released on ~/.cache/torch/adapters/cc572bf7d9898961547ebfe30e6d501b04c32e6a1e60ec59aaae75a18b3a3590.50448f00b5bcda4df943da413ec313f7a77c0ed75146a1a38189cd6dfffac6ca.lock\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558622507216 acquired on ~/.cache/torch/adapters/796b1323bf803d85c1f67e01d72ed7d89e346491b4c5447b499b36c53a74a48c.efda12f289afd269c1e3e7a3e0f31fa2bf419c4a6f71309b7ec88e09134c201b.lock\n",
      "Downloading: 100% 3.33M/3.33M [00:00<00:00, 16.0MB/s]\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558622507216 released on ~/.cache/torch/adapters/796b1323bf803d85c1f67e01d72ed7d89e346491b4c5447b499b36c53a74a48c.efda12f289afd269c1e3e7a3e0f31fa2bf419c4a6f71309b7ec88e09134c201b.lock\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558777956112 acquired on ~/.cache/torch/adapters/796b1323bf803d85c1f67e01d72ed7d89e346491b4c5447b499b36c53a74a48c.efda12f289afd269c1e3e7a3e0f31fa2bf419c4a6f71309b7ec88e09134c201b.lock\n",
      "08/02/2021 22:06:12 - INFO - filelock - Lock 140558777956112 released on ~/.cache/torch/adapters/796b1323bf803d85c1f67e01d72ed7d89e346491b4c5447b499b36c53a74a48c.efda12f289afd269c1e3e7a3e0f31fa2bf419c4a6f71309b7ec88e09134c201b.lock\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5805\n",
      "{'loss': 0.5416, 'learning_rate': 1.827734711455642e-05, 'epoch': 3.88}\n",
      "{'loss': 0.4624, 'learning_rate': 1.6554694229112837e-05, 'epoch': 7.75}\n",
      "{'loss': 0.3105, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.2467, 'learning_rate': 1.3109388458225669e-05, 'epoch': 15.5}\n",
      "{'loss': 0.2164, 'learning_rate': 1.1386735572782086e-05, 'epoch': 19.38}\n",
      "{'loss': 0.1158, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0748, 'learning_rate': 7.941429801894918e-06, 'epoch': 27.13}\n",
      "{'loss': 0.0486, 'learning_rate': 6.218776916451336e-06, 'epoch': 31.01}\n",
      "{'loss': 0.0344, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "{'loss': 0.0237, 'learning_rate': 2.773471145564169e-06, 'epoch': 38.76}\n",
      "{'loss': 0.0102, 'learning_rate': 1.0508182601205856e-06, 'epoch': 42.64}\n",
      "100% 5805/5805 [17:34<00:00,  5.49it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1054.8, 'train_samples_per_second': 22.014, 'train_steps_per_second': 5.503, 'train_loss': 0.18022266547295884, 'epoch': 45.0}\n",
      "100% 5805/5805 [17:34<00:00,  5.50it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters1/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters1/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters1/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters1/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters1/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters1/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters1/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters1/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters1/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters1/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters1/special_tokens_map.json\n",
      "08/02/2021 22:23:52 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.35it/s]\n",
      "08/02/2021 22:23:53 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_loss = 1.4877088069915771\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_f1 = 0.78125\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_precision = 0.78125\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_recall = 0.78125\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_runtime = 1.0916\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_samples_per_second = 58.632\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   eval_steps_per_second = 7.329\n",
      "08/02/2021 22:23:53 - INFO - __main__ -   epoch = 45.0\n",
      "08/02/2021 22:23:53 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.14it/s]\n",
      "08/02/2021 22:23:54 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_loss = 0.531856119632721\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_acc = 0.9384615384615385\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_f1 = 0.9384615384615385\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_precision = 0.9384615384615385\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_recall = 0.9384615384615385\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_runtime = 1.1218\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_samples_per_second = 57.941\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   eval_steps_per_second = 8.023\n",
      "08/02/2021 22:23:54 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mce6cXo9-JLF",
    "outputId": "0f570638-ed9a-406d-a6bc-e66b5359f7f4"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters2/ \\\n",
    "--task_name mlm \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 22:25:31.189391: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 22:25:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2925\n",
      "{'loss': 0.4267, 'learning_rate': 1.6581196581196585e-05, 'epoch': 7.69}\n",
      "{'loss': 0.2091, 'learning_rate': 1.3162393162393164e-05, 'epoch': 15.38}\n",
      "{'loss': 0.0924, 'learning_rate': 9.743589743589744e-06, 'epoch': 23.08}\n",
      "{'loss': 0.0465, 'learning_rate': 6.324786324786325e-06, 'epoch': 30.77}\n",
      "{'loss': 0.0301, 'learning_rate': 2.9059829059829063e-06, 'epoch': 38.46}\n",
      "100% 2925/2925 [16:05<00:00,  3.49it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 965.2643, 'train_samples_per_second': 24.056, 'train_steps_per_second': 3.03, 'train_loss': 0.13856756837958964, 'epoch': 45.0}\n",
      "100% 2925/2925 [16:05<00:00,  3.03it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters2/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters2/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters2/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters2/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters2/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters2/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters2/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters2/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters2/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters2/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters2/special_tokens_map.json\n",
      "08/02/2021 22:41:49 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.38it/s]\n",
      "08/02/2021 22:41:50 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_loss = 1.4983899593353271\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_f1 = 0.78125\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_precision = 0.78125\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_recall = 0.78125\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_runtime = 1.0884\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_samples_per_second = 58.802\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   eval_steps_per_second = 7.35\n",
      "08/02/2021 22:41:50 - INFO - __main__ -   epoch = 45.0\n",
      "08/02/2021 22:41:50 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.18it/s]\n",
      "08/02/2021 22:41:51 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_loss = 0.3434101939201355\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_f1 = 0.9538461538461539\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_precision = 0.9538461538461539\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_recall = 0.9538461538461539\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_runtime = 1.1187\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_samples_per_second = 58.101\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   eval_steps_per_second = 8.045\n",
      "08/02/2021 22:41:51 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yazz_K16CAhA",
    "outputId": "efa46720-3a75-470e-874f-a436e3c3b22c"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters3/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 22:42:22.342056: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 22:42:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2925\n",
      "{'loss': 0.4267, 'learning_rate': 1.6581196581196585e-05, 'epoch': 7.69}\n",
      "{'loss': 0.2091, 'learning_rate': 1.3162393162393164e-05, 'epoch': 15.38}\n",
      "{'loss': 0.0924, 'learning_rate': 9.743589743589744e-06, 'epoch': 23.08}\n",
      "{'loss': 0.0465, 'learning_rate': 6.324786324786325e-06, 'epoch': 30.77}\n",
      "{'loss': 0.0301, 'learning_rate': 2.9059829059829063e-06, 'epoch': 38.46}\n",
      "100% 2925/2925 [16:06<00:00,  3.49it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 966.093, 'train_samples_per_second': 24.035, 'train_steps_per_second': 3.028, 'train_loss': 0.13856756837958964, 'epoch': 45.0}\n",
      "100% 2925/2925 [16:06<00:00,  3.03it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters3/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters3/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters3/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters3/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters3/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters3/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters3/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters3/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters3/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters3/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters3/special_tokens_map.json\n",
      "08/02/2021 22:58:41 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.36it/s]\n",
      "08/02/2021 22:58:42 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_loss = 1.4983899593353271\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_f1 = 0.78125\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_precision = 0.78125\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_recall = 0.78125\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_runtime = 1.0894\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_samples_per_second = 58.747\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   eval_steps_per_second = 7.343\n",
      "08/02/2021 22:58:42 - INFO - __main__ -   epoch = 45.0\n",
      "08/02/2021 22:58:42 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.16it/s]\n",
      "08/02/2021 22:58:43 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_loss = 0.3434101939201355\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_f1 = 0.9538461538461539\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_precision = 0.9538461538461539\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_recall = 0.9538461538461539\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_runtime = 1.1206\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_samples_per_second = 58.004\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   eval_steps_per_second = 8.031\n",
      "08/02/2021 22:58:43 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy0wDpASGupu"
   },
   "source": [
    "5. Got the best result using adapterhub adapters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vuxqe-cCFmU1",
    "outputId": "3f8c83b6-1e5c-4491-f67d-a9f78e7a25df"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters4/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 23:01:09.062824: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 23:01:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3404, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0911, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0472, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [15:35<00:00,  2.07it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 935.4169, 'train_samples_per_second': 24.823, 'train_steps_per_second': 2.069, 'train_loss': 0.1294691021744287, 'epoch': 45.0}\n",
      "100% 1935/1935 [15:35<00:00,  2.07it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters4/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters4/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters4/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters4/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters4/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters4/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters4/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters4/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters4/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters4/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters4/special_tokens_map.json\n",
      "08/02/2021 23:16:57 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.34it/s]\n",
      "08/02/2021 23:16:58 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_loss = 1.3568799495697021\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_acc = 0.765625\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_f1 = 0.765625\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_precision = 0.765625\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_recall = 0.765625\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_runtime = 1.092\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_samples_per_second = 58.609\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   eval_steps_per_second = 7.326\n",
      "08/02/2021 23:16:58 - INFO - __main__ -   epoch = 45.0\n",
      "08/02/2021 23:16:58 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.13it/s]\n",
      "08/02/2021 23:16:59 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_loss = 0.21356406807899475\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_acc = 0.9692307692307692\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_f1 = 0.9692307692307692\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_precision = 0.9692307692307692\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_recall = 0.9692307692307692\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_runtime = 1.1231\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_samples_per_second = 57.876\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   eval_steps_per_second = 8.014\n",
      "08/02/2021 23:16:59 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nudpotxOKbFt",
    "outputId": "ffdab9ac-9b5c-421e-84c1-f26969e12d81"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters5/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-02 23:19:18.129710: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/02/2021 23:19:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.4357, 'learning_rate': 7.416020671834626e-06, 'epoch': 11.63}\n",
      "{'loss': 0.2072, 'learning_rate': 4.832041343669251e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0991, 'learning_rate': 2.2480620155038763e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [15:35<00:00,  2.07it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 935.8808, 'train_samples_per_second': 24.811, 'train_steps_per_second': 2.068, 'train_loss': 0.2044000453111121, 'epoch': 45.0}\n",
      "100% 1935/1935 [15:35<00:00,  2.07it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters5/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters5/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters5/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters5/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters5/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters5/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters5/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters5/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters5/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters5/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters5/special_tokens_map.json\n",
      "08/02/2021 23:35:06 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.35it/s]\n",
      "08/02/2021 23:35:07 - INFO - __main__ - ***** Eval results *****\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_loss = 1.0730494260787964\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_f1 = 0.78125\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_precision = 0.78125\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_recall = 0.78125\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_runtime = 1.0926\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_samples_per_second = 58.577\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   eval_steps_per_second = 7.322\n",
      "08/02/2021 23:35:07 - INFO - __main__ -   epoch = 45.0\n",
      "08/02/2021 23:35:07 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.14it/s]\n",
      "08/02/2021 23:35:08 - INFO - __main__ - ***** Test results {} *****\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_loss = 0.41700243949890137\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_acc = 0.9076923076923077\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_f1 = 0.9076923076923076\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_precision = 0.9076923076923077\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_recall = 0.9076923076923077\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_runtime = 1.1232\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_samples_per_second = 57.868\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   eval_steps_per_second = 8.013\n",
      "08/02/2021 23:35:08 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0EXGfK1Jhg-",
    "outputId": "1a1842b9-632d-4acb-c157-19373d0f78f3"
   },
   "source": [
    "%cd \"/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\"\n",
    "!ls\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\n",
      "'~'\t       __pycache__   run_mlm.py\t\t      tapos-training\n",
      " data\t       README.md     run_multiple_choice.py   utils_multiple_choice.py\n",
      " experiments   results\t     tapos_test.ipynb\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPWdZnz6JjUb",
    "outputId": "7e508997-a36b-4571-f74a-dc4781c1f797"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters6/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 2"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 03:56:03.595854: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 03:56:05 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007486288 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "Downloading: 100% 481/481 [00:00<00:00, 578kB/s]\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007486288 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033102451920 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "Downloading: 100% 899k/899k [00:00<00:00, 13.5MB/s]\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033102451920 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007486928 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 8.86MB/s]\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007486928 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007444880 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 18.0MB/s]\n",
      "08/03/2021 03:56:06 - INFO - filelock - Lock 140033007444880 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "08/03/2021 03:56:07 - INFO - filelock - Lock 140033007486928 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Downloading: 100% 501M/501M [00:12<00:00, 39.6MB/s]\n",
      "08/03/2021 03:56:20 - INFO - filelock - Lock 140033007486928 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3356, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.1059, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0319, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [28:26<00:00,  1.14it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1706.8929, 'train_samples_per_second': 13.604, 'train_steps_per_second': 1.134, 'train_loss': 0.12501164074091947, 'epoch': 45.0}\n",
      "100% 1935/1935 [28:26<00:00,  1.13it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters6/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters6/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters6/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters6/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters6/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters6/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters6/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters6/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters6/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters6/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters6/special_tokens_map.json\n",
      "08/03/2021 04:25:03 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.61it/s]\n",
      "08/03/2021 04:25:05 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_loss = 1.230163812637329\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_acc = 0.828125\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_f1 = 0.828125\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_precision = 0.828125\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_recall = 0.828125\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_runtime = 1.9849\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_samples_per_second = 32.243\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   eval_steps_per_second = 4.03\n",
      "08/03/2021 04:25:05 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 04:25:05 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  5.09it/s]\n",
      "08/03/2021 04:25:07 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_loss = 0.12309622764587402\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_acc = 0.9692307692307692\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_f1 = 0.9692307692307692\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_precision = 0.9692307692307692\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_recall = 0.9692307692307692\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_runtime = 2.0184\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_samples_per_second = 32.204\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   eval_steps_per_second = 4.459\n",
      "08/03/2021 04:25:07 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVHo2nQrUdQD",
    "outputId": "5562fea0-65e1-4b47-bd9d-618918cb8237"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters7/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 3"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 04:42:31.202372: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 04:42:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3576, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0892, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0245, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1708.3774, 'train_samples_per_second': 13.592, 'train_steps_per_second': 1.133, 'train_loss': 0.12333046841683006, 'epoch': 45.0}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters7/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters7/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters7/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters7/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters7/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters7/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters7/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters7/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters7/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters7/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters7/special_tokens_map.json\n",
      "08/03/2021 05:11:10 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.61it/s]\n",
      "08/03/2021 05:11:12 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_loss = 1.095016360282898\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_acc = 0.828125\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_f1 = 0.828125\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_precision = 0.828125\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_recall = 0.828125\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_runtime = 1.9796\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_samples_per_second = 32.33\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   eval_steps_per_second = 4.041\n",
      "08/03/2021 05:11:12 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 05:11:12 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  5.09it/s]\n",
      "08/03/2021 05:11:14 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_loss = 0.1204051822423935\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_acc = 0.9846153846153847\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_f1 = 0.9846153846153847\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_precision = 0.9846153846153847\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_recall = 0.9846153846153847\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_runtime = 2.0174\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_samples_per_second = 32.219\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   eval_steps_per_second = 4.461\n",
      "08/03/2021 05:11:14 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9xghxcQUhyd",
    "outputId": "a1a49bb2-10f0-459d-f5af-034a9506200c"
   },
   "source": [
    " !python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters8/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 4"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 05:11:18.409976: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 05:11:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3113, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0535, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0217, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1708.5904, 'train_samples_per_second': 13.59, 'train_steps_per_second': 1.133, 'train_loss': 0.10196253680443579, 'epoch': 45.0}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters8/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters8/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters8/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters8/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters8/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters8/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters8/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters8/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters8/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters8/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters8/special_tokens_map.json\n",
      "08/03/2021 05:39:58 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.60it/s]\n",
      "08/03/2021 05:40:00 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_loss = 1.1120259761810303\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_runtime = 1.985\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_samples_per_second = 32.241\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   eval_steps_per_second = 4.03\n",
      "08/03/2021 05:40:00 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 05:40:00 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  5.08it/s]\n",
      "08/03/2021 05:40:02 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_loss = 0.1906178891658783\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_acc = 0.9384615384615385\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_f1 = 0.9384615384615385\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_precision = 0.9384615384615385\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_recall = 0.9384615384615385\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_runtime = 2.0215\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_samples_per_second = 32.155\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   eval_steps_per_second = 4.452\n",
      "08/03/2021 05:40:02 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64wj80hvUvTz",
    "outputId": "99eeacb6-0767-4057-b52f-edf282409656"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters9/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 05:40:04.958064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 05:40:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3647, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0821, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0169, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1708.6885, 'train_samples_per_second': 13.589, 'train_steps_per_second': 1.132, 'train_loss': 0.12093316787897154, 'epoch': 45.0}\n",
      "100% 1935/1935 [28:28<00:00,  1.13it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters9/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters9/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters9/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters9/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters9/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters9/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters9/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters9/multinli,qqp,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters9/multinli,qqp,scitail/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters9/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters9/special_tokens_map.json\n",
      "08/03/2021 06:08:44 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.60it/s]\n",
      "08/03/2021 06:08:46 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_loss = 1.2788139581680298\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_runtime = 1.9853\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_samples_per_second = 32.237\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   eval_steps_per_second = 4.03\n",
      "08/03/2021 06:08:46 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 06:08:46 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  5.06it/s]\n",
      "08/03/2021 06:08:48 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_loss = 0.2934716045856476\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_f1 = 0.9538461538461539\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_precision = 0.9538461538461539\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_recall = 0.9538461538461539\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_runtime = 2.0258\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_samples_per_second = 32.085\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   eval_steps_per_second = 4.443\n",
      "08/03/2021 06:08:48 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Ufm3qNoyGQ"
   },
   "source": [
    "6. Got the best result using new adapter combining with adapterhub adapters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c-kzRPAoU-v",
    "outputId": "c6a5870f-59dc-40f4-fdca-fd394afec7fc"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters10/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 06:12:00.885859: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 06:12:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.2939, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0795, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0414, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [30:57<00:00,  1.04it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1857.6373, 'train_samples_per_second': 12.5, 'train_steps_per_second': 1.042, 'train_loss': 0.11096527459393483, 'epoch': 45.0}\n",
      "100% 1935/1935 [30:57<00:00,  1.04it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters10/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters10/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters10/special_tokens_map.json\n",
      "08/03/2021 06:43:11 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.28it/s]\n",
      "08/03/2021 06:43:13 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_loss = 1.399237871170044\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_runtime = 2.1345\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_samples_per_second = 29.983\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   eval_steps_per_second = 3.748\n",
      "08/03/2021 06:43:13 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 06:43:13 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  4.69it/s]\n",
      "08/03/2021 06:43:15 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_loss = 0.14436683058738708\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_acc = 0.9846153846153847\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_f1 = 0.9846153846153847\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_precision = 0.9846153846153847\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_recall = 0.9846153846153847\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_runtime = 2.1865\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_samples_per_second = 29.728\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   eval_steps_per_second = 4.116\n",
      "08/03/2021 06:43:15 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2LjoGsUwTP6"
   },
   "source": [
    "7. Running only with the new ag **adaptar**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Js2AgzfwXPJ",
    "outputId": "f2609134-bb1e-4672-b9f5-35d2ac35ffd1"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters11/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 06:45:16.507174: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 06:45:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.5087, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.3394, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.2595, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [22:16<00:00,  1.45it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1336.068, 'train_samples_per_second': 17.379, 'train_steps_per_second': 1.448, 'train_loss': 0.33768503351729046, 'epoch': 45.0}\n",
      "100% 1935/1935 [22:16<00:00,  1.45it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters11/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters11/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters11/special_tokens_map.json\n",
      "08/03/2021 07:07:41 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  5.72it/s]\n",
      "08/03/2021 07:07:43 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_loss = 0.8011829853057861\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_acc = 0.75\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_f1 = 0.75\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_precision = 0.75\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_recall = 0.75\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_runtime = 1.5979\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_samples_per_second = 40.052\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   eval_steps_per_second = 5.007\n",
      "08/03/2021 07:07:43 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 07:07:43 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  6.29it/s]\n",
      "08/03/2021 07:07:44 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_loss = 0.5381713509559631\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_acc = 0.8615384615384616\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_f1 = 0.8615384615384615\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_precision = 0.8615384615384616\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_recall = 0.8615384615384616\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_runtime = 1.6302\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_samples_per_second = 39.872\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   eval_steps_per_second = 5.521\n",
      "08/03/2021 07:07:44 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZUvWs-q2AIo"
   },
   "source": [
    "Added one `[\"nli/scitail@ukp\"]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pO9iSmyH16-n",
    "outputId": "5fb2a2a6-c23e-4854-896a-d1dd0f4a7b66"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters12/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 07:09:08.697535: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 07:09:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3362, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0573, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0143, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [25:49<00:00,  1.25it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1549.24, 'train_samples_per_second': 14.988, 'train_steps_per_second': 1.249, 'train_loss': 0.10635265687947433, 'epoch': 45.0}\n",
      "100% 1935/1935 [25:49<00:00,  1.25it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters12/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters12/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters12/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters12/scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters12/scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters12/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters12/special_tokens_map.json\n",
      "08/03/2021 07:35:07 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  5.12it/s]\n",
      "08/03/2021 07:35:09 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_loss = 1.4180727005004883\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_acc = 0.796875\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_f1 = 0.796875\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_precision = 0.796875\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_recall = 0.796875\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_runtime = 1.7835\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_samples_per_second = 35.885\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   eval_steps_per_second = 4.486\n",
      "08/03/2021 07:35:09 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 07:35:09 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  5.63it/s]\n",
      "08/03/2021 07:35:11 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_loss = 0.4320991635322571\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_acc = 0.9384615384615385\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_f1 = 0.9384615384615385\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_precision = 0.9384615384615385\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_recall = 0.9384615384615385\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_runtime = 1.8209\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_samples_per_second = 35.697\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   eval_steps_per_second = 4.943\n",
      "08/03/2021 07:35:11 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SjipffQDCSJ"
   },
   "source": [
    "with all adapter including new one"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqSsm-vf8XQl",
    "outputId": "7b486fc6-dda8-416c-d6a0-d26e3e3dcb2f"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters13/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--weight_decay 0.01 \\\n",
    "--seed 5"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 07:38:45.506552: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 07:38:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3081, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0755, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0335, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [30:56<00:00,  1.04it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1856.1923, 'train_samples_per_second': 12.509, 'train_steps_per_second': 1.042, 'train_loss': 0.11079558118677263, 'epoch': 45.0}\n",
      "100% 1935/1935 [30:56<00:00,  1.04it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters13/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters13/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters13/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters13/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters13/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters13/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters13/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters13/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters13/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters13/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters13/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters13/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters13/special_tokens_map.json\n",
      "08/03/2021 08:09:52 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.29it/s]\n",
      "08/03/2021 08:09:54 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_loss = 1.3927960395812988\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_runtime = 2.13\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_samples_per_second = 30.047\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   eval_steps_per_second = 3.756\n",
      "08/03/2021 08:09:54 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 08:09:54 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  4.73it/s]\n",
      "08/03/2021 08:09:56 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_loss = 0.22502243518829346\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_acc = 0.9692307692307692\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_f1 = 0.9692307692307692\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_precision = 0.9692307692307692\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_recall = 0.9692307692307692\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_runtime = 2.1704\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_samples_per_second = 29.948\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   eval_steps_per_second = 4.147\n",
      "08/03/2021 08:09:56 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bm4F5CEHddY"
   },
   "source": [
    "8. Running created adapter for rct-sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ak7_86qSEKf0",
    "outputId": "b1453401-4b01-4d62-c950-222f02661e23"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/rct-sample_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters14/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 08:11:39.431595: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 08:11:41 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1890\n",
      "{'loss': 0.7281, 'learning_rate': 1.470899470899471e-05, 'epoch': 11.9}\n",
      "{'loss': 0.2327, 'learning_rate': 9.417989417989418e-06, 'epoch': 23.81}\n",
      "{'loss': 0.1296, 'learning_rate': 4.126984126984127e-06, 'epoch': 35.71}\n",
      "100% 1890/1890 [30:02<00:00,  1.15it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1802.7267, 'train_samples_per_second': 12.481, 'train_steps_per_second': 1.048, 'train_loss': 0.3069705599830264, 'epoch': 45.0}\n",
      "100% 1890/1890 [30:02<00:00,  1.05it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters14/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters14/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters14/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters14/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters14/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters14/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters14/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters14/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters14/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters14/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters14/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters14/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters14/special_tokens_map.json\n",
      "08/03/2021 08:41:59 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30212\n",
      "  Batch size = 8\n",
      "100% 3777/3777 [16:44<00:00,  3.76it/s]\n",
      "08/03/2021 08:58:45 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_loss = 1.0639758110046387\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_acc = 0.7764795445518337\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_f1 = 0.7764795445518338\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_precision = 0.7764795445518337\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_recall = 0.7764795445518337\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_runtime = 1005.181\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_samples_per_second = 30.056\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   eval_steps_per_second = 3.758\n",
      "08/03/2021 08:58:45 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 08:58:45 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30135\n",
      "  Batch size = 8\n",
      "100% 3767/3767 [16:42<00:00,  3.76it/s]\n",
      "08/03/2021 09:15:27 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_loss = 1.0992480516433716\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_acc = 0.7707980753276921\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_f1 = 0.7707980753276921\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_precision = 0.7707980753276921\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_recall = 0.7707980753276921\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_runtime = 1002.8901\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_samples_per_second = 30.048\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   eval_steps_per_second = 3.756\n",
      "08/03/2021 09:15:27 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGtahuTdZ_TT",
    "outputId": "2d1dc414-5e4a-4d79-dd80-a6cf3dc7225b"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters16/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 09:47:06.155481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 09:47:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.2939, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0795, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0414, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [30:57<00:00,  1.04it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1857.3596, 'train_samples_per_second': 12.502, 'train_steps_per_second': 1.042, 'train_loss': 0.11096527459393483, 'epoch': 45.0}\n",
      "100% 1935/1935 [30:57<00:00,  1.04it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters16/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters16/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters16/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters16/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters16/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters16/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters16/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters16/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters16/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters16/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters16/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters16/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters16/special_tokens_map.json\n",
      "08/03/2021 10:18:14 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  4.28it/s]\n",
      "08/03/2021 10:18:17 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_loss = 1.399237871170044\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_f1 = 0.7963944856839873\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_precision = 0.8227272727272728\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_recall = 0.7874493927125505\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_runtime = 2.1312\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_samples_per_second = 30.03\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   eval_steps_per_second = 3.754\n",
      "08/03/2021 10:18:17 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 10:18:17 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  4.73it/s]\n",
      "08/03/2021 10:18:19 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_loss = 0.14436683058738708\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_acc = 0.9846153846153847\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_f1 = 0.9840725312423425\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_precision = 0.9871794871794872\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_recall = 0.9814814814814814\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_runtime = 2.1728\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_samples_per_second = 29.915\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   eval_steps_per_second = 4.142\n",
      "08/03/2021 10:18:19 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdlSxT6uIAlm"
   },
   "source": [
    "9. Running Combination of newly created adapter `ag` and adapterhub adapters on citation_intent dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iblo4TCMiZ6n",
    "outputId": "f6b62e43-7a6c-4ceb-e2b0-d195a0e8d3d8"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/citation-intent_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 10 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters17/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 10:23:27.114523: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 10:23:28 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1688\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6345\n",
      "{'loss': 1.1408, 'learning_rate': 1.8423955870764383e-05, 'epoch': 3.55}\n",
      "{'loss': 0.8214, 'learning_rate': 1.6847911741528765e-05, 'epoch': 7.09}\n",
      "{'loss': 0.6743, 'learning_rate': 1.5271867612293146e-05, 'epoch': 10.64}\n",
      "{'loss': 0.5702, 'learning_rate': 1.3695823483057526e-05, 'epoch': 14.18}\n",
      "{'loss': 0.4779, 'learning_rate': 1.2119779353821908e-05, 'epoch': 17.73}\n",
      "{'loss': 0.4053, 'learning_rate': 1.054373522458629e-05, 'epoch': 21.28}\n",
      "{'loss': 0.3522, 'learning_rate': 8.967691095350671e-06, 'epoch': 24.82}\n",
      "{'loss': 0.3176, 'learning_rate': 7.391646966115052e-06, 'epoch': 28.37}\n",
      "{'loss': 0.2802, 'learning_rate': 5.815602836879432e-06, 'epoch': 31.91}\n",
      "{'loss': 0.2542, 'learning_rate': 4.239558707643815e-06, 'epoch': 35.46}\n",
      "{'loss': 0.2517, 'learning_rate': 2.6635145784081955e-06, 'epoch': 39.01}\n",
      "{'loss': 0.2211, 'learning_rate': 1.087470449172577e-06, 'epoch': 42.55}\n",
      "100% 6345/6345 [1:41:23<00:00,  1.15it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 6083.2937, 'train_samples_per_second': 12.487, 'train_steps_per_second': 1.043, 'train_loss': 0.46627292978754187, 'epoch': 45.0}\n",
      "100% 6345/6345 [1:41:23<00:00,  1.04it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters17/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters17/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters17/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters17/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters17/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters17/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters17/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters17/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters17/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters17/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters17/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters17/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters17/special_tokens_map.json\n",
      "08/03/2021 12:05:02 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 8\n",
      " 93% 14/15 [00:03<00:00,  3.76it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100% 15/15 [00:03<00:00,  4.22it/s]\n",
      "08/03/2021 12:05:06 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_loss = 1.0229520797729492\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_acc = 0.7631578947368421\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_f1 = 0.5335428695598187\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_precision = 0.5440749863163655\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_recall = 0.5362368328470023\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_runtime = 3.8147\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_samples_per_second = 29.885\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   eval_steps_per_second = 3.932\n",
      "08/03/2021 12:05:06 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 12:05:06 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 8\n",
      "100% 18/18 [00:04<00:00,  4.11it/s]\n",
      "08/03/2021 12:05:10 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_loss = 0.9662972688674927\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_acc = 0.7697841726618705\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_f1 = 0.5639359323244261\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_precision = 0.5615079365079364\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_recall = 0.5770066865841513\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_runtime = 4.649\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_samples_per_second = 29.899\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   eval_steps_per_second = 3.872\n",
      "08/03/2021 12:05:10 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS594NFc5vDz",
    "outputId": "d7122421-30d6-4f4c-822e-6e01a2200db0"
   },
   "source": [
    "%cd \"/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\"\n",
    "!ls"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\n",
      "'~'\t       README.md\t\t\t       run_multiple_choice.py\n",
      " data\t       results\t\t\t\t       tapos_test.ipynb\n",
      " experiments   run_mlm.py\t\t\t       tapos-training\n",
      " __pycache__   run_multiple_choice_adapter_fusion.py   utils_multiple_choice.py\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjgTaGNs6IXY",
    "outputId": "4a8c7af5-dd9d-4cfd-faf7-c76cc39c43c6"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/citation-intent_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 10 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters18/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 21:27:08.425129: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 21:27:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/03/2021 21:27:12 - INFO - filelock - Lock 140171582913040 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "Downloading: 100% 481/481 [00:00<00:00, 517kB/s]\n",
      "08/03/2021 21:27:12 - INFO - filelock - Lock 140171582913040 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "08/03/2021 21:27:13 - INFO - filelock - Lock 140171582934352 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "Downloading: 100% 899k/899k [00:00<00:00, 3.66MB/s]\n",
      "08/03/2021 21:27:14 - INFO - filelock - Lock 140171582934352 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "08/03/2021 21:27:14 - INFO - filelock - Lock 140171582892560 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 2.73MB/s]\n",
      "08/03/2021 21:27:15 - INFO - filelock - Lock 140171582892560 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "08/03/2021 21:27:15 - INFO - filelock - Lock 140171582810768 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 4.20MB/s]\n",
      "08/03/2021 21:27:16 - INFO - filelock - Lock 140171582810768 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "08/03/2021 21:27:17 - INFO - filelock - Lock 140171582870608 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Downloading: 100% 501M/501M [00:07<00:00, 70.2MB/s]\n",
      "08/03/2021 21:27:24 - INFO - filelock - Lock 140171582870608 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1688\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1410\n",
      "{'loss': 1.1315, 'learning_rate': 1.2907801418439719e-05, 'epoch': 3.55}\n",
      "{'loss': 0.8646, 'learning_rate': 5.815602836879432e-06, 'epoch': 7.09}\n",
      "100% 1410/1410 [12:17<00:00,  2.11it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 737.6024, 'train_samples_per_second': 22.885, 'train_steps_per_second': 1.912, 'train_loss': 0.9299257968334441, 'epoch': 10.0}\n",
      "100% 1410/1410 [12:17<00:00,  1.91it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters18/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters18/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters18/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters18/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters18/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters18/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters18/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters18/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters18/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters18/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters18/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters18/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters18/special_tokens_map.json\n",
      "08/03/2021 21:40:04 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 8\n",
      "100% 15/15 [00:01<00:00,  7.78it/s]\n",
      "08/03/2021 21:40:06 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_loss = 0.8216890096664429\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_acc = 0.6754385964912281\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_f1 = 0.6754385964912281\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_precision = 0.6754385964912281\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_recall = 0.6754385964912281\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_runtime = 2.0765\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_samples_per_second = 54.899\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   eval_steps_per_second = 7.224\n",
      "08/03/2021 21:40:06 - INFO - __main__ -   epoch = 10.0\n",
      "08/03/2021 21:40:06 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 8\n",
      "100% 18/18 [00:02<00:00,  7.58it/s]\n",
      "08/03/2021 21:40:08 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_loss = 0.8520898818969727\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_acc = 0.7194244604316546\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_f1 = 0.7194244604316546\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_precision = 0.7194244604316546\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_recall = 0.7194244604316546\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_runtime = 2.5207\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_samples_per_second = 55.142\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   eval_steps_per_second = 7.141\n",
      "08/03/2021 21:40:08 - INFO - __main__ -   epoch = 10.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH0EpX57Ia79"
   },
   "source": [
    "10. Running Combination of newly created adapter `ag` and adapterhub adapters on hyperpartisan news with lower number of epocs to compare with citation_intent dataset performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6GLpHja6Q5H",
    "outputId": "5ebad138-60bf-4852-e99d-fb5d50f0e342"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 10 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters19/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 21:40:11.465087: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 21:40:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 430\n",
      "100% 430/430 [03:46<00:00,  1.89it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 226.9778, 'train_samples_per_second': 22.734, 'train_steps_per_second': 1.894, 'train_loss': 0.34493287552234736, 'epoch': 10.0}\n",
      "100% 430/430 [03:46<00:00,  1.89it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters19/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters19/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters19/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters19/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters19/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters19/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters19/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters19/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters19/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters19/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters19/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters19/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters19/special_tokens_map.json\n",
      "08/03/2021 21:44:13 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  7.79it/s]\n",
      "08/03/2021 21:44:14 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_loss = 0.520887553691864\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_runtime = 1.1722\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_samples_per_second = 54.598\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   eval_steps_per_second = 6.825\n",
      "08/03/2021 21:44:14 - INFO - __main__ -   epoch = 10.0\n",
      "08/03/2021 21:44:14 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  8.55it/s]\n",
      "08/03/2021 21:44:15 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_loss = 0.3732253611087799\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_acc = 0.9076923076923077\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_f1 = 0.9076923076923076\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_precision = 0.9076923076923077\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_recall = 0.9076923076923077\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_runtime = 1.1993\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_samples_per_second = 54.199\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   eval_steps_per_second = 7.505\n",
      "08/03/2021 21:44:15 - INFO - __main__ -   epoch = 10.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d46rC1_AMDv",
    "outputId": "9914fe9c-9723-49e8-c35c-43189492f858"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/citation-intent_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 10 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters20/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 21:53:07.383590: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 21:53:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1688\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1410\n",
      "{'loss': 1.1068, 'learning_rate': 1.2907801418439719e-05, 'epoch': 3.55}\n",
      "{'loss': 0.8422, 'learning_rate': 5.815602836879432e-06, 'epoch': 7.09}\n",
      "100% 1410/1410 [12:16<00:00,  2.11it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 736.9447, 'train_samples_per_second': 22.905, 'train_steps_per_second': 1.913, 'train_loss': 0.9086809307125443, 'epoch': 10.0}\n",
      "100% 1410/1410 [12:16<00:00,  1.91it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters20/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters20/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters20/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters20/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters20/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters20/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters20/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters20/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters20/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters20/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters20/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters20/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters20/special_tokens_map.json\n",
      "08/03/2021 22:05:38 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 8\n",
      "100% 15/15 [00:01<00:00,  7.77it/s]\n",
      "08/03/2021 22:05:40 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_loss = 0.7913118600845337\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_acc = 0.7105263157894737\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_f1 = 0.7105263157894737\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_precision = 0.7105263157894737\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_recall = 0.7105263157894737\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_runtime = 2.0735\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_samples_per_second = 54.981\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   eval_steps_per_second = 7.234\n",
      "08/03/2021 22:05:40 - INFO - __main__ -   epoch = 10.0\n",
      "08/03/2021 22:05:40 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 8\n",
      "100% 18/18 [00:02<00:00,  7.57it/s]\n",
      "08/03/2021 22:05:43 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_loss = 0.8254960775375366\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_acc = 0.7194244604316546\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_f1 = 0.7194244604316546\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_precision = 0.7194244604316546\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_recall = 0.7194244604316546\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_runtime = 2.5231\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_samples_per_second = 55.092\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   eval_steps_per_second = 7.134\n",
      "08/03/2021 22:05:43 - INFO - __main__ -   epoch = 10.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGYFnAQ4J0TM",
    "outputId": "07879b15-b94a-4d38-851d-b1d11901bcc5"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters21/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 22:35:06.656747: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 22:35:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3281, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0903, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0345, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [17:00<00:00,  1.90it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1020.5452, 'train_samples_per_second': 22.753, 'train_steps_per_second': 1.896, 'train_loss': 0.11978166811842018, 'epoch': 45.0}\n",
      "100% 1935/1935 [17:00<00:00,  1.90it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters21/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters21/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters21/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters21/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters21/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters21/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters21/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters21/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters21/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters21/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters21/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters21/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters21/special_tokens_map.json\n",
      "08/03/2021 22:52:20 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  7.77it/s]\n",
      "08/03/2021 22:52:21 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_loss = 1.1362582445144653\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_f1 = 0.8125\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_precision = 0.8125\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_recall = 0.8125\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_runtime = 1.1736\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_samples_per_second = 54.532\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   eval_steps_per_second = 6.817\n",
      "08/03/2021 22:52:21 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 22:52:21 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  8.54it/s]\n",
      "08/03/2021 22:52:23 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_loss = 0.3018531799316406\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_f1 = 0.9538461538461539\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_precision = 0.9538461538461539\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_recall = 0.9538461538461539\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_runtime = 1.2013\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_samples_per_second = 54.107\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   eval_steps_per_second = 7.492\n",
      "08/03/2021 22:52:23 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0V1Sk60N7_b"
   },
   "source": [
    "11. Running experiment using created chemprot adapter only on hyperpartisan dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkGhYvsFOCUK",
    "outputId": "ba889cdf-22a4-407f-d593-7f5a4405114e"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters22/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 22:54:26.013502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 22:54:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.4805, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.2474, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.1787, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [11:56<00:00,  2.70it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 716.5287, 'train_samples_per_second': 32.406, 'train_steps_per_second': 2.701, 'train_loss': 0.26549685968601117, 'epoch': 45.0}\n",
      "100% 1935/1935 [11:56<00:00,  2.70it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters22/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters22/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters22/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters22/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters22/mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters22/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters22/special_tokens_map.json\n",
      "08/03/2021 23:06:33 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00, 10.62it/s]\n",
      "08/03/2021 23:06:34 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_loss = 0.668175995349884\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_acc = 0.84375\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_f1 = 0.84375\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_precision = 0.84375\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_recall = 0.84375\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_runtime = 0.8582\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_samples_per_second = 74.574\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   eval_steps_per_second = 9.322\n",
      "08/03/2021 23:06:34 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 23:06:34 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 11.71it/s]\n",
      "08/03/2021 23:06:35 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_loss = 0.43813344836235046\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_acc = 0.9076923076923077\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_f1 = 0.9076923076923076\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_precision = 0.9076923076923077\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_recall = 0.9076923076923077\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_runtime = 0.8755\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_samples_per_second = 74.241\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   eval_steps_per_second = 10.279\n",
      "08/03/2021 23:06:35 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VBAnlwGOGtW",
    "outputId": "3157428f-7c9c-4b6d-f386-4281c698319b"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters23/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 23:06:37.799043: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 23:06:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.6347, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.5608, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.5029, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [11:55<00:00,  2.70it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 715.907, 'train_samples_per_second': 32.434, 'train_steps_per_second': 2.703, 'train_loss': 0.5438151357093831, 'epoch': 45.0}\n",
      "100% 1935/1935 [11:55<00:00,  2.70it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters23/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters23/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters23/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters23/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters23/mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters23/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters23/special_tokens_map.json\n",
      "08/03/2021 23:18:44 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00, 10.57it/s]\n",
      "08/03/2021 23:18:45 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_loss = 0.5359475612640381\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_acc = 0.75\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_f1 = 0.75\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_precision = 0.75\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_recall = 0.75\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_runtime = 0.8595\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_samples_per_second = 74.462\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   eval_steps_per_second = 9.308\n",
      "08/03/2021 23:18:45 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 23:18:45 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 11.67it/s]\n",
      "08/03/2021 23:18:46 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_loss = 0.5092417001724243\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_acc = 0.7538461538461538\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_f1 = 0.7538461538461538\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_precision = 0.7538461538461538\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_recall = 0.7538461538461538\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_runtime = 0.8793\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_samples_per_second = 73.926\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   eval_steps_per_second = 10.236\n",
      "08/03/2021 23:18:46 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMgxxR_nOL_i",
    "outputId": "ac4ae5da-f463-4b14-905e-80b4b6c8d9fd"
   },
   "source": [
    "!python3 run_multiple_choice.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters24/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 23:18:49.370625: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 23:18:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.52, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.3279, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.2607, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [11:56<00:00,  2.70it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 716.1154, 'train_samples_per_second': 32.425, 'train_steps_per_second': 2.702, 'train_loss': 0.33886110369857275, 'epoch': 45.0}\n",
      "100% 1935/1935 [11:56<00:00,  2.70it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters24/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters24/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters24/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters24/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters24/mlm/pytorch_model_adapter_fusion.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters24/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters24/special_tokens_map.json\n",
      "08/03/2021 23:30:56 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00, 10.66it/s]\n",
      "08/03/2021 23:30:57 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_loss = 0.770103931427002\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_acc = 0.796875\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_f1 = 0.796875\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_precision = 0.796875\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_recall = 0.796875\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_runtime = 0.8554\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_samples_per_second = 74.819\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   eval_steps_per_second = 9.352\n",
      "08/03/2021 23:30:57 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 23:30:57 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 11.80it/s]\n",
      "08/03/2021 23:30:58 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_loss = 0.5413658618927002\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_acc = 0.8615384615384616\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_f1 = 0.8615384615384615\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_precision = 0.8615384615384616\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_recall = 0.8615384615384616\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_runtime = 0.8694\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_samples_per_second = 74.764\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   eval_steps_per_second = 10.352\n",
      "08/03/2021 23:30:58 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCwwzgbNJTA8"
   },
   "source": [
    "11. Running experiment using created chemprot adapter only on hyperpartisan dataset and combining with only \"nli/scitail@ukp\" adapter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVMQ7Rc8XxAh",
    "outputId": "8acad85a-e905-4918-9c41-eb69af0827a7"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters27/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--adapter_2 \"nli/scitail@ukp\" \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-03 23:45:06.322306: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/03/2021 23:45:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3763, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0764, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0111, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [13:49<00:00,  2.34it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 829.4297, 'train_samples_per_second': 27.995, 'train_steps_per_second': 2.333, 'train_loss': 0.12063315191934275, 'epoch': 45.0}\n",
      "100% 1935/1935 [13:49<00:00,  2.33it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters27/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters27/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters27/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters27/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters27/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters27/mlm,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters27/mlm,scitail/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters27/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters27/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters27/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters27/special_tokens_map.json\n",
      "08/03/2021 23:59:10 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  9.38it/s]\n",
      "08/03/2021 23:59:11 - INFO - __main__ - ***** Eval results *****\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_loss = 1.1109331846237183\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_acc = 0.859375\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_f1 = 0.8457831325301204\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_precision = 0.8847953216374269\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_recall = 0.832995951417004\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_runtime = 0.984\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_samples_per_second = 65.043\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   eval_steps_per_second = 8.13\n",
      "08/03/2021 23:59:11 - INFO - __main__ -   epoch = 45.0\n",
      "08/03/2021 23:59:11 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 10.44it/s]\n",
      "08/03/2021 23:59:12 - INFO - __main__ - ***** Test results {} *****\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_loss = 0.3639879822731018\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_f1 = 0.9522175937270277\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_precision = 0.9551282051282051\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_recall = 0.9498050682261209\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_runtime = 0.9828\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_samples_per_second = 66.136\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   eval_steps_per_second = 9.157\n",
      "08/03/2021 23:59:12 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWnvz9RDdUQe",
    "outputId": "e4ec62ac-9e07-4384-b5d9-d4c07576e772"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters28/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--adapter_2 \"nli/scitail@ukp\" \\\n",
    "--adapter_3 \"nli/multinli@ukp\" \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 00:00:16.714138: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 00:00:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3161, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.0499, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0197, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [15:25<00:00,  2.09it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 925.9319, 'train_samples_per_second': 25.077, 'train_steps_per_second': 2.09, 'train_loss': 0.10105861175891966, 'epoch': 45.0}\n",
      "100% 1935/1935 [15:25<00:00,  2.09it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters28/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters28/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters28/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters28/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters28/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters28/multinli/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters28/multinli/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters28/mlm,scitail,multinli/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters28/mlm,scitail,multinli/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters28/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters28/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters28/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters28/special_tokens_map.json\n",
      "08/04/2021 00:15:57 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.32it/s]\n",
      "08/04/2021 00:15:59 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_loss = 1.5372681617736816\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_f1 = 0.7624602332979852\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_precision = 0.7863636363636364\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_recall = 0.7550607287449393\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_runtime = 1.1086\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_samples_per_second = 57.73\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   eval_steps_per_second = 7.216\n",
      "08/04/2021 00:15:59 - INFO - __main__ -   epoch = 45.0\n",
      "08/04/2021 00:15:59 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.28it/s]\n",
      "08/04/2021 00:16:00 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_loss = 0.19586485624313354\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_acc = 0.9538461538461539\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_f1 = 0.9522175937270277\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_precision = 0.9551282051282051\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_recall = 0.9498050682261209\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_runtime = 1.1061\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_samples_per_second = 58.766\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   eval_steps_per_second = 8.137\n",
      "08/04/2021 00:16:00 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LqeZ8_BhU1F",
    "outputId": "e7fc62ee-2407-4398-c884-19b7c66a1ae5"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters29/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--adapter_2 \"nli/scitail@ukp\" \\\n",
    "--adapter_3 \"sts/qqp@ukp\" \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 00:17:46.281322: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 00:17:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.3426, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.1314, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.0497, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [15:25<00:00,  2.09it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 925.5725, 'train_samples_per_second': 25.087, 'train_steps_per_second': 2.091, 'train_loss': 0.14104917560744964, 'epoch': 45.0}\n",
      "100% 1935/1935 [15:25<00:00,  2.09it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters29/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters29/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters29/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters29/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters29/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters29/qqp/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters29/qqp/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters29/mlm,scitail,qqp/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters29/mlm,scitail,qqp/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters29/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters29/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters29/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters29/special_tokens_map.json\n",
      "08/04/2021 00:33:25 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  8.45it/s]\n",
      "08/04/2021 00:33:26 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_loss = 1.1500147581100464\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_acc = 0.8125\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_f1 = 0.7963944856839873\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_precision = 0.8227272727272728\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_recall = 0.7874493927125505\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_runtime = 1.0786\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_samples_per_second = 59.334\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   eval_steps_per_second = 7.417\n",
      "08/04/2021 00:33:26 - INFO - __main__ -   epoch = 45.0\n",
      "08/04/2021 00:33:26 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00,  9.27it/s]\n",
      "08/04/2021 00:33:28 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_loss = 0.3648495674133301\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_acc = 0.9076923076923077\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_f1 = 0.9038461538461539\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_precision = 0.91\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_recall = 0.8996101364522417\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_runtime = 1.1081\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_samples_per_second = 58.658\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   eval_steps_per_second = 8.122\n",
      "08/04/2021 00:33:28 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eaM4CLy72Tn",
    "outputId": "4ce952b0-d691-4c78-80c7-32678a2dda7a"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 10 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters30/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--adapter_2 \"nli/scitail@ukp\" \\\n",
    "--load_best_model_at_end \\\n",
    "--weight_decay 0.1 \\\n",
    "--adam_beta1 0.9 \\\n",
    "--adam_beta2 0.98 \\\n",
    "--adam_epsilon 1.e-6 \\\n",
    "--seed 5 \\\n",
    "--overwrite_output_dir"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 02:20:53.831671: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 02:20:55 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 430\n",
      "100% 430/430 [03:04<00:00,  2.33it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 184.6093, 'train_samples_per_second': 27.951, 'train_steps_per_second': 2.329, 'train_loss': 0.42643358097519984, 'epoch': 10.0}\n",
      "100% 430/430 [03:04<00:00,  2.33it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters30/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters30/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters30/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters30/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters30/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters30/mlm,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters30/mlm,scitail/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters30/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters30/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters30/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters30/special_tokens_map.json\n",
      "08/04/2021 02:24:13 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  9.35it/s]\n",
      "08/04/2021 02:24:14 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_loss = 0.6546000242233276\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_acc = 0.78125\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_f1 = 0.7454545454545455\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_precision = 0.8342857142857143\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_recall = 0.736842105263158\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_runtime = 0.9871\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_samples_per_second = 64.837\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   eval_steps_per_second = 8.105\n",
      "08/04/2021 02:24:14 - INFO - __main__ -   epoch = 10.0\n",
      "08/04/2021 02:24:14 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 10.50it/s]\n",
      "08/04/2021 02:24:15 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_loss = 0.609829843044281\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_acc = 0.7692307692307693\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_f1 = 0.732803507810359\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_precision = 0.8270308123249299\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_recall = 0.7275828460038987\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_runtime = 0.9769\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_samples_per_second = 66.535\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   eval_steps_per_second = 9.213\n",
      "08/04/2021 02:24:15 - INFO - __main__ -   epoch = 10.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQz9lI5-9eFZ",
    "outputId": "346111b7-c410-47c5-9942-cb90a8367491"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 20 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters31/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/chemprot/mlm \\\n",
    "--adapter_2 \"nli/scitail@ukp\" \\\n",
    "--load_best_model_at_end \\\n",
    "--weight_decay 0.1 \\\n",
    "--adam_beta1 0.9 \\\n",
    "--adam_beta2 0.98 \\\n",
    "--adam_epsilon 1.e-6 \\\n",
    "--seed 5"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 02:24:18.699486: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 02:24:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 860\n",
      "{'loss': 0.3807, 'learning_rate': 8.372093023255815e-06, 'epoch': 11.63}\n",
      "100% 860/860 [06:09<00:00,  2.32it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 369.6803, 'train_samples_per_second': 27.916, 'train_steps_per_second': 2.326, 'train_loss': 0.28076648712158203, 'epoch': 20.0}\n",
      "100% 860/860 [06:09<00:00,  2.33it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters31/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters31/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters31/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters31/scitail/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters31/scitail/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters31/mlm,scitail/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters31/mlm,scitail/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters31/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters31/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters31/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters31/special_tokens_map.json\n",
      "08/04/2021 02:30:42 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:00<00:00,  9.50it/s]\n",
      "08/04/2021 02:30:43 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_loss = 0.8560089468955994\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_acc = 0.828125\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_f1 = 0.8073871409028728\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_precision = 0.8642052565707135\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_recall = 0.7945344129554657\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_runtime = 0.96\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_samples_per_second = 66.665\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   eval_steps_per_second = 8.333\n",
      "08/04/2021 02:30:43 - INFO - __main__ -   epoch = 20.0\n",
      "08/04/2021 02:30:43 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:00<00:00, 10.44it/s]\n",
      "08/04/2021 02:30:44 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_loss = 0.5558419227600098\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_acc = 0.8769230769230769\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_f1 = 0.8678861788617886\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_precision = 0.8966450216450217\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_recall = 0.8572124756335282\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_runtime = 0.982\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_samples_per_second = 66.188\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   eval_steps_per_second = 9.165\n",
      "08/04/2021 02:30:44 - INFO - __main__ -   epoch = 20.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd \"/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\"\n",
    "!ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. Best Score Using ag adapater"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters32/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "12. Best Score Using hyperpartisan adapater"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters33/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cBaBqkPqb0Y",
    "outputId": "255b6d68-1a8e-4a8b-ce93-92fa270ae078"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters33/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Masters/CS7643/final_project/gatech_deep_final\n",
      "'~'\t       README.md\t\t\t       run_multiple_choice.py\n",
      " data\t       results\t\t\t\t       tapos_test.ipynb\n",
      " experiments   run_mlm.py\t\t\t       tapos-training\n",
      " __pycache__   run_multiple_choice_adapter_fusion.py   utils_multiple_choice.py\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InrtjFFcw3Sz"
   },
   "source": [
    "11. Best Score Using ag adapater"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUDebq3Sqduu",
    "outputId": "b42df21a-6171-4a24-bdc7-852d48844b2f"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters32/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/ag/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 05:38:14.488494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 05:38:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/04/2021 05:38:20 - INFO - filelock - Lock 139631717989776 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "Downloading: 100% 481/481 [00:00<00:00, 480kB/s]\n",
      "08/04/2021 05:38:21 - INFO - filelock - Lock 139631717989776 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "08/04/2021 05:38:23 - INFO - filelock - Lock 139631717914960 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "Downloading: 100% 899k/899k [00:00<00:00, 1.70MB/s]\n",
      "08/04/2021 05:38:24 - INFO - filelock - Lock 139631717914960 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "08/04/2021 05:38:25 - INFO - filelock - Lock 139631717926288 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 1.17MB/s]\n",
      "08/04/2021 05:38:26 - INFO - filelock - Lock 139631717926288 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "08/04/2021 05:38:27 - INFO - filelock - Lock 139631717968784 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 1.94MB/s]\n",
      "08/04/2021 05:38:28 - INFO - filelock - Lock 139631717968784 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
      "08/04/2021 05:38:31 - INFO - filelock - Lock 139631717927248 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Downloading: 100% 501M/501M [00:06<00:00, 72.7MB/s]\n",
      "08/04/2021 05:38:38 - INFO - filelock - Lock 139631717927248 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.5087, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.3394, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.2595, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [22:05<00:00,  1.46it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1325.8541, 'train_samples_per_second': 17.513, 'train_steps_per_second': 1.459, 'train_loss': 0.33768503351729046, 'epoch': 45.0}\n",
      "100% 1935/1935 [22:05<00:00,  1.46it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters32/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters32/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters32/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters32/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters32/mlm/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters32/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters32/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters32/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters32/special_tokens_map.json\n",
      "08/04/2021 06:00:59 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  5.75it/s]\n",
      "08/04/2021 06:01:00 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_loss = 0.8011829853057861\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_acc = 0.75\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_f1 = 0.7285259809119831\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_precision = 0.75\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_recall = 0.7226720647773279\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_runtime = 1.6096\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_samples_per_second = 39.762\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   eval_steps_per_second = 4.97\n",
      "08/04/2021 06:01:00 - INFO - __main__ -   epoch = 45.0\n",
      "08/04/2021 06:01:00 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  6.34it/s]\n",
      "08/04/2021 06:01:02 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_loss = 0.5381713509559631\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_acc = 0.8615384615384616\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_f1 = 0.8526077097505669\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_precision = 0.8731501057082452\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_recall = 0.8440545808966862\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_runtime = 1.6184\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_samples_per_second = 40.162\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   eval_steps_per_second = 5.561\n",
      "08/04/2021 06:01:02 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-z95ZNRxHhL"
   },
   "source": [
    "12. Best Score Using hyperpartisan adapater"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmL9YcGLqn-j",
    "outputId": "ef94f4b9-71f4-4984-bd5f-9789ccfa5a61"
   },
   "source": [
    "!python3 run_multiple_choice_adapter_fusion.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir data/hyperpartisan_news_ \\\n",
    "--max_seq_length 512 \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--num_train_epochs 45 \\\n",
    "--output_dir results/hyperpartisan_news_adapterhub_adapters33/ \\\n",
    "--task_name hb3 \\\n",
    "--do_predict \\\n",
    "--model_name_or_path roberta-base \\\n",
    "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
    "--load_best_model_at_end \\\n",
    "--seed 5"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-08-04 06:01:32.392336: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "08/04/2021 06:01:33 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 516\n",
      "  Num Epochs = 45\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1935\n",
      "{'loss': 0.6319, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
      "{'loss': 0.5614, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
      "{'loss': 0.5007, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
      "100% 1935/1935 [22:05<00:00,  1.46it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1325.1736, 'train_samples_per_second': 17.522, 'train_steps_per_second': 1.46, 'train_loss': 0.5423720248909883, 'epoch': 45.0}\n",
      "100% 1935/1935 [22:05<00:00,  1.46it/s]\n",
      "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters33/\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters33/mlm/adapter_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters33/mlm/pytorch_adapter.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters33/mlm/adapter_fusion_config.json\n",
      "Module weights saved in results/hyperpartisan_news_adapterhub_adapters33/mlm/pytorch_model_adapter_fusion.bin\n",
      "Configuration saved in results/hyperpartisan_news_adapterhub_adapters33/config.json\n",
      "Model weights saved in results/hyperpartisan_news_adapterhub_adapters33/pytorch_model.bin\n",
      "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters33/tokenizer_config.json\n",
      "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters33/special_tokens_map.json\n",
      "08/04/2021 06:23:55 - INFO - __main__ - *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "100% 8/8 [00:01<00:00,  5.75it/s]\n",
      "08/04/2021 06:23:57 - INFO - __main__ - ***** Eval results *****\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_loss = 0.5267142057418823\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_acc = 0.75\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_f1 = 0.7005847953216375\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_precision = 0.8141025641025641\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_recall = 0.6983805668016194\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_runtime = 1.5873\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_samples_per_second = 40.319\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   eval_steps_per_second = 5.04\n",
      "08/04/2021 06:23:57 - INFO - __main__ -   epoch = 45.0\n",
      "08/04/2021 06:23:57 - INFO - root - *** Test ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 8\n",
      "100% 9/9 [00:01<00:00,  6.35it/s]\n",
      "08/04/2021 06:23:58 - INFO - __main__ - ***** Test results {} *****\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_loss = 0.5086063742637634\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_acc = 0.7384615384615385\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_f1 = 0.7046244319700614\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_precision = 0.7633928571428572\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_recall = 0.7012670565302144\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_runtime = 1.6158\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_samples_per_second = 40.228\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   eval_steps_per_second = 5.57\n",
      "08/04/2021 06:23:58 - INFO - __main__ -   epoch = 45.0\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}