{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experiment_II-part1",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iGSno326vIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1dbe52-d00f-489b-ef92-d300ad83a738"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/My Drive/DL/final_project/\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/DL/final_project\n",
            "'~'\t       run_multiple_choice2_obsolete.py\n",
            "'=1.1.3'       run_multiple_choice_adapter_fusion.py\n",
            " data\t       run_multiple_choice_adapter_fusion_wo_pfeiffer.py\n",
            " experiments   run.py\n",
            " __pycache__   tapos_test.ipynb\n",
            " README.md     tapos-training\n",
            " results       utils_multiple_choice.py\n",
            " run_mlm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmBoBpl8LAXJ",
        "outputId": "5a519c43-86f2-4bb2-ab08-4d778b998537"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siw8GzKUbnwq",
        "outputId": "132a410d-ffff-495b-aaa0-c23c02894743"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.4 MB/s eta 0:14:25tcmalloc: large alloc 1147494400 bytes == 0x55e518cba000 @  0x7f046448d615 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05caf40 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0558b99 0x55e4e059be79 0x55e4e05577b2 0x55e4e05cae65 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6b0e 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 1.6 MB/s eta 0:10:31tcmalloc: large alloc 1434370048 bytes == 0x55e55d310000 @  0x7f046448d615 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05caf40 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0558b99 0x55e4e059be79 0x55e4e05577b2 0x55e4e05cae65 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6b0e 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.4 MB/s eta 0:08:36tcmalloc: large alloc 1792966656 bytes == 0x55e4e2142000 @  0x7f046448d615 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05caf40 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0558b99 0x55e4e059be79 0x55e4e05577b2 0x55e4e05cae65 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6b0e 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.3 MB/s eta 0:04:21tcmalloc: large alloc 2241208320 bytes == 0x55e54cf2a000 @  0x7f046448d615 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05caf40 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0649a56 0x55e4e05c6fb3 0x55e4e0558b99 0x55e4e059be79 0x55e4e05577b2 0x55e4e05cae65 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6b0e 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x55e5d288c000 @  0x7f046448c1e7 0x55e4e0589ae7 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35\n",
            "tcmalloc: large alloc 2551685120 bytes == 0x55e64c354000 @  0x7f046448d615 0x55e4e055402c 0x55e4e063417a 0x55e4e0556e4d 0x55e4e0648c0d 0x55e4e05cb0d8 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c6d67 0x55e4e055865a 0x55e4e05c6d67 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e055873a 0x55e4e05c793b 0x55e4e05c5c35 0x55e4e0558dd1\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 2.9 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 29 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.20.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBUo6nXre2DJ",
        "outputId": "cd19138d-8380-4810-ed23-10411991d736"
      },
      "source": [
        "!pip3 install torch==1.9.0\n",
        "!pip3 install torchaudio==0.7.0\n",
        "!pip3 uninstall --yes folium\n",
        "!pip3 install folium==0.2.1\n",
        "# # !pip3 uninstall --yes adapter-transformers\n",
        "!pip3 install transformers\n",
        "# # !pip3 uninstall --yes pytorch-transformers\n",
        "# # !pip install gputil \n",
        "!pip3 install -U adapter-transformers\n",
        "!pip3 install datasets==1.8.0\n",
        "!pip3 install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Collecting torchaudio==0.7.0\n",
            "  Downloading torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.1 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.5)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0 torchaudio-0.7.0\n",
            "Found existing installation: folium 0.8.3\n",
            "Uninstalling folium-0.8.3:\n",
            "  Successfully uninstalled folium-0.8.3\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=41218a8bd81d82da5b6c72c079ca5398eca6f4350621674b35be003ee3ca8fa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 78.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 74.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 75.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-2.1.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.41.1)\n",
            "Collecting huggingface-hub>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.0.1)\n",
            "Installing collected packages: huggingface-hub, adapter-transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.0.12\n",
            "    Uninstalling huggingface-hub-0.0.12:\n",
            "      Successfully uninstalled huggingface-hub-0.0.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.9.1 requires huggingface-hub==0.0.12, but you have huggingface-hub 0.0.15 which is incompatible.\u001b[0m\n",
            "Successfully installed adapter-transformers-2.1.2 huggingface-hub-0.0.15\n",
            "Collecting datasets==1.8.0\n",
            "  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.0.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.19.5)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.41.1)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.8.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.8.0) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.8.0 fsspec-2021.7.0 xxhash-2.0.2\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ2HIgD9tKOT",
        "outputId": "3b0283b2-5793-4c47-e17d-4a5a14a15dde"
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.7 GB  | Proc size: 161.0 MB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total 15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ln12BF4HsPr",
        "outputId": "b93a5193-9651-4754-e272-4016074b467e"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Sun Aug  1 20:12:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    24W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivr9KhLdoAuj"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def jsonl2txt(path):\n",
        "    df = pd.DataFrame()\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            df = df.append(json.loads(line), ignore_index=True)\n",
        "    df.text.to_csv(path.replace(\"jsonl\", \"txt\"), header=False, index=False)\n",
        "# jsonl2txt('data/sciie_train.jsonl')\n",
        "# jsonl2txt('data/sciie_dev.jsonl')\n",
        "# jsonl2txt('data/sciie_test.jsonl')\n",
        "jsonl2txt('data/citation-intent_train.jsonl')\n",
        "jsonl2txt('data/citation-intent_dev.jsonl')\n",
        "jsonl2txt('data/citation-intent_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiYiIMOMk1BA",
        "outputId": "47781b36-bd7b-4121-a69b-e81a2b66d9c4"
      },
      "source": [
        "# jsonl2txt('data/rct-sample_train.jsonl')\n",
        "# jsonl2txt('data/rct-sample_dev.jsonl')\n",
        "# jsonl2txt('data/rct-sample_test.jsonl')\n",
        "!python3 run_mlm.py \\\n",
        "--train_file data/rct-sample_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/rct-sample_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/rct-sample \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 100 \\\n",
        "--learning_rate 1e-4 \\\n",
        "--logging_steps 50 \\\n",
        "--per_gpu_train_batch_size 8 \\\n",
        "--per_gpu_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 8  \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-01 23:41:10.733793: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/01/2021 23:41:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/01/2021 23:41:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=50,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=8,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/rct-sample/runs/Aug01_23-41-14_4449eb89c498,\n",
            "logging_first_step=False,\n",
            "logging_steps=50,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "output_dir=results/adapters/rct-sample,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=rct-sample,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/rct-sample,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/01/2021 23:41:14 - WARNING - datasets.builder -   Using custom data configuration default-ed34d87a4817818c\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-ed34d87a4817818c/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-ed34d87a4817818c/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:531] 2021-08-01 23:41:14,918 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-01 23:41:14,919 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-01 23:41:15,269 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-01 23:41:15,614 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-01 23:41:15,615 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-01 23:41:17,688 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|modeling_utils.py:1163] 2021-08-01 23:41:18,105 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-01 23:41:26,559 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-01 23:41:26,560 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-01 23:41:26,567 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00,  6.79ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 31/31 [00:01<00:00, 20.85ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-01 23:41:31,439 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-01 23:41:31,442 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-01 23:41:31,442 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-01 23:41:31,450 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-01 23:41:31,450 >>   Num examples = 500\n",
            "[INFO|trainer.py:1201] 2021-08-01 23:41:31,450 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1202] 2021-08-01 23:41:31,450 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-01 23:41:31,450 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1204] 2021-08-01 23:41:31,450 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1205] 2021-08-01 23:41:31,450 >>   Total optimization steps = 700\n",
            "[WARNING|training_args.py:760] 2021-08-01 23:41:31,463 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-01 23:41:31,463 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.5756, 'learning_rate': 9.285714285714286e-05, 'epoch': 7.13}\n",
            "{'loss': 2.3805, 'learning_rate': 8.571428571428571e-05, 'epoch': 14.25}\n",
            "{'loss': 2.3689, 'learning_rate': 7.857142857142858e-05, 'epoch': 21.38}\n",
            "{'loss': 2.2512, 'learning_rate': 7.142857142857143e-05, 'epoch': 28.51}\n",
            "{'loss': 2.2404, 'learning_rate': 6.428571428571429e-05, 'epoch': 35.63}\n",
            "{'loss': 2.1639, 'learning_rate': 5.714285714285714e-05, 'epoch': 42.76}\n",
            "{'loss': 2.1371, 'learning_rate': 5e-05, 'epoch': 49.89}\n",
            "{'loss': 2.1757, 'learning_rate': 4.2857142857142856e-05, 'epoch': 57.13}\n",
            " 59% 411/700 [02:41<01:57,  2.47it/s]\n",
            " 59% 411/700 [02:41<01:53,  2.54it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezk9wj-4WuQu"
      },
      "source": [
        "# jsonl2txt('data/rct-20k_train.jsonl')\n",
        "# jsonl2txt('data/rct-20k_dev.jsonl')\n",
        "# jsonl2txt('data/rct-20k_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb_ncAibmo8j"
      },
      "source": [
        "jsonl2txt('data/chemprot_train.jsonl')\n",
        "jsonl2txt('data/chemprot_dev.jsonl')\n",
        "jsonl2txt('data/chemprot_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETDCz6pdUqwg",
        "outputId": "d5678f46-d38f-4537-b032-5eeabb50f54f"
      },
      "source": [
        "# Chemprot new adapter\n",
        "!python3 run_mlm.py \\\n",
        "--train_file data/chemprot_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/chemprot_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/chemprot \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 90 \\\n",
        "--learning_rate 5e-4 \\\n",
        "--logging_steps 40 \\\n",
        "--per_gpu_train_batch_size 8 \\\n",
        "--per_gpu_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 8  \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 16:03:53.360657: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 16:03:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/02/2021 16:03:54 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=40,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=8,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/chemprot/runs/Aug02_16-03-54_9f6c530227e8,\n",
            "logging_first_step=False,\n",
            "logging_steps=40,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=90.0,\n",
            "output_dir=results/adapters/chemprot,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=chemprot,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/chemprot,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/02/2021 16:03:54 - WARNING - datasets.builder -   Using custom data configuration default-8be8d2c4c16004d0\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-8be8d2c4c16004d0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8be8d2c4c16004d0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1590] 2021-08-02 16:03:55,108 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmply2hljye\n",
            "Downloading: 100% 481/481 [00:00<00:00, 478kB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 16:03:55,241 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|file_utils.py:1602] 2021-08-02 16:03:55,242 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:531] 2021-08-02 16:03:55,242 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-02 16:03:55,243 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-02 16:03:55,378 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-02 16:03:55,513 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-02 16:03:55,513 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-08-02 16:03:55,650 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp04no27ez\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 11.4MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 16:03:55,870 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1602] 2021-08-02 16:03:55,870 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1590] 2021-08-02 16:03:55,997 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8qt43m1o\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 7.94MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 16:03:56,188 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1602] 2021-08-02 16:03:56,188 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1590] 2021-08-02 16:03:56,318 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpms97ajl3\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 15.4MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 16:03:56,541 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|file_utils.py:1602] 2021-08-02 16:03:56,541 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 16:03:56,938 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1590] 2021-08-02 16:03:57,133 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp670m8bio\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 66.6MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 16:04:04,834 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|file_utils.py:1602] 2021-08-02 16:04:04,834 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1163] 2021-08-02 16:04:04,834 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-02 16:04:06,478 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-02 16:04:06,478 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-02 16:04:06,485 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 5/5 [00:00<00:00, 11.18ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 3/3 [00:00<00:00, 17.82ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-02 16:04:09,700 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-02 16:04:09,702 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-02 16:04:09,702 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-02 16:04:09,711 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-02 16:04:09,711 >>   Num examples = 4169\n",
            "[INFO|trainer.py:1201] 2021-08-02 16:04:09,711 >>   Num Epochs = 90\n",
            "[INFO|trainer.py:1202] 2021-08-02 16:04:09,711 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-02 16:04:09,711 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1204] 2021-08-02 16:04:09,711 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1205] 2021-08-02 16:04:09,711 >>   Total optimization steps = 5850\n",
            "[WARNING|training_args.py:760] 2021-08-02 16:04:09,723 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-02 16:04:09,723 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.1601, 'learning_rate': 0.0004965811965811966, 'epoch': 0.61}\n",
            "{'loss': 1.9497, 'learning_rate': 0.0004931623931623932, 'epoch': 1.23}\n",
            "{'loss': 1.8538, 'learning_rate': 0.0004897435897435897, 'epoch': 1.84}\n",
            "{'loss': 1.7933, 'learning_rate': 0.0004863247863247863, 'epoch': 2.46}\n",
            "{'loss': 1.6995, 'learning_rate': 0.0004829059829059829, 'epoch': 3.08}\n",
            "{'loss': 1.6119, 'learning_rate': 0.0004794871794871795, 'epoch': 3.69}\n",
            "{'loss': 1.6353, 'learning_rate': 0.00047606837606837605, 'epoch': 4.31}\n",
            "{'loss': 1.5812, 'learning_rate': 0.0004726495726495726, 'epoch': 4.92}\n",
            "{'loss': 1.5114, 'learning_rate': 0.00046923076923076926, 'epoch': 5.54}\n",
            "{'loss': 1.4921, 'learning_rate': 0.00046581196581196583, 'epoch': 6.15}\n",
            "{'loss': 1.4381, 'learning_rate': 0.0004623931623931624, 'epoch': 6.77}\n",
            "{'loss': 1.4281, 'learning_rate': 0.000458974358974359, 'epoch': 7.38}\n",
            "{'loss': 1.4096, 'learning_rate': 0.00045555555555555556, 'epoch': 8.0}\n",
            "{'loss': 1.3451, 'learning_rate': 0.00045213675213675214, 'epoch': 8.61}\n",
            "{'loss': 1.34, 'learning_rate': 0.0004487179487179487, 'epoch': 9.23}\n",
            "{'loss': 1.3363, 'learning_rate': 0.0004452991452991453, 'epoch': 9.84}\n",
            "{'loss': 1.3016, 'learning_rate': 0.00044188034188034187, 'epoch': 10.46}\n",
            "{'loss': 1.2687, 'learning_rate': 0.00043846153846153845, 'epoch': 11.08}\n",
            "{'loss': 1.2528, 'learning_rate': 0.0004350427350427351, 'epoch': 11.69}\n",
            "{'loss': 1.2425, 'learning_rate': 0.00043162393162393166, 'epoch': 12.31}\n",
            "{'loss': 1.1975, 'learning_rate': 0.0004282051282051282, 'epoch': 12.92}\n",
            "{'loss': 1.1819, 'learning_rate': 0.0004247863247863248, 'epoch': 13.54}\n",
            "{'loss': 1.1907, 'learning_rate': 0.0004213675213675214, 'epoch': 14.15}\n",
            "{'loss': 1.1673, 'learning_rate': 0.00041794871794871796, 'epoch': 14.77}\n",
            "{'loss': 1.1408, 'learning_rate': 0.00041452991452991454, 'epoch': 15.38}\n",
            "{'loss': 1.137, 'learning_rate': 0.0004111111111111111, 'epoch': 16.0}\n",
            "{'loss': 1.1068, 'learning_rate': 0.0004076923076923077, 'epoch': 16.61}\n",
            "{'loss': 1.1069, 'learning_rate': 0.00040427350427350427, 'epoch': 17.23}\n",
            "{'loss': 1.0963, 'learning_rate': 0.0004008547008547009, 'epoch': 17.84}\n",
            "{'loss': 1.0991, 'learning_rate': 0.0003974358974358974, 'epoch': 18.46}\n",
            "{'loss': 1.0637, 'learning_rate': 0.000394017094017094, 'epoch': 19.08}\n",
            "{'loss': 1.0369, 'learning_rate': 0.00039059829059829063, 'epoch': 19.69}\n",
            "{'loss': 1.0596, 'learning_rate': 0.0003871794871794872, 'epoch': 20.31}\n",
            "{'loss': 0.983, 'learning_rate': 0.0003837606837606838, 'epoch': 20.92}\n",
            "{'loss': 1.0094, 'learning_rate': 0.0003803418803418803, 'epoch': 21.54}\n",
            "{'loss': 1.0291, 'learning_rate': 0.00037692307692307694, 'epoch': 22.15}\n",
            "{'loss': 0.9759, 'learning_rate': 0.0003735042735042735, 'epoch': 22.77}\n",
            "{'loss': 1.0048, 'learning_rate': 0.0003700854700854701, 'epoch': 23.38}\n",
            "{'loss': 1.0113, 'learning_rate': 0.00036666666666666667, 'epoch': 24.0}\n",
            "{'loss': 0.9856, 'learning_rate': 0.00036324786324786325, 'epoch': 24.61}\n",
            "{'loss': 0.9662, 'learning_rate': 0.0003598290598290598, 'epoch': 25.23}\n",
            "{'loss': 0.9527, 'learning_rate': 0.0003564102564102564, 'epoch': 25.84}\n",
            "{'loss': 0.9258, 'learning_rate': 0.00035299145299145303, 'epoch': 26.46}\n",
            "{'loss': 0.9116, 'learning_rate': 0.00034957264957264955, 'epoch': 27.08}\n",
            "{'loss': 0.9084, 'learning_rate': 0.00034615384615384613, 'epoch': 27.69}\n",
            "{'loss': 0.8928, 'learning_rate': 0.00034273504273504276, 'epoch': 28.31}\n",
            "{'loss': 0.883, 'learning_rate': 0.00033931623931623934, 'epoch': 28.92}\n",
            "{'loss': 0.8889, 'learning_rate': 0.00033589743589743586, 'epoch': 29.54}\n",
            "{'loss': 0.9053, 'learning_rate': 0.0003324786324786325, 'epoch': 30.15}\n",
            "{'loss': 0.8624, 'learning_rate': 0.00032905982905982907, 'epoch': 30.77}\n",
            "{'loss': 0.8656, 'learning_rate': 0.00032564102564102565, 'epoch': 31.38}\n",
            "{'loss': 0.8334, 'learning_rate': 0.0003222222222222222, 'epoch': 32.0}\n",
            "{'loss': 0.8731, 'learning_rate': 0.0003188034188034188, 'epoch': 32.61}\n",
            "{'loss': 0.8744, 'learning_rate': 0.0003153846153846154, 'epoch': 33.23}\n",
            "{'loss': 0.8359, 'learning_rate': 0.00031196581196581195, 'epoch': 33.84}\n",
            "{'loss': 0.8467, 'learning_rate': 0.0003085470085470086, 'epoch': 34.46}\n",
            "{'loss': 0.8174, 'learning_rate': 0.00030512820512820516, 'epoch': 35.08}\n",
            "{'loss': 0.812, 'learning_rate': 0.0003017094017094017, 'epoch': 35.69}\n",
            "{'loss': 0.8437, 'learning_rate': 0.0002982905982905983, 'epoch': 36.31}\n",
            "{'loss': 0.8001, 'learning_rate': 0.0002948717948717949, 'epoch': 36.92}\n",
            "{'loss': 0.7835, 'learning_rate': 0.00029145299145299147, 'epoch': 37.54}\n",
            "{'loss': 0.8101, 'learning_rate': 0.000288034188034188, 'epoch': 38.15}\n",
            "{'loss': 0.8033, 'learning_rate': 0.0002846153846153846, 'epoch': 38.77}\n",
            "{'loss': 0.7962, 'learning_rate': 0.0002811965811965812, 'epoch': 39.38}\n",
            "{'loss': 0.7571, 'learning_rate': 0.0002777777777777778, 'epoch': 40.0}\n",
            "{'loss': 0.7985, 'learning_rate': 0.0002743589743589744, 'epoch': 40.61}\n",
            "{'loss': 0.8071, 'learning_rate': 0.00027094017094017093, 'epoch': 41.23}\n",
            "{'loss': 0.7686, 'learning_rate': 0.0002675213675213675, 'epoch': 41.84}\n",
            "{'loss': 0.7592, 'learning_rate': 0.00026410256410256414, 'epoch': 42.46}\n",
            "{'loss': 0.7627, 'learning_rate': 0.0002606837606837607, 'epoch': 43.08}\n",
            "{'loss': 0.7793, 'learning_rate': 0.00025726495726495724, 'epoch': 43.69}\n",
            "{'loss': 0.7528, 'learning_rate': 0.0002538461538461538, 'epoch': 44.31}\n",
            "{'loss': 0.7601, 'learning_rate': 0.00025042735042735045, 'epoch': 44.92}\n",
            "{'loss': 0.7406, 'learning_rate': 0.000247008547008547, 'epoch': 45.54}\n",
            "{'loss': 0.7538, 'learning_rate': 0.0002435897435897436, 'epoch': 46.15}\n",
            "{'loss': 0.7261, 'learning_rate': 0.00024017094017094018, 'epoch': 46.77}\n",
            "{'loss': 0.7203, 'learning_rate': 0.00023675213675213675, 'epoch': 47.38}\n",
            "{'loss': 0.7124, 'learning_rate': 0.00023333333333333333, 'epoch': 48.0}\n",
            "{'loss': 0.7507, 'learning_rate': 0.0002299145299145299, 'epoch': 48.61}\n",
            "{'loss': 0.7197, 'learning_rate': 0.0002264957264957265, 'epoch': 49.23}\n",
            "{'loss': 0.7251, 'learning_rate': 0.0002230769230769231, 'epoch': 49.84}\n",
            "{'loss': 0.6859, 'learning_rate': 0.00021965811965811967, 'epoch': 50.46}\n",
            "{'loss': 0.6833, 'learning_rate': 0.00021623931623931624, 'epoch': 51.08}\n",
            "{'loss': 0.6695, 'learning_rate': 0.00021282051282051282, 'epoch': 51.69}\n",
            "{'loss': 0.698, 'learning_rate': 0.00020940170940170942, 'epoch': 52.31}\n",
            "{'loss': 0.6871, 'learning_rate': 0.00020598290598290597, 'epoch': 52.92}\n",
            "{'loss': 0.6947, 'learning_rate': 0.00020256410256410258, 'epoch': 53.54}\n",
            "{'loss': 0.6654, 'learning_rate': 0.00019914529914529913, 'epoch': 54.15}\n",
            "{'loss': 0.6687, 'learning_rate': 0.00019572649572649573, 'epoch': 54.77}\n",
            "{'loss': 0.7011, 'learning_rate': 0.00019230769230769233, 'epoch': 55.38}\n",
            "{'loss': 0.6542, 'learning_rate': 0.00018888888888888888, 'epoch': 56.0}\n",
            "{'loss': 0.6511, 'learning_rate': 0.0001854700854700855, 'epoch': 56.61}\n",
            "{'loss': 0.6654, 'learning_rate': 0.00018205128205128204, 'epoch': 57.23}\n",
            "{'loss': 0.6538, 'learning_rate': 0.00017863247863247864, 'epoch': 57.84}\n",
            "{'loss': 0.6678, 'learning_rate': 0.00017521367521367522, 'epoch': 58.46}\n",
            "{'loss': 0.6533, 'learning_rate': 0.0001717948717948718, 'epoch': 59.08}\n",
            "{'loss': 0.6726, 'learning_rate': 0.0001683760683760684, 'epoch': 59.69}\n",
            "{'loss': 0.6681, 'learning_rate': 0.00016495726495726495, 'epoch': 60.31}\n",
            "{'loss': 0.6712, 'learning_rate': 0.00016153846153846155, 'epoch': 60.92}\n",
            "{'loss': 0.6504, 'learning_rate': 0.0001581196581196581, 'epoch': 61.54}\n",
            "{'loss': 0.6684, 'learning_rate': 0.0001547008547008547, 'epoch': 62.15}\n",
            "{'loss': 0.6463, 'learning_rate': 0.00015128205128205128, 'epoch': 62.77}\n",
            "{'loss': 0.6363, 'learning_rate': 0.00014786324786324786, 'epoch': 63.38}\n",
            "{'loss': 0.6179, 'learning_rate': 0.00014444444444444444, 'epoch': 64.0}\n",
            "{'loss': 0.6474, 'learning_rate': 0.00014102564102564101, 'epoch': 64.61}\n",
            "{'loss': 0.6261, 'learning_rate': 0.00013760683760683762, 'epoch': 65.23}\n",
            "{'loss': 0.6401, 'learning_rate': 0.0001341880341880342, 'epoch': 65.84}\n",
            "{'loss': 0.637, 'learning_rate': 0.00013076923076923077, 'epoch': 66.46}\n",
            "{'loss': 0.6238, 'learning_rate': 0.00012735042735042735, 'epoch': 67.08}\n",
            "{'loss': 0.635, 'learning_rate': 0.00012393162393162393, 'epoch': 67.69}\n",
            "{'loss': 0.617, 'learning_rate': 0.00012051282051282052, 'epoch': 68.31}\n",
            "{'loss': 0.6407, 'learning_rate': 0.0001170940170940171, 'epoch': 68.92}\n",
            "{'loss': 0.628, 'learning_rate': 0.00011367521367521367, 'epoch': 69.54}\n",
            "{'loss': 0.6155, 'learning_rate': 0.00011025641025641026, 'epoch': 70.15}\n",
            "{'loss': 0.612, 'learning_rate': 0.00010683760683760684, 'epoch': 70.77}\n",
            "{'loss': 0.6012, 'learning_rate': 0.00010341880341880341, 'epoch': 71.38}\n",
            "{'loss': 0.6006, 'learning_rate': 0.0001, 'epoch': 72.0}\n",
            "{'loss': 0.6182, 'learning_rate': 9.658119658119658e-05, 'epoch': 72.61}\n",
            "{'loss': 0.6081, 'learning_rate': 9.316239316239317e-05, 'epoch': 73.23}\n",
            "{'loss': 0.6029, 'learning_rate': 8.974358974358975e-05, 'epoch': 73.84}\n",
            "{'loss': 0.607, 'learning_rate': 8.632478632478633e-05, 'epoch': 74.46}\n",
            "{'loss': 0.5936, 'learning_rate': 8.29059829059829e-05, 'epoch': 75.08}\n",
            "{'loss': 0.6077, 'learning_rate': 7.948717948717948e-05, 'epoch': 75.69}\n",
            "{'loss': 0.5993, 'learning_rate': 7.606837606837607e-05, 'epoch': 76.31}\n",
            "{'loss': 0.61, 'learning_rate': 7.264957264957266e-05, 'epoch': 76.92}\n",
            "{'loss': 0.6217, 'learning_rate': 6.923076923076924e-05, 'epoch': 77.54}\n",
            "{'loss': 0.5995, 'learning_rate': 6.581196581196581e-05, 'epoch': 78.15}\n",
            "{'loss': 0.5936, 'learning_rate': 6.239316239316239e-05, 'epoch': 78.77}\n",
            "{'loss': 0.5959, 'learning_rate': 5.8974358974358975e-05, 'epoch': 79.38}\n",
            "{'loss': 0.6008, 'learning_rate': 5.555555555555555e-05, 'epoch': 80.0}\n",
            "{'loss': 0.5916, 'learning_rate': 5.213675213675214e-05, 'epoch': 80.61}\n",
            "{'loss': 0.5939, 'learning_rate': 4.871794871794872e-05, 'epoch': 81.23}\n",
            "{'loss': 0.5942, 'learning_rate': 4.5299145299145296e-05, 'epoch': 81.84}\n",
            "{'loss': 0.5979, 'learning_rate': 4.188034188034188e-05, 'epoch': 82.46}\n",
            "{'loss': 0.58, 'learning_rate': 3.846153846153846e-05, 'epoch': 83.08}\n",
            "{'loss': 0.5771, 'learning_rate': 3.5042735042735046e-05, 'epoch': 83.69}\n",
            "{'loss': 0.5711, 'learning_rate': 3.162393162393162e-05, 'epoch': 84.31}\n",
            "{'loss': 0.5896, 'learning_rate': 2.8205128205128207e-05, 'epoch': 84.92}\n",
            "{'loss': 0.5986, 'learning_rate': 2.4786324786324784e-05, 'epoch': 85.54}\n",
            "{'loss': 0.5677, 'learning_rate': 2.1367521367521368e-05, 'epoch': 86.15}\n",
            "{'loss': 0.6017, 'learning_rate': 1.7948717948717948e-05, 'epoch': 86.77}\n",
            "{'loss': 0.5976, 'learning_rate': 1.4529914529914531e-05, 'epoch': 87.38}\n",
            "{'loss': 0.5941, 'learning_rate': 1.1111111111111112e-05, 'epoch': 88.0}\n",
            "{'loss': 0.592, 'learning_rate': 7.692307692307694e-06, 'epoch': 88.61}\n",
            "{'loss': 0.5786, 'learning_rate': 4.273504273504274e-06, 'epoch': 89.23}\n",
            "{'loss': 0.5706, 'learning_rate': 8.547008547008547e-07, 'epoch': 89.84}\n",
            "100% 5850/5850 [1:11:35<00:00,  1.39it/s][INFO|trainer.py:1403] 2021-08-02 17:15:45,725 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4296.0142, 'train_samples_per_second': 87.339, 'train_steps_per_second': 1.362, 'train_loss': 0.8654778985080556, 'epoch': 90.0}\n",
            "100% 5850/5850 [1:11:35<00:00,  1.36it/s]\n",
            "[INFO|trainer.py:1989] 2021-08-02 17:15:45,730 >> Saving model checkpoint to results/adapters/chemprot\n",
            "[INFO|loading.py:59] 2021-08-02 17:15:45,735 >> Configuration saved in results/adapters/chemprot/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 17:15:45,761 >> Module weights saved in results/adapters/chemprot/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-02 17:15:45,765 >> Configuration saved in results/adapters/chemprot/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 17:15:46,312 >> Module weights saved in results/adapters/chemprot/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-02 17:15:46,315 >> Configuration saved in results/adapters/chemprot/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 17:15:46,876 >> Module weights saved in results/adapters/chemprot/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-02 17:15:46,894 >> tokenizer config file saved in results/adapters/chemprot/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-02 17:15:49,129 >> Special tokens file saved in results/adapters/chemprot/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       90.0\n",
            "  train_loss               =     0.8655\n",
            "  train_runtime            = 1:11:36.01\n",
            "  train_samples            =       4169\n",
            "  train_samples_per_second =     87.339\n",
            "  train_steps_per_second   =      1.362\n",
            "08/02/2021 17:15:49 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:547] 2021-08-02 17:15:49,250 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-02 17:15:49,253 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-02 17:15:49,253 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-02 17:15:49,253 >>   Num examples = 2427\n",
            "[INFO|trainer.py:2244] 2021-08-02 17:15:49,253 >>   Batch size = 8\n",
            "100% 303/304 [00:11<00:00, 21.85it/s][WARNING|training_args.py:774] 2021-08-02 17:16:00,412 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 304/304 [00:11<00:00, 27.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       90.0\n",
            "  eval_loss               =     1.8613\n",
            "  eval_runtime            = 0:00:11.15\n",
            "  eval_samples            =       2427\n",
            "  eval_samples_per_second =    217.478\n",
            "  eval_steps_per_second   =     27.241\n",
            "  perplexity              =     6.4321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Py1H5KiAl2k",
        "outputId": "e5739a63-8eb5-48b3-9604-11dafaab1576"
      },
      "source": [
        "#chemprot_1 average='micro'\n",
        "# Experiment_3\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/chemprot_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/chemprot_1/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/chemprot/mlm \\\n",
        "--per_device_eval_batch_size 12 \\\n",
        "--weight_decay 0.12 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.95 \\\n",
        "--adam_epsilon 5e-4 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 1 \\\n",
        "--avg_type micro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.40it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.41it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.41it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.40it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.41it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.41it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6690123081207275, 'eval_acc': 0.8051091882983107, 'eval_f1': 0.8051091882983107, 'eval_precision': 0.8051091882983107, 'eval_recall': 0.8051091882983107, 'eval_runtime': 31.6599, 'eval_samples_per_second': 76.659, 'eval_steps_per_second': 6.412, 'epoch': 18.0}\n",
            " 45% 6264/13920 [50:15<40:39,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-6264\n",
            "Configuration saved in results/chemprot_1/checkpoint-6264/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6264/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6264/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6264/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6264/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6264/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6264/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6264/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6264/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-6264/pytorch_model.bin\n",
            "{'loss': 0.4019, 'learning_rate': 2.132183908045977e-05, 'epoch': 18.68}\n",
            " 48% 6612/13920 [52:31<38:41,  3.15it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.84it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.86it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.47it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.67it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.58it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.48it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.44it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.40it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.41it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.41it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.41it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:19,  6.41it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.41it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.41it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.40it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.40it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.41it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.40it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.40it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.41it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.41it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.41it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 116/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.40it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.40it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.40it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.40it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6381774544715881, 'eval_acc': 0.8273588792748249, 'eval_f1': 0.8273588792748249, 'eval_precision': 0.8273588792748249, 'eval_recall': 0.8273588792748249, 'eval_runtime': 31.6397, 'eval_samples_per_second': 76.707, 'eval_steps_per_second': 6.416, 'epoch': 19.0}\n",
            " 48% 6612/13920 [53:02<38:41,  3.15it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-6612\n",
            "Configuration saved in results/chemprot_1/checkpoint-6612/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6612/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6612/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6612/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6612/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6612/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6612/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6612/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6612/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-6612/pytorch_model.bin\n",
            " 50% 6960/13920 [55:18<37:01,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.77it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.80it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.69it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.37it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.36it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.40it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.41it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.41it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.41it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.41it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.41it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.40it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6817213296890259, 'eval_acc': 0.814997939843428, 'eval_f1': 0.814997939843428, 'eval_precision': 0.814997939843428, 'eval_recall': 0.814997939843428, 'eval_runtime': 31.6886, 'eval_samples_per_second': 76.589, 'eval_steps_per_second': 6.406, 'epoch': 20.0}\n",
            " 50% 6960/13920 [55:50<37:01,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-6960\n",
            "Configuration saved in results/chemprot_1/checkpoint-6960/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6960/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6960/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6960/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6960/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-6960/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-6960/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-6960/pytorch_model.bin\n",
            "{'loss': 0.3936, 'learning_rate': 1.9885057471264367e-05, 'epoch': 20.11}\n",
            " 52% 7308/13920 [58:06<35:12,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.82it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.85it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.68it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.23it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.95it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:27,  6.40it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.36it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.37it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.36it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.37it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.40it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6690061688423157, 'eval_acc': 0.8215904408735064, 'eval_f1': 0.8215904408735064, 'eval_precision': 0.8215904408735064, 'eval_recall': 0.8215904408735064, 'eval_runtime': 31.6982, 'eval_samples_per_second': 76.566, 'eval_steps_per_second': 6.404, 'epoch': 21.0}\n",
            " 52% 7308/13920 [58:37<35:12,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-7308\n",
            "Configuration saved in results/chemprot_1/checkpoint-7308/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7308/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7308/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7308/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7308/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7308/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7308/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7308/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7308/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-7308/pytorch_model.bin\n",
            "{'loss': 0.3734, 'learning_rate': 1.8448275862068967e-05, 'epoch': 21.55}\n",
            " 55% 7656/13920 [1:00:53<33:10,  3.15it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.78it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.81it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.47it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.50it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.44it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.41it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.40it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:27,  6.40it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.40it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.40it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.40it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.41it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.41it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.41it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.41it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:22,  6.41it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.41it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.41it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.41it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.40it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.40it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.41it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.41it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.41it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.41it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.41it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.41it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.41it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.40it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.40it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.41it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.41it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.42it/s]\u001b[A\n",
            " 57% 116/203 [00:17<00:13,  6.41it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.41it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.41it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.41it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.41it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.41it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 73% 148/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6785973906517029, 'eval_acc': 0.8182941903584673, 'eval_f1': 0.8182941903584674, 'eval_precision': 0.8182941903584673, 'eval_recall': 0.8182941903584673, 'eval_runtime': 31.6455, 'eval_samples_per_second': 76.693, 'eval_steps_per_second': 6.415, 'epoch': 22.0}\n",
            " 55% 7656/13920 [1:01:25<33:10,  3.15it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-7656\n",
            "Configuration saved in results/chemprot_1/checkpoint-7656/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7656/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7656/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7656/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7656/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7656/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7656/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-7656/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-7656/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-7656/pytorch_model.bin\n",
            "{'loss': 0.3756, 'learning_rate': 1.7011494252873563e-05, 'epoch': 22.99}\n",
            " 57% 8004/13920 [1:03:41<31:24,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.82it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.85it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.48it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.73it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.28it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.58it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.43it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.40it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.40it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.41it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:27,  6.41it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.42it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.41it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.41it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.40it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.41it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.41it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:24,  6.41it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.41it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:14,  6.40it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.40it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 116/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6781356334686279, 'eval_acc': 0.80758137618459, 'eval_f1': 0.8075813761845899, 'eval_precision': 0.80758137618459, 'eval_recall': 0.80758137618459, 'eval_runtime': 31.6636, 'eval_samples_per_second': 76.649, 'eval_steps_per_second': 6.411, 'epoch': 23.0}\n",
            " 57% 8004/13920 [1:04:12<31:24,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-8004\n",
            "Configuration saved in results/chemprot_1/checkpoint-8004/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8004/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8004/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8004/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8004/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8004/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8004/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8004/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8004/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-8004/pytorch_model.bin\n",
            " 60% 8352/13920 [1:06:28<29:43,  3.12it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.82it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.86it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.48it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.73it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.27it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.98it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.37it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.36it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.36it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.36it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.36it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.37it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.40it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.40it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.36it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6785818934440613, 'eval_acc': 0.8170580964153276, 'eval_f1': 0.8170580964153276, 'eval_precision': 0.8170580964153276, 'eval_recall': 0.8170580964153276, 'eval_runtime': 31.7009, 'eval_samples_per_second': 76.559, 'eval_steps_per_second': 6.404, 'epoch': 24.0}\n",
            " 60% 8352/13920 [1:07:00<29:43,  3.12it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-8352\n",
            "Configuration saved in results/chemprot_1/checkpoint-8352/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8352/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8352/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8352/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8352/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8352/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8352/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8352/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8352/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-8352/pytorch_model.bin\n",
            "{'loss': 0.351, 'learning_rate': 1.5574712643678163e-05, 'epoch': 24.43}\n",
            " 62% 8700/13920 [1:09:16<27:47,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.79it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.84it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.48it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.35it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.35it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.36it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.41it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.41it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.41it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.41it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.41it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.41it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.41it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.41it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.41it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.41it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.41it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.41it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.41it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.41it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.41it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.41it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.41it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.41it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.41it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.41it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.41it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.41it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.40it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6959707736968994, 'eval_acc': 0.8191182529872271, 'eval_f1': 0.8191182529872271, 'eval_precision': 0.8191182529872271, 'eval_recall': 0.8191182529872271, 'eval_runtime': 31.6714, 'eval_samples_per_second': 76.631, 'eval_steps_per_second': 6.41, 'epoch': 25.0}\n",
            " 62% 8700/13920 [1:09:48<27:47,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-8700\n",
            "Configuration saved in results/chemprot_1/checkpoint-8700/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8700/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8700/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8700/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8700/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8700/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8700/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-8700/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-8700/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-8700/pytorch_model.bin\n",
            "{'loss': 0.3392, 'learning_rate': 1.4137931034482759e-05, 'epoch': 25.86}\n",
            " 65% 9048/13920 [1:12:03<25:54,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.78it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.84it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.70it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.76it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.60it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.42it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.38it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.37it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:29,  6.36it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.36it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.35it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.36it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.37it/s]\u001b[A\n",
            " 26% 52/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.36it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.35it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.35it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.35it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.37it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.36it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:17,  6.35it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.35it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.35it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:16,  6.34it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.34it/s]\u001b[A\n",
            " 51% 103/203 [00:16<00:15,  6.34it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.34it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.34it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.34it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.34it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.34it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.34it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.32it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.34it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.34it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.35it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:14,  6.36it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.36it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 60% 122/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.36it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.36it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.36it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.36it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.36it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:11,  6.36it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.35it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.35it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.35it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.35it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.34it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.34it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.34it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.34it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.34it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.35it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.35it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.36it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.36it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.36it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 154/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.36it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.36it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.36it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 192/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.677442729473114, 'eval_acc': 0.8141738772146683, 'eval_f1': 0.8141738772146682, 'eval_precision': 0.8141738772146683, 'eval_recall': 0.8141738772146683, 'eval_runtime': 31.7792, 'eval_samples_per_second': 76.371, 'eval_steps_per_second': 6.388, 'epoch': 26.0}\n",
            " 65% 9048/13920 [1:12:35<25:54,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-9048\n",
            "Configuration saved in results/chemprot_1/checkpoint-9048/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9048/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9048/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9048/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9048/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9048/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9048/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9048/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9048/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-9048/pytorch_model.bin\n",
            " 68% 9396/13920 [1:14:52<24:02,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.81it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.84it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.47it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.98it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.49it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.46it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.44it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.40it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.36it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.36it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.36it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.36it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.36it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.36it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.35it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.34it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.34it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.35it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:11,  6.35it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.35it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.36it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.36it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.36it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6918150782585144, 'eval_acc': 0.8092295014421096, 'eval_f1': 0.8092295014421096, 'eval_precision': 0.8092295014421096, 'eval_recall': 0.8092295014421096, 'eval_runtime': 31.7158, 'eval_samples_per_second': 76.523, 'eval_steps_per_second': 6.401, 'epoch': 27.0}\n",
            " 68% 9396/13920 [1:15:24<24:02,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-9396\n",
            "Configuration saved in results/chemprot_1/checkpoint-9396/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9396/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9396/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9396/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9396/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9396/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9396/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9396/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9396/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-9396/pytorch_model.bin\n",
            "{'loss': 0.3275, 'learning_rate': 1.2701149425287359e-05, 'epoch': 27.3}\n",
            " 70% 9744/13920 [1:17:41<22:17,  3.12it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.77it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.80it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.43it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.68it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.24it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.49it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.44it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.40it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.36it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.35it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.36it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.36it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.36it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.35it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.36it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.37it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.36it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:19,  6.36it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.37it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.35it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 103/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.36it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.36it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.36it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.36it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.35it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.6958600878715515, 'eval_acc': 0.814997939843428, 'eval_f1': 0.814997939843428, 'eval_precision': 0.814997939843428, 'eval_recall': 0.814997939843428, 'eval_runtime': 31.7305, 'eval_samples_per_second': 76.488, 'eval_steps_per_second': 6.398, 'epoch': 28.0}\n",
            " 70% 9744/13920 [1:18:12<22:17,  3.12it/s]\n",
            "100% 203/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-9744\n",
            "Configuration saved in results/chemprot_1/checkpoint-9744/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9744/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9744/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9744/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9744/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9744/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9744/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-9744/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-9744/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-9744/pytorch_model.bin\n",
            "{'loss': 0.3174, 'learning_rate': 1.1264367816091955e-05, 'epoch': 28.74}\n",
            " 72% 10092/13920 [1:20:29<20:22,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.78it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.81it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.58it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.48it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.44it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.35it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.35it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.36it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.36it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.36it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.36it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.36it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.36it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.36it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6918607354164124, 'eval_acc': 0.8174701277297075, 'eval_f1': 0.8174701277297076, 'eval_precision': 0.8174701277297075, 'eval_recall': 0.8174701277297075, 'eval_runtime': 31.7032, 'eval_samples_per_second': 76.554, 'eval_steps_per_second': 6.403, 'epoch': 29.0}\n",
            " 72% 10092/13920 [1:21:01<20:22,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-10092\n",
            "Configuration saved in results/chemprot_1/checkpoint-10092/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10092/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10092/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10092/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10092/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10092/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10092/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10092/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10092/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-10092/pytorch_model.bin\n",
            " 75% 10440/13920 [1:23:17<18:35,  3.12it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.82it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.85it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.67it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.58it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.53it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.48it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.36it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.36it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.36it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.36it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.37it/s]\u001b[A\n",
            " 26% 52/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.37it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.37it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.36it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.35it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.36it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.35it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.36it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.36it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.37it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.36it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.36it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.37it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6848452091217041, 'eval_acc': 0.8195302843016069, 'eval_f1': 0.8195302843016069, 'eval_precision': 0.8195302843016069, 'eval_recall': 0.8195302843016069, 'eval_runtime': 31.7345, 'eval_samples_per_second': 76.478, 'eval_steps_per_second': 6.397, 'epoch': 30.0}\n",
            " 75% 10440/13920 [1:23:49<18:35,  3.12it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-10440\n",
            "Configuration saved in results/chemprot_1/checkpoint-10440/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10440/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10440/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10440/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10440/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10440/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10440/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10440/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10440/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-10440/pytorch_model.bin\n",
            "{'loss': 0.3189, 'learning_rate': 9.827586206896553e-06, 'epoch': 30.17}\n",
            " 78% 10788/13920 [1:26:06<16:38,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.72it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.80it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.43it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.69it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.56it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.50it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.46it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.41it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 52/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.35it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.37it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.37it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 103/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.37it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.36it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.40it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.40it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7236763834953308, 'eval_acc': 0.8137618459002884, 'eval_f1': 0.8137618459002884, 'eval_precision': 0.8137618459002884, 'eval_recall': 0.8137618459002884, 'eval_runtime': 31.7202, 'eval_samples_per_second': 76.513, 'eval_steps_per_second': 6.4, 'epoch': 31.0}\n",
            " 78% 10788/13920 [1:26:37<16:38,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-10788\n",
            "Configuration saved in results/chemprot_1/checkpoint-10788/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10788/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10788/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10788/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10788/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10788/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10788/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-10788/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-10788/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-10788/pytorch_model.bin\n",
            "{'loss': 0.3159, 'learning_rate': 8.390804597701149e-06, 'epoch': 31.61}\n",
            " 80% 11136/13920 [1:28:54<14:50,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.73it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.82it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.69it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.24it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.95it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.76it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.64it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.55it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.50it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.46it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.43it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.38it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.38it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.37it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.37it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.36it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.36it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.35it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.36it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.36it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.36it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.36it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.36it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.36it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.36it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.36it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.36it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.37it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:16,  6.36it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.36it/s]\u001b[A\n",
            " 51% 103/203 [00:16<00:15,  6.36it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.36it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.36it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.36it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.37it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.36it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.36it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 154/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.37it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.35it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.36it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.40it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.40it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6922101378440857, 'eval_acc': 0.8195302843016069, 'eval_f1': 0.8195302843016069, 'eval_precision': 0.8195302843016069, 'eval_recall': 0.8195302843016069, 'eval_runtime': 31.7319, 'eval_samples_per_second': 76.484, 'eval_steps_per_second': 6.397, 'epoch': 32.0}\n",
            " 80% 11136/13920 [1:29:26<14:50,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-11136\n",
            "Configuration saved in results/chemprot_1/checkpoint-11136/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11136/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11136/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11136/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11136/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11136/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11136/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-11136/pytorch_model.bin\n",
            " 82% 11484/13920 [1:31:42<12:56,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.76it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.81it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.44it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.69it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.24it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.95it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.38it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:29,  6.37it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.36it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.36it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.35it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.36it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.36it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.36it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.36it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.37it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.37it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.36it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.36it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.36it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.36it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.37it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.36it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.36it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.36it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.35it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6870788931846619, 'eval_acc': 0.8166460651009477, 'eval_f1': 0.8166460651009477, 'eval_precision': 0.8166460651009477, 'eval_recall': 0.8166460651009477, 'eval_runtime': 31.7304, 'eval_samples_per_second': 76.488, 'eval_steps_per_second': 6.398, 'epoch': 33.0}\n",
            " 82% 11484/13920 [1:32:14<12:56,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.36it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-11484\n",
            "Configuration saved in results/chemprot_1/checkpoint-11484/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11484/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11484/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11484/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11484/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11484/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11484/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11484/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11484/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-11484/pytorch_model.bin\n",
            "{'loss': 0.2986, 'learning_rate': 6.954022988505748e-06, 'epoch': 33.05}\n",
            " 85% 11832/13920 [1:34:31<11:07,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.81it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.83it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.98it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.38it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.36it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.37it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.37it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.35it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.34it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.35it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.36it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.36it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.36it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:19,  6.36it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.36it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.37it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.37it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.40it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.40it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.37it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6983209252357483, 'eval_acc': 0.8158220024721878, 'eval_f1': 0.8158220024721878, 'eval_precision': 0.8158220024721878, 'eval_recall': 0.8158220024721878, 'eval_runtime': 31.7121, 'eval_samples_per_second': 76.532, 'eval_steps_per_second': 6.401, 'epoch': 34.0}\n",
            " 85% 11832/13920 [1:35:02<11:07,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-11832\n",
            "Configuration saved in results/chemprot_1/checkpoint-11832/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11832/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11832/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11832/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11832/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11832/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11832/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-11832/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-11832/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-11832/pytorch_model.bin\n",
            "{'loss': 0.2932, 'learning_rate': 5.517241379310345e-06, 'epoch': 34.48}\n",
            " 88% 12180/13920 [1:37:19<09:18,  3.11it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.78it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.83it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.70it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.56it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.48it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.41it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.37it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.37it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.37it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.36it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.36it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.37it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.37it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.37it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.37it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.37it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.36it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.39it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.40it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7032691240310669, 'eval_acc': 0.8129377832715287, 'eval_f1': 0.8129377832715287, 'eval_precision': 0.8129377832715287, 'eval_recall': 0.8129377832715287, 'eval_runtime': 31.6921, 'eval_samples_per_second': 76.581, 'eval_steps_per_second': 6.405, 'epoch': 35.0}\n",
            " 88% 12180/13920 [1:37:50<09:18,  3.11it/s]\n",
            "100% 203/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-12180\n",
            "Configuration saved in results/chemprot_1/checkpoint-12180/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12180/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12180/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12180/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12180/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12180/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12180/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12180/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12180/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-12180/pytorch_model.bin\n",
            "{'loss': 0.2954, 'learning_rate': 4.080459770114943e-06, 'epoch': 35.92}\n",
            " 90% 12528/13920 [1:40:06<07:24,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.75it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.81it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.70it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.98it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.79it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.52it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.44it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.41it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.37it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.36it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.37it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.38it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.38it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.38it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.37it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.37it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.38it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.38it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.40it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.40it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.41it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.41it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.41it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.39it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.39it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7174939513206482, 'eval_acc': 0.8178821590440873, 'eval_f1': 0.8178821590440873, 'eval_precision': 0.8178821590440873, 'eval_recall': 0.8178821590440873, 'eval_runtime': 31.6902, 'eval_samples_per_second': 76.585, 'eval_steps_per_second': 6.406, 'epoch': 36.0}\n",
            " 90% 12528/13920 [1:40:38<07:24,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-12528\n",
            "Configuration saved in results/chemprot_1/checkpoint-12528/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12528/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12528/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12528/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12528/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12528/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12528/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12528/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12528/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-12528/pytorch_model.bin\n",
            " 92% 12876/13920 [1:42:54<05:32,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.77it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.82it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.70it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.23it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.95it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.64it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.43it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:27,  6.40it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.40it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.40it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.38it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.40it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.40it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.40it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.40it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.40it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.40it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.41it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:19,  6.41it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.40it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.37it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.38it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.37it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.37it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.37it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.36it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.37it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.37it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.37it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.36it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.36it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.37it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.37it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.36it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.36it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.35it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.35it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.36it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.37it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.37it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.37it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7232611179351807, 'eval_acc': 0.8100535640708694, 'eval_f1': 0.8100535640708695, 'eval_precision': 0.8100535640708694, 'eval_recall': 0.8100535640708694, 'eval_runtime': 31.7, 'eval_samples_per_second': 76.561, 'eval_steps_per_second': 6.404, 'epoch': 37.0}\n",
            " 92% 12876/13920 [1:43:25<05:32,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-12876\n",
            "Configuration saved in results/chemprot_1/checkpoint-12876/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12876/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12876/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12876/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12876/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12876/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12876/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-12876/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-12876/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-12876/pytorch_model.bin\n",
            "{'loss': 0.2834, 'learning_rate': 2.6436781609195404e-06, 'epoch': 37.36}\n",
            " 95% 13224/13920 [1:45:41<03:41,  3.14it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.74it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.80it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.44it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.69it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.24it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.96it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.77it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.46it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.42it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.39it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.39it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.39it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.39it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.40it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.40it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.39it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.40it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.40it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.36it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.37it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.40it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.39it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.37it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.38it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.38it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.38it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.38it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.37it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.37it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.37it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.38it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.37it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.36it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.36it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.38it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.39it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.37it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.38it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:08,  6.37it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.37it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.37it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.38it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.37it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.36it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.37it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.38it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.39it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.40it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.38it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7070009112358093, 'eval_acc': 0.8182941903584673, 'eval_f1': 0.8182941903584674, 'eval_precision': 0.8182941903584673, 'eval_recall': 0.8182941903584673, 'eval_runtime': 31.7005, 'eval_samples_per_second': 76.56, 'eval_steps_per_second': 6.404, 'epoch': 38.0}\n",
            " 95% 13224/13920 [1:46:13<03:41,  3.14it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-13224\n",
            "Configuration saved in results/chemprot_1/checkpoint-13224/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13224/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13224/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13224/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13224/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13224/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13224/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-13224/pytorch_model.bin\n",
            "{'loss': 0.2991, 'learning_rate': 1.2068965517241381e-06, 'epoch': 38.79}\n",
            " 98% 13572/13920 [1:48:29<01:51,  3.12it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.72it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.81it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.45it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.71it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.26it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.65it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.50it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.42it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.37it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:29,  6.38it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.37it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.37it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.37it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.36it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.36it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.36it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.37it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.38it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:26,  6.37it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.37it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.36it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.37it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.37it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.39it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 52/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.39it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.37it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.38it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.38it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.39it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.39it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:21,  6.38it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.39it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:13<00:18,  6.38it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.38it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.39it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.39it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.39it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.40it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.41it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.40it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.40it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.39it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.38it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.37it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.38it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.38it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:21<00:10,  6.39it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:10,  6.38it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.38it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.40it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.40it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.38it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.38it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.39it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.37it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.38it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.38it/s]\u001b[A\n",
            " 82% 167/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.39it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:05,  6.38it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.39it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.39it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.38it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.38it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.38it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.37it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.39it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 199/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7088598608970642, 'eval_acc': 0.8174701277297075, 'eval_f1': 0.8174701277297076, 'eval_precision': 0.8174701277297075, 'eval_recall': 0.8174701277297075, 'eval_runtime': 31.6997, 'eval_samples_per_second': 76.562, 'eval_steps_per_second': 6.404, 'epoch': 39.0}\n",
            " 98% 13572/13920 [1:49:00<01:51,  3.12it/s]\n",
            "100% 203/203 [00:31<00:00,  6.38it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-13572\n",
            "Configuration saved in results/chemprot_1/checkpoint-13572/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13572/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13572/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13572/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13572/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13572/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13572/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13572/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13572/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-13572/pytorch_model.bin\n",
            "100% 13920/13920 [1:51:17<00:00,  3.13it/s]***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/203 [00:00<00:15, 12.77it/s]\u001b[A\n",
            "  1% 3/203 [00:00<00:20,  9.83it/s]\u001b[A\n",
            "  2% 4/203 [00:00<00:23,  8.46it/s]\u001b[A\n",
            "  2% 5/203 [00:00<00:25,  7.70it/s]\u001b[A\n",
            "  3% 6/203 [00:00<00:27,  7.25it/s]\u001b[A\n",
            "  3% 7/203 [00:00<00:28,  6.97it/s]\u001b[A\n",
            "  4% 8/203 [00:01<00:28,  6.78it/s]\u001b[A\n",
            "  4% 9/203 [00:01<00:29,  6.66it/s]\u001b[A\n",
            "  5% 10/203 [00:01<00:29,  6.57it/s]\u001b[A\n",
            "  5% 11/203 [00:01<00:29,  6.51it/s]\u001b[A\n",
            "  6% 12/203 [00:01<00:29,  6.47it/s]\u001b[A\n",
            "  6% 13/203 [00:01<00:29,  6.45it/s]\u001b[A\n",
            "  7% 14/203 [00:02<00:29,  6.42it/s]\u001b[A\n",
            "  7% 15/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 16/203 [00:02<00:29,  6.40it/s]\u001b[A\n",
            "  8% 17/203 [00:02<00:29,  6.39it/s]\u001b[A\n",
            "  9% 18/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            "  9% 19/203 [00:02<00:28,  6.40it/s]\u001b[A\n",
            " 10% 20/203 [00:02<00:28,  6.39it/s]\u001b[A\n",
            " 10% 21/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 22/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 11% 23/203 [00:03<00:28,  6.38it/s]\u001b[A\n",
            " 12% 24/203 [00:03<00:28,  6.39it/s]\u001b[A\n",
            " 12% 25/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 26/203 [00:03<00:27,  6.38it/s]\u001b[A\n",
            " 13% 27/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 28/203 [00:04<00:27,  6.38it/s]\u001b[A\n",
            " 14% 29/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 30/203 [00:04<00:27,  6.39it/s]\u001b[A\n",
            " 15% 31/203 [00:04<00:26,  6.40it/s]\u001b[A\n",
            " 16% 32/203 [00:04<00:26,  6.41it/s]\u001b[A\n",
            " 16% 33/203 [00:05<00:26,  6.40it/s]\u001b[A\n",
            " 17% 34/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 17% 35/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 18% 36/203 [00:05<00:26,  6.41it/s]\u001b[A\n",
            " 18% 37/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 38/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 19% 39/203 [00:05<00:25,  6.40it/s]\u001b[A\n",
            " 20% 40/203 [00:06<00:25,  6.40it/s]\u001b[A\n",
            " 20% 41/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 42/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 21% 43/203 [00:06<00:25,  6.39it/s]\u001b[A\n",
            " 22% 44/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 22% 45/203 [00:06<00:24,  6.39it/s]\u001b[A\n",
            " 23% 46/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 23% 47/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 48/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 24% 49/203 [00:07<00:24,  6.38it/s]\u001b[A\n",
            " 25% 50/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 25% 51/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 52/203 [00:07<00:23,  6.38it/s]\u001b[A\n",
            " 26% 53/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 54/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 27% 55/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 56/203 [00:08<00:23,  6.38it/s]\u001b[A\n",
            " 28% 57/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 58/203 [00:08<00:22,  6.39it/s]\u001b[A\n",
            " 29% 59/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 60/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 30% 61/203 [00:09<00:22,  6.39it/s]\u001b[A\n",
            " 31% 62/203 [00:09<00:22,  6.40it/s]\u001b[A\n",
            " 31% 63/203 [00:09<00:21,  6.40it/s]\u001b[A\n",
            " 32% 64/203 [00:09<00:21,  6.40it/s]\u001b[A\n",
            " 32% 65/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 66/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 67/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 33% 68/203 [00:10<00:21,  6.40it/s]\u001b[A\n",
            " 34% 69/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 34% 70/203 [00:10<00:20,  6.39it/s]\u001b[A\n",
            " 35% 71/203 [00:10<00:20,  6.38it/s]\u001b[A\n",
            " 35% 72/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 73/203 [00:11<00:20,  6.37it/s]\u001b[A\n",
            " 36% 74/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 75/203 [00:11<00:20,  6.38it/s]\u001b[A\n",
            " 37% 76/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 77/203 [00:11<00:19,  6.39it/s]\u001b[A\n",
            " 38% 78/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 39% 79/203 [00:12<00:19,  6.38it/s]\u001b[A\n",
            " 39% 80/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 81/203 [00:12<00:19,  6.39it/s]\u001b[A\n",
            " 40% 82/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 41% 83/203 [00:12<00:18,  6.38it/s]\u001b[A\n",
            " 41% 84/203 [00:12<00:18,  6.39it/s]\u001b[A\n",
            " 42% 85/203 [00:13<00:18,  6.39it/s]\u001b[A\n",
            " 42% 86/203 [00:13<00:18,  6.40it/s]\u001b[A\n",
            " 43% 87/203 [00:13<00:18,  6.41it/s]\u001b[A\n",
            " 43% 88/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 89/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 44% 90/203 [00:13<00:17,  6.40it/s]\u001b[A\n",
            " 45% 91/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 45% 92/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 93/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 46% 94/203 [00:14<00:17,  6.40it/s]\u001b[A\n",
            " 47% 95/203 [00:14<00:16,  6.41it/s]\u001b[A\n",
            " 47% 96/203 [00:14<00:16,  6.41it/s]\u001b[A\n",
            " 48% 97/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 48% 98/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 99/203 [00:15<00:16,  6.40it/s]\u001b[A\n",
            " 49% 100/203 [00:15<00:16,  6.39it/s]\u001b[A\n",
            " 50% 101/203 [00:15<00:15,  6.39it/s]\u001b[A\n",
            " 50% 102/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 103/203 [00:15<00:15,  6.40it/s]\u001b[A\n",
            " 51% 104/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 105/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 52% 106/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 107/203 [00:16<00:15,  6.39it/s]\u001b[A\n",
            " 53% 108/203 [00:16<00:14,  6.39it/s]\u001b[A\n",
            " 54% 109/203 [00:16<00:14,  6.38it/s]\u001b[A\n",
            " 54% 110/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 111/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 55% 112/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 113/203 [00:17<00:14,  6.37it/s]\u001b[A\n",
            " 56% 114/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 115/203 [00:17<00:13,  6.38it/s]\u001b[A\n",
            " 57% 116/203 [00:18<00:13,  6.37it/s]\u001b[A\n",
            " 58% 117/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 58% 118/203 [00:18<00:13,  6.38it/s]\u001b[A\n",
            " 59% 119/203 [00:18<00:13,  6.39it/s]\u001b[A\n",
            " 59% 120/203 [00:18<00:12,  6.39it/s]\u001b[A\n",
            " 60% 121/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 60% 122/203 [00:18<00:12,  6.40it/s]\u001b[A\n",
            " 61% 123/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 61% 124/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 62% 125/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 62% 126/203 [00:19<00:12,  6.40it/s]\u001b[A\n",
            " 63% 127/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 63% 128/203 [00:19<00:11,  6.40it/s]\u001b[A\n",
            " 64% 129/203 [00:20<00:11,  6.40it/s]\u001b[A\n",
            " 64% 130/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 131/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 65% 132/203 [00:20<00:11,  6.39it/s]\u001b[A\n",
            " 66% 133/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 66% 134/203 [00:20<00:10,  6.39it/s]\u001b[A\n",
            " 67% 135/203 [00:20<00:10,  6.40it/s]\u001b[A\n",
            " 67% 136/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 67% 137/203 [00:21<00:10,  6.40it/s]\u001b[A\n",
            " 68% 138/203 [00:21<00:10,  6.41it/s]\u001b[A\n",
            " 68% 139/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 140/203 [00:21<00:09,  6.40it/s]\u001b[A\n",
            " 69% 141/203 [00:21<00:09,  6.39it/s]\u001b[A\n",
            " 70% 142/203 [00:22<00:09,  6.38it/s]\u001b[A\n",
            " 70% 143/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 144/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 71% 145/203 [00:22<00:09,  6.39it/s]\u001b[A\n",
            " 72% 146/203 [00:22<00:08,  6.39it/s]\u001b[A\n",
            " 72% 147/203 [00:22<00:08,  6.38it/s]\u001b[A\n",
            " 73% 148/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 73% 149/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 74% 150/203 [00:23<00:08,  6.39it/s]\u001b[A\n",
            " 74% 151/203 [00:23<00:08,  6.38it/s]\u001b[A\n",
            " 75% 152/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 75% 153/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 154/203 [00:23<00:07,  6.39it/s]\u001b[A\n",
            " 76% 155/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 156/203 [00:24<00:07,  6.39it/s]\u001b[A\n",
            " 77% 157/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 78% 158/203 [00:24<00:07,  6.40it/s]\u001b[A\n",
            " 78% 159/203 [00:24<00:06,  6.40it/s]\u001b[A\n",
            " 79% 160/203 [00:24<00:06,  6.40it/s]\u001b[A\n",
            " 79% 161/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 162/203 [00:25<00:06,  6.40it/s]\u001b[A\n",
            " 80% 163/203 [00:25<00:06,  6.41it/s]\u001b[A\n",
            " 81% 164/203 [00:25<00:06,  6.41it/s]\u001b[A\n",
            " 81% 165/203 [00:25<00:05,  6.41it/s]\u001b[A\n",
            " 82% 166/203 [00:25<00:05,  6.41it/s]\u001b[A\n",
            " 82% 167/203 [00:25<00:05,  6.40it/s]\u001b[A\n",
            " 83% 168/203 [00:26<00:05,  6.40it/s]\u001b[A\n",
            " 83% 169/203 [00:26<00:05,  6.41it/s]\u001b[A\n",
            " 84% 170/203 [00:26<00:05,  6.41it/s]\u001b[A\n",
            " 84% 171/203 [00:26<00:04,  6.41it/s]\u001b[A\n",
            " 85% 172/203 [00:26<00:04,  6.41it/s]\u001b[A\n",
            " 85% 173/203 [00:26<00:04,  6.41it/s]\u001b[A\n",
            " 86% 174/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 86% 175/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 176/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 87% 177/203 [00:27<00:04,  6.40it/s]\u001b[A\n",
            " 88% 178/203 [00:27<00:03,  6.41it/s]\u001b[A\n",
            " 88% 179/203 [00:27<00:03,  6.41it/s]\u001b[A\n",
            " 89% 180/203 [00:28<00:03,  6.40it/s]\u001b[A\n",
            " 89% 181/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 182/203 [00:28<00:03,  6.39it/s]\u001b[A\n",
            " 90% 183/203 [00:28<00:03,  6.38it/s]\u001b[A\n",
            " 91% 184/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 91% 185/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 186/203 [00:28<00:02,  6.39it/s]\u001b[A\n",
            " 92% 187/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 93% 188/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 93% 189/203 [00:29<00:02,  6.38it/s]\u001b[A\n",
            " 94% 190/203 [00:29<00:02,  6.39it/s]\u001b[A\n",
            " 94% 191/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 192/203 [00:29<00:01,  6.38it/s]\u001b[A\n",
            " 95% 193/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 194/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 96% 195/203 [00:30<00:01,  6.38it/s]\u001b[A\n",
            " 97% 196/203 [00:30<00:01,  6.39it/s]\u001b[A\n",
            " 97% 197/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 98% 198/203 [00:30<00:00,  6.40it/s]\u001b[A\n",
            " 98% 199/203 [00:30<00:00,  6.39it/s]\u001b[A\n",
            " 99% 200/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            " 99% 201/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7075365781784058, 'eval_acc': 0.8162340337865678, 'eval_f1': 0.8162340337865678, 'eval_precision': 0.8162340337865678, 'eval_recall': 0.8162340337865678, 'eval_runtime': 31.6556, 'eval_samples_per_second': 76.669, 'eval_steps_per_second': 6.413, 'epoch': 40.0}\n",
            "100% 13920/13920 [1:51:49<00:00,  3.13it/s]\n",
            "100% 203/203 [00:31<00:00,  6.39it/s]\u001b[A\n",
            "                                     \u001b[ASaving model checkpoint to results/chemprot_1/checkpoint-13920\n",
            "Configuration saved in results/chemprot_1/checkpoint-13920/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13920/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13920/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13920/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13920/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13920/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13920/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/checkpoint-13920/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/checkpoint-13920/config.json\n",
            "Model weights saved in results/chemprot_1/checkpoint-13920/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/chemprot_1/checkpoint-4872 (score: 0.6355000138282776).\n",
            "Loading best adapter(s) from results/chemprot_1/checkpoint-4872 (score: 0.6355000138282776).\n",
            "Loading module configuration from results/chemprot_1/checkpoint-4872/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/chemprot_1/checkpoint-4872/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/chemprot_1/checkpoint-4872/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 13, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/chemprot_1/checkpoint-4872/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/chemprot_1/checkpoint-4872 (score: 0.6355000138282776).\n",
            "Loading module configuration from results/chemprot_1/checkpoint-4872/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/chemprot_1/checkpoint-4872/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 6725.2534, 'train_samples_per_second': 24.796, 'train_steps_per_second': 2.07, 'train_loss': 0.542579575790756, 'epoch': 40.0}\n",
            "100% 13920/13920 [1:52:05<00:00,  2.07it/s]\n",
            "Saving model checkpoint to results/chemprot_1/\n",
            "Configuration saved in results/chemprot_1/mlm/adapter_config.json\n",
            "Module weights saved in results/chemprot_1/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/chemprot_1/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/mlm/head_config.json\n",
            "Module weights saved in results/chemprot_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/chemprot_1/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/chemprot_1/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/chemprot_1/config.json\n",
            "Model weights saved in results/chemprot_1/pytorch_model.bin\n",
            "tokenizer config file saved in results/chemprot_1/tokenizer_config.json\n",
            "Special tokens file saved in results/chemprot_1/special_tokens_map.json\n",
            "08/03/2021 07:11:33 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2427\n",
            "  Batch size = 12\n",
            "100% 203/203 [00:31<00:00,  6.43it/s]\n",
            "08/03/2021 07:12:05 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_loss = 0.6355000138282776\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_acc = 0.814997939843428\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_f1 = 0.814997939843428\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_precision = 0.814997939843428\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_recall = 0.814997939843428\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_runtime = 31.7576\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_samples_per_second = 76.423\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   eval_steps_per_second = 6.392\n",
            "08/03/2021 07:12:05 - INFO - __main__ -   epoch = 40.0\n",
            "08/03/2021 07:12:05 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3469\n",
            "  Batch size = 12\n",
            "100% 290/290 [00:45<00:00,  6.42it/s]\n",
            "08/03/2021 07:12:51 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_loss = 0.672048807144165\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_acc = 0.8057076967425771\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_f1 = 0.8057076967425771\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_precision = 0.8057076967425771\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_recall = 0.8057076967425771\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_runtime = 45.3508\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_samples_per_second = 76.493\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   eval_steps_per_second = 6.395\n",
            "08/03/2021 07:12:51 - INFO - __main__ -   epoch = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8rpqQDAcyMa",
        "outputId": "19d5252f-2f39-4af6-b79a-9d19f637f642"
      },
      "source": [
        "# Sciie new adapter\n",
        "!python3 run_mlm.py \\\n",
        "--train_file data/sciie_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/sciie_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/sciie \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 90 \\\n",
        "--learning_rate 5e-4 \\\n",
        "--logging_steps 40 \\\n",
        "--per_gpu_train_batch_size 8 \\\n",
        "--per_gpu_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 8  \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 01:12:44.346655: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 01:12:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/03/2021 01:12:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=40,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=8,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/sciie/runs/Aug03_01-12-45_b79b4b505e94,\n",
            "logging_first_step=False,\n",
            "logging_steps=40,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=90.0,\n",
            "output_dir=results/adapters/sciie,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=sciie,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/sciie,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/03/2021 01:12:46 - WARNING - datasets.builder -   Using custom data configuration default-949d0279ad309551\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-949d0279ad309551/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-949d0279ad309551/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1590] 2021-08-03 01:12:46,536 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi_zfaook\n",
            "Downloading: 100% 481/481 [00:00<00:00, 412kB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-03 01:12:46,884 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|file_utils.py:1602] 2021-08-03 01:12:46,884 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 01:12:46,884 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 01:12:46,885 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-03 01:12:47,238 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 01:12:47,589 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 01:12:47,590 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-08-03 01:12:47,936 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo_5o988x\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 3.62MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-03 01:12:48,536 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1602] 2021-08-03 01:12:48,536 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1590] 2021-08-03 01:12:48,878 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7qc8pxok\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 2.63MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-03 01:12:49,405 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1602] 2021-08-03 01:12:49,405 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1590] 2021-08-03 01:12:49,750 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfj0cvoun\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 4.11MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-03 01:12:50,437 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|file_utils.py:1602] 2021-08-03 01:12:50,437 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 01:12:51,494 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1590] 2021-08-03 01:12:51,914 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpy_0o1cm7\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 67.9MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-03 01:12:59,356 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|file_utils.py:1602] 2021-08-03 01:12:59,356 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1163] 2021-08-03 01:12:59,356 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-03 01:13:01,068 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-03 01:13:01,068 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-03 01:13:01,076 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 4/4 [00:00<00:00, 11.39ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00, 32.96ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-03 01:13:04,786 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-03 01:13:04,788 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-03 01:13:04,789 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-03 01:13:04,797 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-03 01:13:04,797 >>   Num examples = 3219\n",
            "[INFO|trainer.py:1201] 2021-08-03 01:13:04,797 >>   Num Epochs = 90\n",
            "[INFO|trainer.py:1202] 2021-08-03 01:13:04,797 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-03 01:13:04,797 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1204] 2021-08-03 01:13:04,797 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1205] 2021-08-03 01:13:04,797 >>   Total optimization steps = 4500\n",
            "[WARNING|training_args.py:760] 2021-08-03 01:13:04,810 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-03 01:13:04,811 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.479, 'learning_rate': 0.0004955555555555556, 'epoch': 0.79}\n",
            "{'loss': 2.1424, 'learning_rate': 0.0004911111111111111, 'epoch': 1.6}\n",
            "{'loss': 2.0208, 'learning_rate': 0.0004866666666666667, 'epoch': 2.4}\n",
            "{'loss': 1.9343, 'learning_rate': 0.0004822222222222222, 'epoch': 3.2}\n",
            "{'loss': 1.8173, 'learning_rate': 0.0004777777777777778, 'epoch': 3.99}\n",
            "{'loss': 1.802, 'learning_rate': 0.00047333333333333336, 'epoch': 4.79}\n",
            "{'loss': 1.7507, 'learning_rate': 0.0004688888888888889, 'epoch': 5.6}\n",
            "{'loss': 1.6998, 'learning_rate': 0.00046444444444444446, 'epoch': 6.4}\n",
            "{'loss': 1.6469, 'learning_rate': 0.00046, 'epoch': 7.2}\n",
            "{'loss': 1.5826, 'learning_rate': 0.00045555555555555556, 'epoch': 7.99}\n",
            "{'loss': 1.5537, 'learning_rate': 0.0004511111111111111, 'epoch': 8.79}\n",
            "{'loss': 1.5223, 'learning_rate': 0.00044666666666666666, 'epoch': 9.6}\n",
            "{'loss': 1.5181, 'learning_rate': 0.00044222222222222227, 'epoch': 10.4}\n",
            "{'loss': 1.4283, 'learning_rate': 0.00043777777777777776, 'epoch': 11.2}\n",
            "{'loss': 1.359, 'learning_rate': 0.00043333333333333337, 'epoch': 11.99}\n",
            "{'loss': 1.3769, 'learning_rate': 0.00042888888888888886, 'epoch': 12.79}\n",
            "{'loss': 1.3084, 'learning_rate': 0.00042444444444444447, 'epoch': 13.6}\n",
            "{'loss': 1.3314, 'learning_rate': 0.00042, 'epoch': 14.4}\n",
            "{'loss': 1.3028, 'learning_rate': 0.00041555555555555557, 'epoch': 15.2}\n",
            "{'loss': 1.2865, 'learning_rate': 0.0004111111111111111, 'epoch': 15.99}\n",
            "{'loss': 1.2721, 'learning_rate': 0.00040666666666666667, 'epoch': 16.79}\n",
            "{'loss': 1.2243, 'learning_rate': 0.0004022222222222222, 'epoch': 17.6}\n",
            "{'loss': 1.1542, 'learning_rate': 0.00039777777777777777, 'epoch': 18.4}\n",
            "{'loss': 1.2072, 'learning_rate': 0.0003933333333333333, 'epoch': 19.2}\n",
            "{'loss': 1.1452, 'learning_rate': 0.0003888888888888889, 'epoch': 19.99}\n",
            "{'loss': 1.1342, 'learning_rate': 0.0003844444444444444, 'epoch': 20.79}\n",
            "{'loss': 1.1016, 'learning_rate': 0.00038, 'epoch': 21.6}\n",
            "{'loss': 1.0601, 'learning_rate': 0.0003755555555555555, 'epoch': 22.4}\n",
            "{'loss': 1.0858, 'learning_rate': 0.0003711111111111111, 'epoch': 23.2}\n",
            "{'loss': 1.0423, 'learning_rate': 0.00036666666666666667, 'epoch': 23.99}\n",
            "{'loss': 1.0721, 'learning_rate': 0.0003622222222222222, 'epoch': 24.79}\n",
            "{'loss': 1.0088, 'learning_rate': 0.00035777777777777777, 'epoch': 25.6}\n",
            "{'loss': 1.0138, 'learning_rate': 0.0003533333333333333, 'epoch': 26.4}\n",
            "{'loss': 0.9841, 'learning_rate': 0.0003488888888888889, 'epoch': 27.2}\n",
            "{'loss': 0.99, 'learning_rate': 0.0003444444444444445, 'epoch': 27.99}\n",
            "{'loss': 0.9673, 'learning_rate': 0.00034, 'epoch': 28.79}\n",
            "{'loss': 0.9738, 'learning_rate': 0.0003355555555555556, 'epoch': 29.6}\n",
            "{'loss': 0.9515, 'learning_rate': 0.0003311111111111111, 'epoch': 30.4}\n",
            "{'loss': 0.9564, 'learning_rate': 0.0003266666666666667, 'epoch': 31.2}\n",
            "{'loss': 0.9268, 'learning_rate': 0.0003222222222222222, 'epoch': 31.99}\n",
            "{'loss': 0.925, 'learning_rate': 0.0003177777777777778, 'epoch': 32.79}\n",
            "{'loss': 0.8924, 'learning_rate': 0.0003133333333333334, 'epoch': 33.6}\n",
            "{'loss': 0.8837, 'learning_rate': 0.0003088888888888889, 'epoch': 34.4}\n",
            "{'loss': 0.876, 'learning_rate': 0.0003044444444444445, 'epoch': 35.2}\n",
            "{'loss': 0.8702, 'learning_rate': 0.0003, 'epoch': 35.99}\n",
            "{'loss': 0.8855, 'learning_rate': 0.0002955555555555556, 'epoch': 36.79}\n",
            "{'loss': 0.8558, 'learning_rate': 0.00029111111111111113, 'epoch': 37.6}\n",
            "{'loss': 0.8473, 'learning_rate': 0.0002866666666666667, 'epoch': 38.4}\n",
            "{'loss': 0.8196, 'learning_rate': 0.00028222222222222223, 'epoch': 39.2}\n",
            "{'loss': 0.8463, 'learning_rate': 0.0002777777777777778, 'epoch': 39.99}\n",
            "{'loss': 0.8098, 'learning_rate': 0.00027333333333333333, 'epoch': 40.79}\n",
            "{'loss': 0.8576, 'learning_rate': 0.00026888888888888893, 'epoch': 41.6}\n",
            "{'loss': 0.7797, 'learning_rate': 0.00026444444444444443, 'epoch': 42.4}\n",
            "{'loss': 0.8043, 'learning_rate': 0.00026000000000000003, 'epoch': 43.2}\n",
            "{'loss': 0.7641, 'learning_rate': 0.00025555555555555553, 'epoch': 43.99}\n",
            "{'loss': 0.7419, 'learning_rate': 0.00025111111111111113, 'epoch': 44.79}\n",
            "{'loss': 0.7795, 'learning_rate': 0.0002466666666666667, 'epoch': 45.6}\n",
            "{'loss': 0.7787, 'learning_rate': 0.00024222222222222223, 'epoch': 46.4}\n",
            "{'loss': 0.7205, 'learning_rate': 0.00023777777777777778, 'epoch': 47.2}\n",
            "{'loss': 0.7552, 'learning_rate': 0.00023333333333333333, 'epoch': 47.99}\n",
            "{'loss': 0.7611, 'learning_rate': 0.0002288888888888889, 'epoch': 48.79}\n",
            "{'loss': 0.7238, 'learning_rate': 0.00022444444444444446, 'epoch': 49.6}\n",
            "{'loss': 0.7392, 'learning_rate': 0.00022, 'epoch': 50.4}\n",
            "{'loss': 0.7029, 'learning_rate': 0.00021555555555555556, 'epoch': 51.2}\n",
            "{'loss': 0.7029, 'learning_rate': 0.0002111111111111111, 'epoch': 51.99}\n",
            "{'loss': 0.6873, 'learning_rate': 0.00020666666666666666, 'epoch': 52.79}\n",
            "{'loss': 0.7253, 'learning_rate': 0.00020222222222222223, 'epoch': 53.6}\n",
            "{'loss': 0.7048, 'learning_rate': 0.00019777777777777778, 'epoch': 54.4}\n",
            "{'loss': 0.6688, 'learning_rate': 0.00019333333333333333, 'epoch': 55.2}\n",
            "{'loss': 0.7087, 'learning_rate': 0.00018888888888888888, 'epoch': 55.99}\n",
            "{'loss': 0.6877, 'learning_rate': 0.00018444444444444443, 'epoch': 56.79}\n",
            "{'loss': 0.7126, 'learning_rate': 0.00017999999999999998, 'epoch': 57.6}\n",
            "{'loss': 0.6789, 'learning_rate': 0.00017555555555555556, 'epoch': 58.4}\n",
            "{'loss': 0.6671, 'learning_rate': 0.0001711111111111111, 'epoch': 59.2}\n",
            "{'loss': 0.6241, 'learning_rate': 0.00016666666666666666, 'epoch': 59.99}\n",
            "{'loss': 0.6722, 'learning_rate': 0.0001622222222222222, 'epoch': 60.79}\n",
            "{'loss': 0.6746, 'learning_rate': 0.00015777777777777776, 'epoch': 61.6}\n",
            "{'loss': 0.6577, 'learning_rate': 0.00015333333333333334, 'epoch': 62.4}\n",
            "{'loss': 0.6711, 'learning_rate': 0.0001488888888888889, 'epoch': 63.2}\n",
            "{'loss': 0.6337, 'learning_rate': 0.00014444444444444444, 'epoch': 63.99}\n",
            "{'loss': 0.5863, 'learning_rate': 0.00014000000000000001, 'epoch': 64.79}\n",
            "{'loss': 0.6709, 'learning_rate': 0.00013555555555555556, 'epoch': 65.6}\n",
            "{'loss': 0.6356, 'learning_rate': 0.00013111111111111111, 'epoch': 66.4}\n",
            "{'loss': 0.6683, 'learning_rate': 0.0001266666666666667, 'epoch': 67.2}\n",
            "{'loss': 0.6149, 'learning_rate': 0.00012222222222222221, 'epoch': 67.99}\n",
            "{'loss': 0.5996, 'learning_rate': 0.00011777777777777778, 'epoch': 68.79}\n",
            "{'loss': 0.6236, 'learning_rate': 0.00011333333333333333, 'epoch': 69.6}\n",
            "{'loss': 0.6063, 'learning_rate': 0.00010888888888888888, 'epoch': 70.4}\n",
            "{'loss': 0.6051, 'learning_rate': 0.00010444444444444445, 'epoch': 71.2}\n",
            "{'loss': 0.5878, 'learning_rate': 0.0001, 'epoch': 71.99}\n",
            "{'loss': 0.6118, 'learning_rate': 9.555555555555557e-05, 'epoch': 72.79}\n",
            "{'loss': 0.5883, 'learning_rate': 9.111111111111112e-05, 'epoch': 73.6}\n",
            "{'loss': 0.5843, 'learning_rate': 8.666666666666667e-05, 'epoch': 74.4}\n",
            "{'loss': 0.5933, 'learning_rate': 8.222222222222223e-05, 'epoch': 75.2}\n",
            "{'loss': 0.5928, 'learning_rate': 7.777777777777778e-05, 'epoch': 75.99}\n",
            "{'loss': 0.599, 'learning_rate': 7.333333333333333e-05, 'epoch': 76.79}\n",
            "{'loss': 0.5897, 'learning_rate': 6.88888888888889e-05, 'epoch': 77.6}\n",
            "{'loss': 0.5758, 'learning_rate': 6.444444444444444e-05, 'epoch': 78.4}\n",
            "{'loss': 0.5906, 'learning_rate': 6e-05, 'epoch': 79.2}\n",
            "{'loss': 0.5949, 'learning_rate': 5.555555555555555e-05, 'epoch': 79.99}\n",
            "{'loss': 0.5566, 'learning_rate': 5.1111111111111115e-05, 'epoch': 80.79}\n",
            "{'loss': 0.5708, 'learning_rate': 4.666666666666667e-05, 'epoch': 81.6}\n",
            "{'loss': 0.5998, 'learning_rate': 4.222222222222222e-05, 'epoch': 82.4}\n",
            "{'loss': 0.5858, 'learning_rate': 3.777777777777778e-05, 'epoch': 83.2}\n",
            "{'loss': 0.5789, 'learning_rate': 3.3333333333333335e-05, 'epoch': 83.99}\n",
            "{'loss': 0.592, 'learning_rate': 2.8888888888888888e-05, 'epoch': 84.79}\n",
            "{'loss': 0.5743, 'learning_rate': 2.4444444444444445e-05, 'epoch': 85.6}\n",
            "{'loss': 0.5668, 'learning_rate': 2e-05, 'epoch': 86.4}\n",
            "{'loss': 0.5739, 'learning_rate': 1.5555555555555555e-05, 'epoch': 87.2}\n",
            "{'loss': 0.5453, 'learning_rate': 1.1111111111111112e-05, 'epoch': 87.99}\n",
            "{'loss': 0.5823, 'learning_rate': 6.6666666666666675e-06, 'epoch': 88.79}\n",
            "{'loss': 0.5935, 'learning_rate': 2.222222222222222e-06, 'epoch': 89.6}\n",
            "100% 4500/4500 [25:13<00:00,  2.89it/s][INFO|trainer.py:1403] 2021-08-03 01:38:18,338 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1513.541, 'train_samples_per_second': 191.412, 'train_steps_per_second': 2.973, 'train_loss': 0.9222937914530436, 'epoch': 89.99}\n",
            "100% 4500/4500 [25:13<00:00,  2.97it/s]\n",
            "[INFO|trainer.py:1989] 2021-08-03 01:38:18,343 >> Saving model checkpoint to results/adapters/sciie\n",
            "[INFO|loading.py:59] 2021-08-03 01:38:18,348 >> Configuration saved in results/adapters/sciie/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 01:38:18,373 >> Module weights saved in results/adapters/sciie/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 01:38:18,377 >> Configuration saved in results/adapters/sciie/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 01:38:18,944 >> Module weights saved in results/adapters/sciie/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 01:38:18,948 >> Configuration saved in results/adapters/sciie/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 01:38:19,557 >> Module weights saved in results/adapters/sciie/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 01:38:19,576 >> tokenizer config file saved in results/adapters/sciie/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 01:38:19,580 >> Special tokens file saved in results/adapters/sciie/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      89.99\n",
            "  train_loss               =     0.9223\n",
            "  train_runtime            = 0:25:13.54\n",
            "  train_samples            =       3219\n",
            "  train_samples_per_second =    191.412\n",
            "  train_steps_per_second   =      2.973\n",
            "08/03/2021 01:38:19 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:547] 2021-08-03 01:38:19,713 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 01:38:19,716 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 01:38:19,717 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 01:38:19,717 >>   Num examples = 455\n",
            "[INFO|trainer.py:2244] 2021-08-03 01:38:19,717 >>   Batch size = 8\n",
            " 98% 56/57 [00:01<00:00, 48.98it/s][WARNING|training_args.py:774] 2021-08-03 01:38:20,861 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 57/57 [00:01<00:00, 50.50it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      89.99\n",
            "  eval_loss               =     2.1565\n",
            "  eval_runtime            = 0:00:01.14\n",
            "  eval_samples            =        455\n",
            "  eval_samples_per_second =     397.25\n",
            "  eval_steps_per_second   =     49.765\n",
            "  perplexity              =      8.641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN2JIRzhroxB",
        "outputId": "43f920e6-5dcc-4a4e-c499-79914bb28e53"
      },
      "source": [
        "# import torch\n",
        "# torch.cuda.empty_cache() \n",
        "# torch.cuda.synchronize()\n",
        "# Experiment_4_sciie sciie_1 average='macro'\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/sciie_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/sciie_1/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/sciie/mlm \\\n",
        "--per_device_eval_batch_size 12 \\\n",
        "--weight_decay 0.12 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.95 \\\n",
        "--adam_epsilon 5e-4 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 1 \\\n",
        "--avg_type macro \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 01:56:34.745824: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 01:56:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3219\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10760\n",
            "  2% 269/10760 [01:41<52:31,  3.33it/s]  ***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.88it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.91it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.53it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.77it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.31it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.02it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.83it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.70it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.62it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.56it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.52it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.49it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.47it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6196889877319336, 'eval_acc': 0.4703296703296703, 'eval_f1': 0.09139440529575059, 'eval_precision': 0.06718995290423861, 'eval_recall': 0.14285714285714285, 'eval_runtime': 5.9096, 'eval_samples_per_second': 76.993, 'eval_steps_per_second': 6.43, 'epoch': 1.0}\n",
            "  2% 269/10760 [01:47<52:31,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 1.5773, 'learning_rate': 3.814126394052045e-05, 'epoch': 1.86}\n",
            "  5% 500/10760 [03:14<1:04:39,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-500\n",
            "Configuration saved in results/sciie_1/checkpoint-500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-500/pytorch_model.bin\n",
            "  5% 538/10760 [03:33<51:53,  3.28it/s]  ***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.62it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.71it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.37it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.65it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.21it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.92it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.73it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.62it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.46it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.44it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.39it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.38it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.38it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.38it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.38it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.37it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.36it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.36it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.35it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.35it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.37it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.37it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.587307333946228, 'eval_acc': 0.4703296703296703, 'eval_f1': 0.09139440529575059, 'eval_precision': 0.06718995290423861, 'eval_recall': 0.14285714285714285, 'eval_runtime': 5.9549, 'eval_samples_per_second': 76.408, 'eval_steps_per_second': 6.381, 'epoch': 2.0}\n",
            "  5% 538/10760 [03:39<51:53,  3.28it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "  8% 807/10760 [05:20<49:51,  3.33it/s]  ***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.81it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.85it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.38it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.66it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.23it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.96it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.78it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.48it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4822314977645874, 'eval_acc': 0.4747252747252747, 'eval_f1': 0.10265746247755245, 'eval_precision': 0.21034374014506466, 'eval_recall': 0.14857142857142858, 'eval_runtime': 5.9194, 'eval_samples_per_second': 76.866, 'eval_steps_per_second': 6.42, 'epoch': 3.0}\n",
            "  8% 807/10760 [05:26<49:51,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.63it/s]\u001b[A\n",
            "{'loss': 1.4266, 'learning_rate': 3.62825278810409e-05, 'epoch': 3.72}\n",
            "  9% 1000/10760 [06:39<1:01:26,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-1000\n",
            "Configuration saved in results/sciie_1/checkpoint-1000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-1000/pytorch_model.bin\n",
            " 10% 1076/10760 [07:12<48:39,  3.32it/s]  ***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.79it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.3403891324996948, 'eval_acc': 0.5406593406593406, 'eval_f1': 0.21474806372765556, 'eval_precision': 0.191001221001221, 'eval_recall': 0.2632173972075762, 'eval_runtime': 5.9271, 'eval_samples_per_second': 76.767, 'eval_steps_per_second': 6.411, 'epoch': 4.0}\n",
            " 10% 1076/10760 [07:18<48:39,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            " 12% 1345/10760 [09:00<47:25,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.81it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.78it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.57it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.51it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.47it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.283643364906311, 'eval_acc': 0.5494505494505495, 'eval_f1': 0.22861993693912197, 'eval_precision': 0.2004901084655686, 'eval_recall': 0.2720309565296101, 'eval_runtime': 5.9344, 'eval_samples_per_second': 76.672, 'eval_steps_per_second': 6.403, 'epoch': 5.0}\n",
            " 12% 1345/10760 [09:06<47:25,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 1.2791, 'learning_rate': 3.442379182156134e-05, 'epoch': 5.58}\n",
            " 14% 1500/10760 [10:04<58:11,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-1500\n",
            "Configuration saved in results/sciie_1/checkpoint-1500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-1500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-1500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-1500/pytorch_model.bin\n",
            " 15% 1614/10760 [10:51<45:52,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.85it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.86it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.2489527463912964, 'eval_acc': 0.5538461538461539, 'eval_f1': 0.23775356296364705, 'eval_precision': 0.34536611228695396, 'eval_recall': 0.2732454494166779, 'eval_runtime': 5.9238, 'eval_samples_per_second': 76.809, 'eval_steps_per_second': 6.415, 'epoch': 6.0}\n",
            " 15% 1614/10760 [10:57<45:52,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            " 18% 1883/10760 [12:39<44:25,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.83it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.1470870971679688, 'eval_acc': 0.5868131868131868, 'eval_f1': 0.32873247171869835, 'eval_precision': 0.38994174811072513, 'eval_recall': 0.33073313414574196, 'eval_runtime': 5.9199, 'eval_samples_per_second': 76.859, 'eval_steps_per_second': 6.419, 'epoch': 7.0}\n",
            " 18% 1883/10760 [12:45<44:25,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 1.1318, 'learning_rate': 3.2565055762081785e-05, 'epoch': 7.43}\n",
            " 19% 2000/10760 [13:29<55:16,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-2000\n",
            "Configuration saved in results/sciie_1/checkpoint-2000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-2000/pytorch_model.bin\n",
            " 20% 2152/10760 [14:31<43:21,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.77it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.9658957123756409, 'eval_acc': 0.6593406593406593, 'eval_f1': 0.4195197094595183, 'eval_precision': 0.49546193089500173, 'eval_recall': 0.4439332481563892, 'eval_runtime': 5.9305, 'eval_samples_per_second': 76.722, 'eval_steps_per_second': 6.408, 'epoch': 8.0}\n",
            " 20% 2152/10760 [14:37<43:21,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            " 22% 2421/10760 [16:18<41:45,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.87it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.90it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.50it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.42it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.8190645575523376, 'eval_acc': 0.7054945054945055, 'eval_f1': 0.5083630464172286, 'eval_precision': 0.6947866053232599, 'eval_recall': 0.4844792206817475, 'eval_runtime': 5.9171, 'eval_samples_per_second': 76.896, 'eval_steps_per_second': 6.422, 'epoch': 9.0}\n",
            " 22% 2421/10760 [16:24<41:45,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.9344, 'learning_rate': 3.0706319702602235e-05, 'epoch': 9.29}\n",
            " 23% 2500/10760 [16:54<52:17,  2.63it/s]Saving model checkpoint to results/sciie_1/checkpoint-2500\n",
            "Configuration saved in results/sciie_1/checkpoint-2500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-2500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-2500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-2500/pytorch_model.bin\n",
            " 25% 2690/10760 [18:10<40:27,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.86it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.88it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.51it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.75it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.00it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.78it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6513662338256836, 'eval_acc': 0.7626373626373626, 'eval_f1': 0.6026934165747758, 'eval_precision': 0.7839810421776365, 'eval_recall': 0.5790043415858716, 'eval_runtime': 5.9246, 'eval_samples_per_second': 76.799, 'eval_steps_per_second': 6.414, 'epoch': 10.0}\n",
            " 25% 2690/10760 [18:16<40:27,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            " 28% 2959/10760 [19:57<38:58,  3.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.84it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.87it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.49it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.5727083086967468, 'eval_acc': 0.7912087912087912, 'eval_f1': 0.6687443125723325, 'eval_precision': 0.7948000009720486, 'eval_recall': 0.6383760556932228, 'eval_runtime': 5.915, 'eval_samples_per_second': 76.923, 'eval_steps_per_second': 6.424, 'epoch': 11.0}\n",
            " 28% 2959/10760 [20:03<38:58,  3.34it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            "{'loss': 0.6981, 'learning_rate': 2.884758364312268e-05, 'epoch': 11.15}\n",
            " 28% 3000/10760 [20:19<48:47,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-3000\n",
            "Configuration saved in results/sciie_1/checkpoint-3000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-3000/pytorch_model.bin\n",
            " 30% 3228/10760 [21:49<37:50,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.85it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.88it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.50it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.49161386489868164, 'eval_acc': 0.8483516483516483, 'eval_f1': 0.7860097928215816, 'eval_precision': 0.8251103363809893, 'eval_recall': 0.7618332979979515, 'eval_runtime': 5.9226, 'eval_samples_per_second': 76.824, 'eval_steps_per_second': 6.416, 'epoch': 12.0}\n",
            " 30% 3228/10760 [21:55<37:50,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.59it/s]\u001b[A\n",
            " 32% 3497/10760 [23:37<36:19,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.84it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.88it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.50it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.00it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.43it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.43it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.4906291663646698, 'eval_acc': 0.8505494505494505, 'eval_f1': 0.7975152646391753, 'eval_precision': 0.805880504279519, 'eval_recall': 0.7990527746392561, 'eval_runtime': 5.9152, 'eval_samples_per_second': 76.92, 'eval_steps_per_second': 6.424, 'epoch': 13.0}\n",
            " 32% 3497/10760 [23:43<36:19,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.5774, 'learning_rate': 2.6988847583643126e-05, 'epoch': 13.01}\n",
            " 33% 3500/10760 [23:44<2:27:46,  1.22s/it]Saving model checkpoint to results/sciie_1/checkpoint-3500\n",
            "Configuration saved in results/sciie_1/checkpoint-3500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-3500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-3500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-3500/pytorch_model.bin\n",
            " 35% 3766/10760 [25:29<35:12,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.82it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.48it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.4434226155281067, 'eval_acc': 0.8593406593406593, 'eval_f1': 0.8155837513877721, 'eval_precision': 0.8160146148130607, 'eval_recall': 0.8161808663321347, 'eval_runtime': 5.9262, 'eval_samples_per_second': 76.778, 'eval_steps_per_second': 6.412, 'epoch': 14.0}\n",
            " 35% 3766/10760 [25:35<35:12,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.4909, 'learning_rate': 2.5130111524163573e-05, 'epoch': 14.87}\n",
            " 37% 4000/10760 [27:04<42:29,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-4000\n",
            "Configuration saved in results/sciie_1/checkpoint-4000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-4000/pytorch_model.bin\n",
            " 38% 4035/10760 [27:21<34:18,  3.27it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.81it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.45it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.69it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.23it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.94it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.76it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.63it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.55it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.51it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.4417535066604614, 'eval_acc': 0.8505494505494505, 'eval_f1': 0.7866897553756756, 'eval_precision': 0.8232786222993597, 'eval_recall': 0.7739171716348056, 'eval_runtime': 5.9267, 'eval_samples_per_second': 76.772, 'eval_steps_per_second': 6.412, 'epoch': 15.0}\n",
            " 38% 4035/10760 [27:27<34:18,  3.27it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            " 40% 4304/10760 [29:08<32:20,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.83it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.88it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.51it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.00it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.43362540006637573, 'eval_acc': 0.8637362637362638, 'eval_f1': 0.8107303610392776, 'eval_precision': 0.8680163686715192, 'eval_recall': 0.7749061350623876, 'eval_runtime': 5.9207, 'eval_samples_per_second': 76.849, 'eval_steps_per_second': 6.418, 'epoch': 16.0}\n",
            " 40% 4304/10760 [29:14<32:20,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.4316, 'learning_rate': 2.3271375464684017e-05, 'epoch': 16.73}\n",
            " 42% 4500/10760 [30:28<39:31,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-4500\n",
            "Configuration saved in results/sciie_1/checkpoint-4500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-4500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-4500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-4500/pytorch_model.bin\n",
            " 42% 4573/10760 [30:59<30:56,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.89it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.90it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.52it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.76it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.30it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.01it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.83it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.70it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.62it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.56it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.51it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.48it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.44it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.43it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.43it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.43it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.43it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.43it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.43it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.43it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.43it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.43it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.43it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.43it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.43it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.43it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.43it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.4314243793487549, 'eval_acc': 0.8571428571428571, 'eval_f1': 0.8075276855428576, 'eval_precision': 0.8023880078903508, 'eval_recall': 0.8167773052142538, 'eval_runtime': 5.9012, 'eval_samples_per_second': 77.103, 'eval_steps_per_second': 6.439, 'epoch': 17.0}\n",
            " 42% 4573/10760 [31:05<30:56,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            " 45% 4842/10760 [32:46<29:44,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.84it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.89it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.50it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.43it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.38404375314712524, 'eval_acc': 0.8637362637362638, 'eval_f1': 0.8121783215636649, 'eval_precision': 0.8248811396269023, 'eval_recall': 0.8011408333024033, 'eval_runtime': 5.9137, 'eval_samples_per_second': 76.94, 'eval_steps_per_second': 6.426, 'epoch': 18.0}\n",
            " 45% 4842/10760 [32:52<29:44,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.3893, 'learning_rate': 2.141263940520446e-05, 'epoch': 18.59}\n",
            " 46% 5000/10760 [33:52<36:24,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-5000\n",
            "Configuration saved in results/sciie_1/checkpoint-5000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-5000/pytorch_model.bin\n",
            " 48% 5111/10760 [34:37<28:16,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.82it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.86it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.49it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3942338824272156, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.824847231431914, 'eval_precision': 0.8468212039728268, 'eval_recall': 0.8120255750449331, 'eval_runtime': 5.9199, 'eval_samples_per_second': 76.859, 'eval_steps_per_second': 6.419, 'epoch': 19.0}\n",
            " 48% 5111/10760 [34:43<28:16,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            " 50% 5380/10760 [36:25<26:57,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.79it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.85it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3835775852203369, 'eval_acc': 0.8637362637362638, 'eval_f1': 0.8141231698329792, 'eval_precision': 0.8211458631818667, 'eval_recall': 0.8112404372854963, 'eval_runtime': 5.9249, 'eval_samples_per_second': 76.795, 'eval_steps_per_second': 6.414, 'epoch': 20.0}\n",
            " 50% 5380/10760 [36:31<26:57,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.3566, 'learning_rate': 1.955390334572491e-05, 'epoch': 20.45}\n",
            " 51% 5500/10760 [37:16<33:06,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-5500\n",
            "Configuration saved in results/sciie_1/checkpoint-5500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-5500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-5500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-5500/pytorch_model.bin\n",
            " 52% 5649/10760 [38:17<25:32,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.86it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.86it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.48it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.42it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.42it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3801996111869812, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8282368542349717, 'eval_precision': 0.8369731299726221, 'eval_recall': 0.8242424815375405, 'eval_runtime': 5.9146, 'eval_samples_per_second': 76.928, 'eval_steps_per_second': 6.425, 'epoch': 21.0}\n",
            " 52% 5649/10760 [38:22<25:32,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            " 55% 5918/10760 [40:04<24:18,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.74it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.80it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.45it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3852074146270752, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.823910254912069, 'eval_precision': 0.8472688658949071, 'eval_recall': 0.8095639595576168, 'eval_runtime': 5.9275, 'eval_samples_per_second': 76.761, 'eval_steps_per_second': 6.411, 'epoch': 22.0}\n",
            " 55% 5918/10760 [40:10<24:18,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.3483, 'learning_rate': 1.7695167286245355e-05, 'epoch': 22.3}\n",
            " 56% 6000/10760 [40:41<29:59,  2.65it/s]Saving model checkpoint to results/sciie_1/checkpoint-6000\n",
            "Configuration saved in results/sciie_1/checkpoint-6000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-6000/pytorch_model.bin\n",
            " 57% 6187/10760 [41:55<22:52,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.83it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.87it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:03,  8.50it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.51it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.48it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.46it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.42it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.42it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3775372803211212, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.8266778657988284, 'eval_precision': 0.8433848704008521, 'eval_recall': 0.8181047576758397, 'eval_runtime': 5.9156, 'eval_samples_per_second': 76.916, 'eval_steps_per_second': 6.424, 'epoch': 23.0}\n",
            " 57% 6187/10760 [42:01<22:52,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            " 60% 6456/10760 [43:42<21:45,  3.30it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.78it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.83it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3597326874732971, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.8304290563771406, 'eval_precision': 0.8354915673229103, 'eval_recall': 0.8270857956308546, 'eval_runtime': 5.9283, 'eval_samples_per_second': 76.751, 'eval_steps_per_second': 6.41, 'epoch': 24.0}\n",
            " 60% 6456/10760 [43:48<21:45,  3.30it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.3172, 'learning_rate': 1.5836431226765802e-05, 'epoch': 24.16}\n",
            " 60% 6500/10760 [44:05<26:57,  2.63it/s]Saving model checkpoint to results/sciie_1/checkpoint-6500\n",
            "Configuration saved in results/sciie_1/checkpoint-6500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-6500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-6500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-6500/pytorch_model.bin\n",
            " 62% 6725/10760 [45:34<20:15,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.77it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.55it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.52it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.49it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.42it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.41it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3738100230693817, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8181061280366267, 'eval_precision': 0.8300146337838543, 'eval_recall': 0.8153463776414365, 'eval_runtime': 5.9179, 'eval_samples_per_second': 76.885, 'eval_steps_per_second': 6.421, 'epoch': 25.0}\n",
            " 62% 6725/10760 [45:40<20:15,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.62it/s]\u001b[A\n",
            " 65% 6994/10760 [47:21<18:52,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.81it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.83it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.38268932700157166, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8273651898424687, 'eval_precision': 0.822490340747858, 'eval_recall': 0.8411271163701287, 'eval_runtime': 5.9308, 'eval_samples_per_second': 76.718, 'eval_steps_per_second': 6.407, 'epoch': 26.0}\n",
            " 65% 6994/10760 [47:27<18:52,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.58it/s]\u001b[A\n",
            "{'loss': 0.3061, 'learning_rate': 1.3977695167286247e-05, 'epoch': 26.02}\n",
            " 65% 7000/10760 [47:30<41:58,  1.49it/s]Saving model checkpoint to results/sciie_1/checkpoint-7000\n",
            "Configuration saved in results/sciie_1/checkpoint-7000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-7000/pytorch_model.bin\n",
            " 68% 7263/10760 [49:13<17:31,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.74it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.83it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.25it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.97it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.77it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.65it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.57it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.51it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.46it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.38it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3630470335483551, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8228090892074862, 'eval_precision': 0.8417490468606822, 'eval_recall': 0.8104184321877902, 'eval_runtime': 5.9393, 'eval_samples_per_second': 76.608, 'eval_steps_per_second': 6.398, 'epoch': 27.0}\n",
            " 68% 7263/10760 [49:19<17:31,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.2932, 'learning_rate': 1.211895910780669e-05, 'epoch': 27.88}\n",
            " 70% 7500/10760 [50:49<20:37,  2.63it/s]Saving model checkpoint to results/sciie_1/checkpoint-7500\n",
            "Configuration saved in results/sciie_1/checkpoint-7500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-7500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-7500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-7500/pytorch_model.bin\n",
            " 70% 7532/10760 [51:05<16:25,  3.28it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.73it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.78it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.40it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.66it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.22it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.95it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.76it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.64it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.55it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.47it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.44it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.38it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3683420419692993, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8220407235137764, 'eval_precision': 0.8470295946896932, 'eval_recall': 0.8081437325883243, 'eval_runtime': 5.943, 'eval_samples_per_second': 76.56, 'eval_steps_per_second': 6.394, 'epoch': 28.0}\n",
            " 70% 7532/10760 [51:11<16:25,  3.28it/s]\n",
            "100% 38/38 [00:05<00:00,  6.59it/s]\u001b[A\n",
            " 72% 7801/10760 [52:52<14:51,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.83it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.86it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.49it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.74it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  7.00it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.81it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.60it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.41it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3602180778980255, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8176981715473384, 'eval_precision': 0.8342302084554267, 'eval_recall': 0.8104042571376125, 'eval_runtime': 5.923, 'eval_samples_per_second': 76.819, 'eval_steps_per_second': 6.416, 'epoch': 29.0}\n",
            " 72% 7801/10760 [52:58<14:51,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.59it/s]\u001b[A\n",
            "{'loss': 0.2844, 'learning_rate': 1.0260223048327138e-05, 'epoch': 29.74}\n",
            " 74% 8000/10760 [54:14<17:25,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-8000\n",
            "Configuration saved in results/sciie_1/checkpoint-8000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-8000/pytorch_model.bin\n",
            " 75% 8070/10760 [54:43<13:33,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.82it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.85it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.97it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.39it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3559102714061737, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8231035223175178, 'eval_precision': 0.8335047982097665, 'eval_recall': 0.8233769447724709, 'eval_runtime': 5.9302, 'eval_samples_per_second': 76.726, 'eval_steps_per_second': 6.408, 'epoch': 30.0}\n",
            " 75% 8070/10760 [54:49<13:33,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            " 78% 8339/10760 [56:31<12:09,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.80it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.41it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.3542000353336334, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8231728437640683, 'eval_precision': 0.836857629981577, 'eval_recall': 0.8152753678721509, 'eval_runtime': 5.9307, 'eval_samples_per_second': 76.72, 'eval_steps_per_second': 6.407, 'epoch': 31.0}\n",
            " 78% 8339/10760 [56:37<12:09,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.2749, 'learning_rate': 8.401486988847585e-06, 'epoch': 31.6}\n",
            " 79% 8500/10760 [57:38<14:17,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-8500\n",
            "Configuration saved in results/sciie_1/checkpoint-8500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-8500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-8500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-8500/pytorch_model.bin\n",
            " 80% 8608/10760 [58:22<10:48,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.78it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.82it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.57it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.38it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.351177841424942, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8342108909959995, 'eval_precision': 0.8532475719816464, 'eval_recall': 0.8222041464735045, 'eval_runtime': 5.9357, 'eval_samples_per_second': 76.655, 'eval_steps_per_second': 6.402, 'epoch': 32.0}\n",
            " 80% 8608/10760 [58:28<10:48,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.59it/s]\u001b[A\n",
            " 82% 8877/10760 [1:00:10<09:25,  3.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.78it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.48it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.57it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3555254638195038, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8235511600770408, 'eval_precision': 0.8452884885808088, 'eval_recall': 0.8112191644555008, 'eval_runtime': 5.9323, 'eval_samples_per_second': 76.699, 'eval_steps_per_second': 6.406, 'epoch': 33.0}\n",
            " 82% 8877/10760 [1:00:16<09:25,  3.33it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.2656, 'learning_rate': 6.54275092936803e-06, 'epoch': 33.46}\n",
            " 84% 9000/10760 [1:01:03<11:06,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-9000\n",
            "Configuration saved in results/sciie_1/checkpoint-9000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-9000/pytorch_model.bin\n",
            " 85% 9146/10760 [1:02:01<08:08,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.75it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.82it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.54it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:03,  6.50it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.48it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.45it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.38it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3571719229221344, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.8307071897639606, 'eval_precision': 0.8368755117177246, 'eval_recall': 0.8346015434066842, 'eval_runtime': 5.932, 'eval_samples_per_second': 76.702, 'eval_steps_per_second': 6.406, 'epoch': 34.0}\n",
            " 85% 9146/10760 [1:02:07<08:08,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.59it/s]\u001b[A\n",
            " 88% 9415/10760 [1:03:49<06:45,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.76it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.82it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.99it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.68it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.59it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.47it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.39it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.41it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3522724509239197, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8356234291797408, 'eval_precision': 0.8462739732800657, 'eval_recall': 0.8336169348315877, 'eval_runtime': 5.9287, 'eval_samples_per_second': 76.745, 'eval_steps_per_second': 6.409, 'epoch': 35.0}\n",
            " 88% 9415/10760 [1:03:55<06:45,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.61it/s]\u001b[A\n",
            "{'loss': 0.2664, 'learning_rate': 4.684014869888477e-06, 'epoch': 35.32}\n",
            " 88% 9500/10760 [1:04:27<07:56,  2.64it/s]Saving model checkpoint to results/sciie_1/checkpoint-9500\n",
            "Configuration saved in results/sciie_1/checkpoint-9500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-9500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-9500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-9500/pytorch_model.bin\n",
            " 90% 9684/10760 [1:05:40<05:26,  3.29it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.84it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.86it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.47it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.27it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.80it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.38it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.38it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.38it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.35692137479782104, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8332785278017936, 'eval_precision': 0.8574825437421344, 'eval_recall': 0.8192341370782613, 'eval_runtime': 5.939, 'eval_samples_per_second': 76.612, 'eval_steps_per_second': 6.398, 'epoch': 36.0}\n",
            " 90% 9684/10760 [1:05:46<05:26,  3.29it/s]\n",
            "100% 38/38 [00:05<00:00,  6.58it/s]\u001b[A\n",
            " 92% 9953/10760 [1:07:28<04:03,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.76it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.84it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.48it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.41it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3540659546852112, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.8249268781687237, 'eval_precision': 0.8467348684460916, 'eval_recall': 0.8110859889301133, 'eval_runtime': 5.9278, 'eval_samples_per_second': 76.758, 'eval_steps_per_second': 6.411, 'epoch': 37.0}\n",
            " 92% 9953/10760 [1:07:34<04:03,  3.32it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.2605, 'learning_rate': 2.8252788104089224e-06, 'epoch': 37.17}\n",
            " 93% 10000/10760 [1:07:52<04:48,  2.63it/s]Saving model checkpoint to results/sciie_1/checkpoint-10000\n",
            "Configuration saved in results/sciie_1/checkpoint-10000/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10000/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10000/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10000/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10000/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10000/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10000/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-10000/pytorch_model.bin\n",
            " 95% 10222/10760 [1:09:19<02:42,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.76it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.83it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.48it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.72it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.97it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.48it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.41it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.40it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.40it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.35872405767440796, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8374224552378523, 'eval_precision': 0.8448109250153824, 'eval_recall': 0.8387506143674585, 'eval_runtime': 5.9312, 'eval_samples_per_second': 76.712, 'eval_steps_per_second': 6.407, 'epoch': 38.0}\n",
            " 95% 10222/10760 [1:09:25<02:42,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            " 98% 10491/10760 [1:11:07<01:21,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.79it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.82it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.45it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.98it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.66it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.53it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.49it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.46it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.44it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.42it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.39it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.40it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.39it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.38it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.38it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.40it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.35265544056892395, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.8269175493034363, 'eval_precision': 0.8361110516415675, 'eval_recall': 0.8245901638235191, 'eval_runtime': 5.9331, 'eval_samples_per_second': 76.689, 'eval_steps_per_second': 6.405, 'epoch': 39.0}\n",
            " 98% 10491/10760 [1:11:13<01:21,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "{'loss': 0.2528, 'learning_rate': 9.665427509293682e-07, 'epoch': 39.03}\n",
            " 98% 10500/10760 [1:11:17<02:04,  2.09it/s]Saving model checkpoint to results/sciie_1/checkpoint-10500\n",
            "Configuration saved in results/sciie_1/checkpoint-10500/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10500/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10500/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10500/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10500/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/checkpoint-10500/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/checkpoint-10500/config.json\n",
            "Model weights saved in results/sciie_1/checkpoint-10500/pytorch_model.bin\n",
            "100% 10760/10760 [1:12:59<00:00,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 2/38 [00:00<00:02, 12.77it/s]\u001b[A\n",
            "  8% 3/38 [00:00<00:03,  9.83it/s]\u001b[A\n",
            " 11% 4/38 [00:00<00:04,  8.46it/s]\u001b[A\n",
            " 13% 5/38 [00:00<00:04,  7.71it/s]\u001b[A\n",
            " 16% 6/38 [00:00<00:04,  7.26it/s]\u001b[A\n",
            " 18% 7/38 [00:00<00:04,  6.97it/s]\u001b[A\n",
            " 21% 8/38 [00:01<00:04,  6.79it/s]\u001b[A\n",
            " 24% 9/38 [00:01<00:04,  6.67it/s]\u001b[A\n",
            " 26% 10/38 [00:01<00:04,  6.58it/s]\u001b[A\n",
            " 29% 11/38 [00:01<00:04,  6.52it/s]\u001b[A\n",
            " 32% 12/38 [00:01<00:04,  6.47it/s]\u001b[A\n",
            " 34% 13/38 [00:01<00:03,  6.45it/s]\u001b[A\n",
            " 37% 14/38 [00:02<00:03,  6.43it/s]\u001b[A\n",
            " 39% 15/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 42% 16/38 [00:02<00:03,  6.41it/s]\u001b[A\n",
            " 45% 17/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 47% 18/38 [00:02<00:03,  6.40it/s]\u001b[A\n",
            " 50% 19/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 53% 20/38 [00:02<00:02,  6.40it/s]\u001b[A\n",
            " 55% 21/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 58% 22/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 61% 23/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 63% 24/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 66% 25/38 [00:03<00:02,  6.39it/s]\u001b[A\n",
            " 68% 26/38 [00:03<00:01,  6.40it/s]\u001b[A\n",
            " 71% 27/38 [00:04<00:01,  6.39it/s]\u001b[A\n",
            " 74% 28/38 [00:04<00:01,  6.37it/s]\u001b[A\n",
            " 76% 29/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 79% 30/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 82% 31/38 [00:04<00:01,  6.38it/s]\u001b[A\n",
            " 84% 32/38 [00:04<00:00,  6.38it/s]\u001b[A\n",
            " 87% 33/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 89% 34/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 92% 35/38 [00:05<00:00,  6.37it/s]\u001b[A\n",
            " 95% 36/38 [00:05<00:00,  6.36it/s]\u001b[A\n",
            " 97% 37/38 [00:05<00:00,  6.36it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3541638255119324, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.828219407767543, 'eval_precision': 0.8401781142564552, 'eval_recall': 0.8237647180828193, 'eval_runtime': 5.941, 'eval_samples_per_second': 76.587, 'eval_steps_per_second': 6.396, 'epoch': 40.0}\n",
            "100% 10760/10760 [1:13:04<00:00,  3.31it/s]\n",
            "100% 38/38 [00:05<00:00,  6.57it/s]\u001b[A\n",
            "                                   \u001b[A\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4385.0073, 'train_samples_per_second': 29.364, 'train_steps_per_second': 2.454, 'train_loss': 0.5712211197636827, 'epoch': 40.0}\n",
            "100% 10760/10760 [1:13:04<00:00,  2.45it/s]\n",
            "Saving model checkpoint to results/sciie_1/\n",
            "Configuration saved in results/sciie_1/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_1/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_1/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/mlm/head_config.json\n",
            "Module weights saved in results/sciie_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_1/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_1/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_1/config.json\n",
            "Model weights saved in results/sciie_1/pytorch_model.bin\n",
            "tokenizer config file saved in results/sciie_1/tokenizer_config.json\n",
            "Special tokens file saved in results/sciie_1/special_tokens_map.json\n",
            "08/03/2021 03:09:53 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 12\n",
            "100% 38/38 [00:05<00:00,  6.56it/s]\n",
            "08/03/2021 03:09:59 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_loss = 0.3541638255119324\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_acc = 0.8725274725274725\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_f1 = 0.828219407767543\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_precision = 0.8401781142564552\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_recall = 0.8237647180828193\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_runtime = 5.9711\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_samples_per_second = 76.2\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   eval_steps_per_second = 6.364\n",
            "08/03/2021 03:09:59 - INFO - __main__ -   epoch = 40.0\n",
            "08/03/2021 03:09:59 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 974\n",
            "  Batch size = 12\n",
            "100% 82/82 [00:12<00:00,  6.49it/s]\n",
            "08/03/2021 03:10:12 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_loss = 0.40495166182518005\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_acc = 0.8737166324435318\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_f1 = 0.8093496155426612\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_precision = 0.799790245704181\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_recall = 0.8205920551929212\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_runtime = 12.7847\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_samples_per_second = 76.185\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   eval_steps_per_second = 6.414\n",
            "08/03/2021 03:10:12 - INFO - __main__ -   epoch = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QGdzdI3E3sO",
        "outputId": "01660991-2059-45d0-bb56-f78be0f408fc"
      },
      "source": [
        "# Citation-intent new adapter\n",
        "!python3 run_mlm.py \\\n",
        "--train_file data/citation-intent_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/citation-intent_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/citation-intent \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 100 \\\n",
        "--learning_rate 5e-4 \\\n",
        "--logging_steps 40 \\\n",
        "--per_gpu_train_batch_size 12 \\\n",
        "--per_gpu_eval_batch_size 12 \\\n",
        "--gradient_accumulation_steps 2  \\\n",
        "--load_best_model_at_end \\\n",
        "--per_device_eval_batch_size 12 \\\n",
        "--weight_decay 0.12 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.95 \\\n",
        "--adam_epsilon 5e-4 \\\n",
        "--evaluation_strategy epoch \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 03:47:42.407000: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 03:47:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/03/2021 03:47:43 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.95,\n",
            "adam_epsilon=0.0005,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=40,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=2,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/citation-intent/runs/Aug03_03-47-43_b79b4b505e94,\n",
            "logging_first_step=False,\n",
            "logging_steps=40,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "output_dir=results/adapters/citation-intent,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=12,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=citation-intent,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/citation-intent,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.12,\n",
            ")\n",
            "08/03/2021 03:47:44 - WARNING - datasets.builder -   Using custom data configuration default-b2eba07b36f2e9bd\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-b2eba07b36f2e9bd/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-b2eba07b36f2e9bd/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 03:47:44,664 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 03:47:44,665 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-03 03:47:45,018 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 03:47:45,365 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 03:47:45,366 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 03:47:47,468 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|modeling_utils.py:1163] 2021-08-03 03:47:47,906 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-03 03:47:56,347 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-03 03:47:56,347 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-03 03:47:56,359 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 2/2 [00:00<00:00,  8.40ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00, 110.36ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-03 03:48:00,135 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-03 03:48:00,138 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-03 03:48:00,138 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-03 03:48:00,146 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-03 03:48:00,146 >>   Num examples = 1688\n",
            "[INFO|trainer.py:1201] 2021-08-03 03:48:00,146 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1202] 2021-08-03 03:48:00,146 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-03 03:48:00,146 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
            "[INFO|trainer.py:1204] 2021-08-03 03:48:00,147 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1205] 2021-08-03 03:48:00,147 >>   Total optimization steps = 7000\n",
            "[WARNING|training_args.py:760] 2021-08-03 03:48:00,158 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:48:00,158 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.5478, 'learning_rate': 0.0004971428571428571, 'epoch': 0.57}\n",
            "  1% 70/7000 [00:11<19:23,  5.95it/s][INFO|trainer.py:547] 2021-08-03 03:48:11,664 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:48:11,666 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:48:11,667 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:48:11,667 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:48:11,667 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.30it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.35it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:48:11,997 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.124768018722534, 'eval_runtime': 0.3312, 'eval_samples_per_second': 344.203, 'eval_steps_per_second': 30.193, 'epoch': 0.99}\n",
            "  1% 70/7000 [00:11<19:23,  5.95it/s]\n",
            "100% 10/10 [00:00<00:00, 35.35it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:48:12,002 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-70\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:12,007 >> Configuration saved in results/adapters/citation-intent/checkpoint-70/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:12,035 >> Module weights saved in results/adapters/citation-intent/checkpoint-70/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:12,040 >> Configuration saved in results/adapters/citation-intent/checkpoint-70/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:12,639 >> Module weights saved in results/adapters/citation-intent/checkpoint-70/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:12,643 >> Configuration saved in results/adapters/citation-intent/checkpoint-70/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:13,253 >> Module weights saved in results/adapters/citation-intent/checkpoint-70/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:48:13,274 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-70/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:48:13,277 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-70/special_tokens_map.json\n",
            "{'loss': 2.3956, 'learning_rate': 0.0004942857142857143, 'epoch': 1.14}\n",
            "{'loss': 2.217, 'learning_rate': 0.0004914285714285715, 'epoch': 1.71}\n",
            "  2% 140/7000 [00:24<20:22,  5.61it/s][INFO|trainer.py:547] 2021-08-03 03:48:25,038 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:48:25,040 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:48:25,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:48:25,040 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:48:25,040 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.14it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.29it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:48:25,374 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.124849319458008, 'eval_runtime': 0.3344, 'eval_samples_per_second': 340.94, 'eval_steps_per_second': 29.907, 'epoch': 1.99}\n",
            "  2% 140/7000 [00:25<20:22,  5.61it/s]\n",
            "100% 10/10 [00:00<00:00, 35.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:48:25,378 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-140\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:25,384 >> Configuration saved in results/adapters/citation-intent/checkpoint-140/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:25,411 >> Module weights saved in results/adapters/citation-intent/checkpoint-140/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:25,415 >> Configuration saved in results/adapters/citation-intent/checkpoint-140/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:25,986 >> Module weights saved in results/adapters/citation-intent/checkpoint-140/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:25,990 >> Configuration saved in results/adapters/citation-intent/checkpoint-140/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:26,589 >> Module weights saved in results/adapters/citation-intent/checkpoint-140/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:48:29,065 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-140/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:48:29,068 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-140/special_tokens_map.json\n",
            "{'loss': 2.2995, 'learning_rate': 0.0004885714285714286, 'epoch': 2.28}\n",
            "{'loss': 2.3036, 'learning_rate': 0.0004857142857142857, 'epoch': 2.85}\n",
            "  3% 210/7000 [00:40<17:56,  6.31it/s][INFO|trainer.py:547] 2021-08-03 03:48:40,842 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:48:40,845 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:48:40,845 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:48:40,845 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:48:40,845 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.16it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.82it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:48:41,184 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.1106224060058594, 'eval_runtime': 0.3397, 'eval_samples_per_second': 335.63, 'eval_steps_per_second': 29.441, 'epoch': 2.99}\n",
            "  3% 210/7000 [00:41<17:56,  6.31it/s]\n",
            "100% 10/10 [00:00<00:00, 35.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:48:41,188 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-210\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:41,195 >> Configuration saved in results/adapters/citation-intent/checkpoint-210/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:41,218 >> Module weights saved in results/adapters/citation-intent/checkpoint-210/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:41,222 >> Configuration saved in results/adapters/citation-intent/checkpoint-210/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:41,807 >> Module weights saved in results/adapters/citation-intent/checkpoint-210/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:41,810 >> Configuration saved in results/adapters/citation-intent/checkpoint-210/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:42,404 >> Module weights saved in results/adapters/citation-intent/checkpoint-210/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:48:42,426 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-210/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:48:42,429 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-210/special_tokens_map.json\n",
            "{'loss': 2.1406, 'learning_rate': 0.0004828571428571429, 'epoch': 3.43}\n",
            "{'loss': 2.0606, 'learning_rate': 0.00048, 'epoch': 3.99}\n",
            "  4% 280/7000 [00:56<18:42,  5.99it/s][INFO|trainer.py:547] 2021-08-03 03:48:56,564 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:48:56,566 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:48:56,566 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:48:56,566 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:48:56,566 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.30it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.46it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:48:56,894 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.066183567047119, 'eval_runtime': 0.3283, 'eval_samples_per_second': 347.245, 'eval_steps_per_second': 30.46, 'epoch': 3.99}\n",
            "  4% 280/7000 [00:56<18:42,  5.99it/s]\n",
            "100% 10/10 [00:00<00:00, 35.46it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:48:56,898 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-280\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:56,903 >> Configuration saved in results/adapters/citation-intent/checkpoint-280/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:56,925 >> Module weights saved in results/adapters/citation-intent/checkpoint-280/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:56,928 >> Configuration saved in results/adapters/citation-intent/checkpoint-280/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:57,483 >> Module weights saved in results/adapters/citation-intent/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:48:57,486 >> Configuration saved in results/adapters/citation-intent/checkpoint-280/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:48:58,068 >> Module weights saved in results/adapters/citation-intent/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:48:58,090 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-280/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:48:58,094 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-280/special_tokens_map.json\n",
            "{'loss': 2.1355, 'learning_rate': 0.00047714285714285713, 'epoch': 4.57}\n",
            "  5% 350/7000 [01:11<16:34,  6.68it/s][INFO|trainer.py:547] 2021-08-03 03:49:12,119 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:49:12,123 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:49:12,123 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:49:12,123 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:49:12,123 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.49it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.88it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:49:12,465 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8679534196853638, 'eval_runtime': 0.3429, 'eval_samples_per_second': 332.486, 'eval_steps_per_second': 29.165, 'epoch': 4.99}\n",
            "  5% 350/7000 [01:12<16:34,  6.68it/s]\n",
            "100% 10/10 [00:00<00:00, 35.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:49:12,470 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-350\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:12,475 >> Configuration saved in results/adapters/citation-intent/checkpoint-350/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:12,504 >> Module weights saved in results/adapters/citation-intent/checkpoint-350/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:12,509 >> Configuration saved in results/adapters/citation-intent/checkpoint-350/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:13,148 >> Module weights saved in results/adapters/citation-intent/checkpoint-350/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:13,151 >> Configuration saved in results/adapters/citation-intent/checkpoint-350/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:13,760 >> Module weights saved in results/adapters/citation-intent/checkpoint-350/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:49:13,780 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-350/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:49:13,782 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-350/special_tokens_map.json\n",
            "{'loss': 2.0847, 'learning_rate': 0.0004742857142857143, 'epoch': 5.14}\n",
            "{'loss': 2.0477, 'learning_rate': 0.0004714285714285714, 'epoch': 5.71}\n",
            "  6% 420/7000 [01:27<18:11,  6.03it/s][INFO|trainer.py:547] 2021-08-03 03:49:27,924 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:49:27,926 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:49:27,926 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:49:27,926 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:49:27,926 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 36.27it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.91it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:49:28,280 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.9324277639389038, 'eval_runtime': 0.3548, 'eval_samples_per_second': 321.287, 'eval_steps_per_second': 28.183, 'epoch': 5.99}\n",
            "  6% 420/7000 [01:28<18:11,  6.03it/s]\n",
            "100% 10/10 [00:00<00:00, 34.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:49:28,286 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-420\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:28,292 >> Configuration saved in results/adapters/citation-intent/checkpoint-420/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:28,317 >> Module weights saved in results/adapters/citation-intent/checkpoint-420/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:28,322 >> Configuration saved in results/adapters/citation-intent/checkpoint-420/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:28,930 >> Module weights saved in results/adapters/citation-intent/checkpoint-420/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:28,933 >> Configuration saved in results/adapters/citation-intent/checkpoint-420/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:29,567 >> Module weights saved in results/adapters/citation-intent/checkpoint-420/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:49:29,589 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-420/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:49:29,592 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-420/special_tokens_map.json\n",
            "{'loss': 2.1191, 'learning_rate': 0.0004685714285714286, 'epoch': 6.28}\n",
            "{'loss': 1.9633, 'learning_rate': 0.0004657142857142857, 'epoch': 6.85}\n",
            "  7% 490/7000 [01:43<20:32,  5.28it/s][INFO|trainer.py:547] 2021-08-03 03:49:43,696 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:49:43,699 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:49:43,699 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:49:43,699 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:49:43,699 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.36it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 33.58it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:49:44,044 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6768014430999756, 'eval_runtime': 0.3451, 'eval_samples_per_second': 330.296, 'eval_steps_per_second': 28.973, 'epoch': 6.99}\n",
            "  7% 490/7000 [01:43<20:32,  5.28it/s]\n",
            "100% 10/10 [00:00<00:00, 33.58it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:49:44,049 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-490\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:44,055 >> Configuration saved in results/adapters/citation-intent/checkpoint-490/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:44,084 >> Module weights saved in results/adapters/citation-intent/checkpoint-490/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:44,088 >> Configuration saved in results/adapters/citation-intent/checkpoint-490/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:44,683 >> Module weights saved in results/adapters/citation-intent/checkpoint-490/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:44,686 >> Configuration saved in results/adapters/citation-intent/checkpoint-490/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:45,328 >> Module weights saved in results/adapters/citation-intent/checkpoint-490/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:49:45,348 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-490/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:49:45,352 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-490/special_tokens_map.json\n",
            "{'loss': 2.0359, 'learning_rate': 0.00046285714285714284, 'epoch': 7.43}\n",
            "{'loss': 1.9315, 'learning_rate': 0.00046, 'epoch': 7.99}\n",
            "  8% 560/7000 [01:56<17:13,  6.23it/s][INFO|trainer.py:547] 2021-08-03 03:49:56,815 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:49:56,817 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:49:56,817 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:49:56,818 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:49:56,818 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.84it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.12it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:49:57,150 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7807477712631226, 'eval_runtime': 0.3326, 'eval_samples_per_second': 342.799, 'eval_steps_per_second': 30.07, 'epoch': 7.99}\n",
            "  8% 560/7000 [01:56<17:13,  6.23it/s]\n",
            "100% 10/10 [00:00<00:00, 35.12it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:49:57,153 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-560\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:57,158 >> Configuration saved in results/adapters/citation-intent/checkpoint-560/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:57,179 >> Module weights saved in results/adapters/citation-intent/checkpoint-560/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:57,183 >> Configuration saved in results/adapters/citation-intent/checkpoint-560/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:57,738 >> Module weights saved in results/adapters/citation-intent/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:49:57,741 >> Configuration saved in results/adapters/citation-intent/checkpoint-560/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:49:58,333 >> Module weights saved in results/adapters/citation-intent/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:49:58,352 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:49:58,355 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-560/special_tokens_map.json\n",
            "{'loss': 2.0043, 'learning_rate': 0.00045714285714285713, 'epoch': 8.57}\n",
            "  9% 630/7000 [02:09<18:03,  5.88it/s][INFO|trainer.py:547] 2021-08-03 03:50:10,123 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:50:10,125 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:50:10,125 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:50:10,125 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:50:10,125 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.20it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.26it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:50:10,458 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6846108436584473, 'eval_runtime': 0.3333, 'eval_samples_per_second': 342.004, 'eval_steps_per_second': 30.0, 'epoch': 8.99}\n",
            "  9% 630/7000 [02:10<18:03,  5.88it/s]\n",
            "100% 10/10 [00:00<00:00, 36.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:50:10,462 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-630\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:10,467 >> Configuration saved in results/adapters/citation-intent/checkpoint-630/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:10,494 >> Module weights saved in results/adapters/citation-intent/checkpoint-630/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:10,498 >> Configuration saved in results/adapters/citation-intent/checkpoint-630/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:11,110 >> Module weights saved in results/adapters/citation-intent/checkpoint-630/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:11,114 >> Configuration saved in results/adapters/citation-intent/checkpoint-630/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:11,687 >> Module weights saved in results/adapters/citation-intent/checkpoint-630/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:50:11,706 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-630/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:50:11,708 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-630/special_tokens_map.json\n",
            "{'loss': 1.9342, 'learning_rate': 0.0004542857142857143, 'epoch': 9.14}\n",
            "{'loss': 1.8666, 'learning_rate': 0.00045142857142857143, 'epoch': 9.71}\n",
            " 10% 700/7000 [02:23<18:10,  5.77it/s][INFO|trainer.py:547] 2021-08-03 03:50:23,298 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:50:23,301 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:50:23,301 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:50:23,301 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:50:23,301 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.33it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.05it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:50:23,656 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7483103275299072, 'eval_runtime': 0.355, 'eval_samples_per_second': 321.168, 'eval_steps_per_second': 28.173, 'epoch': 9.99}\n",
            " 10% 700/7000 [02:23<18:10,  5.77it/s]\n",
            "100% 10/10 [00:00<00:00, 34.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:50:23,660 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-700\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:23,685 >> Configuration saved in results/adapters/citation-intent/checkpoint-700/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:23,708 >> Module weights saved in results/adapters/citation-intent/checkpoint-700/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:23,712 >> Configuration saved in results/adapters/citation-intent/checkpoint-700/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:24,320 >> Module weights saved in results/adapters/citation-intent/checkpoint-700/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:24,324 >> Configuration saved in results/adapters/citation-intent/checkpoint-700/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:24,920 >> Module weights saved in results/adapters/citation-intent/checkpoint-700/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:50:24,941 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-700/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:50:24,943 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-700/special_tokens_map.json\n",
            "{'loss': 1.8792, 'learning_rate': 0.0004485714285714286, 'epoch': 10.28}\n",
            "{'loss': 1.951, 'learning_rate': 0.0004457142857142857, 'epoch': 10.85}\n",
            " 11% 770/7000 [02:36<15:09,  6.85it/s][INFO|trainer.py:547] 2021-08-03 03:50:36,476 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:50:36,478 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:50:36,478 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:50:36,478 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:50:36,478 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.05it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.80it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:50:36,889 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.001052141189575, 'eval_runtime': 0.4112, 'eval_samples_per_second': 277.207, 'eval_steps_per_second': 24.316, 'epoch': 10.99}\n",
            " 11% 770/7000 [02:36<15:09,  6.85it/s]\n",
            "100% 10/10 [00:00<00:00, 34.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:50:36,894 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-770\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:36,899 >> Configuration saved in results/adapters/citation-intent/checkpoint-770/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:36,920 >> Module weights saved in results/adapters/citation-intent/checkpoint-770/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:36,925 >> Configuration saved in results/adapters/citation-intent/checkpoint-770/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:37,540 >> Module weights saved in results/adapters/citation-intent/checkpoint-770/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:37,558 >> Configuration saved in results/adapters/citation-intent/checkpoint-770/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:38,180 >> Module weights saved in results/adapters/citation-intent/checkpoint-770/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:50:38,202 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-770/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:50:38,205 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-770/special_tokens_map.json\n",
            "{'loss': 1.933, 'learning_rate': 0.00044285714285714284, 'epoch': 11.43}\n",
            "{'loss': 1.8207, 'learning_rate': 0.00044, 'epoch': 11.99}\n",
            " 12% 840/7000 [02:49<18:51,  5.45it/s][INFO|trainer.py:547] 2021-08-03 03:50:49,841 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:50:49,844 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:50:49,844 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:50:49,844 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:50:49,844 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.94it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.26it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:50:50,175 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7351994514465332, 'eval_runtime': 0.332, 'eval_samples_per_second': 343.325, 'eval_steps_per_second': 30.116, 'epoch': 11.99}\n",
            " 12% 840/7000 [02:50<18:51,  5.45it/s]\n",
            "100% 10/10 [00:00<00:00, 35.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:50:50,180 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-840\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:50,185 >> Configuration saved in results/adapters/citation-intent/checkpoint-840/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:50,206 >> Module weights saved in results/adapters/citation-intent/checkpoint-840/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:50,210 >> Configuration saved in results/adapters/citation-intent/checkpoint-840/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:50,797 >> Module weights saved in results/adapters/citation-intent/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:50:50,963 >> Configuration saved in results/adapters/citation-intent/checkpoint-840/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:50:51,544 >> Module weights saved in results/adapters/citation-intent/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:50:51,566 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-840/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:50:51,570 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-840/special_tokens_map.json\n",
            "{'loss': 1.8971, 'learning_rate': 0.0004371428571428572, 'epoch': 12.57}\n",
            " 13% 910/7000 [03:02<16:19,  6.22it/s][INFO|trainer.py:547] 2021-08-03 03:51:03,096 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:51:03,099 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:51:03,099 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:51:03,099 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:51:03,099 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.49it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.66it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:51:03,436 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.709763526916504, 'eval_runtime': 0.3374, 'eval_samples_per_second': 337.877, 'eval_steps_per_second': 29.638, 'epoch': 12.99}\n",
            " 13% 910/7000 [03:03<16:19,  6.22it/s]\n",
            "100% 10/10 [00:00<00:00, 35.66it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:51:03,440 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-910\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:03,445 >> Configuration saved in results/adapters/citation-intent/checkpoint-910/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:03,467 >> Module weights saved in results/adapters/citation-intent/checkpoint-910/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:03,471 >> Configuration saved in results/adapters/citation-intent/checkpoint-910/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:04,060 >> Module weights saved in results/adapters/citation-intent/checkpoint-910/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:04,069 >> Configuration saved in results/adapters/citation-intent/checkpoint-910/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:04,694 >> Module weights saved in results/adapters/citation-intent/checkpoint-910/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:51:07,160 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-910/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:51:07,165 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-910/special_tokens_map.json\n",
            "{'loss': 1.8131, 'learning_rate': 0.0004342857142857143, 'epoch': 13.14}\n",
            "{'loss': 1.7651, 'learning_rate': 0.00043142857142857143, 'epoch': 13.71}\n",
            " 14% 980/7000 [03:18<19:35,  5.12it/s][INFO|trainer.py:547] 2021-08-03 03:51:18,756 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:51:18,759 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:51:18,759 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:51:18,759 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:51:18,759 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.69it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.67it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:51:19,106 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7345787286758423, 'eval_runtime': 0.3476, 'eval_samples_per_second': 327.921, 'eval_steps_per_second': 28.765, 'epoch': 13.99}\n",
            " 14% 980/7000 [03:18<19:35,  5.12it/s]\n",
            "100% 10/10 [00:00<00:00, 35.67it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:51:19,111 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-980\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:19,117 >> Configuration saved in results/adapters/citation-intent/checkpoint-980/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:19,140 >> Module weights saved in results/adapters/citation-intent/checkpoint-980/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:19,144 >> Configuration saved in results/adapters/citation-intent/checkpoint-980/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:19,737 >> Module weights saved in results/adapters/citation-intent/checkpoint-980/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:19,740 >> Configuration saved in results/adapters/citation-intent/checkpoint-980/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:20,383 >> Module weights saved in results/adapters/citation-intent/checkpoint-980/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:51:20,408 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-980/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:51:20,412 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-980/special_tokens_map.json\n",
            "{'loss': 1.7862, 'learning_rate': 0.00042857142857142855, 'epoch': 14.28}\n",
            "{'loss': 1.7571, 'learning_rate': 0.0004257142857142857, 'epoch': 14.85}\n",
            " 15% 1050/7000 [03:34<16:08,  6.14it/s][INFO|trainer.py:547] 2021-08-03 03:51:34,507 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:51:34,510 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:51:34,510 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:51:34,510 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:51:34,510 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 34.88it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 33.07it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 30.69it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:51:34,868 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.9751719236373901, 'eval_runtime': 0.3584, 'eval_samples_per_second': 318.11, 'eval_steps_per_second': 27.904, 'epoch': 14.99}\n",
            " 15% 1050/7000 [03:34<16:08,  6.14it/s]\n",
            "100% 10/10 [00:00<00:00, 30.69it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:51:34,873 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1050\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:34,882 >> Configuration saved in results/adapters/citation-intent/checkpoint-1050/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:34,911 >> Module weights saved in results/adapters/citation-intent/checkpoint-1050/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:34,916 >> Configuration saved in results/adapters/citation-intent/checkpoint-1050/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:35,584 >> Module weights saved in results/adapters/citation-intent/checkpoint-1050/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:35,588 >> Configuration saved in results/adapters/citation-intent/checkpoint-1050/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:36,283 >> Module weights saved in results/adapters/citation-intent/checkpoint-1050/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:51:36,287 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1050/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:51:36,291 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1050/special_tokens_map.json\n",
            "{'loss': 1.7239, 'learning_rate': 0.0004228571428571429, 'epoch': 15.43}\n",
            "{'loss': 1.8363, 'learning_rate': 0.00042, 'epoch': 15.99}\n",
            " 16% 1120/7000 [03:48<15:06,  6.49it/s][INFO|trainer.py:547] 2021-08-03 03:51:48,848 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:51:48,850 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:51:48,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:51:48,850 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:51:48,850 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 36.65it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.62it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:51:49,210 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6663336753845215, 'eval_runtime': 0.3604, 'eval_samples_per_second': 316.316, 'eval_steps_per_second': 27.747, 'epoch': 15.99}\n",
            " 16% 1120/7000 [03:49<15:06,  6.49it/s]\n",
            "100% 10/10 [00:00<00:00, 34.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:51:49,214 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1120\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:49,220 >> Configuration saved in results/adapters/citation-intent/checkpoint-1120/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:49,247 >> Module weights saved in results/adapters/citation-intent/checkpoint-1120/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:49,251 >> Configuration saved in results/adapters/citation-intent/checkpoint-1120/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:49,866 >> Module weights saved in results/adapters/citation-intent/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:51:49,870 >> Configuration saved in results/adapters/citation-intent/checkpoint-1120/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:51:50,498 >> Module weights saved in results/adapters/citation-intent/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:51:50,519 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1120/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:51:50,523 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1120/special_tokens_map.json\n",
            "{'loss': 1.7689, 'learning_rate': 0.00041714285714285714, 'epoch': 16.57}\n",
            " 17% 1190/7000 [04:04<17:21,  5.58it/s][INFO|trainer.py:547] 2021-08-03 03:52:04,864 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:52:04,867 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:52:04,867 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:52:04,867 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:52:04,868 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.68it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 34.76it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 31.85it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:52:05,220 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6985284090042114, 'eval_runtime': 0.3535, 'eval_samples_per_second': 322.448, 'eval_steps_per_second': 28.285, 'epoch': 16.99}\n",
            " 17% 1190/7000 [04:05<17:21,  5.58it/s]\n",
            "100% 10/10 [00:00<00:00, 31.85it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:52:05,224 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1190\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:05,229 >> Configuration saved in results/adapters/citation-intent/checkpoint-1190/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:05,248 >> Module weights saved in results/adapters/citation-intent/checkpoint-1190/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:05,252 >> Configuration saved in results/adapters/citation-intent/checkpoint-1190/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:05,853 >> Module weights saved in results/adapters/citation-intent/checkpoint-1190/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:05,858 >> Configuration saved in results/adapters/citation-intent/checkpoint-1190/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:06,455 >> Module weights saved in results/adapters/citation-intent/checkpoint-1190/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:52:06,474 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1190/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:52:06,477 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1190/special_tokens_map.json\n",
            "{'loss': 1.7833, 'learning_rate': 0.0004142857142857143, 'epoch': 17.14}\n",
            "{'loss': 1.7119, 'learning_rate': 0.00041142857142857143, 'epoch': 17.71}\n",
            " 18% 1260/7000 [04:20<14:46,  6.48it/s][INFO|trainer.py:547] 2021-08-03 03:52:20,856 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:52:20,858 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:52:20,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:52:20,858 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:52:20,858 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.16it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.23it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:52:21,196 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8584506511688232, 'eval_runtime': 0.3383, 'eval_samples_per_second': 336.954, 'eval_steps_per_second': 29.557, 'epoch': 17.99}\n",
            " 18% 1260/7000 [04:21<14:46,  6.48it/s]\n",
            "100% 10/10 [00:00<00:00, 36.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:52:21,201 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1260\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:21,205 >> Configuration saved in results/adapters/citation-intent/checkpoint-1260/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:21,229 >> Module weights saved in results/adapters/citation-intent/checkpoint-1260/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:21,234 >> Configuration saved in results/adapters/citation-intent/checkpoint-1260/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:21,842 >> Module weights saved in results/adapters/citation-intent/checkpoint-1260/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:21,845 >> Configuration saved in results/adapters/citation-intent/checkpoint-1260/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:22,436 >> Module weights saved in results/adapters/citation-intent/checkpoint-1260/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:52:22,455 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1260/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:52:22,458 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1260/special_tokens_map.json\n",
            "{'loss': 1.7132, 'learning_rate': 0.0004085714285714286, 'epoch': 18.28}\n",
            "{'loss': 1.6521, 'learning_rate': 0.0004057142857142857, 'epoch': 18.85}\n",
            " 19% 1330/7000 [04:36<15:39,  6.04it/s][INFO|trainer.py:547] 2021-08-03 03:52:36,759 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:52:36,761 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:52:36,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:52:36,762 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:52:36,762 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 34.93it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 33.43it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:52:37,120 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6262640953063965, 'eval_runtime': 0.3595, 'eval_samples_per_second': 317.084, 'eval_steps_per_second': 27.814, 'epoch': 18.99}\n",
            " 19% 1330/7000 [04:36<15:39,  6.04it/s]\n",
            "100% 10/10 [00:00<00:00, 33.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:52:37,125 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1330\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:37,132 >> Configuration saved in results/adapters/citation-intent/checkpoint-1330/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:37,157 >> Module weights saved in results/adapters/citation-intent/checkpoint-1330/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:37,162 >> Configuration saved in results/adapters/citation-intent/checkpoint-1330/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:37,809 >> Module weights saved in results/adapters/citation-intent/checkpoint-1330/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:37,813 >> Configuration saved in results/adapters/citation-intent/checkpoint-1330/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:38,434 >> Module weights saved in results/adapters/citation-intent/checkpoint-1330/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:52:38,454 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1330/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:52:38,458 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1330/special_tokens_map.json\n",
            "{'loss': 1.6686, 'learning_rate': 0.00040285714285714285, 'epoch': 19.43}\n",
            "{'loss': 1.6462, 'learning_rate': 0.0004, 'epoch': 19.99}\n",
            " 20% 1400/7000 [04:52<14:01,  6.66it/s][INFO|trainer.py:547] 2021-08-03 03:52:52,939 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:52:52,941 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:52:52,941 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:52:52,942 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:52:52,942 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.85it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.29it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:52:53,283 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8671635389328003, 'eval_runtime': 0.3422, 'eval_samples_per_second': 333.097, 'eval_steps_per_second': 29.219, 'epoch': 19.99}\n",
            " 20% 1400/7000 [04:53<14:01,  6.66it/s]\n",
            "100% 10/10 [00:00<00:00, 36.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:52:53,287 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1400\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:53,293 >> Configuration saved in results/adapters/citation-intent/checkpoint-1400/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:53,319 >> Module weights saved in results/adapters/citation-intent/checkpoint-1400/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:53,323 >> Configuration saved in results/adapters/citation-intent/checkpoint-1400/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:53,909 >> Module weights saved in results/adapters/citation-intent/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:52:53,914 >> Configuration saved in results/adapters/citation-intent/checkpoint-1400/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:52:54,531 >> Module weights saved in results/adapters/citation-intent/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:52:54,553 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:52:54,556 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1400/special_tokens_map.json\n",
            "{'loss': 1.7014, 'learning_rate': 0.00039714285714285714, 'epoch': 20.57}\n",
            " 21% 1470/7000 [05:05<16:42,  5.52it/s][INFO|trainer.py:547] 2021-08-03 03:53:06,154 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:53:06,156 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:53:06,157 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:53:06,157 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:53:06,157 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.57it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.74it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:53:06,499 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6186373233795166, 'eval_runtime': 0.3432, 'eval_samples_per_second': 332.196, 'eval_steps_per_second': 29.14, 'epoch': 20.99}\n",
            " 21% 1470/7000 [05:06<16:42,  5.52it/s]\n",
            "100% 10/10 [00:00<00:00, 36.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:53:06,503 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1470\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:06,509 >> Configuration saved in results/adapters/citation-intent/checkpoint-1470/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:06,530 >> Module weights saved in results/adapters/citation-intent/checkpoint-1470/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:06,534 >> Configuration saved in results/adapters/citation-intent/checkpoint-1470/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:07,125 >> Module weights saved in results/adapters/citation-intent/checkpoint-1470/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:07,128 >> Configuration saved in results/adapters/citation-intent/checkpoint-1470/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:07,726 >> Module weights saved in results/adapters/citation-intent/checkpoint-1470/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:53:07,745 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1470/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:53:07,748 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1470/special_tokens_map.json\n",
            "{'loss': 1.6789, 'learning_rate': 0.0003942857142857143, 'epoch': 21.14}\n",
            "{'loss': 1.6775, 'learning_rate': 0.00039142857142857143, 'epoch': 21.71}\n",
            " 22% 1540/7000 [05:19<13:44,  6.62it/s][INFO|trainer.py:547] 2021-08-03 03:53:19,383 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:53:19,385 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:53:19,385 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:53:19,385 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:53:19,385 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.70it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.93it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:53:19,727 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8753461837768555, 'eval_runtime': 0.3423, 'eval_samples_per_second': 333.012, 'eval_steps_per_second': 29.212, 'epoch': 21.99}\n",
            " 22% 1540/7000 [05:19<13:44,  6.62it/s]\n",
            "100% 10/10 [00:00<00:00, 34.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:53:19,731 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1540\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:19,737 >> Configuration saved in results/adapters/citation-intent/checkpoint-1540/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:19,762 >> Module weights saved in results/adapters/citation-intent/checkpoint-1540/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:19,766 >> Configuration saved in results/adapters/citation-intent/checkpoint-1540/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:20,355 >> Module weights saved in results/adapters/citation-intent/checkpoint-1540/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:20,359 >> Configuration saved in results/adapters/citation-intent/checkpoint-1540/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:20,951 >> Module weights saved in results/adapters/citation-intent/checkpoint-1540/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:53:20,970 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1540/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:53:20,973 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1540/special_tokens_map.json\n",
            "{'loss': 1.6467, 'learning_rate': 0.00038857142857142855, 'epoch': 22.28}\n",
            "{'loss': 1.6338, 'learning_rate': 0.0003857142857142857, 'epoch': 22.85}\n",
            " 23% 1610/7000 [05:32<15:06,  5.95it/s][INFO|trainer.py:547] 2021-08-03 03:53:32,637 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:53:32,640 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:53:32,640 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:53:32,640 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:53:32,640 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 36.52it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 33.72it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 31.13it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:53:33,006 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7329199314117432, 'eval_runtime': 0.367, 'eval_samples_per_second': 310.638, 'eval_steps_per_second': 27.249, 'epoch': 22.99}\n",
            " 23% 1610/7000 [05:32<15:06,  5.95it/s]\n",
            "100% 10/10 [00:00<00:00, 31.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:53:33,013 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1610\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:33,044 >> Configuration saved in results/adapters/citation-intent/checkpoint-1610/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:33,070 >> Module weights saved in results/adapters/citation-intent/checkpoint-1610/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:33,074 >> Configuration saved in results/adapters/citation-intent/checkpoint-1610/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:33,696 >> Module weights saved in results/adapters/citation-intent/checkpoint-1610/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:33,701 >> Configuration saved in results/adapters/citation-intent/checkpoint-1610/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:34,333 >> Module weights saved in results/adapters/citation-intent/checkpoint-1610/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:53:34,356 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1610/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:53:34,360 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1610/special_tokens_map.json\n",
            "{'loss': 1.6089, 'learning_rate': 0.00038285714285714285, 'epoch': 23.43}\n",
            "{'loss': 1.6342, 'learning_rate': 0.00038, 'epoch': 23.99}\n",
            " 24% 1680/7000 [05:45<15:08,  5.86it/s][INFO|trainer.py:547] 2021-08-03 03:53:46,142 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:53:46,144 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:53:46,144 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:53:46,144 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:53:46,144 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.29it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.17it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:53:46,482 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6754388809204102, 'eval_runtime': 0.3386, 'eval_samples_per_second': 336.649, 'eval_steps_per_second': 29.531, 'epoch': 23.99}\n",
            " 24% 1680/7000 [05:46<15:08,  5.86it/s]\n",
            "100% 10/10 [00:00<00:00, 35.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:53:46,486 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1680\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:46,492 >> Configuration saved in results/adapters/citation-intent/checkpoint-1680/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:46,516 >> Module weights saved in results/adapters/citation-intent/checkpoint-1680/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:46,520 >> Configuration saved in results/adapters/citation-intent/checkpoint-1680/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:47,133 >> Module weights saved in results/adapters/citation-intent/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:53:47,137 >> Configuration saved in results/adapters/citation-intent/checkpoint-1680/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:53:47,777 >> Module weights saved in results/adapters/citation-intent/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:53:47,800 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1680/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:53:47,804 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1680/special_tokens_map.json\n",
            "{'loss': 1.5897, 'learning_rate': 0.0003771428571428572, 'epoch': 24.57}\n",
            " 25% 1750/7000 [05:59<15:16,  5.73it/s][INFO|trainer.py:547] 2021-08-03 03:53:59,693 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:53:59,696 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:53:59,696 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:53:59,696 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:53:59,696 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.32it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.19it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:54:00,027 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.847119927406311, 'eval_runtime': 0.3315, 'eval_samples_per_second': 343.862, 'eval_steps_per_second': 30.163, 'epoch': 24.99}\n",
            " 25% 1750/7000 [05:59<15:16,  5.73it/s]\n",
            "100% 10/10 [00:00<00:00, 35.19it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:54:00,031 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1750\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:00,037 >> Configuration saved in results/adapters/citation-intent/checkpoint-1750/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:00,060 >> Module weights saved in results/adapters/citation-intent/checkpoint-1750/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:00,063 >> Configuration saved in results/adapters/citation-intent/checkpoint-1750/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:00,696 >> Module weights saved in results/adapters/citation-intent/checkpoint-1750/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:00,898 >> Configuration saved in results/adapters/citation-intent/checkpoint-1750/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:01,491 >> Module weights saved in results/adapters/citation-intent/checkpoint-1750/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:54:01,510 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:54:01,514 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1750/special_tokens_map.json\n",
            "{'loss': 1.5763, 'learning_rate': 0.00037428571428571426, 'epoch': 25.14}\n",
            "{'loss': 1.5566, 'learning_rate': 0.00037142857142857143, 'epoch': 25.71}\n",
            " 26% 1820/7000 [06:13<14:23,  6.00it/s][INFO|trainer.py:547] 2021-08-03 03:54:13,288 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:54:13,291 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:54:13,291 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:54:13,291 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:54:13,291 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.61it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.39it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:54:13,639 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5942103862762451, 'eval_runtime': 0.3494, 'eval_samples_per_second': 326.229, 'eval_steps_per_second': 28.617, 'epoch': 25.99}\n",
            " 26% 1820/7000 [06:13<14:23,  6.00it/s]\n",
            "100% 10/10 [00:00<00:00, 35.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:54:13,645 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1820\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:13,652 >> Configuration saved in results/adapters/citation-intent/checkpoint-1820/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:13,679 >> Module weights saved in results/adapters/citation-intent/checkpoint-1820/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:13,684 >> Configuration saved in results/adapters/citation-intent/checkpoint-1820/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:14,307 >> Module weights saved in results/adapters/citation-intent/checkpoint-1820/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:14,311 >> Configuration saved in results/adapters/citation-intent/checkpoint-1820/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:14,955 >> Module weights saved in results/adapters/citation-intent/checkpoint-1820/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:54:17,479 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1820/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:54:17,482 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1820/special_tokens_map.json\n",
            "{'loss': 1.5382, 'learning_rate': 0.00036857142857142855, 'epoch': 26.28}\n",
            "{'loss': 1.5486, 'learning_rate': 0.00036571428571428573, 'epoch': 26.85}\n",
            " 27% 1890/7000 [06:29<13:43,  6.20it/s][INFO|trainer.py:547] 2021-08-03 03:54:29,306 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:54:29,308 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:54:29,309 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:54:29,309 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:54:29,309 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.08it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.92it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:54:29,649 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6245152950286865, 'eval_runtime': 0.3408, 'eval_samples_per_second': 334.462, 'eval_steps_per_second': 29.339, 'epoch': 26.99}\n",
            " 27% 1890/7000 [06:29<13:43,  6.20it/s]\n",
            "100% 10/10 [00:00<00:00, 34.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:54:29,653 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1890\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:29,658 >> Configuration saved in results/adapters/citation-intent/checkpoint-1890/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:29,683 >> Module weights saved in results/adapters/citation-intent/checkpoint-1890/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:29,687 >> Configuration saved in results/adapters/citation-intent/checkpoint-1890/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:30,297 >> Module weights saved in results/adapters/citation-intent/checkpoint-1890/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:30,300 >> Configuration saved in results/adapters/citation-intent/checkpoint-1890/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:30,885 >> Module weights saved in results/adapters/citation-intent/checkpoint-1890/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:54:33,155 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1890/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:54:33,158 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1890/special_tokens_map.json\n",
            "{'loss': 1.5514, 'learning_rate': 0.0003628571428571429, 'epoch': 27.43}\n",
            "{'loss': 1.5164, 'learning_rate': 0.00035999999999999997, 'epoch': 27.99}\n",
            " 28% 1960/7000 [06:44<13:20,  6.30it/s][INFO|trainer.py:547] 2021-08-03 03:54:44,913 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:54:44,916 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:54:44,916 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:54:44,916 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:54:44,916 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.29it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.64it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:54:45,271 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4695652723312378, 'eval_runtime': 0.356, 'eval_samples_per_second': 320.227, 'eval_steps_per_second': 28.09, 'epoch': 27.99}\n",
            " 28% 1960/7000 [06:45<13:20,  6.30it/s]\n",
            "100% 10/10 [00:00<00:00, 35.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:54:45,276 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-1960\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:45,281 >> Configuration saved in results/adapters/citation-intent/checkpoint-1960/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:45,306 >> Module weights saved in results/adapters/citation-intent/checkpoint-1960/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:45,310 >> Configuration saved in results/adapters/citation-intent/checkpoint-1960/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:45,915 >> Module weights saved in results/adapters/citation-intent/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:54:45,919 >> Configuration saved in results/adapters/citation-intent/checkpoint-1960/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:54:46,559 >> Module weights saved in results/adapters/citation-intent/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:54:46,583 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-1960/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:54:46,586 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-1960/special_tokens_map.json\n",
            "{'loss': 1.585, 'learning_rate': 0.00035714285714285714, 'epoch': 28.57}\n",
            " 29% 2030/7000 [07:00<13:58,  5.93it/s][INFO|trainer.py:547] 2021-08-03 03:55:00,521 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:55:00,523 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:55:00,523 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:55:00,523 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:55:00,523 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.56it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.74it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:55:00,859 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.684096336364746, 'eval_runtime': 0.3363, 'eval_samples_per_second': 339.02, 'eval_steps_per_second': 29.739, 'epoch': 28.99}\n",
            " 29% 2030/7000 [07:00<13:58,  5.93it/s]\n",
            "100% 10/10 [00:00<00:00, 35.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:55:00,863 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2030\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:00,869 >> Configuration saved in results/adapters/citation-intent/checkpoint-2030/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:00,893 >> Module weights saved in results/adapters/citation-intent/checkpoint-2030/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:00,899 >> Configuration saved in results/adapters/citation-intent/checkpoint-2030/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:01,508 >> Module weights saved in results/adapters/citation-intent/checkpoint-2030/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:01,513 >> Configuration saved in results/adapters/citation-intent/checkpoint-2030/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:02,162 >> Module weights saved in results/adapters/citation-intent/checkpoint-2030/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:55:02,187 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2030/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:55:02,191 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2030/special_tokens_map.json\n",
            "{'loss': 1.6572, 'learning_rate': 0.00035428571428571426, 'epoch': 29.14}\n",
            "{'loss': 1.5449, 'learning_rate': 0.00035142857142857144, 'epoch': 29.71}\n",
            " 30% 2100/7000 [07:16<13:24,  6.09it/s][INFO|trainer.py:547] 2021-08-03 03:55:16,331 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:55:16,334 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:55:16,334 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:55:16,334 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:55:16,334 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.90it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.64it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:55:16,688 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6481174230575562, 'eval_runtime': 0.3547, 'eval_samples_per_second': 321.372, 'eval_steps_per_second': 28.191, 'epoch': 29.99}\n",
            " 30% 2100/7000 [07:16<13:24,  6.09it/s]\n",
            "100% 10/10 [00:00<00:00, 35.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:55:16,693 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2100\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:16,698 >> Configuration saved in results/adapters/citation-intent/checkpoint-2100/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:16,722 >> Module weights saved in results/adapters/citation-intent/checkpoint-2100/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:16,727 >> Configuration saved in results/adapters/citation-intent/checkpoint-2100/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:17,385 >> Module weights saved in results/adapters/citation-intent/checkpoint-2100/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:17,390 >> Configuration saved in results/adapters/citation-intent/checkpoint-2100/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:18,023 >> Module weights saved in results/adapters/citation-intent/checkpoint-2100/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:55:18,047 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:55:18,051 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2100/special_tokens_map.json\n",
            "{'loss': 1.5856, 'learning_rate': 0.0003485714285714286, 'epoch': 30.28}\n",
            "{'loss': 1.5173, 'learning_rate': 0.00034571428571428573, 'epoch': 30.85}\n",
            " 31% 2170/7000 [07:31<14:18,  5.63it/s][INFO|trainer.py:547] 2021-08-03 03:55:31,600 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:55:31,602 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:55:31,602 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:55:31,602 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:55:31,602 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.62it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.63it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:55:31,943 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6166762113571167, 'eval_runtime': 0.341, 'eval_samples_per_second': 334.266, 'eval_steps_per_second': 29.322, 'epoch': 30.99}\n",
            " 31% 2170/7000 [07:31<14:18,  5.63it/s]\n",
            "100% 10/10 [00:00<00:00, 35.63it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:55:31,947 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2170\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:31,952 >> Configuration saved in results/adapters/citation-intent/checkpoint-2170/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:31,979 >> Module weights saved in results/adapters/citation-intent/checkpoint-2170/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:31,984 >> Configuration saved in results/adapters/citation-intent/checkpoint-2170/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:32,597 >> Module weights saved in results/adapters/citation-intent/checkpoint-2170/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:32,601 >> Configuration saved in results/adapters/citation-intent/checkpoint-2170/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:33,221 >> Module weights saved in results/adapters/citation-intent/checkpoint-2170/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:55:33,244 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2170/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:55:33,247 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2170/special_tokens_map.json\n",
            "{'loss': 1.5754, 'learning_rate': 0.00034285714285714285, 'epoch': 31.43}\n",
            "{'loss': 1.4467, 'learning_rate': 0.00034, 'epoch': 31.99}\n",
            " 32% 2240/7000 [07:46<12:24,  6.40it/s][INFO|trainer.py:547] 2021-08-03 03:55:47,144 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:55:47,146 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:55:47,146 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:55:47,146 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:55:47,146 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.16it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.39it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:55:47,484 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7076436281204224, 'eval_runtime': 0.338, 'eval_samples_per_second': 337.292, 'eval_steps_per_second': 29.587, 'epoch': 31.99}\n",
            " 32% 2240/7000 [07:47<12:24,  6.40it/s]\n",
            "100% 10/10 [00:00<00:00, 34.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:55:47,488 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2240\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:47,493 >> Configuration saved in results/adapters/citation-intent/checkpoint-2240/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:47,513 >> Module weights saved in results/adapters/citation-intent/checkpoint-2240/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:47,516 >> Configuration saved in results/adapters/citation-intent/checkpoint-2240/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:48,119 >> Module weights saved in results/adapters/citation-intent/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:55:48,123 >> Configuration saved in results/adapters/citation-intent/checkpoint-2240/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:55:48,707 >> Module weights saved in results/adapters/citation-intent/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:55:48,729 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2240/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:55:48,732 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2240/special_tokens_map.json\n",
            "{'loss': 1.4753, 'learning_rate': 0.00033714285714285714, 'epoch': 32.57}\n",
            " 33% 2310/7000 [08:00<12:28,  6.26it/s][INFO|trainer.py:547] 2021-08-03 03:56:00,437 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:56:00,439 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:56:00,440 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:56:00,440 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:56:00,440 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.05it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.23it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:56:00,775 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5612356662750244, 'eval_runtime': 0.3359, 'eval_samples_per_second': 339.402, 'eval_steps_per_second': 29.772, 'epoch': 32.99}\n",
            " 33% 2310/7000 [08:00<12:28,  6.26it/s]\n",
            "100% 10/10 [00:00<00:00, 35.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:56:00,779 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2310\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:00,784 >> Configuration saved in results/adapters/citation-intent/checkpoint-2310/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:00,808 >> Module weights saved in results/adapters/citation-intent/checkpoint-2310/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:00,817 >> Configuration saved in results/adapters/citation-intent/checkpoint-2310/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:01,388 >> Module weights saved in results/adapters/citation-intent/checkpoint-2310/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:01,392 >> Configuration saved in results/adapters/citation-intent/checkpoint-2310/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:02,026 >> Module weights saved in results/adapters/citation-intent/checkpoint-2310/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:56:02,049 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2310/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:56:02,053 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2310/special_tokens_map.json\n",
            "{'loss': 1.4632, 'learning_rate': 0.0003342857142857143, 'epoch': 33.14}\n",
            "{'loss': 1.562, 'learning_rate': 0.00033142857142857144, 'epoch': 33.71}\n",
            " 34% 2380/7000 [08:13<12:00,  6.41it/s][INFO|trainer.py:547] 2021-08-03 03:56:13,830 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:56:13,832 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:56:13,832 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:56:13,832 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:56:13,833 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.87it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.34it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:56:14,167 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6132550239562988, 'eval_runtime': 0.3354, 'eval_samples_per_second': 339.928, 'eval_steps_per_second': 29.818, 'epoch': 33.99}\n",
            " 34% 2380/7000 [08:13<12:00,  6.41it/s]\n",
            "100% 10/10 [00:00<00:00, 35.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:56:14,174 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2380\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:14,179 >> Configuration saved in results/adapters/citation-intent/checkpoint-2380/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:14,201 >> Module weights saved in results/adapters/citation-intent/checkpoint-2380/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:14,205 >> Configuration saved in results/adapters/citation-intent/checkpoint-2380/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:14,773 >> Module weights saved in results/adapters/citation-intent/checkpoint-2380/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:14,777 >> Configuration saved in results/adapters/citation-intent/checkpoint-2380/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:15,369 >> Module weights saved in results/adapters/citation-intent/checkpoint-2380/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:56:15,391 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2380/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:56:15,394 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2380/special_tokens_map.json\n",
            "{'loss': 1.511, 'learning_rate': 0.00032857142857142856, 'epoch': 34.28}\n",
            "{'loss': 1.4919, 'learning_rate': 0.00032571428571428573, 'epoch': 34.85}\n",
            " 35% 2450/7000 [08:26<12:45,  5.95it/s][INFO|trainer.py:547] 2021-08-03 03:56:27,178 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:56:27,180 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:56:27,181 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:56:27,181 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:56:27,181 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.77it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.45it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:56:27,530 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6639187335968018, 'eval_runtime': 0.3497, 'eval_samples_per_second': 325.956, 'eval_steps_per_second': 28.593, 'epoch': 34.99}\n",
            " 35% 2450/7000 [08:27<12:45,  5.95it/s]\n",
            "100% 10/10 [00:00<00:00, 34.45it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:56:27,534 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2450\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:27,540 >> Configuration saved in results/adapters/citation-intent/checkpoint-2450/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:27,561 >> Module weights saved in results/adapters/citation-intent/checkpoint-2450/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:27,580 >> Configuration saved in results/adapters/citation-intent/checkpoint-2450/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:28,244 >> Module weights saved in results/adapters/citation-intent/checkpoint-2450/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:28,249 >> Configuration saved in results/adapters/citation-intent/checkpoint-2450/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:28,876 >> Module weights saved in results/adapters/citation-intent/checkpoint-2450/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:56:28,880 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2450/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:56:28,883 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2450/special_tokens_map.json\n",
            "{'loss': 1.5191, 'learning_rate': 0.00032285714285714285, 'epoch': 35.43}\n",
            "{'loss': 1.4389, 'learning_rate': 0.00032, 'epoch': 35.99}\n",
            " 36% 2520/7000 [08:40<12:36,  5.92it/s][INFO|trainer.py:547] 2021-08-03 03:56:40,564 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:56:40,567 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:56:40,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:56:40,567 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:56:40,567 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.65it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.29it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:56:40,923 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6406307220458984, 'eval_runtime': 0.3561, 'eval_samples_per_second': 320.096, 'eval_steps_per_second': 28.079, 'epoch': 35.99}\n",
            " 36% 2520/7000 [08:40<12:36,  5.92it/s]\n",
            "100% 10/10 [00:00<00:00, 34.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:56:40,938 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2520\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:40,945 >> Configuration saved in results/adapters/citation-intent/checkpoint-2520/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:40,972 >> Module weights saved in results/adapters/citation-intent/checkpoint-2520/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:40,976 >> Configuration saved in results/adapters/citation-intent/checkpoint-2520/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:41,648 >> Module weights saved in results/adapters/citation-intent/checkpoint-2520/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:41,654 >> Configuration saved in results/adapters/citation-intent/checkpoint-2520/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:42,304 >> Module weights saved in results/adapters/citation-intent/checkpoint-2520/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:56:42,327 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:56:42,331 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2520/special_tokens_map.json\n",
            "{'loss': 1.4627, 'learning_rate': 0.00031714285714285715, 'epoch': 36.57}\n",
            " 37% 2590/7000 [08:53<10:50,  6.78it/s][INFO|trainer.py:547] 2021-08-03 03:56:54,146 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:56:54,148 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:56:54,149 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:56:54,149 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:56:54,149 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.63it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.40it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:56:54,479 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5420976877212524, 'eval_runtime': 0.3311, 'eval_samples_per_second': 344.287, 'eval_steps_per_second': 30.201, 'epoch': 36.99}\n",
            " 37% 2590/7000 [08:54<10:50,  6.78it/s]\n",
            "100% 10/10 [00:00<00:00, 36.40it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:56:54,483 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2590\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:54,488 >> Configuration saved in results/adapters/citation-intent/checkpoint-2590/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:54,512 >> Module weights saved in results/adapters/citation-intent/checkpoint-2590/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:54,515 >> Configuration saved in results/adapters/citation-intent/checkpoint-2590/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:55,126 >> Module weights saved in results/adapters/citation-intent/checkpoint-2590/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:56:55,152 >> Configuration saved in results/adapters/citation-intent/checkpoint-2590/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:56:55,741 >> Module weights saved in results/adapters/citation-intent/checkpoint-2590/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:56:55,761 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2590/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:56:55,764 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2590/special_tokens_map.json\n",
            "{'loss': 1.496, 'learning_rate': 0.00031428571428571427, 'epoch': 37.14}\n",
            "{'loss': 1.4369, 'learning_rate': 0.00031142857142857144, 'epoch': 37.71}\n",
            " 38% 2660/7000 [09:07<14:52,  4.87it/s][INFO|trainer.py:547] 2021-08-03 03:57:07,371 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:57:07,373 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:57:07,373 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:57:07,373 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:57:07,373 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.23it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.27it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:57:07,702 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6192923784255981, 'eval_runtime': 0.3293, 'eval_samples_per_second': 346.189, 'eval_steps_per_second': 30.367, 'epoch': 37.99}\n",
            " 38% 2660/7000 [09:07<14:52,  4.87it/s]\n",
            "100% 10/10 [00:00<00:00, 35.27it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:57:07,706 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2660\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:07,710 >> Configuration saved in results/adapters/citation-intent/checkpoint-2660/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:07,730 >> Module weights saved in results/adapters/citation-intent/checkpoint-2660/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:07,733 >> Configuration saved in results/adapters/citation-intent/checkpoint-2660/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:08,389 >> Module weights saved in results/adapters/citation-intent/checkpoint-2660/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:08,393 >> Configuration saved in results/adapters/citation-intent/checkpoint-2660/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:09,022 >> Module weights saved in results/adapters/citation-intent/checkpoint-2660/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:57:09,041 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2660/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:57:11,767 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2660/special_tokens_map.json\n",
            "{'loss': 1.4334, 'learning_rate': 0.00030857142857142856, 'epoch': 38.28}\n",
            "{'loss': 1.4272, 'learning_rate': 0.00030571428571428573, 'epoch': 38.85}\n",
            " 39% 2730/7000 [09:23<10:54,  6.53it/s][INFO|trainer.py:547] 2021-08-03 03:57:23,271 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:57:23,274 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:57:23,274 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:57:23,274 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:57:23,274 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 36.59it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.42it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:57:23,617 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7421255111694336, 'eval_runtime': 0.3442, 'eval_samples_per_second': 331.205, 'eval_steps_per_second': 29.053, 'epoch': 38.99}\n",
            " 39% 2730/7000 [09:23<10:54,  6.53it/s]\n",
            "100% 10/10 [00:00<00:00, 34.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:57:23,622 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2730\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:23,627 >> Configuration saved in results/adapters/citation-intent/checkpoint-2730/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:23,651 >> Module weights saved in results/adapters/citation-intent/checkpoint-2730/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:23,665 >> Configuration saved in results/adapters/citation-intent/checkpoint-2730/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:24,300 >> Module weights saved in results/adapters/citation-intent/checkpoint-2730/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:24,305 >> Configuration saved in results/adapters/citation-intent/checkpoint-2730/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:24,966 >> Module weights saved in results/adapters/citation-intent/checkpoint-2730/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:57:24,990 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2730/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:57:24,993 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2730/special_tokens_map.json\n",
            "{'loss': 1.4882, 'learning_rate': 0.0003028571428571429, 'epoch': 39.43}\n",
            "{'loss': 1.4467, 'learning_rate': 0.0003, 'epoch': 39.99}\n",
            " 40% 2800/7000 [09:39<11:02,  6.34it/s][INFO|trainer.py:547] 2021-08-03 03:57:39,498 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:57:39,500 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:57:39,500 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:57:39,500 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:57:39,500 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.98it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.18it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:57:39,844 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4328805208206177, 'eval_runtime': 0.344, 'eval_samples_per_second': 331.41, 'eval_steps_per_second': 29.071, 'epoch': 39.99}\n",
            " 40% 2800/7000 [09:39<11:02,  6.34it/s]\n",
            "100% 10/10 [00:00<00:00, 34.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:57:39,848 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2800\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:39,853 >> Configuration saved in results/adapters/citation-intent/checkpoint-2800/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:39,879 >> Module weights saved in results/adapters/citation-intent/checkpoint-2800/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:39,883 >> Configuration saved in results/adapters/citation-intent/checkpoint-2800/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:40,524 >> Module weights saved in results/adapters/citation-intent/checkpoint-2800/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:40,527 >> Configuration saved in results/adapters/citation-intent/checkpoint-2800/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:41,133 >> Module weights saved in results/adapters/citation-intent/checkpoint-2800/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:57:41,153 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:57:41,156 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2800/special_tokens_map.json\n",
            "{'loss': 1.4457, 'learning_rate': 0.00029714285714285715, 'epoch': 40.57}\n",
            " 41% 2870/7000 [09:55<11:40,  5.90it/s][INFO|trainer.py:547] 2021-08-03 03:57:55,622 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:57:55,624 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:57:55,624 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:57:55,624 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:57:55,624 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.51it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.56it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:57:55,966 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6048442125320435, 'eval_runtime': 0.3421, 'eval_samples_per_second': 333.261, 'eval_steps_per_second': 29.233, 'epoch': 40.99}\n",
            " 41% 2870/7000 [09:55<11:40,  5.90it/s]\n",
            "100% 10/10 [00:00<00:00, 35.56it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:57:55,970 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2870\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:55,974 >> Configuration saved in results/adapters/citation-intent/checkpoint-2870/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:55,995 >> Module weights saved in results/adapters/citation-intent/checkpoint-2870/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:55,998 >> Configuration saved in results/adapters/citation-intent/checkpoint-2870/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:56,567 >> Module weights saved in results/adapters/citation-intent/checkpoint-2870/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:57:56,570 >> Configuration saved in results/adapters/citation-intent/checkpoint-2870/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:57:57,147 >> Module weights saved in results/adapters/citation-intent/checkpoint-2870/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:57:57,166 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:57:57,169 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2870/special_tokens_map.json\n",
            "{'loss': 1.4595, 'learning_rate': 0.00029428571428571427, 'epoch': 41.14}\n",
            "{'loss': 1.359, 'learning_rate': 0.00029142857142857144, 'epoch': 41.71}\n",
            " 42% 2940/7000 [10:11<11:03,  6.12it/s][INFO|trainer.py:547] 2021-08-03 03:58:11,545 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:58:11,548 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:58:11,548 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:58:11,548 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:58:11,548 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.61it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.84it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:58:11,893 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6306252479553223, 'eval_runtime': 0.3461, 'eval_samples_per_second': 329.388, 'eval_steps_per_second': 28.894, 'epoch': 41.99}\n",
            " 42% 2940/7000 [10:11<11:03,  6.12it/s]\n",
            "100% 10/10 [00:00<00:00, 35.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:58:11,898 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-2940\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:11,903 >> Configuration saved in results/adapters/citation-intent/checkpoint-2940/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:11,928 >> Module weights saved in results/adapters/citation-intent/checkpoint-2940/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:11,932 >> Configuration saved in results/adapters/citation-intent/checkpoint-2940/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:12,534 >> Module weights saved in results/adapters/citation-intent/checkpoint-2940/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:12,538 >> Configuration saved in results/adapters/citation-intent/checkpoint-2940/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:13,135 >> Module weights saved in results/adapters/citation-intent/checkpoint-2940/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:58:13,155 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-2940/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:58:13,158 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-2940/special_tokens_map.json\n",
            "{'loss': 1.4073, 'learning_rate': 0.0002885714285714286, 'epoch': 42.28}\n",
            "{'loss': 1.4787, 'learning_rate': 0.0002857142857142857, 'epoch': 42.85}\n",
            " 43% 3010/7000 [10:26<12:55,  5.15it/s][INFO|trainer.py:547] 2021-08-03 03:58:27,050 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:58:27,054 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:58:27,054 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:58:27,054 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:58:27,054 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.38it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.57it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:58:27,400 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.718079686164856, 'eval_runtime': 0.3462, 'eval_samples_per_second': 329.309, 'eval_steps_per_second': 28.887, 'epoch': 42.99}\n",
            " 43% 3010/7000 [10:27<12:55,  5.15it/s]\n",
            "100% 10/10 [00:00<00:00, 34.57it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:58:27,404 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3010\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:27,410 >> Configuration saved in results/adapters/citation-intent/checkpoint-3010/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:27,434 >> Module weights saved in results/adapters/citation-intent/checkpoint-3010/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:27,438 >> Configuration saved in results/adapters/citation-intent/checkpoint-3010/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:28,062 >> Module weights saved in results/adapters/citation-intent/checkpoint-3010/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:28,066 >> Configuration saved in results/adapters/citation-intent/checkpoint-3010/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:28,714 >> Module weights saved in results/adapters/citation-intent/checkpoint-3010/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:58:28,733 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3010/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:58:28,737 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3010/special_tokens_map.json\n",
            "{'loss': 1.361, 'learning_rate': 0.00028285714285714286, 'epoch': 43.43}\n",
            "{'loss': 1.4178, 'learning_rate': 0.00028000000000000003, 'epoch': 43.99}\n",
            " 44% 3080/7000 [10:43<12:40,  5.15it/s][INFO|trainer.py:547] 2021-08-03 03:58:43,378 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:58:43,380 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:58:43,380 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:58:43,380 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:58:43,380 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.77it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.05it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:58:43,719 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4806398153305054, 'eval_runtime': 0.3387, 'eval_samples_per_second': 336.604, 'eval_steps_per_second': 29.527, 'epoch': 43.99}\n",
            " 44% 3080/7000 [10:43<12:40,  5.15it/s]\n",
            "100% 10/10 [00:00<00:00, 35.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:58:43,723 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3080\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:43,729 >> Configuration saved in results/adapters/citation-intent/checkpoint-3080/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:43,759 >> Module weights saved in results/adapters/citation-intent/checkpoint-3080/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:43,763 >> Configuration saved in results/adapters/citation-intent/checkpoint-3080/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:44,377 >> Module weights saved in results/adapters/citation-intent/checkpoint-3080/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:44,381 >> Configuration saved in results/adapters/citation-intent/checkpoint-3080/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:44,990 >> Module weights saved in results/adapters/citation-intent/checkpoint-3080/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:58:45,011 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3080/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:58:45,014 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3080/special_tokens_map.json\n",
            "{'loss': 1.3438, 'learning_rate': 0.00027714285714285715, 'epoch': 44.57}\n",
            " 45% 3150/7000 [10:56<09:54,  6.48it/s][INFO|trainer.py:547] 2021-08-03 03:58:56,869 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:58:56,871 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:58:56,871 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:58:56,871 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:58:56,871 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.58it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.34it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:58:57,224 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4977627992630005, 'eval_runtime': 0.3536, 'eval_samples_per_second': 322.418, 'eval_steps_per_second': 28.282, 'epoch': 44.99}\n",
            " 45% 3150/7000 [10:57<09:54,  6.48it/s]\n",
            "100% 10/10 [00:00<00:00, 35.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:58:57,228 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3150\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:57,233 >> Configuration saved in results/adapters/citation-intent/checkpoint-3150/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:57,254 >> Module weights saved in results/adapters/citation-intent/checkpoint-3150/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:57,258 >> Configuration saved in results/adapters/citation-intent/checkpoint-3150/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:57,831 >> Module weights saved in results/adapters/citation-intent/checkpoint-3150/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:58:57,835 >> Configuration saved in results/adapters/citation-intent/checkpoint-3150/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:58:58,479 >> Module weights saved in results/adapters/citation-intent/checkpoint-3150/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:58:58,501 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3150/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:58:58,505 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3150/special_tokens_map.json\n",
            "{'loss': 1.4524, 'learning_rate': 0.0002742857142857143, 'epoch': 45.14}\n",
            "{'loss': 1.3583, 'learning_rate': 0.0002714285714285714, 'epoch': 45.71}\n",
            " 46% 3220/7000 [11:10<09:34,  6.58it/s][INFO|trainer.py:547] 2021-08-03 03:59:10,334 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:59:10,337 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:59:10,337 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:59:10,337 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:59:10,337 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.96it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.54it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:59:10,672 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5620604753494263, 'eval_runtime': 0.3357, 'eval_samples_per_second': 339.565, 'eval_steps_per_second': 29.786, 'epoch': 45.99}\n",
            " 46% 3220/7000 [11:10<09:34,  6.58it/s]\n",
            "100% 10/10 [00:00<00:00, 36.54it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:59:10,677 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3220\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:10,683 >> Configuration saved in results/adapters/citation-intent/checkpoint-3220/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:10,707 >> Module weights saved in results/adapters/citation-intent/checkpoint-3220/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:10,710 >> Configuration saved in results/adapters/citation-intent/checkpoint-3220/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:11,300 >> Module weights saved in results/adapters/citation-intent/checkpoint-3220/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:11,304 >> Configuration saved in results/adapters/citation-intent/checkpoint-3220/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:11,894 >> Module weights saved in results/adapters/citation-intent/checkpoint-3220/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:59:11,914 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3220/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:59:11,917 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3220/special_tokens_map.json\n",
            "{'loss': 1.3839, 'learning_rate': 0.00026857142857142856, 'epoch': 46.28}\n",
            "{'loss': 1.4008, 'learning_rate': 0.00026571428571428574, 'epoch': 46.85}\n",
            " 47% 3290/7000 [11:23<10:27,  5.91it/s][INFO|trainer.py:547] 2021-08-03 03:59:23,887 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:59:23,890 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:59:23,891 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:59:23,891 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:59:23,891 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.36it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.06it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:59:24,233 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4021427631378174, 'eval_runtime': 0.3433, 'eval_samples_per_second': 332.032, 'eval_steps_per_second': 29.126, 'epoch': 46.99}\n",
            " 47% 3290/7000 [11:24<10:27,  5.91it/s]\n",
            "100% 10/10 [00:00<00:00, 34.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:59:24,238 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3290\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:24,243 >> Configuration saved in results/adapters/citation-intent/checkpoint-3290/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:24,292 >> Module weights saved in results/adapters/citation-intent/checkpoint-3290/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:24,296 >> Configuration saved in results/adapters/citation-intent/checkpoint-3290/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:24,853 >> Module weights saved in results/adapters/citation-intent/checkpoint-3290/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:24,857 >> Configuration saved in results/adapters/citation-intent/checkpoint-3290/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:25,440 >> Module weights saved in results/adapters/citation-intent/checkpoint-3290/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:59:25,462 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:59:25,465 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3290/special_tokens_map.json\n",
            "{'loss': 1.3805, 'learning_rate': 0.00026285714285714286, 'epoch': 47.43}\n",
            "{'loss': 1.3322, 'learning_rate': 0.00026000000000000003, 'epoch': 47.99}\n",
            " 48% 3360/7000 [11:37<10:05,  6.01it/s][INFO|trainer.py:547] 2021-08-03 03:59:37,378 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:59:37,381 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:59:37,381 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:59:37,381 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:59:37,381 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.28it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.22it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:59:37,716 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4598184823989868, 'eval_runtime': 0.3357, 'eval_samples_per_second': 339.556, 'eval_steps_per_second': 29.786, 'epoch': 47.99}\n",
            " 48% 3360/7000 [11:37<10:05,  6.01it/s]\n",
            "100% 10/10 [00:00<00:00, 35.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:59:37,720 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3360\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:37,725 >> Configuration saved in results/adapters/citation-intent/checkpoint-3360/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:37,750 >> Module weights saved in results/adapters/citation-intent/checkpoint-3360/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:37,754 >> Configuration saved in results/adapters/citation-intent/checkpoint-3360/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:38,352 >> Module weights saved in results/adapters/citation-intent/checkpoint-3360/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:38,373 >> Configuration saved in results/adapters/citation-intent/checkpoint-3360/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:39,001 >> Module weights saved in results/adapters/citation-intent/checkpoint-3360/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:59:39,023 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3360/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:59:39,027 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3360/special_tokens_map.json\n",
            "{'loss': 1.3829, 'learning_rate': 0.0002571428571428571, 'epoch': 48.57}\n",
            " 49% 3430/7000 [11:50<08:28,  7.03it/s][INFO|trainer.py:547] 2021-08-03 03:59:50,821 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 03:59:50,823 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 03:59:50,823 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 03:59:50,824 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 03:59:50,824 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.82it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.99it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 03:59:51,156 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3890329599380493, 'eval_runtime': 0.3334, 'eval_samples_per_second': 341.953, 'eval_steps_per_second': 29.996, 'epoch': 48.99}\n",
            " 49% 3430/7000 [11:50<08:28,  7.03it/s]\n",
            "100% 10/10 [00:00<00:00, 34.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 03:59:51,161 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3430\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:51,167 >> Configuration saved in results/adapters/citation-intent/checkpoint-3430/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:51,188 >> Module weights saved in results/adapters/citation-intent/checkpoint-3430/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:51,192 >> Configuration saved in results/adapters/citation-intent/checkpoint-3430/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:51,762 >> Module weights saved in results/adapters/citation-intent/checkpoint-3430/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 03:59:51,766 >> Configuration saved in results/adapters/citation-intent/checkpoint-3430/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 03:59:52,374 >> Module weights saved in results/adapters/citation-intent/checkpoint-3430/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 03:59:52,394 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3430/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 03:59:52,397 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3430/special_tokens_map.json\n",
            "{'loss': 1.3616, 'learning_rate': 0.00025428571428571427, 'epoch': 49.14}\n",
            "{'loss': 1.3462, 'learning_rate': 0.00025142857142857145, 'epoch': 49.71}\n",
            " 50% 3500/7000 [12:03<09:02,  6.46it/s][INFO|trainer.py:547] 2021-08-03 04:00:04,146 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:00:04,149 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:00:04,149 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:00:04,149 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:00:04,149 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.61it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.32it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:00:04,492 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4444472789764404, 'eval_runtime': 0.3433, 'eval_samples_per_second': 332.025, 'eval_steps_per_second': 29.125, 'epoch': 49.99}\n",
            " 50% 3500/7000 [12:04<09:02,  6.46it/s]\n",
            "100% 10/10 [00:00<00:00, 35.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:00:04,496 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3500\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:04,502 >> Configuration saved in results/adapters/citation-intent/checkpoint-3500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:04,526 >> Module weights saved in results/adapters/citation-intent/checkpoint-3500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:04,530 >> Configuration saved in results/adapters/citation-intent/checkpoint-3500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:05,160 >> Module weights saved in results/adapters/citation-intent/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:05,163 >> Configuration saved in results/adapters/citation-intent/checkpoint-3500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:05,798 >> Module weights saved in results/adapters/citation-intent/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:00:05,821 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:00:08,653 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 1.3625, 'learning_rate': 0.00024857142857142857, 'epoch': 50.28}\n",
            "{'loss': 1.2961, 'learning_rate': 0.00024571428571428574, 'epoch': 50.85}\n",
            " 51% 3570/7000 [12:20<10:01,  5.71it/s][INFO|trainer.py:547] 2021-08-03 04:00:20,578 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:00:20,580 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:00:20,580 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:00:20,580 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:00:20,580 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.49it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.91it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:00:20,921 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7249386310577393, 'eval_runtime': 0.3417, 'eval_samples_per_second': 333.671, 'eval_steps_per_second': 29.269, 'epoch': 50.99}\n",
            " 51% 3570/7000 [12:20<10:01,  5.71it/s]\n",
            "100% 10/10 [00:00<00:00, 35.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:00:20,928 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3570\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:20,935 >> Configuration saved in results/adapters/citation-intent/checkpoint-3570/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:20,961 >> Module weights saved in results/adapters/citation-intent/checkpoint-3570/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:20,966 >> Configuration saved in results/adapters/citation-intent/checkpoint-3570/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:21,588 >> Module weights saved in results/adapters/citation-intent/checkpoint-3570/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:21,592 >> Configuration saved in results/adapters/citation-intent/checkpoint-3570/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:22,186 >> Module weights saved in results/adapters/citation-intent/checkpoint-3570/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:00:22,208 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3570/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:00:22,211 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3570/special_tokens_map.json\n",
            "{'loss': 1.2672, 'learning_rate': 0.00024285714285714286, 'epoch': 51.43}\n",
            "{'loss': 1.2987, 'learning_rate': 0.00024, 'epoch': 51.99}\n",
            " 52% 3640/7000 [12:35<08:23,  6.68it/s][INFO|trainer.py:547] 2021-08-03 04:00:36,185 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:00:36,187 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:00:36,187 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:00:36,188 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:00:36,188 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.84it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.88it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:00:36,528 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5302411317825317, 'eval_runtime': 0.3412, 'eval_samples_per_second': 334.081, 'eval_steps_per_second': 29.305, 'epoch': 51.99}\n",
            " 52% 3640/7000 [12:36<08:23,  6.68it/s]\n",
            "100% 10/10 [00:00<00:00, 35.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:00:36,533 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3640\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:36,539 >> Configuration saved in results/adapters/citation-intent/checkpoint-3640/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:36,563 >> Module weights saved in results/adapters/citation-intent/checkpoint-3640/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:36,567 >> Configuration saved in results/adapters/citation-intent/checkpoint-3640/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:37,195 >> Module weights saved in results/adapters/citation-intent/checkpoint-3640/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:37,200 >> Configuration saved in results/adapters/citation-intent/checkpoint-3640/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:37,820 >> Module weights saved in results/adapters/citation-intent/checkpoint-3640/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:00:37,840 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3640/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:00:37,843 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3640/special_tokens_map.json\n",
            "{'loss': 1.3415, 'learning_rate': 0.00023714285714285715, 'epoch': 52.57}\n",
            " 53% 3710/7000 [12:51<08:50,  6.20it/s][INFO|trainer.py:547] 2021-08-03 04:00:52,170 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:00:52,173 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:00:52,173 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:00:52,173 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:00:52,173 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.79it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.83it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:00:52,518 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3505481481552124, 'eval_runtime': 0.3453, 'eval_samples_per_second': 330.141, 'eval_steps_per_second': 28.96, 'epoch': 52.99}\n",
            " 53% 3710/7000 [12:52<08:50,  6.20it/s]\n",
            "100% 10/10 [00:00<00:00, 34.83it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:00:52,522 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3710\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:52,527 >> Configuration saved in results/adapters/citation-intent/checkpoint-3710/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:52,550 >> Module weights saved in results/adapters/citation-intent/checkpoint-3710/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:52,554 >> Configuration saved in results/adapters/citation-intent/checkpoint-3710/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:53,214 >> Module weights saved in results/adapters/citation-intent/checkpoint-3710/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:00:53,217 >> Configuration saved in results/adapters/citation-intent/checkpoint-3710/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:00:53,900 >> Module weights saved in results/adapters/citation-intent/checkpoint-3710/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:00:53,912 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3710/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:00:53,915 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3710/special_tokens_map.json\n",
            "{'loss': 1.305, 'learning_rate': 0.0002342857142857143, 'epoch': 53.14}\n",
            "{'loss': 1.3048, 'learning_rate': 0.00023142857142857142, 'epoch': 53.71}\n",
            " 54% 3780/7000 [13:06<09:27,  5.67it/s][INFO|trainer.py:547] 2021-08-03 04:01:06,413 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:01:06,415 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:01:06,415 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:01:06,415 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:01:06,415 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.95it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.74it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:01:06,759 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6390022039413452, 'eval_runtime': 0.3444, 'eval_samples_per_second': 330.999, 'eval_steps_per_second': 29.035, 'epoch': 53.99}\n",
            " 54% 3780/7000 [13:06<09:27,  5.67it/s]\n",
            "100% 10/10 [00:00<00:00, 34.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:01:06,763 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3780\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:06,770 >> Configuration saved in results/adapters/citation-intent/checkpoint-3780/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:06,793 >> Module weights saved in results/adapters/citation-intent/checkpoint-3780/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:06,796 >> Configuration saved in results/adapters/citation-intent/checkpoint-3780/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:07,414 >> Module weights saved in results/adapters/citation-intent/checkpoint-3780/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:07,419 >> Configuration saved in results/adapters/citation-intent/checkpoint-3780/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:08,051 >> Module weights saved in results/adapters/citation-intent/checkpoint-3780/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:01:08,074 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3780/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:01:08,077 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3780/special_tokens_map.json\n",
            "{'loss': 1.3143, 'learning_rate': 0.00022857142857142857, 'epoch': 54.28}\n",
            "{'loss': 1.3655, 'learning_rate': 0.00022571428571428571, 'epoch': 54.85}\n",
            " 55% 3850/7000 [13:22<08:44,  6.00it/s][INFO|trainer.py:547] 2021-08-03 04:01:22,424 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:01:22,426 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:01:22,426 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:01:22,426 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:01:22,426 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 37.62it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.87it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:01:22,772 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7310271263122559, 'eval_runtime': 0.347, 'eval_samples_per_second': 328.547, 'eval_steps_per_second': 28.82, 'epoch': 54.99}\n",
            " 55% 3850/7000 [13:22<08:44,  6.00it/s]\n",
            "100% 10/10 [00:00<00:00, 34.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:01:22,778 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3850\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:22,784 >> Configuration saved in results/adapters/citation-intent/checkpoint-3850/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:22,811 >> Module weights saved in results/adapters/citation-intent/checkpoint-3850/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:22,816 >> Configuration saved in results/adapters/citation-intent/checkpoint-3850/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:23,469 >> Module weights saved in results/adapters/citation-intent/checkpoint-3850/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:23,473 >> Configuration saved in results/adapters/citation-intent/checkpoint-3850/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:24,150 >> Module weights saved in results/adapters/citation-intent/checkpoint-3850/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:01:24,171 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3850/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:01:24,174 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3850/special_tokens_map.json\n",
            "{'loss': 1.3312, 'learning_rate': 0.00022285714285714286, 'epoch': 55.43}\n",
            "{'loss': 1.235, 'learning_rate': 0.00022, 'epoch': 55.99}\n",
            " 56% 3920/7000 [13:38<08:38,  5.94it/s][INFO|trainer.py:547] 2021-08-03 04:01:38,535 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:01:38,538 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:01:38,538 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:01:38,538 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:01:38,538 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.28it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.99it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:01:38,881 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6183639764785767, 'eval_runtime': 0.3437, 'eval_samples_per_second': 331.686, 'eval_steps_per_second': 29.095, 'epoch': 55.99}\n",
            " 56% 3920/7000 [13:38<08:38,  5.94it/s]\n",
            "100% 10/10 [00:00<00:00, 34.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:01:38,886 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3920\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:38,892 >> Configuration saved in results/adapters/citation-intent/checkpoint-3920/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:38,915 >> Module weights saved in results/adapters/citation-intent/checkpoint-3920/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:38,919 >> Configuration saved in results/adapters/citation-intent/checkpoint-3920/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:39,535 >> Module weights saved in results/adapters/citation-intent/checkpoint-3920/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:39,538 >> Configuration saved in results/adapters/citation-intent/checkpoint-3920/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:40,196 >> Module weights saved in results/adapters/citation-intent/checkpoint-3920/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:01:40,218 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3920/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:01:40,223 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3920/special_tokens_map.json\n",
            "{'loss': 1.3177, 'learning_rate': 0.00021714285714285715, 'epoch': 56.57}\n",
            " 57% 3990/7000 [13:51<08:26,  5.95it/s][INFO|trainer.py:547] 2021-08-03 04:01:51,862 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:01:51,864 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:01:51,864 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:01:51,864 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:01:51,864 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.32it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.96it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:01:52,218 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4144803285598755, 'eval_runtime': 0.3543, 'eval_samples_per_second': 321.767, 'eval_steps_per_second': 28.225, 'epoch': 56.99}\n",
            " 57% 3990/7000 [13:52<08:26,  5.95it/s]\n",
            "100% 10/10 [00:00<00:00, 34.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:01:52,222 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-3990\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:52,228 >> Configuration saved in results/adapters/citation-intent/checkpoint-3990/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:52,250 >> Module weights saved in results/adapters/citation-intent/checkpoint-3990/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:52,253 >> Configuration saved in results/adapters/citation-intent/checkpoint-3990/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:52,866 >> Module weights saved in results/adapters/citation-intent/checkpoint-3990/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:01:52,870 >> Configuration saved in results/adapters/citation-intent/checkpoint-3990/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:01:53,517 >> Module weights saved in results/adapters/citation-intent/checkpoint-3990/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:01:53,536 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-3990/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:01:53,539 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-3990/special_tokens_map.json\n",
            "{'loss': 1.3412, 'learning_rate': 0.00021428571428571427, 'epoch': 57.14}\n",
            "{'loss': 1.3199, 'learning_rate': 0.00021142857142857145, 'epoch': 57.71}\n",
            " 58% 4060/7000 [14:05<07:50,  6.25it/s][INFO|trainer.py:547] 2021-08-03 04:02:05,363 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:02:05,365 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:02:05,365 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:02:05,366 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:02:05,366 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.28it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.32it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:02:05,699 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5187723636627197, 'eval_runtime': 0.3344, 'eval_samples_per_second': 340.933, 'eval_steps_per_second': 29.906, 'epoch': 57.99}\n",
            " 58% 4060/7000 [14:05<07:50,  6.25it/s]\n",
            "100% 10/10 [00:00<00:00, 36.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:02:05,704 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4060\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:05,708 >> Configuration saved in results/adapters/citation-intent/checkpoint-4060/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:05,729 >> Module weights saved in results/adapters/citation-intent/checkpoint-4060/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:05,733 >> Configuration saved in results/adapters/citation-intent/checkpoint-4060/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:06,320 >> Module weights saved in results/adapters/citation-intent/checkpoint-4060/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:06,323 >> Configuration saved in results/adapters/citation-intent/checkpoint-4060/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:06,923 >> Module weights saved in results/adapters/citation-intent/checkpoint-4060/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:02:06,943 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4060/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:02:06,946 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4060/special_tokens_map.json\n",
            "{'loss': 1.2428, 'learning_rate': 0.00020857142857142857, 'epoch': 58.28}\n",
            "{'loss': 1.2326, 'learning_rate': 0.00020571428571428572, 'epoch': 58.85}\n",
            " 59% 4130/7000 [14:18<08:05,  5.91it/s][INFO|trainer.py:547] 2021-08-03 04:02:18,527 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:02:18,529 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:02:18,530 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:02:18,530 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:02:18,530 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.00it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.61it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:02:18,874 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.596127986907959, 'eval_runtime': 0.3448, 'eval_samples_per_second': 330.663, 'eval_steps_per_second': 29.006, 'epoch': 58.99}\n",
            " 59% 4130/7000 [14:18<08:05,  5.91it/s]\n",
            "100% 10/10 [00:00<00:00, 34.61it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:02:18,878 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4130\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:18,905 >> Configuration saved in results/adapters/citation-intent/checkpoint-4130/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:18,928 >> Module weights saved in results/adapters/citation-intent/checkpoint-4130/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:18,931 >> Configuration saved in results/adapters/citation-intent/checkpoint-4130/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:19,515 >> Module weights saved in results/adapters/citation-intent/checkpoint-4130/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:19,519 >> Configuration saved in results/adapters/citation-intent/checkpoint-4130/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:20,151 >> Module weights saved in results/adapters/citation-intent/checkpoint-4130/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:02:20,173 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4130/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:02:20,177 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4130/special_tokens_map.json\n",
            "{'loss': 1.2819, 'learning_rate': 0.00020285714285714286, 'epoch': 59.43}\n",
            "{'loss': 1.3306, 'learning_rate': 0.0002, 'epoch': 59.99}\n",
            " 60% 4200/7000 [14:31<07:52,  5.93it/s][INFO|trainer.py:547] 2021-08-03 04:02:32,058 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:02:32,061 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:02:32,061 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:02:32,061 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:02:32,061 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.27it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.30it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:02:32,404 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4757766723632812, 'eval_runtime': 0.3434, 'eval_samples_per_second': 332.004, 'eval_steps_per_second': 29.123, 'epoch': 59.99}\n",
            " 60% 4200/7000 [14:32<07:52,  5.93it/s]\n",
            "100% 10/10 [00:00<00:00, 35.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:02:32,408 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4200\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:32,414 >> Configuration saved in results/adapters/citation-intent/checkpoint-4200/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:32,435 >> Module weights saved in results/adapters/citation-intent/checkpoint-4200/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:32,439 >> Configuration saved in results/adapters/citation-intent/checkpoint-4200/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:33,053 >> Module weights saved in results/adapters/citation-intent/checkpoint-4200/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:33,057 >> Configuration saved in results/adapters/citation-intent/checkpoint-4200/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:33,712 >> Module weights saved in results/adapters/citation-intent/checkpoint-4200/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:02:33,735 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:02:33,740 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4200/special_tokens_map.json\n",
            "{'loss': 1.2627, 'learning_rate': 0.00019714285714285716, 'epoch': 60.57}\n",
            " 61% 4270/7000 [14:45<07:19,  6.22it/s][INFO|trainer.py:547] 2021-08-03 04:02:45,671 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:02:45,673 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:02:45,674 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:02:45,674 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:02:45,674 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.92it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.06it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:02:46,004 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6082707643508911, 'eval_runtime': 0.3313, 'eval_samples_per_second': 344.089, 'eval_steps_per_second': 30.183, 'epoch': 60.99}\n",
            " 61% 4270/7000 [14:45<07:19,  6.22it/s]\n",
            "100% 10/10 [00:00<00:00, 35.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:02:46,009 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4270\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:46,015 >> Configuration saved in results/adapters/citation-intent/checkpoint-4270/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:46,037 >> Module weights saved in results/adapters/citation-intent/checkpoint-4270/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:46,040 >> Configuration saved in results/adapters/citation-intent/checkpoint-4270/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:46,622 >> Module weights saved in results/adapters/citation-intent/checkpoint-4270/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:46,650 >> Configuration saved in results/adapters/citation-intent/checkpoint-4270/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:47,252 >> Module weights saved in results/adapters/citation-intent/checkpoint-4270/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:02:47,272 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4270/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:02:47,275 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4270/special_tokens_map.json\n",
            "{'loss': 1.3083, 'learning_rate': 0.00019428571428571428, 'epoch': 61.14}\n",
            "{'loss': 1.2827, 'learning_rate': 0.00019142857142857142, 'epoch': 61.71}\n",
            " 62% 4340/7000 [14:58<07:15,  6.11it/s][INFO|trainer.py:547] 2021-08-03 04:02:58,985 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:02:58,987 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:02:58,987 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:02:58,987 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:02:58,987 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.77it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.65it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:02:59,326 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2768229246139526, 'eval_runtime': 0.3388, 'eval_samples_per_second': 336.451, 'eval_steps_per_second': 29.513, 'epoch': 61.99}\n",
            " 62% 4340/7000 [14:59<07:15,  6.11it/s]\n",
            "100% 10/10 [00:00<00:00, 35.65it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:02:59,331 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4340\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:59,337 >> Configuration saved in results/adapters/citation-intent/checkpoint-4340/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:59,361 >> Module weights saved in results/adapters/citation-intent/checkpoint-4340/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:59,364 >> Configuration saved in results/adapters/citation-intent/checkpoint-4340/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:02:59,967 >> Module weights saved in results/adapters/citation-intent/checkpoint-4340/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:02:59,971 >> Configuration saved in results/adapters/citation-intent/checkpoint-4340/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:00,617 >> Module weights saved in results/adapters/citation-intent/checkpoint-4340/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:03:03,405 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4340/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:03:03,408 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4340/special_tokens_map.json\n",
            "{'loss': 1.245, 'learning_rate': 0.0001885714285714286, 'epoch': 62.28}\n",
            "{'loss': 1.241, 'learning_rate': 0.00018571428571428572, 'epoch': 62.85}\n",
            " 63% 4410/7000 [15:15<06:33,  6.59it/s][INFO|trainer.py:547] 2021-08-03 04:03:15,241 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:03:15,243 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:03:15,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:03:15,244 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:03:15,244 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.58it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.34it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:03:15,585 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3177798986434937, 'eval_runtime': 0.3416, 'eval_samples_per_second': 333.745, 'eval_steps_per_second': 29.276, 'epoch': 62.99}\n",
            " 63% 4410/7000 [15:15<06:33,  6.59it/s]\n",
            "100% 10/10 [00:00<00:00, 35.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:03:15,589 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4410\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:15,595 >> Configuration saved in results/adapters/citation-intent/checkpoint-4410/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:15,618 >> Module weights saved in results/adapters/citation-intent/checkpoint-4410/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:15,621 >> Configuration saved in results/adapters/citation-intent/checkpoint-4410/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:16,230 >> Module weights saved in results/adapters/citation-intent/checkpoint-4410/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:16,233 >> Configuration saved in results/adapters/citation-intent/checkpoint-4410/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:16,860 >> Module weights saved in results/adapters/citation-intent/checkpoint-4410/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:03:19,680 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4410/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:03:19,693 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4410/special_tokens_map.json\n",
            "{'loss': 1.2453, 'learning_rate': 0.00018285714285714286, 'epoch': 63.43}\n",
            "{'loss': 1.2578, 'learning_rate': 0.00017999999999999998, 'epoch': 63.99}\n",
            " 64% 4480/7000 [15:31<07:27,  5.63it/s][INFO|trainer.py:547] 2021-08-03 04:03:31,563 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:03:31,566 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:03:31,566 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:03:31,566 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:03:31,566 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.54it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.95it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:03:31,915 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.443189024925232, 'eval_runtime': 0.3491, 'eval_samples_per_second': 326.51, 'eval_steps_per_second': 28.641, 'epoch': 63.99}\n",
            " 64% 4480/7000 [15:31<07:27,  5.63it/s]\n",
            "100% 10/10 [00:00<00:00, 35.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:03:31,920 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4480\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:31,926 >> Configuration saved in results/adapters/citation-intent/checkpoint-4480/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:31,952 >> Module weights saved in results/adapters/citation-intent/checkpoint-4480/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:31,958 >> Configuration saved in results/adapters/citation-intent/checkpoint-4480/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:32,599 >> Module weights saved in results/adapters/citation-intent/checkpoint-4480/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:32,603 >> Configuration saved in results/adapters/citation-intent/checkpoint-4480/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:33,261 >> Module weights saved in results/adapters/citation-intent/checkpoint-4480/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:03:33,284 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4480/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:03:35,652 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4480/special_tokens_map.json\n",
            "{'loss': 1.2792, 'learning_rate': 0.00017714285714285713, 'epoch': 64.57}\n",
            " 65% 4550/7000 [15:47<07:26,  5.48it/s][INFO|trainer.py:547] 2021-08-03 04:03:47,525 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:03:47,528 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:03:47,528 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:03:47,528 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:03:47,528 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.91it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.64it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:03:47,877 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5492721796035767, 'eval_runtime': 0.35, 'eval_samples_per_second': 325.74, 'eval_steps_per_second': 28.574, 'epoch': 64.99}\n",
            " 65% 4550/7000 [15:47<07:26,  5.48it/s]\n",
            "100% 10/10 [00:00<00:00, 35.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:03:47,882 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4550\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:47,887 >> Configuration saved in results/adapters/citation-intent/checkpoint-4550/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:47,913 >> Module weights saved in results/adapters/citation-intent/checkpoint-4550/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:47,918 >> Configuration saved in results/adapters/citation-intent/checkpoint-4550/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:48,548 >> Module weights saved in results/adapters/citation-intent/checkpoint-4550/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:03:48,552 >> Configuration saved in results/adapters/citation-intent/checkpoint-4550/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:03:49,165 >> Module weights saved in results/adapters/citation-intent/checkpoint-4550/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:03:49,185 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4550/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:03:49,188 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4550/special_tokens_map.json\n",
            "{'loss': 1.1971, 'learning_rate': 0.0001742857142857143, 'epoch': 65.14}\n",
            "{'loss': 1.2406, 'learning_rate': 0.00017142857142857143, 'epoch': 65.71}\n",
            " 66% 4620/7000 [16:03<07:08,  5.56it/s][INFO|trainer.py:547] 2021-08-03 04:04:03,646 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:04:03,648 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:04:03,648 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:04:03,648 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:04:03,648 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.86it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.72it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:04:03,997 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5559687614440918, 'eval_runtime': 0.3496, 'eval_samples_per_second': 326.09, 'eval_steps_per_second': 28.604, 'epoch': 65.99}\n",
            " 66% 4620/7000 [16:03<07:08,  5.56it/s]\n",
            "100% 10/10 [00:00<00:00, 35.72it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:04:04,002 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4620\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:04,008 >> Configuration saved in results/adapters/citation-intent/checkpoint-4620/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:04,034 >> Module weights saved in results/adapters/citation-intent/checkpoint-4620/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:04,039 >> Configuration saved in results/adapters/citation-intent/checkpoint-4620/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:04,652 >> Module weights saved in results/adapters/citation-intent/checkpoint-4620/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:04,656 >> Configuration saved in results/adapters/citation-intent/checkpoint-4620/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:05,260 >> Module weights saved in results/adapters/citation-intent/checkpoint-4620/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:04:05,279 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4620/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:04:05,283 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4620/special_tokens_map.json\n",
            "{'loss': 1.171, 'learning_rate': 0.00016857142857142857, 'epoch': 66.28}\n",
            "{'loss': 1.2769, 'learning_rate': 0.00016571428571428572, 'epoch': 66.85}\n",
            " 67% 4690/7000 [16:19<06:07,  6.29it/s][INFO|trainer.py:547] 2021-08-03 04:04:19,733 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:04:19,735 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:04:19,736 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:04:19,736 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:04:19,736 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.47it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.75it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:04:20,085 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3601834774017334, 'eval_runtime': 0.3499, 'eval_samples_per_second': 325.762, 'eval_steps_per_second': 28.576, 'epoch': 66.99}\n",
            " 67% 4690/7000 [16:19<06:07,  6.29it/s]\n",
            "100% 10/10 [00:00<00:00, 35.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:04:20,089 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4690\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:20,095 >> Configuration saved in results/adapters/citation-intent/checkpoint-4690/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:20,118 >> Module weights saved in results/adapters/citation-intent/checkpoint-4690/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:20,122 >> Configuration saved in results/adapters/citation-intent/checkpoint-4690/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:20,760 >> Module weights saved in results/adapters/citation-intent/checkpoint-4690/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:20,764 >> Configuration saved in results/adapters/citation-intent/checkpoint-4690/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:21,372 >> Module weights saved in results/adapters/citation-intent/checkpoint-4690/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:04:21,391 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4690/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:04:21,394 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4690/special_tokens_map.json\n",
            "{'loss': 1.2558, 'learning_rate': 0.00016285714285714287, 'epoch': 67.43}\n",
            "{'loss': 1.2122, 'learning_rate': 0.00016, 'epoch': 67.99}\n",
            " 68% 4760/7000 [16:35<07:02,  5.31it/s][INFO|trainer.py:547] 2021-08-03 04:04:35,631 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:04:35,634 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:04:35,634 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:04:35,635 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:04:35,635 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.55it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 32.78it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:04:35,992 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.372376561164856, 'eval_runtime': 0.3585, 'eval_samples_per_second': 317.95, 'eval_steps_per_second': 27.89, 'epoch': 67.99}\n",
            " 68% 4760/7000 [16:35<07:02,  5.31it/s]\n",
            "100% 10/10 [00:00<00:00, 32.78it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:04:35,997 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4760\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:36,002 >> Configuration saved in results/adapters/citation-intent/checkpoint-4760/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:36,026 >> Module weights saved in results/adapters/citation-intent/checkpoint-4760/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:36,030 >> Configuration saved in results/adapters/citation-intent/checkpoint-4760/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:36,685 >> Module weights saved in results/adapters/citation-intent/checkpoint-4760/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:36,689 >> Configuration saved in results/adapters/citation-intent/checkpoint-4760/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:37,320 >> Module weights saved in results/adapters/citation-intent/checkpoint-4760/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:04:37,343 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4760/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:04:37,346 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4760/special_tokens_map.json\n",
            "{'loss': 1.2272, 'learning_rate': 0.00015714285714285713, 'epoch': 68.57}\n",
            " 69% 4830/7000 [16:51<07:07,  5.08it/s][INFO|trainer.py:547] 2021-08-03 04:04:51,623 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:04:51,625 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:04:51,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:04:51,625 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:04:51,625 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 36.88it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 34.29it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 31.67it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:04:51,985 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4025565385818481, 'eval_runtime': 0.3607, 'eval_samples_per_second': 316.059, 'eval_steps_per_second': 27.724, 'epoch': 68.99}\n",
            " 69% 4830/7000 [16:51<07:07,  5.08it/s]\n",
            "100% 10/10 [00:00<00:00, 31.67it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:04:51,990 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4830\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:51,994 >> Configuration saved in results/adapters/citation-intent/checkpoint-4830/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:52,020 >> Module weights saved in results/adapters/citation-intent/checkpoint-4830/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:52,025 >> Configuration saved in results/adapters/citation-intent/checkpoint-4830/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:52,644 >> Module weights saved in results/adapters/citation-intent/checkpoint-4830/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:04:52,648 >> Configuration saved in results/adapters/citation-intent/checkpoint-4830/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:04:53,261 >> Module weights saved in results/adapters/citation-intent/checkpoint-4830/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:04:53,282 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4830/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:04:53,285 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4830/special_tokens_map.json\n",
            "{'loss': 1.2871, 'learning_rate': 0.00015428571428571428, 'epoch': 69.14}\n",
            "{'loss': 1.2396, 'learning_rate': 0.00015142857142857145, 'epoch': 69.71}\n",
            " 70% 4900/7000 [17:07<07:06,  4.93it/s][INFO|trainer.py:547] 2021-08-03 04:05:07,495 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:05:07,497 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:05:07,497 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:05:07,497 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:05:07,497 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.58it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.97it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:05:07,839 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5342177152633667, 'eval_runtime': 0.3421, 'eval_samples_per_second': 333.219, 'eval_steps_per_second': 29.23, 'epoch': 69.99}\n",
            " 70% 4900/7000 [17:07<07:06,  4.93it/s]\n",
            "100% 10/10 [00:00<00:00, 34.97it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:05:07,843 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4900\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:07,848 >> Configuration saved in results/adapters/citation-intent/checkpoint-4900/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:07,873 >> Module weights saved in results/adapters/citation-intent/checkpoint-4900/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:07,877 >> Configuration saved in results/adapters/citation-intent/checkpoint-4900/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:08,549 >> Module weights saved in results/adapters/citation-intent/checkpoint-4900/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:08,554 >> Configuration saved in results/adapters/citation-intent/checkpoint-4900/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:09,215 >> Module weights saved in results/adapters/citation-intent/checkpoint-4900/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:05:09,241 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:05:09,246 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4900/special_tokens_map.json\n",
            "{'loss': 1.1963, 'learning_rate': 0.00014857142857142857, 'epoch': 70.28}\n",
            "{'loss': 1.255, 'learning_rate': 0.00014571428571428572, 'epoch': 70.85}\n",
            " 71% 4970/7000 [17:20<05:26,  6.22it/s][INFO|trainer.py:547] 2021-08-03 04:05:21,109 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:05:21,111 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:05:21,111 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:05:21,112 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:05:21,112 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.90it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.26it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:05:21,447 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3706068992614746, 'eval_runtime': 0.3366, 'eval_samples_per_second': 338.672, 'eval_steps_per_second': 29.708, 'epoch': 70.99}\n",
            " 71% 4970/7000 [17:21<05:26,  6.22it/s]\n",
            "100% 10/10 [00:00<00:00, 34.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:05:21,452 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-4970\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:21,457 >> Configuration saved in results/adapters/citation-intent/checkpoint-4970/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:21,482 >> Module weights saved in results/adapters/citation-intent/checkpoint-4970/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:21,486 >> Configuration saved in results/adapters/citation-intent/checkpoint-4970/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:22,126 >> Module weights saved in results/adapters/citation-intent/checkpoint-4970/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:22,131 >> Configuration saved in results/adapters/citation-intent/checkpoint-4970/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:22,769 >> Module weights saved in results/adapters/citation-intent/checkpoint-4970/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:05:22,792 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-4970/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:05:22,796 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-4970/special_tokens_map.json\n",
            "{'loss': 1.2335, 'learning_rate': 0.00014285714285714284, 'epoch': 71.43}\n",
            "{'loss': 1.2369, 'learning_rate': 0.00014000000000000001, 'epoch': 71.99}\n",
            " 72% 5040/7000 [17:34<04:59,  6.55it/s][INFO|trainer.py:547] 2021-08-03 04:05:34,663 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:05:34,665 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:05:34,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:05:34,666 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:05:34,666 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.02it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.94it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:05:35,000 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2805806398391724, 'eval_runtime': 0.3347, 'eval_samples_per_second': 340.584, 'eval_steps_per_second': 29.876, 'epoch': 71.99}\n",
            " 72% 5040/7000 [17:34<04:59,  6.55it/s]\n",
            "100% 10/10 [00:00<00:00, 35.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:05:35,004 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5040\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:35,033 >> Configuration saved in results/adapters/citation-intent/checkpoint-5040/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:35,062 >> Module weights saved in results/adapters/citation-intent/checkpoint-5040/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:35,068 >> Configuration saved in results/adapters/citation-intent/checkpoint-5040/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:35,648 >> Module weights saved in results/adapters/citation-intent/checkpoint-5040/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:35,651 >> Configuration saved in results/adapters/citation-intent/checkpoint-5040/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:36,255 >> Module weights saved in results/adapters/citation-intent/checkpoint-5040/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:05:36,275 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:05:36,278 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5040/special_tokens_map.json\n",
            "{'loss': 1.2436, 'learning_rate': 0.00013714285714285716, 'epoch': 72.57}\n",
            " 73% 5110/7000 [17:47<05:18,  5.94it/s][INFO|trainer.py:547] 2021-08-03 04:05:48,107 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:05:48,109 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:05:48,109 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:05:48,109 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:05:48,110 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.61it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 35.93it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 32.61it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:05:48,475 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4774696826934814, 'eval_runtime': 0.3666, 'eval_samples_per_second': 310.966, 'eval_steps_per_second': 27.278, 'epoch': 72.99}\n",
            " 73% 5110/7000 [17:48<05:18,  5.94it/s]\n",
            "100% 10/10 [00:00<00:00, 32.61it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:05:48,480 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5110\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:48,499 >> Configuration saved in results/adapters/citation-intent/checkpoint-5110/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:48,525 >> Module weights saved in results/adapters/citation-intent/checkpoint-5110/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:48,530 >> Configuration saved in results/adapters/citation-intent/checkpoint-5110/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:49,144 >> Module weights saved in results/adapters/citation-intent/checkpoint-5110/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:05:49,148 >> Configuration saved in results/adapters/citation-intent/checkpoint-5110/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:05:49,748 >> Module weights saved in results/adapters/citation-intent/checkpoint-5110/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:05:49,768 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5110/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:05:49,772 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5110/special_tokens_map.json\n",
            "{'loss': 1.2383, 'learning_rate': 0.00013428571428571428, 'epoch': 73.14}\n",
            "{'loss': 1.2023, 'learning_rate': 0.00013142857142857143, 'epoch': 73.71}\n",
            " 74% 5180/7000 [18:01<04:43,  6.42it/s][INFO|trainer.py:547] 2021-08-03 04:06:01,557 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:06:01,560 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:06:01,560 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:06:01,560 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:06:01,560 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.14it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.94it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:06:01,902 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4816563129425049, 'eval_runtime': 0.3423, 'eval_samples_per_second': 333.025, 'eval_steps_per_second': 29.213, 'epoch': 73.99}\n",
            " 74% 5180/7000 [18:01<04:43,  6.42it/s]\n",
            "100% 10/10 [00:00<00:00, 34.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:06:01,906 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5180\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:01,911 >> Configuration saved in results/adapters/citation-intent/checkpoint-5180/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:01,936 >> Module weights saved in results/adapters/citation-intent/checkpoint-5180/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:01,959 >> Configuration saved in results/adapters/citation-intent/checkpoint-5180/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:02,678 >> Module weights saved in results/adapters/citation-intent/checkpoint-5180/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:02,682 >> Configuration saved in results/adapters/citation-intent/checkpoint-5180/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:03,326 >> Module weights saved in results/adapters/citation-intent/checkpoint-5180/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:06:03,350 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5180/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:06:03,353 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5180/special_tokens_map.json\n",
            "{'loss': 1.2046, 'learning_rate': 0.00012857142857142855, 'epoch': 74.28}\n",
            "{'loss': 1.1721, 'learning_rate': 0.00012571428571428572, 'epoch': 74.85}\n",
            " 75% 5250/7000 [18:14<05:26,  5.36it/s][INFO|trainer.py:547] 2021-08-03 04:06:14,884 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:06:14,887 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:06:14,887 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:06:14,887 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:06:14,887 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.19it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.24it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:06:15,220 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5608084201812744, 'eval_runtime': 0.3335, 'eval_samples_per_second': 341.845, 'eval_steps_per_second': 29.986, 'epoch': 74.99}\n",
            " 75% 5250/7000 [18:15<05:26,  5.36it/s]\n",
            "100% 10/10 [00:00<00:00, 35.24it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:06:15,224 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5250\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:15,230 >> Configuration saved in results/adapters/citation-intent/checkpoint-5250/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:15,252 >> Module weights saved in results/adapters/citation-intent/checkpoint-5250/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:15,256 >> Configuration saved in results/adapters/citation-intent/checkpoint-5250/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:15,874 >> Module weights saved in results/adapters/citation-intent/checkpoint-5250/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:15,878 >> Configuration saved in results/adapters/citation-intent/checkpoint-5250/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:16,901 >> Module weights saved in results/adapters/citation-intent/checkpoint-5250/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:06:16,922 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:06:16,926 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5250/special_tokens_map.json\n",
            "{'loss': 1.277, 'learning_rate': 0.00012285714285714287, 'epoch': 75.43}\n",
            "{'loss': 1.1633, 'learning_rate': 0.00012, 'epoch': 75.99}\n",
            " 76% 5320/7000 [18:28<04:49,  5.80it/s][INFO|trainer.py:547] 2021-08-03 04:06:28,870 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:06:28,872 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:06:28,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:06:28,872 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:06:28,872 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.17it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.25it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:06:29,227 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3861366510391235, 'eval_runtime': 0.355, 'eval_samples_per_second': 321.145, 'eval_steps_per_second': 28.171, 'epoch': 75.99}\n",
            " 76% 5320/7000 [18:29<04:49,  5.80it/s]\n",
            "100% 10/10 [00:00<00:00, 35.25it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:06:29,231 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5320\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:29,236 >> Configuration saved in results/adapters/citation-intent/checkpoint-5320/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:29,262 >> Module weights saved in results/adapters/citation-intent/checkpoint-5320/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:29,267 >> Configuration saved in results/adapters/citation-intent/checkpoint-5320/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:29,875 >> Module weights saved in results/adapters/citation-intent/checkpoint-5320/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:29,891 >> Configuration saved in results/adapters/citation-intent/checkpoint-5320/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:30,504 >> Module weights saved in results/adapters/citation-intent/checkpoint-5320/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:06:30,524 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5320/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:06:30,527 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5320/special_tokens_map.json\n",
            "{'loss': 1.2475, 'learning_rate': 0.00011714285714285715, 'epoch': 76.57}\n",
            " 77% 5390/7000 [18:41<04:17,  6.24it/s][INFO|trainer.py:547] 2021-08-03 04:06:42,181 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:06:42,183 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:06:42,183 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:06:42,183 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:06:42,184 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 35.81it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00, 33.16it/s]\u001b[A\n",
            "100% 10/10 [00:00<00:00, 30.10it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:06:42,571 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5152390003204346, 'eval_runtime': 0.3886, 'eval_samples_per_second': 293.364, 'eval_steps_per_second': 25.734, 'epoch': 76.99}\n",
            " 77% 5390/7000 [18:42<04:17,  6.24it/s]\n",
            "100% 10/10 [00:00<00:00, 30.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:06:42,576 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5390\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:42,584 >> Configuration saved in results/adapters/citation-intent/checkpoint-5390/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:42,613 >> Module weights saved in results/adapters/citation-intent/checkpoint-5390/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:42,618 >> Configuration saved in results/adapters/citation-intent/checkpoint-5390/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:43,234 >> Module weights saved in results/adapters/citation-intent/checkpoint-5390/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:43,238 >> Configuration saved in results/adapters/citation-intent/checkpoint-5390/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:43,891 >> Module weights saved in results/adapters/citation-intent/checkpoint-5390/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:06:43,914 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5390/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:06:43,918 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5390/special_tokens_map.json\n",
            "{'loss': 1.2192, 'learning_rate': 0.00011428571428571428, 'epoch': 77.14}\n",
            "{'loss': 1.1765, 'learning_rate': 0.00011142857142857143, 'epoch': 77.71}\n",
            " 78% 5460/7000 [18:55<03:52,  6.62it/s][INFO|trainer.py:547] 2021-08-03 04:06:55,589 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:06:55,591 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:06:55,592 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:06:55,592 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:06:55,592 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.64it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.55it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:06:55,929 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6468133926391602, 'eval_runtime': 0.3376, 'eval_samples_per_second': 337.637, 'eval_steps_per_second': 29.617, 'epoch': 77.99}\n",
            " 78% 5460/7000 [18:55<03:52,  6.62it/s]\n",
            "100% 10/10 [00:00<00:00, 35.55it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:06:55,933 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5460\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:55,938 >> Configuration saved in results/adapters/citation-intent/checkpoint-5460/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:55,961 >> Module weights saved in results/adapters/citation-intent/checkpoint-5460/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:55,964 >> Configuration saved in results/adapters/citation-intent/checkpoint-5460/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:56,579 >> Module weights saved in results/adapters/citation-intent/checkpoint-5460/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:06:56,582 >> Configuration saved in results/adapters/citation-intent/checkpoint-5460/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:06:57,203 >> Module weights saved in results/adapters/citation-intent/checkpoint-5460/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:06:57,548 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5460/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:06:57,552 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5460/special_tokens_map.json\n",
            "{'loss': 1.1972, 'learning_rate': 0.00010857142857142858, 'epoch': 78.28}\n",
            "{'loss': 1.2246, 'learning_rate': 0.00010571428571428572, 'epoch': 78.85}\n",
            " 79% 5530/7000 [19:10<04:36,  5.32it/s][INFO|trainer.py:547] 2021-08-03 04:07:10,870 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:07:10,872 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:07:10,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:07:10,872 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:07:10,872 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.63it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.57it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:07:11,206 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5360444784164429, 'eval_runtime': 0.3344, 'eval_samples_per_second': 340.882, 'eval_steps_per_second': 29.902, 'epoch': 78.99}\n",
            " 79% 5530/7000 [19:11<04:36,  5.32it/s]\n",
            "100% 10/10 [00:00<00:00, 34.57it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:07:11,210 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5530\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:11,215 >> Configuration saved in results/adapters/citation-intent/checkpoint-5530/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:11,238 >> Module weights saved in results/adapters/citation-intent/checkpoint-5530/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:11,241 >> Configuration saved in results/adapters/citation-intent/checkpoint-5530/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:11,811 >> Module weights saved in results/adapters/citation-intent/checkpoint-5530/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:11,815 >> Configuration saved in results/adapters/citation-intent/checkpoint-5530/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:12,406 >> Module weights saved in results/adapters/citation-intent/checkpoint-5530/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:07:12,429 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5530/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:07:12,432 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5530/special_tokens_map.json\n",
            "{'loss': 1.2029, 'learning_rate': 0.00010285714285714286, 'epoch': 79.43}\n",
            "{'loss': 1.1776, 'learning_rate': 0.0001, 'epoch': 79.99}\n",
            " 80% 5600/7000 [19:26<03:50,  6.07it/s][INFO|trainer.py:547] 2021-08-03 04:07:26,987 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:07:26,990 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:07:26,990 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:07:26,990 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:07:26,990 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.06it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.05it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:07:27,324 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2875080108642578, 'eval_runtime': 0.3348, 'eval_samples_per_second': 340.527, 'eval_steps_per_second': 29.871, 'epoch': 79.99}\n",
            " 80% 5600/7000 [19:27<03:50,  6.07it/s]\n",
            "100% 10/10 [00:00<00:00, 35.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:07:27,328 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5600\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:27,334 >> Configuration saved in results/adapters/citation-intent/checkpoint-5600/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:27,357 >> Module weights saved in results/adapters/citation-intent/checkpoint-5600/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:27,361 >> Configuration saved in results/adapters/citation-intent/checkpoint-5600/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:27,941 >> Module weights saved in results/adapters/citation-intent/checkpoint-5600/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:27,944 >> Configuration saved in results/adapters/citation-intent/checkpoint-5600/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:28,598 >> Module weights saved in results/adapters/citation-intent/checkpoint-5600/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:07:28,612 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:07:28,615 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5600/special_tokens_map.json\n",
            "{'loss': 1.1742, 'learning_rate': 9.714285714285714e-05, 'epoch': 80.57}\n",
            " 81% 5670/7000 [19:41<03:25,  6.48it/s][INFO|trainer.py:547] 2021-08-03 04:07:41,915 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:07:41,917 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:07:41,918 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:07:41,918 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:07:41,918 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.52it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 32.99it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:07:42,266 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4922504425048828, 'eval_runtime': 0.3492, 'eval_samples_per_second': 326.469, 'eval_steps_per_second': 28.638, 'epoch': 80.99}\n",
            " 81% 5670/7000 [19:42<03:25,  6.48it/s]\n",
            "100% 10/10 [00:00<00:00, 32.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:07:42,271 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5670\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:42,278 >> Configuration saved in results/adapters/citation-intent/checkpoint-5670/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:42,303 >> Module weights saved in results/adapters/citation-intent/checkpoint-5670/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:42,307 >> Configuration saved in results/adapters/citation-intent/checkpoint-5670/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:42,940 >> Module weights saved in results/adapters/citation-intent/checkpoint-5670/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:42,944 >> Configuration saved in results/adapters/citation-intent/checkpoint-5670/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:43,607 >> Module weights saved in results/adapters/citation-intent/checkpoint-5670/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:07:43,631 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5670/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:07:43,635 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5670/special_tokens_map.json\n",
            "{'loss': 1.2217, 'learning_rate': 9.42857142857143e-05, 'epoch': 81.14}\n",
            "{'loss': 1.1534, 'learning_rate': 9.142857142857143e-05, 'epoch': 81.71}\n",
            " 82% 5740/7000 [19:57<03:26,  6.11it/s][INFO|trainer.py:547] 2021-08-03 04:07:57,959 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:07:57,962 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:07:57,962 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:07:57,962 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:07:57,962 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.21it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.06it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:07:58,305 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2979543209075928, 'eval_runtime': 0.3431, 'eval_samples_per_second': 332.26, 'eval_steps_per_second': 29.146, 'epoch': 81.99}\n",
            " 82% 5740/7000 [19:58<03:26,  6.11it/s]\n",
            "100% 10/10 [00:00<00:00, 34.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:07:58,310 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5740\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:58,317 >> Configuration saved in results/adapters/citation-intent/checkpoint-5740/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:58,341 >> Module weights saved in results/adapters/citation-intent/checkpoint-5740/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:58,345 >> Configuration saved in results/adapters/citation-intent/checkpoint-5740/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:58,958 >> Module weights saved in results/adapters/citation-intent/checkpoint-5740/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:07:58,962 >> Configuration saved in results/adapters/citation-intent/checkpoint-5740/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:07:59,571 >> Module weights saved in results/adapters/citation-intent/checkpoint-5740/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:07:59,596 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5740/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:07:59,599 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5740/special_tokens_map.json\n",
            "{'loss': 1.2119, 'learning_rate': 8.857142857142857e-05, 'epoch': 82.28}\n",
            "{'loss': 1.1518, 'learning_rate': 8.571428571428571e-05, 'epoch': 82.85}\n",
            " 83% 5810/7000 [20:13<03:21,  5.89it/s][INFO|trainer.py:547] 2021-08-03 04:08:13,959 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:08:13,962 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:08:13,962 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:08:13,962 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:08:13,962 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.56it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.80it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:08:14,296 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.416928768157959, 'eval_runtime': 0.3341, 'eval_samples_per_second': 341.17, 'eval_steps_per_second': 29.927, 'epoch': 82.99}\n",
            " 83% 5810/7000 [20:14<03:21,  5.89it/s]\n",
            "100% 10/10 [00:00<00:00, 34.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:08:14,300 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5810\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:14,304 >> Configuration saved in results/adapters/citation-intent/checkpoint-5810/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:14,326 >> Module weights saved in results/adapters/citation-intent/checkpoint-5810/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:14,330 >> Configuration saved in results/adapters/citation-intent/checkpoint-5810/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:14,929 >> Module weights saved in results/adapters/citation-intent/checkpoint-5810/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:14,933 >> Configuration saved in results/adapters/citation-intent/checkpoint-5810/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:15,549 >> Module weights saved in results/adapters/citation-intent/checkpoint-5810/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:08:15,580 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5810/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:08:15,590 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5810/special_tokens_map.json\n",
            "{'loss': 1.1612, 'learning_rate': 8.285714285714286e-05, 'epoch': 83.43}\n",
            "{'loss': 1.1312, 'learning_rate': 8e-05, 'epoch': 83.99}\n",
            " 84% 5880/7000 [20:29<03:10,  5.89it/s][INFO|trainer.py:547] 2021-08-03 04:08:29,866 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:08:29,870 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:08:29,870 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:08:29,870 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:08:29,870 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.50it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 33.06it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:08:30,235 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.395896553993225, 'eval_runtime': 0.3654, 'eval_samples_per_second': 311.964, 'eval_steps_per_second': 27.365, 'epoch': 83.99}\n",
            " 84% 5880/7000 [20:30<03:10,  5.89it/s]\n",
            "100% 10/10 [00:00<00:00, 33.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:08:30,240 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5880\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:30,246 >> Configuration saved in results/adapters/citation-intent/checkpoint-5880/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:30,268 >> Module weights saved in results/adapters/citation-intent/checkpoint-5880/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:30,273 >> Configuration saved in results/adapters/citation-intent/checkpoint-5880/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:30,900 >> Module weights saved in results/adapters/citation-intent/checkpoint-5880/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:30,904 >> Configuration saved in results/adapters/citation-intent/checkpoint-5880/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:31,537 >> Module weights saved in results/adapters/citation-intent/checkpoint-5880/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:08:31,560 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5880/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:08:31,564 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5880/special_tokens_map.json\n",
            "{'loss': 1.1895, 'learning_rate': 7.714285714285714e-05, 'epoch': 84.57}\n",
            " 85% 5950/7000 [20:46<03:17,  5.32it/s][INFO|trainer.py:547] 2021-08-03 04:08:46,243 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:08:46,246 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:08:46,246 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:08:46,246 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:08:46,246 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.05it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.77it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:08:46,589 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6374115943908691, 'eval_runtime': 0.3433, 'eval_samples_per_second': 332.029, 'eval_steps_per_second': 29.125, 'epoch': 84.99}\n",
            " 85% 5950/7000 [20:46<03:17,  5.32it/s]\n",
            "100% 10/10 [00:00<00:00, 34.77it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:08:46,594 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-5950\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:46,602 >> Configuration saved in results/adapters/citation-intent/checkpoint-5950/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:46,632 >> Module weights saved in results/adapters/citation-intent/checkpoint-5950/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:46,638 >> Configuration saved in results/adapters/citation-intent/checkpoint-5950/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:47,298 >> Module weights saved in results/adapters/citation-intent/checkpoint-5950/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:08:47,302 >> Configuration saved in results/adapters/citation-intent/checkpoint-5950/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:08:47,957 >> Module weights saved in results/adapters/citation-intent/checkpoint-5950/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:08:47,981 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-5950/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:08:47,984 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-5950/special_tokens_map.json\n",
            "{'loss': 1.1772, 'learning_rate': 7.428571428571429e-05, 'epoch': 85.14}\n",
            "{'loss': 1.1283, 'learning_rate': 7.142857142857142e-05, 'epoch': 85.71}\n",
            " 86% 6020/7000 [21:00<02:50,  5.74it/s][INFO|trainer.py:547] 2021-08-03 04:09:00,570 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:09:00,573 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:09:00,573 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:09:00,574 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:09:00,574 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.34it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.13it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:09:00,907 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5605254173278809, 'eval_runtime': 0.3343, 'eval_samples_per_second': 340.972, 'eval_steps_per_second': 29.91, 'epoch': 85.99}\n",
            " 86% 6020/7000 [21:00<02:50,  5.74it/s]\n",
            "100% 10/10 [00:00<00:00, 36.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:09:00,912 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6020\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:00,918 >> Configuration saved in results/adapters/citation-intent/checkpoint-6020/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:00,940 >> Module weights saved in results/adapters/citation-intent/checkpoint-6020/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:00,944 >> Configuration saved in results/adapters/citation-intent/checkpoint-6020/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:01,532 >> Module weights saved in results/adapters/citation-intent/checkpoint-6020/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:01,536 >> Configuration saved in results/adapters/citation-intent/checkpoint-6020/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:02,185 >> Module weights saved in results/adapters/citation-intent/checkpoint-6020/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:09:02,207 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6020/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:09:02,212 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6020/special_tokens_map.json\n",
            "{'loss': 1.1971, 'learning_rate': 6.857142857142858e-05, 'epoch': 86.28}\n",
            "{'loss': 1.1602, 'learning_rate': 6.571428571428571e-05, 'epoch': 86.85}\n",
            " 87% 6090/7000 [21:13<02:23,  6.35it/s][INFO|trainer.py:547] 2021-08-03 04:09:13,855 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:09:13,858 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:09:13,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:09:13,858 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:09:13,858 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.41it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.51it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:09:14,189 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4568690061569214, 'eval_runtime': 0.3313, 'eval_samples_per_second': 344.105, 'eval_steps_per_second': 30.185, 'epoch': 86.99}\n",
            " 87% 6090/7000 [21:14<02:23,  6.35it/s]\n",
            "100% 10/10 [00:00<00:00, 35.51it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:09:14,193 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6090\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:14,198 >> Configuration saved in results/adapters/citation-intent/checkpoint-6090/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:14,219 >> Module weights saved in results/adapters/citation-intent/checkpoint-6090/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:14,223 >> Configuration saved in results/adapters/citation-intent/checkpoint-6090/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:14,844 >> Module weights saved in results/adapters/citation-intent/checkpoint-6090/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:14,848 >> Configuration saved in results/adapters/citation-intent/checkpoint-6090/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:15,484 >> Module weights saved in results/adapters/citation-intent/checkpoint-6090/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:09:15,504 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6090/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:09:15,507 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6090/special_tokens_map.json\n",
            "{'loss': 1.2075, 'learning_rate': 6.285714285714286e-05, 'epoch': 87.43}\n",
            "{'loss': 1.1271, 'learning_rate': 6e-05, 'epoch': 87.99}\n",
            " 88% 6160/7000 [21:27<02:19,  6.04it/s][INFO|trainer.py:547] 2021-08-03 04:09:27,295 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:09:27,297 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:09:27,297 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:09:27,297 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:09:27,297 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.55it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.74it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:09:27,633 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5333163738250732, 'eval_runtime': 0.3369, 'eval_samples_per_second': 338.401, 'eval_steps_per_second': 29.684, 'epoch': 87.99}\n",
            " 88% 6160/7000 [21:27<02:19,  6.04it/s]\n",
            "100% 10/10 [00:00<00:00, 35.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:09:27,637 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6160\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:27,666 >> Configuration saved in results/adapters/citation-intent/checkpoint-6160/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:27,694 >> Module weights saved in results/adapters/citation-intent/checkpoint-6160/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:27,698 >> Configuration saved in results/adapters/citation-intent/checkpoint-6160/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:28,319 >> Module weights saved in results/adapters/citation-intent/checkpoint-6160/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:28,323 >> Configuration saved in results/adapters/citation-intent/checkpoint-6160/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:28,949 >> Module weights saved in results/adapters/citation-intent/checkpoint-6160/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:09:28,970 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:09:28,973 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6160/special_tokens_map.json\n",
            "{'loss': 1.1583, 'learning_rate': 5.714285714285714e-05, 'epoch': 88.57}\n",
            " 89% 6230/7000 [21:40<01:49,  7.02it/s][INFO|trainer.py:547] 2021-08-03 04:09:40,701 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:09:40,703 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:09:40,703 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:09:40,703 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:09:40,703 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 35.86it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.18it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:09:41,037 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.513246774673462, 'eval_runtime': 0.3346, 'eval_samples_per_second': 340.745, 'eval_steps_per_second': 29.89, 'epoch': 88.99}\n",
            " 89% 6230/7000 [21:40<01:49,  7.02it/s]\n",
            "100% 10/10 [00:00<00:00, 35.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:09:41,042 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6230\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:41,048 >> Configuration saved in results/adapters/citation-intent/checkpoint-6230/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:41,093 >> Module weights saved in results/adapters/citation-intent/checkpoint-6230/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:41,100 >> Configuration saved in results/adapters/citation-intent/checkpoint-6230/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:41,739 >> Module weights saved in results/adapters/citation-intent/checkpoint-6230/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:41,743 >> Configuration saved in results/adapters/citation-intent/checkpoint-6230/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:42,382 >> Module weights saved in results/adapters/citation-intent/checkpoint-6230/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:09:42,404 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6230/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:09:42,407 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6230/special_tokens_map.json\n",
            "{'loss': 1.1412, 'learning_rate': 5.428571428571429e-05, 'epoch': 89.14}\n",
            "{'loss': 1.1192, 'learning_rate': 5.142857142857143e-05, 'epoch': 89.71}\n",
            " 90% 6300/7000 [21:54<01:55,  6.04it/s][INFO|trainer.py:547] 2021-08-03 04:09:54,557 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:09:54,559 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:09:54,559 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:09:54,559 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:09:54,559 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.52it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.32it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:09:54,896 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.437334418296814, 'eval_runtime': 0.3373, 'eval_samples_per_second': 337.95, 'eval_steps_per_second': 29.645, 'epoch': 89.99}\n",
            " 90% 6300/7000 [21:54<01:55,  6.04it/s]\n",
            "100% 10/10 [00:00<00:00, 35.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:09:54,902 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6300\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:54,908 >> Configuration saved in results/adapters/citation-intent/checkpoint-6300/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:54,933 >> Module weights saved in results/adapters/citation-intent/checkpoint-6300/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:54,937 >> Configuration saved in results/adapters/citation-intent/checkpoint-6300/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:55,534 >> Module weights saved in results/adapters/citation-intent/checkpoint-6300/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:09:55,537 >> Configuration saved in results/adapters/citation-intent/checkpoint-6300/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:09:56,143 >> Module weights saved in results/adapters/citation-intent/checkpoint-6300/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:09:56,163 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:09:56,167 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6300/special_tokens_map.json\n",
            "{'loss': 1.1721, 'learning_rate': 4.857142857142857e-05, 'epoch': 90.28}\n",
            "{'loss': 1.1787, 'learning_rate': 4.5714285714285716e-05, 'epoch': 90.85}\n",
            " 91% 6370/7000 [22:07<01:39,  6.35it/s][INFO|trainer.py:547] 2021-08-03 04:10:08,008 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:10:08,010 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:10:08,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:10:08,011 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:10:08,011 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.69it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.82it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:10:08,363 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4577066898345947, 'eval_runtime': 0.3526, 'eval_samples_per_second': 323.301, 'eval_steps_per_second': 28.36, 'epoch': 90.99}\n",
            " 91% 6370/7000 [22:08<01:39,  6.35it/s]\n",
            "100% 10/10 [00:00<00:00, 34.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:10:08,369 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6370\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:08,375 >> Configuration saved in results/adapters/citation-intent/checkpoint-6370/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:08,399 >> Module weights saved in results/adapters/citation-intent/checkpoint-6370/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:08,403 >> Configuration saved in results/adapters/citation-intent/checkpoint-6370/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:09,038 >> Module weights saved in results/adapters/citation-intent/checkpoint-6370/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:09,230 >> Configuration saved in results/adapters/citation-intent/checkpoint-6370/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:09,867 >> Module weights saved in results/adapters/citation-intent/checkpoint-6370/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:10:09,888 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6370/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:10:09,892 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6370/special_tokens_map.json\n",
            "{'loss': 1.0786, 'learning_rate': 4.2857142857142856e-05, 'epoch': 91.43}\n",
            "{'loss': 1.1542, 'learning_rate': 4e-05, 'epoch': 91.99}\n",
            " 92% 6440/7000 [22:21<01:33,  6.01it/s][INFO|trainer.py:547] 2021-08-03 04:10:21,526 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:10:21,528 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:10:21,528 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:10:21,529 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:10:21,529 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 34.93it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.44it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:10:21,888 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3677217960357666, 'eval_runtime': 0.3596, 'eval_samples_per_second': 317.0, 'eval_steps_per_second': 27.807, 'epoch': 91.99}\n",
            " 92% 6440/7000 [22:21<01:33,  6.01it/s]\n",
            "100% 10/10 [00:00<00:00, 34.44it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:10:21,892 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6440\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:21,897 >> Configuration saved in results/adapters/citation-intent/checkpoint-6440/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:21,928 >> Module weights saved in results/adapters/citation-intent/checkpoint-6440/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:21,934 >> Configuration saved in results/adapters/citation-intent/checkpoint-6440/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:22,584 >> Module weights saved in results/adapters/citation-intent/checkpoint-6440/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:22,587 >> Configuration saved in results/adapters/citation-intent/checkpoint-6440/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:23,238 >> Module weights saved in results/adapters/citation-intent/checkpoint-6440/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:10:25,761 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6440/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:10:25,773 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6440/special_tokens_map.json\n",
            "{'loss': 1.208, 'learning_rate': 3.7142857142857143e-05, 'epoch': 92.57}\n",
            " 93% 6510/7000 [22:37<01:36,  5.10it/s][INFO|trainer.py:547] 2021-08-03 04:10:37,389 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:10:37,392 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:10:37,392 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:10:37,392 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:10:37,393 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.23it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.86it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:10:37,754 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.552558183670044, 'eval_runtime': 0.3626, 'eval_samples_per_second': 314.364, 'eval_steps_per_second': 27.576, 'epoch': 92.99}\n",
            " 93% 6510/7000 [22:37<01:36,  5.10it/s]\n",
            "100% 10/10 [00:00<00:00, 35.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:10:37,758 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6510\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:37,764 >> Configuration saved in results/adapters/citation-intent/checkpoint-6510/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:37,790 >> Module weights saved in results/adapters/citation-intent/checkpoint-6510/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:37,795 >> Configuration saved in results/adapters/citation-intent/checkpoint-6510/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:38,435 >> Module weights saved in results/adapters/citation-intent/checkpoint-6510/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:38,439 >> Configuration saved in results/adapters/citation-intent/checkpoint-6510/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:39,066 >> Module weights saved in results/adapters/citation-intent/checkpoint-6510/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:10:39,090 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6510/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:10:41,798 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6510/special_tokens_map.json\n",
            "{'loss': 1.2056, 'learning_rate': 3.428571428571429e-05, 'epoch': 93.14}\n",
            "{'loss': 1.1251, 'learning_rate': 3.142857142857143e-05, 'epoch': 93.71}\n",
            " 94% 6580/7000 [22:53<01:06,  6.34it/s][INFO|trainer.py:547] 2021-08-03 04:10:53,512 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:10:53,514 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:10:53,515 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:10:53,515 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:10:53,515 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.34it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.84it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:10:53,858 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.239810585975647, 'eval_runtime': 0.3444, 'eval_samples_per_second': 330.973, 'eval_steps_per_second': 29.033, 'epoch': 93.99}\n",
            " 94% 6580/7000 [22:53<01:06,  6.34it/s]\n",
            "100% 10/10 [00:00<00:00, 34.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:10:53,863 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6580\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:53,869 >> Configuration saved in results/adapters/citation-intent/checkpoint-6580/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:53,896 >> Module weights saved in results/adapters/citation-intent/checkpoint-6580/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:53,901 >> Configuration saved in results/adapters/citation-intent/checkpoint-6580/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:54,478 >> Module weights saved in results/adapters/citation-intent/checkpoint-6580/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:10:54,481 >> Configuration saved in results/adapters/citation-intent/checkpoint-6580/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:10:55,063 >> Module weights saved in results/adapters/citation-intent/checkpoint-6580/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:10:55,082 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:10:55,085 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6580/special_tokens_map.json\n",
            "{'loss': 1.1721, 'learning_rate': 2.857142857142857e-05, 'epoch': 94.28}\n",
            "{'loss': 1.2092, 'learning_rate': 2.5714285714285714e-05, 'epoch': 94.85}\n",
            " 95% 6650/7000 [23:09<00:57,  6.08it/s][INFO|trainer.py:547] 2021-08-03 04:11:09,475 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:11:09,478 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:11:09,478 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:11:09,478 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:11:09,478 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.59it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.84it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:11:09,823 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4164174795150757, 'eval_runtime': 0.3459, 'eval_samples_per_second': 329.548, 'eval_steps_per_second': 28.908, 'epoch': 94.99}\n",
            " 95% 6650/7000 [23:09<00:57,  6.08it/s]\n",
            "100% 10/10 [00:00<00:00, 35.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:11:09,828 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6650\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:09,834 >> Configuration saved in results/adapters/citation-intent/checkpoint-6650/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:09,860 >> Module weights saved in results/adapters/citation-intent/checkpoint-6650/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:09,864 >> Configuration saved in results/adapters/citation-intent/checkpoint-6650/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:10,504 >> Module weights saved in results/adapters/citation-intent/checkpoint-6650/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:10,508 >> Configuration saved in results/adapters/citation-intent/checkpoint-6650/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:11,127 >> Module weights saved in results/adapters/citation-intent/checkpoint-6650/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:11:11,148 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6650/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:11:11,151 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6650/special_tokens_map.json\n",
            "{'loss': 1.1349, 'learning_rate': 2.2857142857142858e-05, 'epoch': 95.43}\n",
            "{'loss': 1.1325, 'learning_rate': 2e-05, 'epoch': 95.99}\n",
            " 96% 6720/7000 [23:25<00:43,  6.42it/s][INFO|trainer.py:547] 2021-08-03 04:11:25,661 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:11:25,664 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:11:25,664 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:11:25,664 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:11:25,664 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.02it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 35.00it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:11:25,999 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.5108915567398071, 'eval_runtime': 0.3353, 'eval_samples_per_second': 339.945, 'eval_steps_per_second': 29.82, 'epoch': 95.99}\n",
            " 96% 6720/7000 [23:25<00:43,  6.42it/s]\n",
            "100% 10/10 [00:00<00:00, 35.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:11:26,004 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6720\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:26,009 >> Configuration saved in results/adapters/citation-intent/checkpoint-6720/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:26,031 >> Module weights saved in results/adapters/citation-intent/checkpoint-6720/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:26,035 >> Configuration saved in results/adapters/citation-intent/checkpoint-6720/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:26,620 >> Module weights saved in results/adapters/citation-intent/checkpoint-6720/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:26,624 >> Configuration saved in results/adapters/citation-intent/checkpoint-6720/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:27,245 >> Module weights saved in results/adapters/citation-intent/checkpoint-6720/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:11:27,265 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6720/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:11:27,268 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6720/special_tokens_map.json\n",
            "{'loss': 1.1669, 'learning_rate': 1.7142857142857145e-05, 'epoch': 96.57}\n",
            " 97% 6790/7000 [23:41<00:33,  6.36it/s][INFO|trainer.py:547] 2021-08-03 04:11:41,692 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:11:41,694 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:11:41,694 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:11:41,694 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:11:41,694 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 37.54it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 34.27it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:11:42,061 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4223538637161255, 'eval_runtime': 0.3669, 'eval_samples_per_second': 310.711, 'eval_steps_per_second': 27.255, 'epoch': 96.99}\n",
            " 97% 6790/7000 [23:41<00:33,  6.36it/s]\n",
            "100% 10/10 [00:00<00:00, 34.27it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:11:42,065 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6790\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:42,071 >> Configuration saved in results/adapters/citation-intent/checkpoint-6790/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:42,094 >> Module weights saved in results/adapters/citation-intent/checkpoint-6790/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:42,097 >> Configuration saved in results/adapters/citation-intent/checkpoint-6790/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:42,683 >> Module weights saved in results/adapters/citation-intent/checkpoint-6790/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:42,686 >> Configuration saved in results/adapters/citation-intent/checkpoint-6790/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:43,271 >> Module weights saved in results/adapters/citation-intent/checkpoint-6790/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:11:43,293 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6790/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:11:43,295 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6790/special_tokens_map.json\n",
            "{'loss': 1.1813, 'learning_rate': 1.4285714285714285e-05, 'epoch': 97.14}\n",
            "{'loss': 1.1128, 'learning_rate': 1.1428571428571429e-05, 'epoch': 97.71}\n",
            " 98% 6860/7000 [23:57<00:24,  5.80it/s][INFO|trainer.py:547] 2021-08-03 04:11:57,566 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:11:57,570 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:11:57,570 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:11:57,570 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:11:57,570 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 38.01it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 35.11it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:11:57,982 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.519567608833313, 'eval_runtime': 0.4126, 'eval_samples_per_second': 276.316, 'eval_steps_per_second': 24.238, 'epoch': 97.99}\n",
            " 98% 6860/7000 [23:57<00:24,  5.80it/s]\n",
            "100% 10/10 [00:00<00:00, 35.11it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:11:57,987 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6860\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:57,993 >> Configuration saved in results/adapters/citation-intent/checkpoint-6860/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:58,020 >> Module weights saved in results/adapters/citation-intent/checkpoint-6860/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:58,025 >> Configuration saved in results/adapters/citation-intent/checkpoint-6860/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:58,625 >> Module weights saved in results/adapters/citation-intent/checkpoint-6860/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:11:58,628 >> Configuration saved in results/adapters/citation-intent/checkpoint-6860/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:11:59,201 >> Module weights saved in results/adapters/citation-intent/checkpoint-6860/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:11:59,219 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6860/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:11:59,223 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6860/special_tokens_map.json\n",
            "{'loss': 1.1535, 'learning_rate': 8.571428571428573e-06, 'epoch': 98.28}\n",
            "{'loss': 1.1371, 'learning_rate': 5.7142857142857145e-06, 'epoch': 98.85}\n",
            " 99% 6930/7000 [24:12<00:10,  7.00it/s][INFO|trainer.py:547] 2021-08-03 04:12:13,151 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:12:13,154 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:12:13,154 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:12:13,154 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:12:13,154 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00, 36.00it/s]\u001b[A\n",
            " 90% 9/10 [00:00<00:00, 34.78it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:12:13,499 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3726205825805664, 'eval_runtime': 0.3456, 'eval_samples_per_second': 329.882, 'eval_steps_per_second': 28.937, 'epoch': 98.99}\n",
            " 99% 6930/7000 [24:13<00:10,  7.00it/s]\n",
            "100% 10/10 [00:00<00:00, 34.78it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:12:13,503 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-6930\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:13,508 >> Configuration saved in results/adapters/citation-intent/checkpoint-6930/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:13,532 >> Module weights saved in results/adapters/citation-intent/checkpoint-6930/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:13,536 >> Configuration saved in results/adapters/citation-intent/checkpoint-6930/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:14,152 >> Module weights saved in results/adapters/citation-intent/checkpoint-6930/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:14,156 >> Configuration saved in results/adapters/citation-intent/checkpoint-6930/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:14,764 >> Module weights saved in results/adapters/citation-intent/checkpoint-6930/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:12:14,778 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-6930/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:12:14,781 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-6930/special_tokens_map.json\n",
            "{'loss': 1.1587, 'learning_rate': 2.8571428571428573e-06, 'epoch': 99.43}\n",
            "{'loss': 1.1739, 'learning_rate': 0.0, 'epoch': 99.99}\n",
            "100% 7000/7000 [24:26<00:00,  6.49it/s][INFO|trainer.py:547] 2021-08-03 04:12:26,554 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:12:26,557 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:12:26,557 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:12:26,557 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:12:26,557 >>   Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00, 39.95it/s]\u001b[A\n",
            " 80% 8/10 [00:00<00:00, 36.00it/s]\u001b[A[WARNING|training_args.py:774] 2021-08-03 04:12:26,895 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4340306520462036, 'eval_runtime': 0.3389, 'eval_samples_per_second': 336.398, 'eval_steps_per_second': 29.509, 'epoch': 99.99}\n",
            "100% 7000/7000 [24:26<00:00,  6.49it/s]\n",
            "100% 10/10 [00:00<00:00, 36.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1989] 2021-08-03 04:12:26,900 >> Saving model checkpoint to results/adapters/citation-intent/checkpoint-7000\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:26,906 >> Configuration saved in results/adapters/citation-intent/checkpoint-7000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:26,928 >> Module weights saved in results/adapters/citation-intent/checkpoint-7000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:26,932 >> Configuration saved in results/adapters/citation-intent/checkpoint-7000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:27,522 >> Module weights saved in results/adapters/citation-intent/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:27,526 >> Configuration saved in results/adapters/citation-intent/checkpoint-7000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:28,160 >> Module weights saved in results/adapters/citation-intent/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:12:28,181 >> tokenizer config file saved in results/adapters/citation-intent/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:12:28,185 >> Special tokens file saved in results/adapters/citation-intent/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:1403] 2021-08-03 04:12:28,416 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1429] 2021-08-03 04:12:28,417 >> Loading best adapter(s) from results/adapters/citation-intent/checkpoint-6580 (score: 1.239810585975647).\n",
            "[INFO|loading.py:76] 2021-08-03 04:12:28,418 >> Loading module configuration from results/adapters/citation-intent/checkpoint-6580/mlm/adapter_config.json\n",
            "[WARNING|loading.py:430] 2021-08-03 04:12:28,419 >> Overwriting existing adapter 'mlm'.\n",
            "[INFO|loading.py:135] 2021-08-03 04:12:28,443 >> Loading module weights from results/adapters/citation-intent/checkpoint-6580/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:76] 2021-08-03 04:12:28,452 >> Loading module configuration from results/adapters/citation-intent/checkpoint-6580/mlm/head_config.json\n",
            "[INFO|loading.py:135] 2021-08-03 04:12:28,655 >> Loading module weights from results/adapters/citation-intent/checkpoint-6580/mlm/pytorch_model_head.bin\n",
            "[INFO|trainer.py:1438] 2021-08-03 04:12:28,698 >> Loading best adapter fusion(s) from results/adapters/citation-intent/checkpoint-6580 (score: 1.239810585975647).\n",
            "{'train_runtime': 1468.5576, 'train_samples_per_second': 114.943, 'train_steps_per_second': 4.767, 'train_loss': 1.446519249507359, 'epoch': 99.99}\n",
            "100% 7000/7000 [24:28<00:00,  4.77it/s]\n",
            "[INFO|trainer.py:1989] 2021-08-03 04:12:28,725 >> Saving model checkpoint to results/adapters/citation-intent\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:28,732 >> Configuration saved in results/adapters/citation-intent/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:28,759 >> Module weights saved in results/adapters/citation-intent/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:30,912 >> Configuration saved in results/adapters/citation-intent/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:31,511 >> Module weights saved in results/adapters/citation-intent/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 04:12:31,527 >> Configuration saved in results/adapters/citation-intent/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 04:12:32,178 >> Module weights saved in results/adapters/citation-intent/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 04:12:32,200 >> tokenizer config file saved in results/adapters/citation-intent/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 04:12:32,203 >> Special tokens file saved in results/adapters/citation-intent/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      99.99\n",
            "  train_loss               =     1.4465\n",
            "  train_runtime            = 0:24:28.55\n",
            "  train_samples            =       1688\n",
            "  train_samples_per_second =    114.943\n",
            "  train_steps_per_second   =      4.767\n",
            "08/03/2021 04:12:32 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:547] 2021-08-03 04:12:32,349 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 04:12:32,351 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 04:12:32,352 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 04:12:32,352 >>   Num examples = 114\n",
            "[INFO|trainer.py:2244] 2021-08-03 04:12:32,352 >>   Batch size = 12\n",
            " 90% 9/10 [00:00<00:00, 35.03it/s][WARNING|training_args.py:774] 2021-08-03 04:12:32,683 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 10/10 [00:00<00:00, 29.44it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      99.99\n",
            "  eval_loss               =     1.6555\n",
            "  eval_runtime            = 0:00:00.33\n",
            "  eval_samples            =        114\n",
            "  eval_samples_per_second =    343.218\n",
            "  eval_steps_per_second   =     30.107\n",
            "  perplexity              =     5.2354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkmms-1WOxdi",
        "outputId": "96b8344d-340c-412d-f944-e2d35b2509a8"
      },
      "source": [
        "# Experiment_5_citation_intent_fusion_adapter\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/citation-intent_1/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/sciie/mlm \\\n",
        "--adapter_2 results/adapters/citation-intent/mlm \\\n",
        "--per_device_eval_batch_size 12 \\\n",
        "--weight_decay 0.12 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.95 \\\n",
        "--adam_epsilon 5e-4 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 1 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 04:20:33.254707: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 04:20:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1688\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5640\n",
            "  2% 141/5640 [00:53<31:33,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.84it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.89it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.51it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.75it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.30it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  7.01it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.83it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.70it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3691352605819702, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4967, 'eval_samples_per_second': 76.165, 'eval_steps_per_second': 6.681, 'epoch': 1.0}\n",
            "  2% 141/5640 [00:54<31:33,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.70it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-141\n",
            "Configuration saved in results/citation-intent_1/checkpoint-141/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-141/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-141/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-141/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-141/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-141/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-141/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-141/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-141/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-141/pytorch_model.bin\n",
            "  5% 282/5640 [01:52<30:48,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.83it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.48it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.68it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3352816104888916, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4896, 'eval_samples_per_second': 76.528, 'eval_steps_per_second': 6.713, 'epoch': 2.0}\n",
            "  5% 282/5640 [01:54<30:48,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-282\n",
            "Configuration saved in results/citation-intent_1/checkpoint-282/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-282/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-282/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-282/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-282/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-282/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-282/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-282/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-282/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-282/pytorch_model.bin\n",
            "  8% 423/5640 [02:51<29:59,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.83it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.83it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3245769739151, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4918, 'eval_samples_per_second': 76.418, 'eval_steps_per_second': 6.703, 'epoch': 3.0}\n",
            "  8% 423/5640 [02:53<29:59,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-423\n",
            "Configuration saved in results/citation-intent_1/checkpoint-423/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-423/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-423/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-423/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-423/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-423/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-423/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-423/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-423/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-423/pytorch_model.bin\n",
            "{'loss': 1.4244, 'learning_rate': 3.645390070921986e-05, 'epoch': 3.55}\n",
            " 10% 564/5640 [03:51<29:16,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3154312372207642, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.495, 'eval_samples_per_second': 76.257, 'eval_steps_per_second': 6.689, 'epoch': 4.0}\n",
            " 10% 564/5640 [03:52<29:16,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-564\n",
            "Configuration saved in results/citation-intent_1/checkpoint-564/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-564/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-564/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-564/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-564/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-564/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-564/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-564/pytorch_model.bin\n",
            " 12% 705/5640 [04:49<28:26,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.79it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.68it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2999018430709839, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4916, 'eval_samples_per_second': 76.428, 'eval_steps_per_second': 6.704, 'epoch': 5.0}\n",
            " 12% 705/5640 [04:51<28:26,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-705\n",
            "Configuration saved in results/citation-intent_1/checkpoint-705/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-705/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-705/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-705/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-705/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-705/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-705/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-705/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-705/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-705/pytorch_model.bin\n",
            " 15% 846/5640 [05:50<27:30,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.81it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.48it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2779184579849243, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4912, 'eval_samples_per_second': 76.45, 'eval_steps_per_second': 6.706, 'epoch': 6.0}\n",
            " 15% 846/5640 [05:51<27:30,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-846\n",
            "Configuration saved in results/citation-intent_1/checkpoint-846/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-846/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-846/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-846/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-846/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-846/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-846/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-846/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-846/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-846/pytorch_model.bin\n",
            " 18% 987/5640 [06:49<26:43,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.80it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.86it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.49it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.74it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.29it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  7.01it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.82it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.69it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2534059286117554, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4868, 'eval_samples_per_second': 76.675, 'eval_steps_per_second': 6.726, 'epoch': 7.0}\n",
            " 18% 987/5640 [06:50<26:43,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.69it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-987\n",
            "Configuration saved in results/citation-intent_1/checkpoint-987/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-987/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-987/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-987/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-987/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-987/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-987/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-987/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-987/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-987/pytorch_model.bin\n",
            "{'loss': 1.3202, 'learning_rate': 3.290780141843972e-05, 'epoch': 7.09}\n",
            " 20% 1128/5640 [07:47<25:54,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.82it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  7.00it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.68it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2283802032470703, 'eval_acc': 0.5526315789473685, 'eval_f1': 0.18195902909278705, 'eval_precision': 0.1681547619047619, 'eval_recall': 0.2137476459510358, 'eval_runtime': 1.4891, 'eval_samples_per_second': 76.558, 'eval_steps_per_second': 6.716, 'epoch': 8.0}\n",
            " 20% 1128/5640 [07:49<25:54,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1128\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1128/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1128/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1128/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1128/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1128/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1128/pytorch_model.bin\n",
            " 22% 1269/5640 [08:46<25:15,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.68it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1603652238845825, 'eval_acc': 0.5614035087719298, 'eval_f1': 0.1897091722595078, 'eval_precision': 0.16435185185185186, 'eval_recall': 0.22679580306698952, 'eval_runtime': 1.4924, 'eval_samples_per_second': 76.389, 'eval_steps_per_second': 6.701, 'epoch': 9.0}\n",
            " 22% 1269/5640 [08:47<25:15,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1269\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1269/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1269/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1269/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1269/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1269/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1269/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1269/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1269/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1269/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1269/pytorch_model.bin\n",
            " 25% 1410/5640 [09:44<24:23,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.72it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1173800230026245, 'eval_acc': 0.5701754385964912, 'eval_f1': 0.19711657966691523, 'eval_precision': 0.1712962962962963, 'eval_recall': 0.23473231100349742, 'eval_runtime': 1.4973, 'eval_samples_per_second': 76.136, 'eval_steps_per_second': 6.679, 'epoch': 10.0}\n",
            " 25% 1410/5640 [09:46<24:23,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1410\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1410/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1410/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1410/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1410/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1410/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1410/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1410/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1410/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1410/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1410/pytorch_model.bin\n",
            "{'loss': 1.2036, 'learning_rate': 2.9361702127659577e-05, 'epoch': 10.64}\n",
            " 28% 1551/5640 [10:43<23:37,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.79it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.080317735671997, 'eval_acc': 0.5877192982456141, 'eval_f1': 0.20890410958904107, 'eval_precision': 0.17943805874840357, 'eval_recall': 0.2506053268765133, 'eval_runtime': 1.4946, 'eval_samples_per_second': 76.272, 'eval_steps_per_second': 6.691, 'epoch': 11.0}\n",
            " 28% 1551/5640 [10:44<23:37,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1551\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1551/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1551/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1551/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1551/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1551/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1551/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1551/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1551/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1551/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1551/pytorch_model.bin\n",
            " 30% 1692/5640 [11:42<22:51,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0574134588241577, 'eval_acc': 0.6403508771929824, 'eval_f1': 0.2738186462324394, 'eval_precision': 0.28648014211394496, 'eval_recall': 0.3135593220338983, 'eval_runtime': 1.4969, 'eval_samples_per_second': 76.157, 'eval_steps_per_second': 6.68, 'epoch': 12.0}\n",
            " 30% 1692/5640 [11:43<22:51,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1692\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1692/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1692/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1692/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1692/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1692/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1692/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1692/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1692/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1692/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1692/pytorch_model.bin\n",
            " 32% 1833/5640 [12:41<21:57,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0354604721069336, 'eval_acc': 0.6403508771929824, 'eval_f1': 0.28302311512542383, 'eval_precision': 0.2880952380952381, 'eval_recall': 0.3186709712133441, 'eval_runtime': 1.4938, 'eval_samples_per_second': 76.314, 'eval_steps_per_second': 6.694, 'epoch': 13.0}\n",
            " 32% 1833/5640 [12:43<21:57,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1833\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1833/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1833/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1833/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1833/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1833/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1833/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1833/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1833/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1833/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1833/pytorch_model.bin\n",
            " 35% 1974/5640 [13:41<21:10,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.82it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.86it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.49it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.81it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0113636255264282, 'eval_acc': 0.6491228070175439, 'eval_f1': 0.2684813055657045, 'eval_precision': 0.36926406926406924, 'eval_recall': 0.31127253161151464, 'eval_runtime': 1.4924, 'eval_samples_per_second': 76.389, 'eval_steps_per_second': 6.701, 'epoch': 14.0}\n",
            " 35% 1974/5640 [13:42<21:10,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-1974\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1974/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1974/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1974/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1974/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1974/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1974/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1974/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-1974/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-1974/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-1974/pytorch_model.bin\n",
            "{'loss': 1.0828, 'learning_rate': 2.5815602836879437e-05, 'epoch': 14.18}\n",
            " 38% 2115/5640 [14:40<20:20,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9974867105484009, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3094659516228144, 'eval_precision': 0.3525461993203929, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.4919, 'eval_samples_per_second': 76.411, 'eval_steps_per_second': 6.703, 'epoch': 15.0}\n",
            " 38% 2115/5640 [14:41<20:20,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2115\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2115/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2115/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2115/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2115/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2115/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2115/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2115/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2115/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2115/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2115/pytorch_model.bin\n",
            " 40% 2256/5640 [15:38<19:30,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.76it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9926894307136536, 'eval_acc': 0.6578947368421053, 'eval_f1': 0.29118137498249763, 'eval_precision': 0.30463320463320465, 'eval_recall': 0.3294323379069142, 'eval_runtime': 1.4959, 'eval_samples_per_second': 76.207, 'eval_steps_per_second': 6.685, 'epoch': 16.0}\n",
            " 40% 2256/5640 [15:40<19:30,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2256\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2256/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2256/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2256/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2256/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2256/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2256/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2256/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2256/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2256/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2256/pytorch_model.bin\n",
            " 42% 2397/5640 [16:37<18:42,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9678933620452881, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.30826927556487305, 'eval_precision': 0.3510599415204678, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.4908, 'eval_samples_per_second': 76.471, 'eval_steps_per_second': 6.708, 'epoch': 17.0}\n",
            " 42% 2397/5640 [16:39<18:42,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2397\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2397/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2397/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2397/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2397/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2397/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2397/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2397/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2397/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2397/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2397/pytorch_model.bin\n",
            "{'loss': 1.0255, 'learning_rate': 2.226950354609929e-05, 'epoch': 17.73}\n",
            " 45% 2538/5640 [17:36<17:54,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.73it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9509561657905579, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3105042016806723, 'eval_precision': 0.337468671679198, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.4954, 'eval_samples_per_second': 76.236, 'eval_steps_per_second': 6.687, 'epoch': 18.0}\n",
            " 45% 2538/5640 [17:38<17:54,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2538\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2538/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2538/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2538/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2538/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2538/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2538/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2538/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2538/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2538/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2538/pytorch_model.bin\n",
            " 48% 2679/5640 [18:35<17:05,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.83it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9390591382980347, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.3156470395276365, 'eval_precision': 0.3411981566820277, 'eval_recall': 0.34301856335754644, 'eval_runtime': 1.4951, 'eval_samples_per_second': 76.249, 'eval_steps_per_second': 6.688, 'epoch': 19.0}\n",
            " 48% 2679/5640 [18:36<17:05,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2679\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2679/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2679/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2679/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2679/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2679/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2679/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2679/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2679/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2679/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2679/pytorch_model.bin\n",
            " 50% 2820/5640 [19:33<16:17,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9364987015724182, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.32678044707717807, 'eval_precision': 0.3288284632034632, 'eval_recall': 0.35324186171643795, 'eval_runtime': 1.4952, 'eval_samples_per_second': 76.244, 'eval_steps_per_second': 6.688, 'epoch': 20.0}\n",
            " 50% 2820/5640 [19:35<16:17,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2820\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2820/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2820/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2820/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2820/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2820/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2820/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2820/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2820/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2820/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2820/pytorch_model.bin\n",
            " 52% 2961/5640 [20:32<15:28,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.91300368309021, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.3307875131404543, 'eval_precision': 0.3271909233176839, 'eval_recall': 0.3560667204735002, 'eval_runtime': 1.4963, 'eval_samples_per_second': 76.186, 'eval_steps_per_second': 6.683, 'epoch': 21.0}\n",
            " 52% 2961/5640 [20:33<15:28,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-2961\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2961/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2961/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2961/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2961/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2961/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2961/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2961/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-2961/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-2961/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-2961/pytorch_model.bin\n",
            "{'loss': 0.9619, 'learning_rate': 1.872340425531915e-05, 'epoch': 21.28}\n",
            " 55% 3102/5640 [21:31<14:35,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.80it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9173847436904907, 'eval_acc': 0.6666666666666666, 'eval_f1': 0.30171165159579594, 'eval_precision': 0.3294388138138138, 'eval_recall': 0.33225719666397635, 'eval_runtime': 1.4945, 'eval_samples_per_second': 76.278, 'eval_steps_per_second': 6.691, 'epoch': 22.0}\n",
            " 55% 3102/5640 [21:32<14:35,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3102\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3102/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3102/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3102/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3102/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3102/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3102/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3102/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3102/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3102/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3102/pytorch_model.bin\n",
            " 57% 3243/5640 [22:29<13:50,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8970374464988708, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.31821036106750394, 'eval_precision': 0.3089369550637156, 'eval_recall': 0.3401937046004843, 'eval_runtime': 1.4923, 'eval_samples_per_second': 76.392, 'eval_steps_per_second': 6.701, 'epoch': 23.0}\n",
            " 57% 3243/5640 [22:31<13:50,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3243\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3243/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3243/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3243/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3243/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3243/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3243/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3243/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3243/pytorch_model.bin\n",
            " 60% 3384/5640 [23:28<13:04,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.33it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.63it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.35it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.21it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.75it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8893696069717407, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.32556712159765594, 'eval_precision': 0.3233556832694764, 'eval_recall': 0.3481302125369922, 'eval_runtime': 1.5036, 'eval_samples_per_second': 75.82, 'eval_steps_per_second': 6.651, 'epoch': 24.0}\n",
            " 60% 3384/5640 [23:30<13:04,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3384\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3384/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3384/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3384/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3384/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3384/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3384/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3384/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3384/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3384/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3384/pytorch_model.bin\n",
            "{'loss': 0.9375, 'learning_rate': 1.5177304964539008e-05, 'epoch': 24.82}\n",
            " 62% 3525/5640 [24:28<12:13,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8820428848266602, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.32686886756654193, 'eval_precision': 0.32318228366615465, 'eval_recall': 0.35324186171643795, 'eval_runtime': 1.4948, 'eval_samples_per_second': 76.262, 'eval_steps_per_second': 6.69, 'epoch': 25.0}\n",
            " 62% 3525/5640 [24:29<12:13,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3525\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3525/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3525/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3525/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3525/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3525/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3525/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3525/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3525/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3525/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3525/pytorch_model.bin\n",
            " 65% 3666/5640 [25:27<11:20,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.80it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.48it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8740155696868896, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.31380700428319475, 'eval_precision': 0.32034948482316905, 'eval_recall': 0.3379069141781006, 'eval_runtime': 1.4941, 'eval_samples_per_second': 76.301, 'eval_steps_per_second': 6.693, 'epoch': 26.0}\n",
            " 65% 3666/5640 [25:29<11:20,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3666\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3666/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3666/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3666/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3666/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3666/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3666/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3666/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3666/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3666/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3666/pytorch_model.bin\n",
            " 68% 3807/5640 [26:27<10:32,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.83it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.894859254360199, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3240621937695181, 'eval_precision': 0.3211805555555556, 'eval_recall': 0.35041700295937583, 'eval_runtime': 1.4917, 'eval_samples_per_second': 76.423, 'eval_steps_per_second': 6.704, 'epoch': 27.0}\n",
            " 68% 3807/5640 [26:28<10:32,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3807\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3807/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3807/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3807/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3807/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3807/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3807/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3807/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3807/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3807/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3807/pytorch_model.bin\n",
            " 70% 3948/5640 [27:26<09:48,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.72it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.66it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.21it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.74it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8895578384399414, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.32428059049179936, 'eval_precision': 0.3168941273779983, 'eval_recall': 0.35041700295937583, 'eval_runtime': 1.5009, 'eval_samples_per_second': 75.955, 'eval_steps_per_second': 6.663, 'epoch': 28.0}\n",
            " 70% 3948/5640 [27:27<09:48,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-3948\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3948/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3948/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3948/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3948/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3948/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3948/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3948/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-3948/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-3948/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-3948/pytorch_model.bin\n",
            "{'loss': 0.8925, 'learning_rate': 1.1631205673758865e-05, 'epoch': 28.37}\n",
            " 72% 4089/5640 [28:25<08:58,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.73it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.77it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8722397685050964, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.32897873789714394, 'eval_precision': 0.31153846153846154, 'eval_recall': 0.3555286521388216, 'eval_runtime': 1.4999, 'eval_samples_per_second': 76.006, 'eval_steps_per_second': 6.667, 'epoch': 29.0}\n",
            " 72% 4089/5640 [28:27<08:58,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4089\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4089/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4089/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4089/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4089/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4089/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4089/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4089/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4089/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4089/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4089/pytorch_model.bin\n",
            " 75% 4230/5640 [29:24<08:08,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.83it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.867676317691803, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.320641434868521, 'eval_precision': 0.31355311355311355, 'eval_recall': 0.34530535377993005, 'eval_runtime': 1.4963, 'eval_samples_per_second': 76.189, 'eval_steps_per_second': 6.683, 'epoch': 30.0}\n",
            " 75% 4230/5640 [29:25<08:08,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4230\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4230/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4230/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4230/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4230/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4230/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4230/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4230/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4230/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4230/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4230/pytorch_model.bin\n",
            " 78% 4371/5640 [30:22<07:19,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.80it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.83it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8650690913200378, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.33774193548387094, 'eval_precision': 0.31937270231281123, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.4919, 'eval_samples_per_second': 76.415, 'eval_steps_per_second': 6.703, 'epoch': 31.0}\n",
            " 78% 4371/5640 [30:24<07:19,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4371\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4371/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4371/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4371/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4371/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4371/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4371/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4371/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4371/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4371/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4371/pytorch_model.bin\n",
            "{'loss': 0.8731, 'learning_rate': 8.085106382978723e-06, 'epoch': 31.91}\n",
            " 80% 4512/5640 [31:21<06:29,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.80it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8579471707344055, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.3336487748252454, 'eval_precision': 0.320615671641791, 'eval_recall': 0.35835351089588374, 'eval_runtime': 1.492, 'eval_samples_per_second': 76.409, 'eval_steps_per_second': 6.703, 'epoch': 32.0}\n",
            " 80% 4512/5640 [31:22<06:29,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4512\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4512/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4512/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4512/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4512/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4512/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4512/pytorch_model.bin\n",
            " 82% 4653/5640 [32:19<05:41,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8488770723342896, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.3426399331662489, 'eval_precision': 0.3283531631901346, 'eval_recall': 0.3662900188323917, 'eval_runtime': 1.4955, 'eval_samples_per_second': 76.226, 'eval_steps_per_second': 6.687, 'epoch': 33.0}\n",
            " 82% 4653/5640 [32:21<05:41,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4653\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4653/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4653/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4653/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4653/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4653/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4653/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4653/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4653/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4653/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4653/pytorch_model.bin\n",
            " 85% 4794/5640 [33:18<04:53,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.81it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8443491458892822, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.33774193548387094, 'eval_precision': 0.31937270231281123, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.4936, 'eval_samples_per_second': 76.326, 'eval_steps_per_second': 6.695, 'epoch': 34.0}\n",
            " 85% 4794/5640 [33:20<04:53,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4794\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4794/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4794/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4794/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4794/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4794/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4794/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4794/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4794/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4794/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4794/pytorch_model.bin\n",
            " 88% 4935/5640 [34:17<04:04,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.73it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.842064619064331, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.495, 'eval_samples_per_second': 76.252, 'eval_steps_per_second': 6.689, 'epoch': 35.0}\n",
            " 88% 4935/5640 [34:18<04:04,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-4935\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4935/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4935/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4935/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4935/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4935/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4935/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4935/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-4935/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-4935/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-4935/pytorch_model.bin\n",
            "{'loss': 0.8496, 'learning_rate': 4.539007092198582e-06, 'epoch': 35.46}\n",
            " 90% 5076/5640 [35:16<03:14,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8416489958763123, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.491, 'eval_samples_per_second': 76.457, 'eval_steps_per_second': 6.707, 'epoch': 36.0}\n",
            " 90% 5076/5640 [35:17<03:14,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-5076\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5076/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5076/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5076/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5076/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5076/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5076/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5076/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5076/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5076/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-5076/pytorch_model.bin\n",
            " 92% 5217/5640 [36:14<02:25,  2.90it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8487074375152588, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.3388034188034188, 'eval_precision': 0.32230349471728786, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.4913, 'eval_samples_per_second': 76.441, 'eval_steps_per_second': 6.705, 'epoch': 37.0}\n",
            " 92% 5217/5640 [36:16<02:25,  2.90it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-5217\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5217/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5217/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5217/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5217/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5217/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5217/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5217/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5217/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5217/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-5217/pytorch_model.bin\n",
            " 95% 5358/5640 [37:13<01:37,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.82it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.67it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8451896905899048, 'eval_acc': 0.7017543859649122, 'eval_f1': 0.346573029722636, 'eval_precision': 0.3350659229208925, 'eval_recall': 0.3691148775894539, 'eval_runtime': 1.4921, 'eval_samples_per_second': 76.4, 'eval_steps_per_second': 6.702, 'epoch': 38.0}\n",
            " 95% 5358/5640 [37:15<01:37,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-5358\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5358/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5358/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5358/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5358/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5358/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5358/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5358/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5358/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5358/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-5358/pytorch_model.bin\n",
            " 98% 5499/5640 [38:13<00:48,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.79it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.842776358127594, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.4926, 'eval_samples_per_second': 76.376, 'eval_steps_per_second': 6.7, 'epoch': 39.0}\n",
            " 98% 5499/5640 [38:14<00:48,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-5499\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5499/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5499/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5499/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5499/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5499/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5499/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5499/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5499/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5499/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-5499/pytorch_model.bin\n",
            "{'loss': 0.8485, 'learning_rate': 9.929078014184399e-07, 'epoch': 39.01}\n",
            "100% 5640/5640 [39:12<00:00,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.83it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.85it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.48it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.28it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.80it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.68it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8425695896148682, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.4917, 'eval_samples_per_second': 76.424, 'eval_steps_per_second': 6.704, 'epoch': 40.0}\n",
            "100% 5640/5640 [39:13<00:00,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_1/checkpoint-5640\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5640/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5640/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5640/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/checkpoint-5640/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/checkpoint-5640/config.json\n",
            "Model weights saved in results/citation-intent_1/checkpoint-5640/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/citation-intent_1/checkpoint-5076 (score: 0.8416489958763123).\n",
            "Loading best adapter(s) from results/citation-intent_1/checkpoint-5076 (score: 0.8416489958763123).\n",
            "Loading module configuration from results/citation-intent_1/checkpoint-5076/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/citation-intent_1/checkpoint-5076/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/citation-intent_1/checkpoint-5076/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/citation-intent_1/checkpoint-5076/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/citation-intent_1/checkpoint-5076 (score: 0.8416489958763123).\n",
            "Loading module configuration from results/citation-intent_1/checkpoint-5076/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/citation-intent_1/checkpoint-5076/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 2359.255, 'train_samples_per_second': 28.619, 'train_steps_per_second': 2.391, 'train_loss': 1.033089748680169, 'epoch': 40.0}\n",
            "100% 5640/5640 [39:19<00:00,  2.39it/s]\n",
            "Saving model checkpoint to results/citation-intent_1/\n",
            "Configuration saved in results/citation-intent_1/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_1/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_1/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_1/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_1/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_1/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_1/config.json\n",
            "Model weights saved in results/citation-intent_1/pytorch_model.bin\n",
            "tokenizer config file saved in results/citation-intent_1/tokenizer_config.json\n",
            "Special tokens file saved in results/citation-intent_1/special_tokens_map.json\n",
            "08/03/2021 05:00:16 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            " 90% 9/10 [00:01<00:00,  6.59it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 10/10 [00:01<00:00,  7.21it/s]\n",
            "08/03/2021 05:00:18 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_loss = 0.8416489958763123\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_acc = 0.6929824561403509\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_f1 = 0.33874999999999994\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_precision = 0.3290521405963685\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_recall = 0.36117836965294586\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_runtime = 1.5358\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_samples_per_second = 74.226\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   eval_steps_per_second = 6.511\n",
            "08/03/2021 05:00:18 - INFO - __main__ -   epoch = 40.0\n",
            "08/03/2021 05:00:18 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 139\n",
            "  Batch size = 12\n",
            "100% 12/12 [00:01<00:00,  7.12it/s]\n",
            "08/03/2021 05:00:19 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_loss = 0.9337549209594727\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_acc = 0.6906474820143885\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_f1 = 0.40429407120298594\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_precision = 0.41074401046311165\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_recall = 0.4077103647526183\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_runtime = 1.845\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_samples_per_second = 75.339\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   eval_steps_per_second = 6.504\n",
            "08/03/2021 05:00:19 - INFO - __main__ -   epoch = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrmLxoHdi3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e190a4a-0e44-4a6a-8882-52cdec320e95"
      },
      "source": [
        "# Experiment_6_citation_intent_fusion_adapter\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/citation-intent_2/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/citation-intent/mlm \\\n",
        "--per_device_eval_batch_size 12 \\\n",
        "--weight_decay 0.1 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.95 \\\n",
        "--adam_epsilon 5e-4 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 1 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 09:11:10.065114: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 09:11:11 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1688\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5640\n",
            "  2% 141/5640 [00:53<31:54,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.65it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.74it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.17it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.72it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3717412948608398, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5099, 'eval_samples_per_second': 75.5, 'eval_steps_per_second': 6.623, 'epoch': 1.0}\n",
            "  2% 141/5640 [00:55<31:54,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-141\n",
            "Configuration saved in results/citation-intent_2/checkpoint-141/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-141/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-141/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-141/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-141/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-141/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-141/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-141/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-141/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-141/pytorch_model.bin\n",
            "  5% 282/5640 [01:52<31:06,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.69it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.77it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3389314413070679, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5013, 'eval_samples_per_second': 75.936, 'eval_steps_per_second': 6.661, 'epoch': 2.0}\n",
            "  5% 282/5640 [01:54<31:06,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-282\n",
            "Configuration saved in results/citation-intent_2/checkpoint-282/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-282/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-282/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-282/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-282/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-282/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-282/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-282/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-282/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-282/pytorch_model.bin\n",
            "  8% 423/5640 [02:52<30:17,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3248933553695679, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5014, 'eval_samples_per_second': 75.927, 'eval_steps_per_second': 6.66, 'epoch': 3.0}\n",
            "  8% 423/5640 [02:53<30:17,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-423\n",
            "Configuration saved in results/citation-intent_2/checkpoint-423/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-423/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-423/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-423/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-423/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-423/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-423/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-423/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-423/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-423/pytorch_model.bin\n",
            "{'loss': 1.4262, 'learning_rate': 3.645390070921986e-05, 'epoch': 3.55}\n",
            " 10% 564/5640 [03:51<29:25,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.76it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3161674737930298, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5034, 'eval_samples_per_second': 75.827, 'eval_steps_per_second': 6.651, 'epoch': 4.0}\n",
            " 10% 564/5640 [03:52<29:25,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-564\n",
            "Configuration saved in results/citation-intent_2/checkpoint-564/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-564/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-564/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-564/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-564/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-564/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-564/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-564/pytorch_model.bin\n",
            " 12% 705/5640 [04:50<28:35,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.69it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.76it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3035677671432495, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.4991, 'eval_samples_per_second': 76.045, 'eval_steps_per_second': 6.671, 'epoch': 5.0}\n",
            " 12% 705/5640 [04:52<28:35,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-705\n",
            "Configuration saved in results/citation-intent_2/checkpoint-705/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-705/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-705/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-705/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-705/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-705/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-705/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-705/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-705/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-705/pytorch_model.bin\n",
            " 15% 846/5640 [05:49<27:45,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.71it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.77it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.40it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.64it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.71it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.58it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2859352827072144, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5089, 'eval_samples_per_second': 75.552, 'eval_steps_per_second': 6.627, 'epoch': 6.0}\n",
            " 15% 846/5640 [05:51<27:45,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.58it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-846\n",
            "Configuration saved in results/citation-intent_2/checkpoint-846/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-846/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-846/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-846/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-846/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-846/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-846/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-846/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-846/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-846/pytorch_model.bin\n",
            " 18% 987/5640 [06:49<26:54,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.65it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.73it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.19it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.75it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2682538032531738, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.5036, 'eval_samples_per_second': 75.816, 'eval_steps_per_second': 6.651, 'epoch': 7.0}\n",
            " 18% 987/5640 [06:50<26:54,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-987\n",
            "Configuration saved in results/citation-intent_2/checkpoint-987/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-987/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-987/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-987/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-987/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-987/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-987/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-987/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-987/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-987/pytorch_model.bin\n",
            "{'loss': 1.326, 'learning_rate': 3.290780141843972e-05, 'epoch': 7.09}\n",
            " 20% 1128/5640 [07:49<26:06,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.72it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.78it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2440506219863892, 'eval_acc': 0.5263157894736842, 'eval_f1': 0.12957516339869282, 'eval_precision': 0.14414414414414414, 'eval_recall': 0.17460317460317462, 'eval_runtime': 1.496, 'eval_samples_per_second': 76.205, 'eval_steps_per_second': 6.685, 'epoch': 8.0}\n",
            " 20% 1128/5640 [07:50<26:06,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1128\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1128/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1128/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1128/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1128/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1128/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1128/pytorch_model.bin\n",
            " 22% 1269/5640 [08:49<25:21,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.68it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.75it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.32it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.57it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.14it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.86it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.71it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1816341876983643, 'eval_acc': 0.5526315789473685, 'eval_f1': 0.18195902909278705, 'eval_precision': 0.1681547619047619, 'eval_recall': 0.2137476459510358, 'eval_runtime': 1.513, 'eval_samples_per_second': 75.346, 'eval_steps_per_second': 6.609, 'epoch': 9.0}\n",
            " 22% 1269/5640 [08:50<25:21,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1269\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1269/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1269/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1269/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1269/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1269/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1269/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1269/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1269/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1269/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1269/pytorch_model.bin\n",
            " 25% 1410/5640 [09:49<24:35,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.57it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.67it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.34it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.61it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.72it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1310482025146484, 'eval_acc': 0.5614035087719298, 'eval_f1': 0.19299643976831926, 'eval_precision': 0.1725177304964539, 'eval_recall': 0.22679580306698952, 'eval_runtime': 1.5114, 'eval_samples_per_second': 75.427, 'eval_steps_per_second': 6.616, 'epoch': 10.0}\n",
            " 25% 1410/5640 [09:50<24:35,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1410\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1410/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1410/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1410/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1410/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1410/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1410/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1410/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1410/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1410/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1410/pytorch_model.bin\n",
            "{'loss': 1.2181, 'learning_rate': 2.9361702127659577e-05, 'epoch': 10.64}\n",
            " 28% 1551/5640 [10:48<23:46,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.090915560722351, 'eval_acc': 0.5877192982456141, 'eval_f1': 0.2108303956130043, 'eval_precision': 0.18299625468164793, 'eval_recall': 0.2506053268765133, 'eval_runtime': 1.5, 'eval_samples_per_second': 76.002, 'eval_steps_per_second': 6.667, 'epoch': 11.0}\n",
            " 28% 1551/5640 [10:49<23:46,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1551\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1551/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1551/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1551/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1551/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1551/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1551/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1551/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1551/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1551/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1551/pytorch_model.bin\n",
            " 30% 1692/5640 [11:47<22:52,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.67it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0675526857376099, 'eval_acc': 0.6491228070175439, 'eval_f1': 0.27716024582235915, 'eval_precision': 0.28935185185185186, 'eval_recall': 0.3163841807909604, 'eval_runtime': 1.4987, 'eval_samples_per_second': 76.068, 'eval_steps_per_second': 6.673, 'epoch': 12.0}\n",
            " 30% 1692/5640 [11:48<22:52,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1692\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1692/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1692/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1692/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1692/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1692/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1692/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1692/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1692/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1692/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1692/pytorch_model.bin\n",
            " 32% 1833/5640 [12:47<21:59,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.77it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0444824695587158, 'eval_acc': 0.6578947368421053, 'eval_f1': 0.28962195565249, 'eval_precision': 0.30363756613756615, 'eval_recall': 0.3243206887274684, 'eval_runtime': 1.4955, 'eval_samples_per_second': 76.231, 'eval_steps_per_second': 6.687, 'epoch': 13.0}\n",
            " 32% 1833/5640 [12:48<21:59,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1833\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1833/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1833/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1833/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1833/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1833/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1833/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1833/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1833/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1833/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1833/pytorch_model.bin\n",
            " 35% 1974/5640 [13:47<21:09,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.68it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.76it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.39it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.64it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.19it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.72it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.015601396560669, 'eval_acc': 0.6403508771929824, 'eval_f1': 0.2634026715905484, 'eval_precision': 0.3653846153846154, 'eval_recall': 0.3033360236750067, 'eval_runtime': 1.5073, 'eval_samples_per_second': 75.63, 'eval_steps_per_second': 6.634, 'epoch': 14.0}\n",
            " 35% 1974/5640 [13:48<21:09,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-1974\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1974/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1974/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1974/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1974/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1974/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1974/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1974/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-1974/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-1974/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-1974/pytorch_model.bin\n",
            "{'loss': 1.0893, 'learning_rate': 2.5815602836879437e-05, 'epoch': 14.18}\n",
            " 38% 2115/5640 [14:47<20:27,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.00232994556427, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.30966305244884595, 'eval_precision': 0.37729978354978355, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.4985, 'eval_samples_per_second': 76.077, 'eval_steps_per_second': 6.673, 'epoch': 15.0}\n",
            " 38% 2115/5640 [14:48<20:27,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2115\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2115/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2115/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2115/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2115/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2115/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2115/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2115/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2115/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2115/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2115/pytorch_model.bin\n",
            " 40% 2256/5640 [15:46<19:37,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.66it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.72it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.71it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.56it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0003066062927246, 'eval_acc': 0.6578947368421053, 'eval_f1': 0.29118137498249763, 'eval_precision': 0.30463320463320465, 'eval_recall': 0.3294323379069142, 'eval_runtime': 1.5132, 'eval_samples_per_second': 75.336, 'eval_steps_per_second': 6.608, 'epoch': 16.0}\n",
            " 40% 2256/5640 [15:47<19:37,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.56it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2256\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2256/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2256/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2256/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2256/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2256/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2256/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2256/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2256/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2256/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2256/pytorch_model.bin\n",
            " 42% 2397/5640 [16:45<18:52,  2.86it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.44it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.64it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.72it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.59it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9738718867301941, 'eval_acc': 0.6666666666666666, 'eval_f1': 0.29572649572649573, 'eval_precision': 0.34266347687400317, 'eval_recall': 0.3271455474845305, 'eval_runtime': 1.5156, 'eval_samples_per_second': 75.218, 'eval_steps_per_second': 6.598, 'epoch': 17.0}\n",
            " 42% 2397/5640 [16:47<18:52,  2.86it/s]\n",
            "100% 10/10 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2397\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2397/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2397/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2397/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2397/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2397/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2397/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2397/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2397/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2397/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2397/pytorch_model.bin\n",
            "{'loss': 1.0334, 'learning_rate': 2.226950354609929e-05, 'epoch': 17.73}\n",
            " 45% 2538/5640 [17:45<18:02,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.58it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.68it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9587441086769104, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.30824175824175826, 'eval_precision': 0.3342428652275851, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.5038, 'eval_samples_per_second': 75.809, 'eval_steps_per_second': 6.65, 'epoch': 18.0}\n",
            " 45% 2538/5640 [17:46<18:02,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2538\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2538/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2538/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2538/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2538/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2538/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2538/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2538/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2538/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2538/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2538/pytorch_model.bin\n",
            " 48% 2679/5640 [18:44<17:12,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.76it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.79it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.67it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9485402703285217, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3105042016806723, 'eval_precision': 0.337468671679198, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.4989, 'eval_samples_per_second': 76.055, 'eval_steps_per_second': 6.671, 'epoch': 19.0}\n",
            " 48% 2679/5640 [18:45<17:12,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2679\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2679/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2679/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2679/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2679/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2679/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2679/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2679/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2679/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2679/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2679/pytorch_model.bin\n",
            " 50% 2820/5640 [19:43<16:17,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9433636665344238, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.32691969288618455, 'eval_precision': 0.33643578643578637, 'eval_recall': 0.35324186171643795, 'eval_runtime': 1.4953, 'eval_samples_per_second': 76.241, 'eval_steps_per_second': 6.688, 'epoch': 20.0}\n",
            " 50% 2820/5640 [19:44<16:17,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2820\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2820/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2820/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2820/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2820/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2820/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2820/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2820/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2820/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2820/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2820/pytorch_model.bin\n",
            " 52% 2961/5640 [20:44<15:33,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.62it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.70it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.64it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.74it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.62it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9222851991653442, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.321543057426533, 'eval_precision': 0.31914983164983163, 'eval_recall': 0.3481302125369922, 'eval_runtime': 1.5085, 'eval_samples_per_second': 75.57, 'eval_steps_per_second': 6.629, 'epoch': 21.0}\n",
            " 52% 2961/5640 [20:45<15:33,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-2961\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2961/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2961/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2961/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2961/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2961/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2961/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2961/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-2961/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-2961/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-2961/pytorch_model.bin\n",
            "{'loss': 0.9699, 'learning_rate': 1.872340425531915e-05, 'epoch': 21.28}\n",
            " 55% 3102/5640 [21:45<14:42,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.78it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.22it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.73it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.61it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9251170754432678, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3064102564102564, 'eval_precision': 0.35390492359932085, 'eval_recall': 0.33508205542103847, 'eval_runtime': 1.5036, 'eval_samples_per_second': 75.818, 'eval_steps_per_second': 6.651, 'epoch': 22.0}\n",
            " 55% 3102/5640 [21:46<14:42,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.61it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3102\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3102/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3102/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3102/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3102/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3102/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3102/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3102/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3102/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3102/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3102/pytorch_model.bin\n",
            " 57% 3243/5640 [22:45<13:50,  2.89it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.70it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.75it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.17it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.88it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.69it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.59it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9077987670898438, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.31821036106750394, 'eval_precision': 0.3089369550637156, 'eval_recall': 0.3401937046004843, 'eval_runtime': 1.512, 'eval_samples_per_second': 75.396, 'eval_steps_per_second': 6.614, 'epoch': 23.0}\n",
            " 57% 3243/5640 [22:46<13:50,  2.89it/s]\n",
            "100% 10/10 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3243\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3243/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3243/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3243/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3243/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3243/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3243/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3243/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3243/pytorch_model.bin\n",
            " 60% 3384/5640 [23:44<13:02,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.79it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.78it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.66it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.73it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8980438709259033, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.3307875131404543, 'eval_precision': 0.3271909233176839, 'eval_recall': 0.3560667204735002, 'eval_runtime': 1.5058, 'eval_samples_per_second': 75.707, 'eval_steps_per_second': 6.641, 'epoch': 24.0}\n",
            " 60% 3384/5640 [23:46<13:02,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3384\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3384/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3384/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3384/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3384/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3384/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3384/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3384/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3384/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3384/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3384/pytorch_model.bin\n",
            "{'loss': 0.9438, 'learning_rate': 1.5177304964539008e-05, 'epoch': 24.82}\n",
            " 62% 3525/5640 [24:44<12:13,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.79it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.21it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.73it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8900171518325806, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.32686886756654193, 'eval_precision': 0.32318228366615465, 'eval_recall': 0.35324186171643795, 'eval_runtime': 1.504, 'eval_samples_per_second': 75.796, 'eval_steps_per_second': 6.649, 'epoch': 25.0}\n",
            " 62% 3525/5640 [24:46<12:13,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3525\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3525/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3525/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3525/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3525/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3525/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3525/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3525/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3525/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3525/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3525/pytorch_model.bin\n",
            " 65% 3666/5640 [25:44<11:29,  2.86it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.73it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.73it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.38it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8811270594596863, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.31380700428319475, 'eval_precision': 0.32034948482316905, 'eval_recall': 0.3379069141781006, 'eval_runtime': 1.5029, 'eval_samples_per_second': 75.854, 'eval_steps_per_second': 6.654, 'epoch': 26.0}\n",
            " 65% 3666/5640 [25:45<11:29,  2.86it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3666\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3666/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3666/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3666/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3666/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3666/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3666/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3666/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3666/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3666/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3666/pytorch_model.bin\n",
            " 68% 3807/5640 [26:43<10:36,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.79it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.74it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.62it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8995527029037476, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.31837990944276945, 'eval_precision': 0.32276785714285716, 'eval_recall': 0.34530535377993005, 'eval_runtime': 1.5023, 'eval_samples_per_second': 75.882, 'eval_steps_per_second': 6.656, 'epoch': 27.0}\n",
            " 68% 3807/5640 [26:45<10:36,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3807\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3807/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3807/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3807/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3807/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3807/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3807/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3807/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3807/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3807/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3807/pytorch_model.bin\n",
            " 70% 3948/5640 [27:43<09:50,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.79it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.78it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8953166604042053, 'eval_acc': 0.6666666666666666, 'eval_f1': 0.314809355345912, 'eval_precision': 0.3130558300395257, 'eval_recall': 0.3424804950228679, 'eval_runtime': 1.4964, 'eval_samples_per_second': 76.183, 'eval_steps_per_second': 6.683, 'epoch': 28.0}\n",
            " 70% 3948/5640 [27:44<09:50,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-3948\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3948/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3948/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3948/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3948/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3948/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3948/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3948/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-3948/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-3948/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-3948/pytorch_model.bin\n",
            "{'loss': 0.899, 'learning_rate': 1.1631205673758865e-05, 'epoch': 28.37}\n",
            " 72% 4089/5640 [28:42<09:00,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.55it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.65it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.35it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.22it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8765608072280884, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.32897873789714394, 'eval_precision': 0.31153846153846154, 'eval_recall': 0.3555286521388216, 'eval_runtime': 1.5068, 'eval_samples_per_second': 75.656, 'eval_steps_per_second': 6.636, 'epoch': 29.0}\n",
            " 72% 4089/5640 [28:43<09:00,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4089\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4089/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4089/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4089/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4089/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4089/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4089/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4089/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4089/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4089/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4089/pytorch_model.bin\n",
            " 75% 4230/5640 [29:41<08:11,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.78it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.40it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.70it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.55it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8722476363182068, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.320641434868521, 'eval_precision': 0.31355311355311355, 'eval_recall': 0.34530535377993005, 'eval_runtime': 1.5137, 'eval_samples_per_second': 75.315, 'eval_steps_per_second': 6.607, 'epoch': 30.0}\n",
            " 75% 4230/5640 [29:42<08:11,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.55it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4230\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4230/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4230/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4230/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4230/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4230/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4230/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4230/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4230/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4230/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4230/pytorch_model.bin\n",
            " 78% 4371/5640 [30:40<07:22,  2.86it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.65it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.73it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.17it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.88it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.69it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.56it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8712178468704224, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.33774193548387094, 'eval_precision': 0.31937270231281123, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.514, 'eval_samples_per_second': 75.296, 'eval_steps_per_second': 6.605, 'epoch': 31.0}\n",
            " 78% 4371/5640 [30:41<07:22,  2.86it/s]\n",
            "100% 10/10 [00:01<00:00,  6.56it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4371\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4371/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4371/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4371/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4371/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4371/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4371/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4371/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4371/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4371/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4371/pytorch_model.bin\n",
            "{'loss': 0.8796, 'learning_rate': 8.085106382978723e-06, 'epoch': 31.91}\n",
            " 80% 4512/5640 [31:39<06:34,  2.86it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.63it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.72it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.38it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.64it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.75it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8658050298690796, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.3336487748252454, 'eval_precision': 0.320615671641791, 'eval_recall': 0.35835351089588374, 'eval_runtime': 1.5048, 'eval_samples_per_second': 75.758, 'eval_steps_per_second': 6.645, 'epoch': 32.0}\n",
            " 80% 4512/5640 [31:41<06:34,  2.86it/s]\n",
            "100% 10/10 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4512\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4512/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4512/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4512/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4512/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4512/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4512/pytorch_model.bin\n",
            " 82% 4653/5640 [32:38<05:43,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.77it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.65it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8560283184051514, 'eval_acc': 0.7017543859649122, 'eval_f1': 0.346573029722636, 'eval_precision': 0.3350659229208925, 'eval_recall': 0.3691148775894539, 'eval_runtime': 1.4978, 'eval_samples_per_second': 76.114, 'eval_steps_per_second': 6.677, 'epoch': 33.0}\n",
            " 82% 4653/5640 [32:40<05:43,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4653\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4653/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4653/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4653/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4653/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4653/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4653/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4653/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4653/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4653/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4653/pytorch_model.bin\n",
            " 85% 4794/5640 [33:37<04:54,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.82it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8502198457717896, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.33774193548387094, 'eval_precision': 0.31937270231281123, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.4986, 'eval_samples_per_second': 76.072, 'eval_steps_per_second': 6.673, 'epoch': 34.0}\n",
            " 85% 4794/5640 [33:39<04:54,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4794\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4794/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4794/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4794/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4794/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4794/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4794/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4794/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4794/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4794/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4794/pytorch_model.bin\n",
            " 88% 4935/5640 [34:37<04:04,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.74it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.80it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.75it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8491584658622742, 'eval_acc': 0.7017543859649122, 'eval_f1': 0.346573029722636, 'eval_precision': 0.3350659229208925, 'eval_recall': 0.3691148775894539, 'eval_runtime': 1.499, 'eval_samples_per_second': 76.049, 'eval_steps_per_second': 6.671, 'epoch': 35.0}\n",
            " 88% 4935/5640 [34:38<04:04,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-4935\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4935/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4935/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4935/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4935/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4935/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4935/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4935/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-4935/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-4935/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-4935/pytorch_model.bin\n",
            "{'loss': 0.8564, 'learning_rate': 4.539007092198582e-06, 'epoch': 35.46}\n",
            " 90% 5076/5640 [35:36<03:15,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.81it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.84it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.75it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.62it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8476404547691345, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.4996, 'eval_samples_per_second': 76.02, 'eval_steps_per_second': 6.668, 'epoch': 36.0}\n",
            " 90% 5076/5640 [35:38<03:15,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-5076\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5076/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5076/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5076/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5076/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5076/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5076/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5076/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5076/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5076/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-5076/pytorch_model.bin\n",
            " 92% 5217/5640 [36:35<02:27,  2.86it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.66it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.74it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.39it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.19it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.71it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8533732891082764, 'eval_acc': 0.6842105263157895, 'eval_f1': 0.3388034188034188, 'eval_precision': 0.32230349471728786, 'eval_recall': 0.3634651600753296, 'eval_runtime': 1.5078, 'eval_samples_per_second': 75.609, 'eval_steps_per_second': 6.632, 'epoch': 37.0}\n",
            " 92% 5217/5640 [36:37<02:27,  2.86it/s]\n",
            "100% 10/10 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-5217\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5217/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5217/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5217/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5217/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5217/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5217/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5217/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5217/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5217/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-5217/pytorch_model.bin\n",
            " 95% 5358/5640 [37:35<01:38,  2.87it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.71it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.79it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.23it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.76it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.64it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8498514294624329, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.3426399331662489, 'eval_precision': 0.3283531631901346, 'eval_recall': 0.3662900188323917, 'eval_runtime': 1.5014, 'eval_samples_per_second': 75.928, 'eval_steps_per_second': 6.66, 'epoch': 38.0}\n",
            " 95% 5358/5640 [37:36<01:38,  2.87it/s]\n",
            "100% 10/10 [00:01<00:00,  6.64it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-5358\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5358/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5358/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5358/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5358/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5358/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5358/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5358/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5358/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5358/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-5358/pytorch_model.bin\n",
            " 98% 5499/5640 [38:34<00:48,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.75it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.81it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.66it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.72it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.59it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8477824926376343, 'eval_acc': 0.7017543859649122, 'eval_f1': 0.346573029722636, 'eval_precision': 0.3350659229208925, 'eval_recall': 0.3691148775894539, 'eval_runtime': 1.5075, 'eval_samples_per_second': 75.623, 'eval_steps_per_second': 6.634, 'epoch': 39.0}\n",
            " 98% 5499/5640 [38:36<00:48,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-5499\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5499/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5499/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5499/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5499/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5499/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5499/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5499/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5499/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5499/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-5499/pytorch_model.bin\n",
            "{'loss': 0.8543, 'learning_rate': 9.929078014184399e-07, 'epoch': 39.01}\n",
            "100% 5640/5640 [39:33<00:00,  2.88it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:00<00:00, 12.69it/s]\u001b[A\n",
            " 30% 3/10 [00:00<00:00,  9.74it/s]\u001b[A\n",
            " 40% 4/10 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 50% 5/10 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 60% 6/10 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 70% 7/10 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 80% 8/10 [00:01<00:00,  6.68it/s]\u001b[A\n",
            " 90% 9/10 [00:01<00:00,  6.55it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8468650579452515, 'eval_acc': 0.6929824561403509, 'eval_f1': 0.33874999999999994, 'eval_precision': 0.3290521405963685, 'eval_recall': 0.36117836965294586, 'eval_runtime': 1.5155, 'eval_samples_per_second': 75.224, 'eval_steps_per_second': 6.599, 'epoch': 40.0}\n",
            "100% 5640/5640 [39:35<00:00,  2.88it/s]\n",
            "100% 10/10 [00:01<00:00,  6.55it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/citation-intent_2/checkpoint-5640\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5640/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5640/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5640/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/checkpoint-5640/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/checkpoint-5640/config.json\n",
            "Model weights saved in results/citation-intent_2/checkpoint-5640/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/citation-intent_2/checkpoint-5640 (score: 0.8468650579452515).\n",
            "Loading best adapter(s) from results/citation-intent_2/checkpoint-5640 (score: 0.8468650579452515).\n",
            "Loading module configuration from results/citation-intent_2/checkpoint-5640/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/citation-intent_2/checkpoint-5640/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/citation-intent_2/checkpoint-5640/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/citation-intent_2/checkpoint-5640/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/citation-intent_2/checkpoint-5640 (score: 0.8468650579452515).\n",
            "Loading module configuration from results/citation-intent_2/checkpoint-5640/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/citation-intent_2/checkpoint-5640/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 2380.6001, 'train_samples_per_second': 28.363, 'train_steps_per_second': 2.369, 'train_loss': 1.0400364057392093, 'epoch': 40.0}\n",
            "100% 5640/5640 [39:40<00:00,  2.37it/s]\n",
            "Saving model checkpoint to results/citation-intent_2/\n",
            "Configuration saved in results/citation-intent_2/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_2/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_2/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_2/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_2/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_2/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_2/config.json\n",
            "Model weights saved in results/citation-intent_2/pytorch_model.bin\n",
            "tokenizer config file saved in results/citation-intent_2/tokenizer_config.json\n",
            "Special tokens file saved in results/citation-intent_2/special_tokens_map.json\n",
            "08/03/2021 09:51:12 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 12\n",
            " 90% 9/10 [00:01<00:00,  6.59it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 10/10 [00:01<00:00,  7.33it/s]\n",
            "08/03/2021 09:51:13 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_loss = 0.8468650579452515\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_acc = 0.6929824561403509\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_f1 = 0.33874999999999994\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_precision = 0.3290521405963685\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_recall = 0.36117836965294586\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_runtime = 1.5455\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_samples_per_second = 73.761\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   eval_steps_per_second = 6.47\n",
            "08/03/2021 09:51:13 - INFO - __main__ -   epoch = 40.0\n",
            "08/03/2021 09:51:13 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 139\n",
            "  Batch size = 12\n",
            "100% 12/12 [00:01<00:00,  6.98it/s]\n",
            "08/03/2021 09:51:15 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_loss = 0.933219850063324\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_acc = 0.6834532374100719\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_f1 = 0.40128750820574727\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_precision = 0.40535999088630664\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_recall = 0.40536294691224267\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_runtime = 1.8827\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_samples_per_second = 73.829\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   eval_steps_per_second = 6.374\n",
            "08/03/2021 09:51:15 - INFO - __main__ -   epoch = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TBgz9CJ783a",
        "outputId": "c37499da-adcb-4556-a302-dbe375d33d89"
      },
      "source": [
        "# Experiment_7_citation_intent_fusion_adapter\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 15 \\\n",
        "--gradient_accumulation_steps 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/citation-intent_3/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/citation-intent/mlm \\\n",
        "--adapter_2 results/adapters/sciie/mlm \\\n",
        "--per_device_eval_batch_size 15 \\\n",
        "--weight_decay 0.0001 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.97 \\\n",
        "--adam_epsilon 5e-5 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 836 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 13:25:25.218714: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 13:25:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1688\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 15\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 30\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2240\n",
            "  2% 56/2240 [00:50<33:03,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.3320956230163574, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6047, 'eval_samples_per_second': 71.041, 'eval_steps_per_second': 4.985, 'epoch': 0.99}\n",
            "  2% 56/2240 [00:52<33:03,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-56\n",
            "Configuration saved in results/citation-intent_3/checkpoint-56/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-56/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-56/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-56/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-56/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-56/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-56/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-56/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-56/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-56/pytorch_model.bin\n",
            "  5% 112/2240 [01:48<32:10,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.291805386543274, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6033, 'eval_samples_per_second': 71.103, 'eval_steps_per_second': 4.99, 'epoch': 1.99}\n",
            "  5% 112/2240 [01:49<32:10,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-112\n",
            "Configuration saved in results/citation-intent_3/checkpoint-112/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-112/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-112/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-112/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-112/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-112/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-112/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-112/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-112/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-112/pytorch_model.bin\n",
            "  8% 168/2240 [02:45<31:21,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.237970232963562, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6056, 'eval_samples_per_second': 71.003, 'eval_steps_per_second': 4.983, 'epoch': 2.99}\n",
            "  8% 168/2240 [02:47<31:21,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-168\n",
            "Configuration saved in results/citation-intent_3/checkpoint-168/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-168/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-168/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-168/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-168/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-168/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-168/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-168/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-168/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-168/pytorch_model.bin\n",
            " 10% 224/2240 [03:42<30:28,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.54it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.1113290786743164, 'eval_acc': 0.5964912280701754, 'eval_f1': 0.21504329004329006, 'eval_precision': 0.19649122807017547, 'eval_recall': 0.24831853645412968, 'eval_runtime': 1.6024, 'eval_samples_per_second': 71.144, 'eval_steps_per_second': 4.993, 'epoch': 3.99}\n",
            " 10% 224/2240 [03:44<30:28,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-224\n",
            "Configuration saved in results/citation-intent_3/checkpoint-224/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-224/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-224/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-224/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-224/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-224/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-224/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-224/pytorch_model.bin\n",
            " 12% 280/2240 [04:38<29:38,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.54it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0133455991744995, 'eval_acc': 0.6666666666666666, 'eval_f1': 0.2805091733663162, 'eval_precision': 0.3369016249451032, 'eval_recall': 0.3066989507667474, 'eval_runtime': 1.6018, 'eval_samples_per_second': 71.168, 'eval_steps_per_second': 4.994, 'epoch': 4.99}\n",
            " 12% 280/2240 [04:40<29:38,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-280\n",
            "Configuration saved in results/citation-intent_3/checkpoint-280/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-280/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-280/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-280/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-280/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-280/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-280/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-280/pytorch_model.bin\n",
            " 15% 336/2240 [05:35<28:48,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.35it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.98it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9637489318847656, 'eval_acc': 0.7105263157894737, 'eval_f1': 0.3462835711948451, 'eval_precision': 0.3427807486631016, 'eval_recall': 0.35660478880817864, 'eval_runtime': 1.6011, 'eval_samples_per_second': 71.2, 'eval_steps_per_second': 4.996, 'epoch': 5.99}\n",
            " 15% 336/2240 [05:36<28:48,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-336\n",
            "Configuration saved in results/citation-intent_3/checkpoint-336/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-336/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-336/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-336/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-336/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-336/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-336/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-336/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-336/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-336/pytorch_model.bin\n",
            " 18% 392/2240 [06:31<27:57,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9191855192184448, 'eval_acc': 0.7017543859649122, 'eval_f1': 0.3416117216117216, 'eval_precision': 0.3350042785448322, 'eval_recall': 0.36400322841000804, 'eval_runtime': 1.6036, 'eval_samples_per_second': 71.09, 'eval_steps_per_second': 4.989, 'epoch': 6.99}\n",
            " 18% 392/2240 [06:33<27:57,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-392\n",
            "Configuration saved in results/citation-intent_3/checkpoint-392/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-392/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-392/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-392/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-392/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-392/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-392/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-392/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-392/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-392/pytorch_model.bin\n",
            " 20% 448/2240 [07:27<27:06,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8958152532577515, 'eval_acc': 0.7105263157894737, 'eval_f1': 0.3455678454587942, 'eval_precision': 0.33928571428571425, 'eval_recall': 0.36682808716707016, 'eval_runtime': 1.6061, 'eval_samples_per_second': 70.981, 'eval_steps_per_second': 4.981, 'epoch': 7.99}\n",
            " 20% 448/2240 [07:29<27:06,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-448\n",
            "Configuration saved in results/citation-intent_3/checkpoint-448/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-448/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-448/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-448/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-448/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-448/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-448/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-448/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-448/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-448/pytorch_model.bin\n",
            "{'loss': 1.181, 'learning_rate': 3.107142857142858e-05, 'epoch': 8.92}\n",
            " 22% 504/2240 [08:24<26:17,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9332170486450195, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.31105651105651105, 'eval_precision': 0.3311531007751938, 'eval_recall': 0.3146354587032553, 'eval_runtime': 1.6008, 'eval_samples_per_second': 71.213, 'eval_steps_per_second': 4.997, 'epoch': 8.99}\n",
            " 22% 504/2240 [08:25<26:17,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-504\n",
            "Configuration saved in results/citation-intent_3/checkpoint-504/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-504/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-504/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-504/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-504/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-504/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-504/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-504/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-504/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-504/pytorch_model.bin\n",
            " 25% 560/2240 [09:20<25:25,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8677116632461548, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.3622655062853985, 'eval_precision': 0.3469405003380663, 'eval_recall': 0.3878127522195319, 'eval_runtime': 1.6034, 'eval_samples_per_second': 71.098, 'eval_steps_per_second': 4.989, 'epoch': 9.99}\n",
            " 25% 560/2240 [09:21<25:25,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-560\n",
            "Configuration saved in results/citation-intent_3/checkpoint-560/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-560/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-560/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-560/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-560/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-560/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-560/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-560/pytorch_model.bin\n",
            " 28% 616/2240 [10:25<24:34,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8117824196815491, 'eval_acc': 0.7192982456140351, 'eval_f1': 0.36272397620377506, 'eval_precision': 0.3535714285714286, 'eval_recall': 0.3747645951035781, 'eval_runtime': 1.6042, 'eval_samples_per_second': 71.062, 'eval_steps_per_second': 4.987, 'epoch': 10.99}\n",
            " 28% 616/2240 [10:27<24:34,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-616\n",
            "Configuration saved in results/citation-intent_3/checkpoint-616/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-616/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-616/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-616/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-616/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-616/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-616/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-616/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-616/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-616/pytorch_model.bin\n",
            " 30% 672/2240 [11:21<23:43,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.52it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7937497496604919, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.4329602239547811, 'eval_precision': 0.525206043956044, 'eval_recall': 0.4243677697067527, 'eval_runtime': 1.6039, 'eval_samples_per_second': 71.075, 'eval_steps_per_second': 4.988, 'epoch': 11.99}\n",
            " 30% 672/2240 [11:23<23:43,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-672\n",
            "Configuration saved in results/citation-intent_3/checkpoint-672/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-672/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-672/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-672/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-672/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-672/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-672/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-672/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-672/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-672/pytorch_model.bin\n",
            " 32% 728/2240 [12:18<22:52,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7964696884155273, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.42538037218888286, 'eval_precision': 0.5199229565426748, 'eval_recall': 0.4164312617702448, 'eval_runtime': 1.6024, 'eval_samples_per_second': 71.143, 'eval_steps_per_second': 4.992, 'epoch': 12.99}\n",
            " 32% 728/2240 [12:19<22:52,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-728\n",
            "Configuration saved in results/citation-intent_3/checkpoint-728/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-728/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-728/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-728/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-728/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-728/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-728/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-728/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-728/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-728/pytorch_model.bin\n",
            " 35% 784/2240 [13:14<22:03,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7842170596122742, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.4289562289562289, 'eval_precision': 0.5260654490106544, 'eval_recall': 0.4164312617702448, 'eval_runtime': 1.6048, 'eval_samples_per_second': 71.036, 'eval_steps_per_second': 4.985, 'epoch': 13.99}\n",
            " 35% 784/2240 [13:16<22:03,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-784\n",
            "Configuration saved in results/citation-intent_3/checkpoint-784/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-784/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-784/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-784/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-784/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-784/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-784/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-784/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-784/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-784/pytorch_model.bin\n",
            " 38% 840/2240 [14:10<21:11,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7583370804786682, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.4510002500625156, 'eval_precision': 0.5336247086247087, 'eval_recall': 0.4453524347592144, 'eval_runtime': 1.6062, 'eval_samples_per_second': 70.976, 'eval_steps_per_second': 4.981, 'epoch': 14.99}\n",
            " 38% 840/2240 [14:12<21:11,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-840\n",
            "Configuration saved in results/citation-intent_3/checkpoint-840/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-840/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-840/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-840/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-840/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-840/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-840/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-840/pytorch_model.bin\n",
            " 40% 896/2240 [15:17<20:21,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.54it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7753779292106628, 'eval_acc': 0.7456140350877193, 'eval_f1': 0.4893156506315894, 'eval_precision': 0.5811237373737373, 'eval_recall': 0.476257734732311, 'eval_runtime': 1.6022, 'eval_samples_per_second': 71.151, 'eval_steps_per_second': 4.993, 'epoch': 15.99}\n",
            " 40% 896/2240 [15:19<20:21,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-896\n",
            "Configuration saved in results/citation-intent_3/checkpoint-896/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-896/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-896/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-896/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-896/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-896/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-896/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-896/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-896/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-896/pytorch_model.bin\n",
            " 42% 952/2240 [16:14<19:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7570986747741699, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.5066257730548536, 'eval_precision': 0.6153554175293307, 'eval_recall': 0.48930589184826473, 'eval_runtime': 1.6026, 'eval_samples_per_second': 71.134, 'eval_steps_per_second': 4.992, 'epoch': 16.99}\n",
            " 42% 952/2240 [16:16<19:29,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-952\n",
            "Configuration saved in results/citation-intent_3/checkpoint-952/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-952/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-952/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-952/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-952/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-952/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-952/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-952/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-952/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-952/pytorch_model.bin\n",
            "{'loss': 0.802, 'learning_rate': 2.2142857142857145e-05, 'epoch': 17.85}\n",
            " 45% 1008/2240 [17:10<18:39,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7965786457061768, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.5183660130718953, 'eval_precision': 0.5619278080258117, 'eval_recall': 0.5094498251277912, 'eval_runtime': 1.6054, 'eval_samples_per_second': 71.01, 'eval_steps_per_second': 4.983, 'epoch': 17.99}\n",
            " 45% 1008/2240 [17:12<18:39,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1008\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1008/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1008/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1008/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1008/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1008/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1008/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1008/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1008/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1008/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1008/pytorch_model.bin\n",
            " 48% 1064/2240 [18:07<17:49,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7404481768608093, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.44399951899951895, 'eval_precision': 0.5357309562029668, 'eval_recall': 0.42947941888619856, 'eval_runtime': 1.6052, 'eval_samples_per_second': 71.02, 'eval_steps_per_second': 4.984, 'epoch': 18.99}\n",
            " 48% 1064/2240 [18:08<17:49,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1064\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1064/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1064/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1064/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1064/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1064/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1064/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1064/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1064/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1064/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1064/pytorch_model.bin\n",
            " 50% 1120/2240 [19:03<16:58,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7187978029251099, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.5477204860642183, 'eval_precision': 0.7106286748077792, 'eval_recall': 0.5040825934893731, 'eval_runtime': 1.6054, 'eval_samples_per_second': 71.009, 'eval_steps_per_second': 4.983, 'epoch': 19.99}\n",
            " 50% 1120/2240 [19:05<16:58,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1120\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1120/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1120/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1120/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1120/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1120/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1120/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1120/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1120/pytorch_model.bin\n",
            " 52% 1176/2240 [20:09<16:07,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7563660740852356, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.5257590841214534, 'eval_precision': 0.5743104345563362, 'eval_recall': 0.5122746838848534, 'eval_runtime': 1.6037, 'eval_samples_per_second': 71.084, 'eval_steps_per_second': 4.988, 'epoch': 20.99}\n",
            " 52% 1176/2240 [20:11<16:07,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1176\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1176/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1176/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1176/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1176/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1176/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1176/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1176/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1176/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1176/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1176/pytorch_model.bin\n",
            " 55% 1232/2240 [21:06<15:17,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7333704233169556, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6348128726425234, 'eval_precision': 0.7604901141743247, 'eval_recall': 0.5845910680656443, 'eval_runtime': 1.6063, 'eval_samples_per_second': 70.969, 'eval_steps_per_second': 4.98, 'epoch': 21.99}\n",
            " 55% 1232/2240 [21:08<15:17,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1232\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1232/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1232/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1232/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1232/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1232/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1232/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1232/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1232/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1232/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1232/pytorch_model.bin\n",
            " 57% 1288/2240 [22:03<14:25,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7227699756622314, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.5860387315053276, 'eval_precision': 0.7027850877192984, 'eval_recall': 0.5429244013989777, 'eval_runtime': 1.6045, 'eval_samples_per_second': 71.049, 'eval_steps_per_second': 4.986, 'epoch': 22.99}\n",
            " 57% 1288/2240 [22:05<14:25,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1288\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1288/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1288/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1288/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1288/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1288/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1288/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1288/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1288/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1288/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1288/pytorch_model.bin\n",
            " 60% 1344/2240 [23:01<13:33,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7580123543739319, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6285576529977427, 'eval_precision': 0.6830316742081447, 'eval_recall': 0.6071630347054076, 'eval_runtime': 1.6058, 'eval_samples_per_second': 70.994, 'eval_steps_per_second': 4.982, 'epoch': 23.99}\n",
            " 60% 1344/2240 [23:02<13:33,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1344\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1344/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1344/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1344/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1344/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1344/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1344/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1344/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1344/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1344/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1344/pytorch_model.bin\n",
            " 62% 1400/2240 [23:57<12:45,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.52it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7310490012168884, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6304570295382675, 'eval_precision': 0.7554053661863508, 'eval_recall': 0.5817662093085821, 'eval_runtime': 1.6058, 'eval_samples_per_second': 70.994, 'eval_steps_per_second': 4.982, 'epoch': 24.99}\n",
            " 62% 1400/2240 [23:59<12:45,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1400\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1400/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1400/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1400/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1400/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1400/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1400/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1400/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1400/pytorch_model.bin\n",
            " 65% 1456/2240 [25:03<11:52,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7122798562049866, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.5972558566638905, 'eval_precision': 0.6463506028022157, 'eval_recall': 0.576257734732311, 'eval_runtime': 1.6054, 'eval_samples_per_second': 71.01, 'eval_steps_per_second': 4.983, 'epoch': 25.99}\n",
            " 65% 1456/2240 [25:05<11:52,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1456\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1456/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1456/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1456/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1456/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1456/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1456/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1456/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1456/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1456/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1456/pytorch_model.bin\n",
            "{'loss': 0.6186, 'learning_rate': 1.3214285714285716e-05, 'epoch': 26.78}\n",
            " 68% 1512/2240 [25:59<11:02,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.35it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7143893241882324, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6129476584022038, 'eval_precision': 0.6729975066230326, 'eval_recall': 0.5841942426688189, 'eval_runtime': 1.6037, 'eval_samples_per_second': 71.085, 'eval_steps_per_second': 4.988, 'epoch': 26.99}\n",
            " 68% 1512/2240 [26:01<11:02,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1512\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1512/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1512/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1512/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1512/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1512/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1512/pytorch_model.bin\n",
            " 70% 1568/2240 [26:56<10:10,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7430142760276794, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6479986645049483, 'eval_precision': 0.6501068376068376, 'eval_recall': 0.651116491794458, 'eval_runtime': 1.6062, 'eval_samples_per_second': 70.974, 'eval_steps_per_second': 4.981, 'epoch': 27.99}\n",
            " 70% 1568/2240 [26:57<10:10,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1568\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1568/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1568/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1568/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1568/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1568/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1568/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1568/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1568/pytorch_model.bin\n",
            " 72% 1624/2240 [27:52<09:19,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6982042193412781, 'eval_acc': 0.7894736842105263, 'eval_f1': 0.6744555949101403, 'eval_precision': 0.7702197288452548, 'eval_recall': 0.6258609093354855, 'eval_runtime': 1.6059, 'eval_samples_per_second': 70.986, 'eval_steps_per_second': 4.981, 'epoch': 28.99}\n",
            " 72% 1624/2240 [27:54<09:19,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1624\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1624/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1624/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1624/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1624/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1624/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1624/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1624/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1624/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1624/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1624/pytorch_model.bin\n",
            " 75% 1680/2240 [28:48<08:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7048327922821045, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6529446728106562, 'eval_precision': 0.7195257661359357, 'eval_recall': 0.6202111918213613, 'eval_runtime': 1.6072, 'eval_samples_per_second': 70.932, 'eval_steps_per_second': 4.978, 'epoch': 29.99}\n",
            " 75% 1680/2240 [28:50<08:29,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1680\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1680/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1680/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1680/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1680/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1680/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1680/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1680/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1680/pytorch_model.bin\n",
            " 78% 1736/2240 [29:54<07:37,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7450715899467468, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6777547734069472, 'eval_precision': 0.7123015873015873, 'eval_recall': 0.6562281409739037, 'eval_runtime': 1.6069, 'eval_samples_per_second': 70.943, 'eval_steps_per_second': 4.978, 'epoch': 30.99}\n",
            " 78% 1736/2240 [29:55<07:37,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1736\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1736/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1736/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1736/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1736/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1736/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1736/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1736/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1736/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1736/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1736/pytorch_model.bin\n",
            " 80% 1792/2240 [30:50<06:47,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7501193881034851, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6553411521496627, 'eval_precision': 0.6852388958946336, 'eval_recall': 0.6437180521926285, 'eval_runtime': 1.6067, 'eval_samples_per_second': 70.954, 'eval_steps_per_second': 4.979, 'epoch': 31.99}\n",
            " 80% 1792/2240 [30:52<06:47,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1792\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1792/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1792/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1792/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1792/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1792/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1792/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1792/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1792/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1792/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1792/pytorch_model.bin\n",
            " 82% 1848/2240 [31:46<05:56,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7450780272483826, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6467273183374879, 'eval_precision': 0.6724929378531073, 'eval_recall': 0.6380683346785042, 'eval_runtime': 1.6038, 'eval_samples_per_second': 71.082, 'eval_steps_per_second': 4.988, 'epoch': 32.99}\n",
            " 82% 1848/2240 [31:48<05:56,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1848\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1848/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1848/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1848/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1848/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1848/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1848/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1848/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1848/pytorch_model.bin\n",
            " 85% 1904/2240 [32:42<05:05,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7193192839622498, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6529446728106562, 'eval_precision': 0.7195257661359357, 'eval_recall': 0.6202111918213613, 'eval_runtime': 1.603, 'eval_samples_per_second': 71.114, 'eval_steps_per_second': 4.99, 'epoch': 33.99}\n",
            " 85% 1904/2240 [32:44<05:05,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1904\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1904/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1904/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1904/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1904/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1904/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1904/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1904/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1904/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1904/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1904/pytorch_model.bin\n",
            " 88% 1960/2240 [33:39<04:14,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7600626945495605, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6626035397774528, 'eval_precision': 0.6832688041943941, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6055, 'eval_samples_per_second': 71.007, 'eval_steps_per_second': 4.983, 'epoch': 34.99}\n",
            " 88% 1960/2240 [33:40<04:14,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-1960\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1960/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1960/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1960/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1960/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1960/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-1960/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-1960/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-1960/pytorch_model.bin\n",
            "{'loss': 0.5186, 'learning_rate': 4.2857142857142855e-06, 'epoch': 35.71}\n",
            " 90% 2016/2240 [34:44<03:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7454191446304321, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6626035397774528, 'eval_precision': 0.6832688041943941, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.606, 'eval_samples_per_second': 70.982, 'eval_steps_per_second': 4.981, 'epoch': 35.99}\n",
            " 90% 2016/2240 [34:45<03:23,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-2016\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2016/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2016/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2016/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2016/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2016/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2016/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2016/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2016/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2016/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-2016/pytorch_model.bin\n",
            " 92% 2072/2240 [35:40<02:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7426416277885437, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6669989717152576, 'eval_precision': 0.6899937225360956, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.6074, 'eval_samples_per_second': 70.923, 'eval_steps_per_second': 4.977, 'epoch': 36.99}\n",
            " 92% 2072/2240 [35:42<02:32,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-2072\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2072/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2072/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2072/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2072/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2072/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2072/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2072/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2072/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2072/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-2072/pytorch_model.bin\n",
            " 95% 2128/2240 [36:36<01:41,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7421879172325134, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6669989717152576, 'eval_precision': 0.6899937225360956, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.603, 'eval_samples_per_second': 71.118, 'eval_steps_per_second': 4.991, 'epoch': 37.99}\n",
            " 95% 2128/2240 [36:38<01:41,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-2128\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2128/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2128/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2128/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2128/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2128/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-2128/pytorch_model.bin\n",
            " 98% 2184/2240 [37:33<00:50,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.35it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7404576539993286, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6669989717152576, 'eval_precision': 0.6899937225360956, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.6018, 'eval_samples_per_second': 71.169, 'eval_steps_per_second': 4.994, 'epoch': 38.99}\n",
            " 98% 2184/2240 [37:35<00:50,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-2184\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2184/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2184/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2184/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2184/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2184/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2184/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2184/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2184/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2184/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-2184/pytorch_model.bin\n",
            "100% 2240/2240 [38:30<00:00,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7469727396965027, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6669989717152576, 'eval_precision': 0.6899937225360956, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.4576, 'eval_samples_per_second': 78.21, 'eval_steps_per_second': 5.488, 'epoch': 39.99}\n",
            "100% 2240/2240 [38:32<00:00,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_3/checkpoint-2240\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2240/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2240/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2240/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2240/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2240/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/checkpoint-2240/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/checkpoint-2240/config.json\n",
            "Model weights saved in results/citation-intent_3/checkpoint-2240/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/citation-intent_3/checkpoint-1624 (score: 0.6982042193412781).\n",
            "Loading best adapter(s) from results/citation-intent_3/checkpoint-1624 (score: 0.6982042193412781).\n",
            "Loading module configuration from results/citation-intent_3/checkpoint-1624/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/citation-intent_3/checkpoint-1624/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/citation-intent_3/checkpoint-1624/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/citation-intent_3/checkpoint-1624/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/citation-intent_3/checkpoint-1624 (score: 0.6982042193412781).\n",
            "Loading module configuration from results/citation-intent_3/checkpoint-1624/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/citation-intent_3/checkpoint-1624/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 2336.6447, 'train_samples_per_second': 28.896, 'train_steps_per_second': 0.959, 'train_loss': 0.7463709694998605, 'epoch': 39.99}\n",
            "100% 2240/2240 [38:56<00:00,  1.04s/it]\n",
            "Saving model checkpoint to results/citation-intent_3/\n",
            "Configuration saved in results/citation-intent_3/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_3/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_3/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_3/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_3/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_3/config.json\n",
            "Model weights saved in results/citation-intent_3/pytorch_model.bin\n",
            "tokenizer config file saved in results/citation-intent_3/tokenizer_config.json\n",
            "Special tokens file saved in results/citation-intent_3/special_tokens_map.json\n",
            "08/03/2021 14:04:48 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "100% 8/8 [00:01<00:00,  6.27it/s]\n",
            "08/03/2021 14:04:50 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_loss = 0.6982042193412781\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_acc = 0.7894736842105263\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_f1 = 0.6744555949101403\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_precision = 0.7702197288452548\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_recall = 0.6258609093354855\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_runtime = 1.4859\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_samples_per_second = 76.719\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   eval_steps_per_second = 5.384\n",
            "08/03/2021 14:04:50 - INFO - __main__ -   epoch = 39.99\n",
            "08/03/2021 14:04:50 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 139\n",
            "  Batch size = 15\n",
            "100% 10/10 [00:01<00:00,  6.31it/s]\n",
            "08/03/2021 14:04:52 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_loss = 0.8888688087463379\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_acc = 0.7769784172661871\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_f1 = 0.6199338476214177\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_precision = 0.6509539842873177\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_recall = 0.5979332404684518\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_runtime = 1.777\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_samples_per_second = 78.222\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   eval_steps_per_second = 5.627\n",
            "08/03/2021 14:04:52 - INFO - __main__ -   epoch = 39.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IXRVJvdVa9-",
        "outputId": "8d591b17-b60f-49eb-a847-c0d7117f5c93"
      },
      "source": [
        "# Experiment_8_citation-intent_4\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 15 \\\n",
        "--gradient_accumulation_steps 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 70 \\\n",
        "--output_dir results/citation-intent_4/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/citation-intent/mlm \\\n",
        "--adapter_2 results/adapters/sciie/mlm \\\n",
        "--per_device_eval_batch_size 15 \\\n",
        "--weight_decay 0.0001 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.97 \\\n",
        "--adam_epsilon 5e-5 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 836 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\\n",
        "# --overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 14:15:02.750255: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 14:15:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1688\n",
            "  Num Epochs = 70\n",
            "  Instantaneous batch size per device = 15\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 30\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 3920\n",
            "  1% 56/3920 [00:50<58:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.324438452720642, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6057, 'eval_samples_per_second': 70.995, 'eval_steps_per_second': 4.982, 'epoch': 0.99}\n",
            "  1% 56/3920 [00:52<58:23,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-56\n",
            "Configuration saved in results/citation-intent_4/checkpoint-56/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-56/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-56/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-56/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-56/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-56/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-56/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-56/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-56/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-56/pytorch_model.bin\n",
            "  3% 112/3920 [01:46<57:42,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.52it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.36it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2967473268508911, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6014, 'eval_samples_per_second': 71.189, 'eval_steps_per_second': 4.996, 'epoch': 1.99}\n",
            "  3% 112/3920 [01:48<57:42,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.36it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-112\n",
            "Configuration saved in results/citation-intent_4/checkpoint-112/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-112/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-112/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-112/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-112/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-112/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-112/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-112/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-112/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-112/pytorch_model.bin\n",
            "  4% 168/3920 [02:43<56:49,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2356685400009155, 'eval_acc': 0.5175438596491229, 'eval_f1': 0.11368015414258188, 'eval_precision': 0.08625730994152048, 'eval_recall': 0.16666666666666666, 'eval_runtime': 1.6026, 'eval_samples_per_second': 71.134, 'eval_steps_per_second': 4.992, 'epoch': 2.99}\n",
            "  4% 168/3920 [02:45<56:49,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-168\n",
            "Configuration saved in results/citation-intent_4/checkpoint-168/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-168/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-168/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-168/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-168/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-168/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-168/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-168/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-168/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-168/pytorch_model.bin\n",
            "  6% 224/3920 [03:40<55:58,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0935180187225342, 'eval_acc': 0.6140350877192983, 'eval_f1': 0.22554112554112557, 'eval_precision': 0.2070175438596491, 'eval_recall': 0.25907990314769974, 'eval_runtime': 1.6032, 'eval_samples_per_second': 71.109, 'eval_steps_per_second': 4.99, 'epoch': 3.99}\n",
            "  6% 224/3920 [03:41<55:58,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-224\n",
            "Configuration saved in results/citation-intent_4/checkpoint-224/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-224/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-224/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-224/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-224/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-224/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-224/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-224/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-224/pytorch_model.bin\n",
            "  7% 280/3920 [04:36<55:11,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0089731216430664, 'eval_acc': 0.6666666666666666, 'eval_f1': 0.2805091733663162, 'eval_precision': 0.3369016249451032, 'eval_recall': 0.3066989507667474, 'eval_runtime': 1.6053, 'eval_samples_per_second': 71.016, 'eval_steps_per_second': 4.984, 'epoch': 4.99}\n",
            "  7% 280/3920 [04:38<55:11,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-280\n",
            "Configuration saved in results/citation-intent_4/checkpoint-280/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-280/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-280/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-280/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-280/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-280/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-280/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-280/pytorch_model.bin\n",
            "  9% 336/3920 [05:32<54:17,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9611841440200806, 'eval_acc': 0.7192982456140351, 'eval_f1': 0.3549382716049383, 'eval_precision': 0.3508771929824561, 'eval_recall': 0.3645412967446866, 'eval_runtime': 1.605, 'eval_samples_per_second': 71.029, 'eval_steps_per_second': 4.985, 'epoch': 5.99}\n",
            "  9% 336/3920 [05:34<54:17,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-336\n",
            "Configuration saved in results/citation-intent_4/checkpoint-336/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-336/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-336/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-336/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-336/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-336/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-336/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-336/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-336/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-336/pytorch_model.bin\n",
            " 10% 392/3920 [06:28<53:18,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.98it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9077313542366028, 'eval_acc': 0.7105263157894737, 'eval_f1': 0.34563987427031884, 'eval_precision': 0.34383289124668437, 'eval_recall': 0.36682808716707016, 'eval_runtime': 1.6025, 'eval_samples_per_second': 71.141, 'eval_steps_per_second': 4.992, 'epoch': 6.99}\n",
            " 10% 392/3920 [06:30<53:18,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.35it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-392\n",
            "Configuration saved in results/citation-intent_4/checkpoint-392/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-392/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-392/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-392/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-392/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-392/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-392/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-392/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-392/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-392/pytorch_model.bin\n",
            " 11% 448/3920 [07:25<52:37,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8963795900344849, 'eval_acc': 0.7192982456140351, 'eval_f1': 0.35350311064596784, 'eval_precision': 0.34501453163424994, 'eval_recall': 0.3747645951035781, 'eval_runtime': 1.6043, 'eval_samples_per_second': 71.06, 'eval_steps_per_second': 4.987, 'epoch': 7.99}\n",
            " 11% 448/3920 [07:27<52:37,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-448\n",
            "Configuration saved in results/citation-intent_4/checkpoint-448/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-448/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-448/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-448/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-448/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-448/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-448/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-448/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-448/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-448/pytorch_model.bin\n",
            "{'loss': 1.1738, 'learning_rate': 3.4897959183673475e-05, 'epoch': 8.92}\n",
            " 13% 504/3920 [08:21<52:00,  1.09it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9491472840309143, 'eval_acc': 0.6754385964912281, 'eval_f1': 0.3130718954248366, 'eval_precision': 0.3354800238521169, 'eval_recall': 0.3146354587032553, 'eval_runtime': 1.6044, 'eval_samples_per_second': 71.052, 'eval_steps_per_second': 4.986, 'epoch': 8.99}\n",
            " 13% 504/3920 [08:23<52:00,  1.09it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-504\n",
            "Configuration saved in results/citation-intent_4/checkpoint-504/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-504/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-504/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-504/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-504/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-504/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-504/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-504/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-504/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-504/pytorch_model.bin\n",
            " 14% 560/3920 [09:18<50:59,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8456057906150818, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.43154004398098883, 'eval_precision': 0.5203473630831643, 'eval_recall': 0.42947941888619856, 'eval_runtime': 1.6084, 'eval_samples_per_second': 70.878, 'eval_steps_per_second': 4.974, 'epoch': 9.99}\n",
            " 14% 560/3920 [09:19<50:59,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-560\n",
            "Configuration saved in results/citation-intent_4/checkpoint-560/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-560/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-560/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-560/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-560/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-560/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-560/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-560/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-560/pytorch_model.bin\n",
            " 16% 616/3920 [10:15<49:57,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8010345101356506, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.4310553738034654, 'eval_precision': 0.5239139515455306, 'eval_recall': 0.4164312617702448, 'eval_runtime': 1.6051, 'eval_samples_per_second': 71.025, 'eval_steps_per_second': 4.984, 'epoch': 10.99}\n",
            " 16% 616/3920 [10:17<49:57,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-616\n",
            "Configuration saved in results/citation-intent_4/checkpoint-616/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-616/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-616/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-616/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-616/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-616/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-616/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-616/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-616/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-616/pytorch_model.bin\n",
            " 17% 672/3920 [11:13<49:17,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7826324105262756, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.4329602239547811, 'eval_precision': 0.525206043956044, 'eval_recall': 0.4243677697067527, 'eval_runtime': 1.607, 'eval_samples_per_second': 70.938, 'eval_steps_per_second': 4.978, 'epoch': 11.99}\n",
            " 17% 672/3920 [11:15<49:17,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-672\n",
            "Configuration saved in results/citation-intent_4/checkpoint-672/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-672/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-672/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-672/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-672/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-672/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-672/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-672/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-672/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-672/pytorch_model.bin\n",
            " 19% 728/3920 [12:10<48:28,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7972590327262878, 'eval_acc': 0.7105263157894737, 'eval_f1': 0.41240458015267173, 'eval_precision': 0.5239898989898989, 'eval_recall': 0.40055824589722894, 'eval_runtime': 1.6096, 'eval_samples_per_second': 70.826, 'eval_steps_per_second': 4.97, 'epoch': 12.99}\n",
            " 19% 728/3920 [12:12<48:28,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-728\n",
            "Configuration saved in results/citation-intent_4/checkpoint-728/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-728/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-728/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-728/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-728/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-728/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-728/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-728/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-728/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-728/pytorch_model.bin\n",
            " 20% 784/3920 [13:06<47:33,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7830657958984375, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.44829449510300573, 'eval_precision': 0.5367500106306077, 'eval_recall': 0.4402407855797686, 'eval_runtime': 1.6065, 'eval_samples_per_second': 70.964, 'eval_steps_per_second': 4.98, 'epoch': 13.99}\n",
            " 20% 784/3920 [13:08<47:33,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-784\n",
            "Configuration saved in results/citation-intent_4/checkpoint-784/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-784/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-784/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-784/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-784/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-784/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-784/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-784/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-784/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-784/pytorch_model.bin\n",
            " 21% 840/3920 [14:03<46:39,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.54it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7532588243484497, 'eval_acc': 0.7456140350877193, 'eval_f1': 0.49860352155434123, 'eval_precision': 0.6071428571428571, 'eval_recall': 0.48136938391175677, 'eval_runtime': 1.6041, 'eval_samples_per_second': 71.069, 'eval_steps_per_second': 4.987, 'epoch': 14.99}\n",
            " 21% 840/3920 [14:04<46:39,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-840\n",
            "Configuration saved in results/citation-intent_4/checkpoint-840/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-840/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-840/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-840/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-840/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-840/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-840/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-840/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-840/pytorch_model.bin\n",
            " 23% 896/3920 [15:00<45:47,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7563078999519348, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.48544815839897804, 'eval_precision': 0.5763457556935818, 'eval_recall': 0.47343287597524886, 'eval_runtime': 1.6021, 'eval_samples_per_second': 71.156, 'eval_steps_per_second': 4.993, 'epoch': 15.99}\n",
            " 23% 896/3920 [15:01<45:47,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-896\n",
            "Configuration saved in results/citation-intent_4/checkpoint-896/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-896/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-896/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-896/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-896/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-896/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-896/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-896/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-896/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-896/pytorch_model.bin\n",
            " 24% 952/3920 [15:56<45:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7595167756080627, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.4978815768731735, 'eval_precision': 0.605831608005521, 'eval_recall': 0.4836561743341405, 'eval_runtime': 1.6069, 'eval_samples_per_second': 70.944, 'eval_steps_per_second': 4.979, 'epoch': 16.99}\n",
            " 24% 952/3920 [15:58<45:01,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-952\n",
            "Configuration saved in results/citation-intent_4/checkpoint-952/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-952/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-952/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-952/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-952/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-952/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-952/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-952/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-952/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-952/pytorch_model.bin\n",
            "{'loss': 0.7802, 'learning_rate': 2.9795918367346944e-05, 'epoch': 17.85}\n",
            " 26% 1008/3920 [16:52<44:10,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7456352114677429, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6272798567264971, 'eval_precision': 0.7461318619128466, 'eval_recall': 0.5817662093085821, 'eval_runtime': 1.6051, 'eval_samples_per_second': 71.023, 'eval_steps_per_second': 4.984, 'epoch': 17.99}\n",
            " 26% 1008/3920 [16:54<44:10,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1008\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1008/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1008/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1008/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1008/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1008/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1008/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1008/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1008/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1008/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1008/pytorch_model.bin\n",
            " 27% 1064/3920 [17:49<43:15,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.43it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7255094051361084, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.5889372822299652, 'eval_precision': 0.7077850877192983, 'eval_recall': 0.5429244013989777, 'eval_runtime': 1.6083, 'eval_samples_per_second': 70.881, 'eval_steps_per_second': 4.974, 'epoch': 18.99}\n",
            " 27% 1064/3920 [17:50<43:15,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1064\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1064/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1064/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1064/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1064/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1064/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1064/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1064/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1064/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1064/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1064/pytorch_model.bin\n",
            " 29% 1120/3920 [18:45<42:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6965065002441406, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6383374298859267, 'eval_precision': 0.7652867965367965, 'eval_recall': 0.5845910680656443, 'eval_runtime': 1.6063, 'eval_samples_per_second': 70.972, 'eval_steps_per_second': 4.98, 'epoch': 19.99}\n",
            " 29% 1120/3920 [18:47<42:23,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1120\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1120/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1120/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1120/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1120/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1120/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1120/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1120/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1120/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1120/pytorch_model.bin\n",
            " 30% 1176/3920 [19:41<41:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7696411609649658, 'eval_acc': 0.7368421052631579, 'eval_f1': 0.6183792815371763, 'eval_precision': 0.66504329004329, 'eval_recall': 0.6038001076136669, 'eval_runtime': 1.603, 'eval_samples_per_second': 71.115, 'eval_steps_per_second': 4.991, 'epoch': 20.99}\n",
            " 30% 1176/3920 [19:43<41:32,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1176\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1176/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1176/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1176/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1176/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1176/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1176/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1176/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1176/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1176/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1176/pytorch_model.bin\n",
            " 31% 1232/3920 [20:38<40:42,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7313854694366455, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6173122455723691, 'eval_precision': 0.745940170940171, 'eval_recall': 0.5687180521926284, 'eval_runtime': 1.6043, 'eval_samples_per_second': 71.059, 'eval_steps_per_second': 4.987, 'epoch': 21.99}\n",
            " 31% 1232/3920 [20:39<40:42,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1232\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1232/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1232/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1232/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1232/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1232/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1232/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1232/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1232/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1232/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1232/pytorch_model.bin\n",
            " 33% 1288/3920 [21:35<39:57,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7234888076782227, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6582151300236406, 'eval_precision': 0.7500992480500678, 'eval_recall': 0.6150995426419156, 'eval_runtime': 1.6044, 'eval_samples_per_second': 71.053, 'eval_steps_per_second': 4.986, 'epoch': 22.99}\n",
            " 33% 1288/3920 [21:37<39:57,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1288\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1288/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1288/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1288/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1288/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1288/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1288/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1288/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1288/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1288/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1288/pytorch_model.bin\n",
            " 34% 1344/3920 [22:32<39:02,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.762111485004425, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6595734780517389, 'eval_precision': 0.6904642342571403, 'eval_recall': 0.6765133171912833, 'eval_runtime': 1.6025, 'eval_samples_per_second': 71.14, 'eval_steps_per_second': 4.992, 'epoch': 23.99}\n",
            " 34% 1344/3920 [22:33<39:02,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1344\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1344/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1344/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1344/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1344/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1344/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1344/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1344/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1344/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1344/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1344/pytorch_model.bin\n",
            " 36% 1400/3920 [23:29<38:11,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.52it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7357154488563538, 'eval_acc': 0.7456140350877193, 'eval_f1': 0.652641767019174, 'eval_precision': 0.6702020202020202, 'eval_recall': 0.6454667742803336, 'eval_runtime': 1.6041, 'eval_samples_per_second': 71.069, 'eval_steps_per_second': 4.987, 'epoch': 24.99}\n",
            " 36% 1400/3920 [23:31<38:11,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1400\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1400/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1400/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1400/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1400/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1400/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1400/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1400/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1400/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1400/pytorch_model.bin\n",
            " 37% 1456/3920 [24:25<37:22,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7058613300323486, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6855833377572508, 'eval_precision': 0.7220114942528735, 'eval_recall': 0.6618778584880279, 'eval_runtime': 1.6044, 'eval_samples_per_second': 71.053, 'eval_steps_per_second': 4.986, 'epoch': 25.99}\n",
            " 37% 1456/3920 [24:27<37:22,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1456\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1456/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1456/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1456/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1456/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1456/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1456/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1456/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1456/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1456/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1456/pytorch_model.bin\n",
            "{'loss': 0.567, 'learning_rate': 2.469387755102041e-05, 'epoch': 26.78}\n",
            " 39% 1512/3920 [25:22<36:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.53it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7156229019165039, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.631220744025622, 'eval_precision': 0.6859953703703704, 'eval_recall': 0.6077011030400861, 'eval_runtime': 1.6048, 'eval_samples_per_second': 71.039, 'eval_steps_per_second': 4.985, 'epoch': 26.99}\n",
            " 39% 1512/3920 [25:23<36:32,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1512\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1512/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1512/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1512/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1512/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1512/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1512/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1512/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1512/pytorch_model.bin\n",
            " 40% 1568/3920 [26:18<35:41,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7163981199264526, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6079777865273582, 'eval_precision': 0.585737812911726, 'eval_recall': 0.6427831584611247, 'eval_runtime': 1.607, 'eval_samples_per_second': 70.938, 'eval_steps_per_second': 4.978, 'epoch': 27.99}\n",
            " 40% 1568/3920 [26:19<35:41,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1568\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1568/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1568/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1568/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1568/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1568/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1568/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1568/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1568/pytorch_model.bin\n",
            " 41% 1624/3920 [27:14<34:47,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6966009140014648, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6504516711833784, 'eval_precision': 0.742167577413479, 'eval_recall': 0.6043381759483454, 'eval_runtime': 1.6042, 'eval_samples_per_second': 71.063, 'eval_steps_per_second': 4.987, 'epoch': 28.99}\n",
            " 41% 1624/3920 [27:16<34:47,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.34it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1624\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1624/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1624/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1624/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1624/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1624/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1624/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1624/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1624/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1624/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1624/pytorch_model.bin\n",
            " 43% 1680/3920 [28:11<33:58,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.69it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7222517728805542, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6420904904537766, 'eval_precision': 0.7186420836991513, 'eval_recall': 0.5997645951035782, 'eval_runtime': 1.6119, 'eval_samples_per_second': 70.724, 'eval_steps_per_second': 4.963, 'epoch': 29.99}\n",
            " 43% 1680/3920 [28:12<33:58,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.29it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1680\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1680/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1680/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1680/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1680/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1680/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1680/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1680/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1680/pytorch_model.bin\n",
            " 44% 1736/3920 [29:07<33:08,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.41it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.65it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.696053147315979, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6383244206773618, 'eval_precision': 0.69472049689441, 'eval_recall': 0.6122746838848534, 'eval_runtime': 1.617, 'eval_samples_per_second': 70.5, 'eval_steps_per_second': 4.947, 'epoch': 30.99}\n",
            " 44% 1736/3920 [29:09<33:08,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.26it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1736\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1736/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1736/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1736/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1736/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1736/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1736/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1736/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1736/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1736/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1736/pytorch_model.bin\n",
            " 46% 1792/3920 [30:03<32:15,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7969327569007874, 'eval_acc': 0.7982456140350878, 'eval_f1': 0.6648805281200149, 'eval_precision': 0.6862698412698413, 'eval_recall': 0.6573042776432606, 'eval_runtime': 1.609, 'eval_samples_per_second': 70.85, 'eval_steps_per_second': 4.972, 'epoch': 31.99}\n",
            " 46% 1792/3920 [30:05<32:15,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1792\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1792/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1792/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1792/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1792/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1792/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1792/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1792/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1792/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1792/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1792/pytorch_model.bin\n",
            " 47% 1848/3920 [31:00<31:22,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.776543140411377, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6603535353535354, 'eval_precision': 0.6916666666666668, 'eval_recall': 0.6488297013720742, 'eval_runtime': 1.6085, 'eval_samples_per_second': 70.872, 'eval_steps_per_second': 4.973, 'epoch': 32.99}\n",
            " 47% 1848/3920 [31:01<31:22,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1848\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1848/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1848/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1848/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1848/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1848/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1848/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1848/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1848/pytorch_model.bin\n",
            " 49% 1904/3920 [31:56<30:31,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7185845375061035, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6671197446621174, 'eval_precision': 0.6886584398850233, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.6093, 'eval_samples_per_second': 70.837, 'eval_steps_per_second': 4.971, 'epoch': 33.99}\n",
            " 49% 1904/3920 [31:58<30:31,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1904\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1904/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1904/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1904/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1904/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1904/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1904/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1904/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1904/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1904/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1904/pytorch_model.bin\n",
            " 50% 1960/3920 [32:53<29:44,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7519382834434509, 'eval_acc': 0.8070175438596491, 'eval_f1': 0.7147722397722397, 'eval_precision': 0.7385587431693988, 'eval_recall': 0.7040825934893732, 'eval_runtime': 1.6102, 'eval_samples_per_second': 70.8, 'eval_steps_per_second': 4.968, 'epoch': 34.99}\n",
            " 50% 1960/3920 [32:55<29:44,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-1960\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1960/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1960/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1960/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1960/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1960/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1960/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-1960/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-1960/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-1960/pytorch_model.bin\n",
            "{'loss': 0.4384, 'learning_rate': 1.9591836734693877e-05, 'epoch': 35.71}\n",
            " 51% 2016/3920 [33:51<28:49,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.51it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7369639277458191, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6707435185732002, 'eval_precision': 0.696798493408663, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6055, 'eval_samples_per_second': 71.004, 'eval_steps_per_second': 4.983, 'epoch': 35.99}\n",
            " 51% 2016/3920 [33:52<28:49,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.33it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2016\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2016/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2016/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2016/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2016/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2016/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2016/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2016/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2016/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2016/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2016/pytorch_model.bin\n",
            " 53% 2072/3920 [34:47<28:00,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.45it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8009293079376221, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6626540120048955, 'eval_precision': 0.6885198723122453, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.6089, 'eval_samples_per_second': 70.857, 'eval_steps_per_second': 4.972, 'epoch': 36.99}\n",
            " 53% 2072/3920 [34:49<28:00,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2072\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2072/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2072/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2072/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2072/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2072/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2072/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2072/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2072/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2072/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2072/pytorch_model.bin\n",
            " 54% 2128/3920 [35:45<27:10,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.41it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7932368516921997, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6479570874126569, 'eval_precision': 0.657051282051282, 'eval_recall': 0.6516545601291364, 'eval_runtime': 1.6121, 'eval_samples_per_second': 70.714, 'eval_steps_per_second': 4.962, 'epoch': 37.99}\n",
            " 54% 2128/3920 [35:47<27:10,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2128\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2128/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2128/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2128/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2128/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2128/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2128/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2128/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2128/pytorch_model.bin\n",
            " 56% 2184/3920 [36:41<26:19,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.69it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7470316290855408, 'eval_acc': 0.7982456140350878, 'eval_f1': 0.6901635401635401, 'eval_precision': 0.7500836912744765, 'eval_recall': 0.6521926284638149, 'eval_runtime': 1.6126, 'eval_samples_per_second': 70.692, 'eval_steps_per_second': 4.961, 'epoch': 38.99}\n",
            " 56% 2184/3920 [36:43<26:19,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.29it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2184\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2184/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2184/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2184/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2184/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2184/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2184/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2184/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2184/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2184/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2184/pytorch_model.bin\n",
            " 57% 2240/3920 [37:38<25:30,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7459046244621277, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6523619785814908, 'eval_precision': 0.6668665667166417, 'eval_recall': 0.6460048426150121, 'eval_runtime': 1.6083, 'eval_samples_per_second': 70.883, 'eval_steps_per_second': 4.974, 'epoch': 39.99}\n",
            " 57% 2240/3920 [37:39<25:30,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2240\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2240/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2240/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2240/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2240/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2240/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2240/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2240/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2240/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2240/pytorch_model.bin\n",
            " 59% 2296/3920 [38:35<24:37,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7966565489768982, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6413252040593045, 'eval_precision': 0.6410185531702559, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6089, 'eval_samples_per_second': 70.857, 'eval_steps_per_second': 4.972, 'epoch': 40.99}\n",
            " 59% 2296/3920 [38:36<24:37,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2296\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2296/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2296/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2296/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2296/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2296/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2296/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2296/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2296/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2296/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2296/pytorch_model.bin\n",
            " 60% 2352/3920 [39:31<23:46,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7954073548316956, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6608652900688299, 'eval_precision': 0.6695326278659612, 'eval_recall': 0.6788001076136668, 'eval_runtime': 1.6078, 'eval_samples_per_second': 70.906, 'eval_steps_per_second': 4.976, 'epoch': 41.99}\n",
            " 60% 2352/3920 [39:33<23:46,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2352\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2352/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2352/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2352/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2352/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2352/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2352/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2352/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2352/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2352/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2352/pytorch_model.bin\n",
            " 61% 2408/3920 [40:27<22:58,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7893692255020142, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6096, 'eval_samples_per_second': 70.823, 'eval_steps_per_second': 4.97, 'epoch': 42.99}\n",
            " 61% 2408/3920 [40:29<22:58,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2408\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2408/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2408/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2408/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2408/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2408/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2408/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2408/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2408/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2408/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2408/pytorch_model.bin\n",
            " 63% 2464/3920 [41:24<22:05,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.42it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.768552303314209, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6750279955207167, 'eval_precision': 0.7067244846656612, 'eval_recall': 0.6567662093085822, 'eval_runtime': 1.6123, 'eval_samples_per_second': 70.705, 'eval_steps_per_second': 4.962, 'epoch': 43.99}\n",
            " 63% 2464/3920 [41:25<22:05,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2464\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2464/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2464/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2464/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2464/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2464/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2464/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2464/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2464/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2464/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2464/pytorch_model.bin\n",
            "{'loss': 0.3464, 'learning_rate': 1.448979591836735e-05, 'epoch': 44.64}\n",
            " 64% 2520/3920 [42:20<21:15,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.45it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7961702942848206, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6498985272878002, 'eval_precision': 0.6708326718001641, 'eval_recall': 0.651116491794458, 'eval_runtime': 1.6112, 'eval_samples_per_second': 70.754, 'eval_steps_per_second': 4.965, 'epoch': 44.99}\n",
            " 64% 2520/3920 [42:22<21:15,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2520\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2520/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2520/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2520/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2520/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2520/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2520/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2520/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2520/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2520/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2520/pytorch_model.bin\n",
            " 66% 2576/3920 [43:17<20:22,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8842610120773315, 'eval_acc': 0.7456140350877193, 'eval_f1': 0.636154011154011, 'eval_precision': 0.6196581196581197, 'eval_recall': 0.6759752488566048, 'eval_runtime': 1.6081, 'eval_samples_per_second': 70.889, 'eval_steps_per_second': 4.975, 'epoch': 45.99}\n",
            " 66% 2576/3920 [43:18<20:22,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2576\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2576/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2576/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2576/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2576/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2576/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2576/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2576/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2576/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2576/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2576/pytorch_model.bin\n",
            " 67% 2632/3920 [44:13<19:33,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.69it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8208027482032776, 'eval_acc': 0.7982456140350878, 'eval_f1': 0.6815381507852346, 'eval_precision': 0.725704365079365, 'eval_recall': 0.6573042776432606, 'eval_runtime': 1.6122, 'eval_samples_per_second': 70.713, 'eval_steps_per_second': 4.962, 'epoch': 46.99}\n",
            " 67% 2632/3920 [44:15<19:33,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.29it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2632\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2632/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2632/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2632/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2632/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2632/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2632/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2632/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2632/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2632/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2632/pytorch_model.bin\n",
            " 69% 2688/3920 [45:10<18:41,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8888141512870789, 'eval_acc': 0.7105263157894737, 'eval_f1': 0.5861421724002243, 'eval_precision': 0.5624216524216524, 'eval_recall': 0.634167339252085, 'eval_runtime': 1.6106, 'eval_samples_per_second': 70.783, 'eval_steps_per_second': 4.967, 'epoch': 47.99}\n",
            " 69% 2688/3920 [45:11<18:41,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2688\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2688/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2688/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2688/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2688/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2688/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2688/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2688/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2688/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2688/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2688/pytorch_model.bin\n",
            " 70% 2744/3920 [46:06<17:50,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.42it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.69it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7761430144309998, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6497497588202236, 'eval_precision': 0.6683674463937622, 'eval_recall': 0.651116491794458, 'eval_runtime': 1.614, 'eval_samples_per_second': 70.63, 'eval_steps_per_second': 4.956, 'epoch': 48.99}\n",
            " 70% 2744/3920 [46:08<17:50,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.29it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2744\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2744/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2744/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2744/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2744/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2744/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2744/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2744/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2744/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2744/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2744/pytorch_model.bin\n",
            " 71% 2800/3920 [47:03<16:59,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.45it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8052912950515747, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6453003877610723, 'eval_precision': 0.6633343508343509, 'eval_recall': 0.6482916330373958, 'eval_runtime': 1.6083, 'eval_samples_per_second': 70.884, 'eval_steps_per_second': 4.974, 'epoch': 49.99}\n",
            " 71% 2800/3920 [47:05<16:59,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2800\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2800/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2800/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2800/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2800/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2800/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2800/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2800/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2800/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2800/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2800/pytorch_model.bin\n",
            " 73% 2856/3920 [48:00<16:07,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7418441772460938, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6696168597898603, 'eval_precision': 0.6986876966385163, 'eval_recall': 0.6488297013720743, 'eval_runtime': 1.6076, 'eval_samples_per_second': 70.914, 'eval_steps_per_second': 4.976, 'epoch': 50.99}\n",
            " 73% 2856/3920 [48:02<16:07,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2856\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2856/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2856/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2856/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2856/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2856/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2856/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2856/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2856/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2856/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2856/pytorch_model.bin\n",
            " 74% 2912/3920 [48:58<15:17,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8122003674507141, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6998746867167919, 'eval_precision': 0.7074915824915825, 'eval_recall': 0.7232916330373959, 'eval_runtime': 1.6083, 'eval_samples_per_second': 70.881, 'eval_steps_per_second': 4.974, 'epoch': 51.99}\n",
            " 74% 2912/3920 [48:59<15:17,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2912\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2912/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2912/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2912/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2912/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2912/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2912/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2912/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2912/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2912/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2912/pytorch_model.bin\n",
            " 76% 2968/3920 [49:55<14:25,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.42it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7855496406555176, 'eval_acc': 0.7631578947368421, 'eval_f1': 0.6648358484864644, 'eval_precision': 0.6775058275058274, 'eval_recall': 0.6816249663707291, 'eval_runtime': 1.6111, 'eval_samples_per_second': 70.757, 'eval_steps_per_second': 4.965, 'epoch': 52.99}\n",
            " 76% 2968/3920 [49:56<14:25,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-2968\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2968/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2968/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2968/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2968/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2968/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2968/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2968/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-2968/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-2968/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-2968/pytorch_model.bin\n",
            "{'loss': 0.2981, 'learning_rate': 9.387755102040818e-06, 'epoch': 53.57}\n",
            " 77% 3024/3920 [50:51<13:35,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7667174339294434, 'eval_acc': 0.7894736842105263, 'eval_f1': 0.6793905052486288, 'eval_precision': 0.7117497053466195, 'eval_recall': 0.6595910680656444, 'eval_runtime': 1.6108, 'eval_samples_per_second': 70.77, 'eval_steps_per_second': 4.966, 'epoch': 53.99}\n",
            " 77% 3024/3920 [50:53<13:35,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3024\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3024/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3024/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3024/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3024/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3024/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3024/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3024/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3024/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3024/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3024/pytorch_model.bin\n",
            " 79% 3080/3920 [51:48<12:44,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.45it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7881816625595093, 'eval_acc': 0.7982456140350878, 'eval_f1': 0.7180033416875521, 'eval_precision': 0.7445274831243974, 'eval_recall': 0.701257734732311, 'eval_runtime': 1.6091, 'eval_samples_per_second': 70.847, 'eval_steps_per_second': 4.972, 'epoch': 54.99}\n",
            " 79% 3080/3920 [51:49<12:44,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3080\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3080/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3080/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3080/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3080/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3080/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3080/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3080/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3080/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3080/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3080/pytorch_model.bin\n",
            " 80% 3136/3920 [52:44<11:53,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8341517448425293, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6628705115547221, 'eval_precision': 0.6877958079783637, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6088, 'eval_samples_per_second': 70.862, 'eval_steps_per_second': 4.973, 'epoch': 55.99}\n",
            " 80% 3136/3920 [52:46<11:53,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3136\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3136/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3136/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3136/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3136/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3136/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3136/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3136/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3136/pytorch_model.bin\n",
            " 81% 3192/3920 [53:40<11:02,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9174873232841492, 'eval_acc': 0.7280701754385965, 'eval_f1': 0.6240690465788762, 'eval_precision': 0.6153415103415104, 'eval_recall': 0.6703255313424804, 'eval_runtime': 1.6095, 'eval_samples_per_second': 70.831, 'eval_steps_per_second': 4.971, 'epoch': 56.99}\n",
            " 81% 3192/3920 [53:42<11:02,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3192\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3192/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3192/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3192/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3192/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3192/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3192/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3192/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3192/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3192/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3192/pytorch_model.bin\n",
            " 83% 3248/3920 [54:37<10:11,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7895618677139282, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6096, 'eval_samples_per_second': 70.823, 'eval_steps_per_second': 4.97, 'epoch': 57.99}\n",
            " 83% 3248/3920 [54:38<10:11,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3248\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3248/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3248/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3248/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3248/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3248/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3248/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3248/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3248/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3248/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3248/pytorch_model.bin\n",
            " 84% 3304/3920 [55:33<09:19,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8154832124710083, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6103, 'eval_samples_per_second': 70.794, 'eval_steps_per_second': 4.968, 'epoch': 58.99}\n",
            " 84% 3304/3920 [55:35<09:19,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3304\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3304/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3304/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3304/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3304/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3304/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3304/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3304/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3304/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3304/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3304/pytorch_model.bin\n",
            " 86% 3360/3920 [56:29<08:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8262286186218262, 'eval_acc': 0.7543859649122807, 'eval_f1': 0.6571756581877086, 'eval_precision': 0.6632241215574549, 'eval_recall': 0.6788001076136668, 'eval_runtime': 1.6076, 'eval_samples_per_second': 70.913, 'eval_steps_per_second': 4.976, 'epoch': 59.99}\n",
            " 86% 3360/3920 [56:31<08:29,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3360\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3360/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3360/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3360/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3360/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3360/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3360/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3360/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3360/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3360/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3360/pytorch_model.bin\n",
            " 87% 3416/3920 [57:26<07:38,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.46it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.778089165687561, 'eval_acc': 0.7807017543859649, 'eval_f1': 0.6736878928266488, 'eval_precision': 0.7075137044064937, 'eval_recall': 0.6516545601291365, 'eval_runtime': 1.6098, 'eval_samples_per_second': 70.815, 'eval_steps_per_second': 4.969, 'epoch': 60.99}\n",
            " 87% 3416/3920 [57:28<07:38,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3416\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3416/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3416/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3416/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3416/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3416/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3416/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3416/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3416/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3416/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3416/pytorch_model.bin\n",
            " 89% 3472/3920 [58:23<06:48,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8516239523887634, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.676299880060942, 'eval_precision': 0.6876218323586745, 'eval_recall': 0.689561474307237, 'eval_runtime': 1.6086, 'eval_samples_per_second': 70.871, 'eval_steps_per_second': 4.973, 'epoch': 61.99}\n",
            " 89% 3472/3920 [58:25<06:48,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3472\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3472/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3472/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3472/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3472/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3472/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3472/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3472/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3472/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3472/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3472/pytorch_model.bin\n",
            "{'loss': 0.2603, 'learning_rate': 4.2857142857142855e-06, 'epoch': 62.5}\n",
            " 90% 3528/3920 [59:20<05:56,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8049717545509338, 'eval_acc': 0.7894736842105263, 'eval_f1': 0.6852855233983819, 'eval_precision': 0.6993793243793244, 'eval_recall': 0.6952111918213614, 'eval_runtime': 1.6111, 'eval_samples_per_second': 70.76, 'eval_steps_per_second': 4.966, 'epoch': 62.99}\n",
            " 90% 3528/3920 [59:22<05:56,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3528\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3528/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3528/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3528/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3528/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3528/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3528/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3528/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3528/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3528/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3528/pytorch_model.bin\n",
            " 91% 3584/3920 [1:00:17<05:05,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.50it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8087584972381592, 'eval_acc': 0.7894736842105263, 'eval_f1': 0.6852855233983819, 'eval_precision': 0.6993793243793244, 'eval_recall': 0.6952111918213614, 'eval_runtime': 1.6077, 'eval_samples_per_second': 70.908, 'eval_steps_per_second': 4.976, 'epoch': 63.99}\n",
            " 91% 3584/3920 [1:00:19<05:05,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3584\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3584/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3584/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3584/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3584/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3584/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3584/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3584/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3584/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3584/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3584/pytorch_model.bin\n",
            " 93% 3640/3920 [1:01:14<04:14,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8433485627174377, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6694379334089153, 'eval_precision': 0.684965225406402, 'eval_recall': 0.6844498251277913, 'eval_runtime': 1.6093, 'eval_samples_per_second': 70.839, 'eval_steps_per_second': 4.971, 'epoch': 64.99}\n",
            " 93% 3640/3920 [1:01:16<04:14,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3640\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3640/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3640/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3640/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3640/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3640/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3640/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3640/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3640/pytorch_model.bin\n",
            " 94% 3696/3920 [1:02:10<03:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8092105388641357, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6108, 'eval_samples_per_second': 70.774, 'eval_steps_per_second': 4.967, 'epoch': 65.99}\n",
            " 94% 3696/3920 [1:02:12<03:23,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.29it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3696\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3696/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3696/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3696/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3696/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3696/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3696/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3696/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3696/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3696/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3696/pytorch_model.bin\n",
            " 96% 3752/3920 [1:03:07<02:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.42it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.71it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8243421316146851, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6116, 'eval_samples_per_second': 70.737, 'eval_steps_per_second': 4.964, 'epoch': 66.99}\n",
            " 96% 3752/3920 [1:03:08<02:32,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3752\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3752/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3752/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3752/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3752/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3752/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3752/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3752/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3752/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3752/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3752/pytorch_model.bin\n",
            " 97% 3808/3920 [1:04:03<01:41,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.49it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.840514600276947, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6694379334089153, 'eval_precision': 0.684965225406402, 'eval_recall': 0.6844498251277913, 'eval_runtime': 1.6101, 'eval_samples_per_second': 70.804, 'eval_steps_per_second': 4.969, 'epoch': 67.99}\n",
            " 97% 3808/3920 [1:04:05<01:41,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.30it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3808\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3808/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3808/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3808/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3808/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3808/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3808/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3808/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3808/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3808/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3808/pytorch_model.bin\n",
            " 99% 3864/3920 [1:04:59<00:50,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.47it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.95it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8331855535507202, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.6084, 'eval_samples_per_second': 70.877, 'eval_steps_per_second': 4.974, 'epoch': 68.99}\n",
            " 99% 3864/3920 [1:05:01<00:50,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.32it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3864\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3864/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3864/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3864/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3864/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3864/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3864/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3864/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3864/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3864/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3864/pytorch_model.bin\n",
            "100% 3920/3920 [1:05:56<00:00,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 2/8 [00:00<00:00, 10.45it/s]\u001b[A\n",
            " 38% 3/8 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 50% 4/8 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 62% 5/8 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 75% 6/8 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 88% 7/8 [00:01<00:00,  5.72it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.8361179828643799, 'eval_acc': 0.7719298245614035, 'eval_f1': 0.6579543284470496, 'eval_precision': 0.6801645503471061, 'eval_recall': 0.65394135055152, 'eval_runtime': 1.4616, 'eval_samples_per_second': 77.994, 'eval_steps_per_second': 5.473, 'epoch': 69.99}\n",
            "100% 3920/3920 [1:05:57<00:00,  1.10it/s]\n",
            "100% 8/8 [00:01<00:00,  6.31it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to results/citation-intent_4/checkpoint-3920\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3920/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3920/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3920/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3920/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3920/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3920/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3920/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/checkpoint-3920/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/checkpoint-3920/config.json\n",
            "Model weights saved in results/citation-intent_4/checkpoint-3920/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/citation-intent_4/checkpoint-1736 (score: 0.696053147315979).\n",
            "Loading best adapter(s) from results/citation-intent_4/checkpoint-1736 (score: 0.696053147315979).\n",
            "Loading module configuration from results/citation-intent_4/checkpoint-1736/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/citation-intent_4/checkpoint-1736/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/citation-intent_4/checkpoint-1736/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/citation-intent_4/checkpoint-1736/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/citation-intent_4/checkpoint-1736 (score: 0.696053147315979).\n",
            "Loading module configuration from results/citation-intent_4/checkpoint-1736/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/citation-intent_4/checkpoint-1736/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 3977.2679, 'train_samples_per_second': 29.709, 'train_steps_per_second': 0.986, 'train_loss': 0.5187287953435158, 'epoch': 69.99}\n",
            "100% 3920/3920 [1:06:17<00:00,  1.01s/it]\n",
            "Saving model checkpoint to results/citation-intent_4/\n",
            "Configuration saved in results/citation-intent_4/mlm/adapter_config.json\n",
            "Module weights saved in results/citation-intent_4/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/citation-intent_4/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/mlm/head_config.json\n",
            "Module weights saved in results/citation-intent_4/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/citation-intent_4/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/citation-intent_4/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/citation-intent_4/config.json\n",
            "Model weights saved in results/citation-intent_4/pytorch_model.bin\n",
            "tokenizer config file saved in results/citation-intent_4/tokenizer_config.json\n",
            "Special tokens file saved in results/citation-intent_4/special_tokens_map.json\n",
            "08/03/2021 15:21:42 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 15\n",
            "100% 8/8 [00:01<00:00,  6.25it/s]\n",
            "08/03/2021 15:21:44 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_loss = 0.696053147315979\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_acc = 0.7631578947368421\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_f1 = 0.6383244206773618\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_precision = 0.69472049689441\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_recall = 0.6122746838848534\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_runtime = 1.489\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_samples_per_second = 76.562\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   eval_steps_per_second = 5.373\n",
            "08/03/2021 15:21:44 - INFO - __main__ -   epoch = 69.99\n",
            "08/03/2021 15:21:44 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 139\n",
            "  Batch size = 15\n",
            "100% 10/10 [00:01<00:00,  6.29it/s]\n",
            "08/03/2021 15:21:46 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_loss = 0.8924270868301392\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_acc = 0.7841726618705036\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_f1 = 0.6635604701469948\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_precision = 0.6846014492753624\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_recall = 0.6489222514574627\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_runtime = 1.7818\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_samples_per_second = 78.01\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   eval_steps_per_second = 5.612\n",
            "08/03/2021 15:21:46 - INFO - __main__ -   epoch = 69.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXsQ82ePKVld"
      },
      "source": [
        "# Experiment_8_1_citation-intent_5\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 15 \\\n",
        "--gradient_accumulation_steps 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 70 \\\n",
        "--output_dir results/citation-intent_5/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path results/citation-intent_4/checkpoint-3920 \\\n",
        "--adapter_1 results/adapters/citation-intent/mlm \\\n",
        "--adapter_2 results/adapters/sciie/mlm \\\n",
        "--per_device_eval_batch_size 15 \\\n",
        "--weight_decay 0.0001 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.97 \\\n",
        "--adam_epsilon 5e-5 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 836 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6cBPa1RrugH",
        "outputId": "c03ec673-2a09-47a0-9de1-25fb4dc3626f"
      },
      "source": [
        "# Experiment_9_sciie_3\n",
        "!python3 run_multiple_choice_adapter_fusion.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/sciie_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 15 \\\n",
        "--gradient_accumulation_steps 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 70 \\\n",
        "--output_dir results/sciie_3/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/citation-intent/mlm \\\n",
        "--adapter_2 results/adapters/sciie/mlm \\\n",
        "--per_device_eval_batch_size 15 \\\n",
        "--weight_decay 0.0001 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.97 \\\n",
        "--adam_epsilon 5e-5 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 836 \\\n",
        "--avg_type macro \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 15:44:21.768214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 15:44:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3219\n",
            "  Num Epochs = 70\n",
            "  Instantaneous batch size per device = 15\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 30\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 7490\n",
            "  1% 107/7490 [01:37<1:51:38,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.24it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.5781819820404053, 'eval_acc': 0.4703296703296703, 'eval_f1': 0.09139440529575059, 'eval_precision': 0.06718995290423861, 'eval_recall': 0.14285714285714285, 'eval_runtime': 5.9727, 'eval_samples_per_second': 76.18, 'eval_steps_per_second': 5.19, 'epoch': 1.0}\n",
            "  1% 107/7490 [01:43<1:51:38,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-107\n",
            "Configuration saved in results/sciie_3/checkpoint-107/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-107/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-107/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-107/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-107/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-107/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-107/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-107/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-107/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-107/pytorch_model.bin\n",
            "  3% 214/7490 [03:25<1:50:09,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.96it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.23it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.430959701538086, 'eval_acc': 0.4703296703296703, 'eval_f1': 0.09139440529575059, 'eval_precision': 0.06718995290423861, 'eval_recall': 0.14285714285714285, 'eval_runtime': 5.9746, 'eval_samples_per_second': 76.155, 'eval_steps_per_second': 5.189, 'epoch': 2.0}\n",
            "  3% 214/7490 [03:31<1:50:09,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-214\n",
            "Configuration saved in results/sciie_3/checkpoint-214/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-214/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-214/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-214/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-214/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-214/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-214/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-214/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-214/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-214/pytorch_model.bin\n",
            "  4% 321/7490 [05:12<1:48:42,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.52it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.97it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.58it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.48it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.41it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.27it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.27it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.27it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.26it/s]\u001b[A\n",
            " 71% 22/31 [00:03<00:01,  5.26it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.24it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.2515171766281128, 'eval_acc': 0.589010989010989, 'eval_f1': 0.25984162895927604, 'eval_precision': 0.2316620260282232, 'eval_recall': 0.29984114412436924, 'eval_runtime': 5.957, 'eval_samples_per_second': 76.381, 'eval_steps_per_second': 5.204, 'epoch': 3.0}\n",
            "  4% 321/7490 [05:18<1:48:42,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-321\n",
            "Configuration saved in results/sciie_3/checkpoint-321/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-321/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-321/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-321/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-321/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-321/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-321/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-321/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-321/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-321/pytorch_model.bin\n",
            "  6% 428/7490 [07:00<1:47:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.36it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.26it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.26it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.26it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.26it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.26it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.25it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.129380464553833, 'eval_acc': 0.6263736263736264, 'eval_f1': 0.3135358436839112, 'eval_precision': 0.551972353753576, 'eval_recall': 0.34957405190875257, 'eval_runtime': 5.9595, 'eval_samples_per_second': 76.348, 'eval_steps_per_second': 5.202, 'epoch': 4.0}\n",
            "  6% 428/7490 [07:06<1:47:01,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-428\n",
            "Configuration saved in results/sciie_3/checkpoint-428/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-428/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-428/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-428/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-428/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-428/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-428/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-428/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-428/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-428/pytorch_model.bin\n",
            "{'loss': 1.3158, 'learning_rate': 3.732977303070761e-05, 'epoch': 4.67}\n",
            "  7% 535/7490 [08:48<1:45:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.24it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.9675968885421753, 'eval_acc': 0.6747252747252748, 'eval_f1': 0.4286448752310097, 'eval_precision': 0.4991272684622438, 'eval_recall': 0.4391711464248703, 'eval_runtime': 5.9676, 'eval_samples_per_second': 76.245, 'eval_steps_per_second': 5.195, 'epoch': 5.0}\n",
            "  7% 535/7490 [08:54<1:45:32,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-535\n",
            "Configuration saved in results/sciie_3/checkpoint-535/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-535/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-535/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-535/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-535/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-535/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-535/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-535/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-535/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-535/pytorch_model.bin\n",
            "  9% 642/7490 [10:36<1:43:55,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.29it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.23it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.9016360640525818, 'eval_acc': 0.7208791208791209, 'eval_f1': 0.5147523829258185, 'eval_precision': 0.516970998925886, 'eval_recall': 0.5317075655272152, 'eval_runtime': 5.9808, 'eval_samples_per_second': 76.077, 'eval_steps_per_second': 5.183, 'epoch': 6.0}\n",
            "  9% 642/7490 [10:42<1:43:55,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-642\n",
            "Configuration saved in results/sciie_3/checkpoint-642/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-642/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-642/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-642/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-642/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-642/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-642/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-642/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-642/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-642/pytorch_model.bin\n",
            " 10% 749/7490 [12:23<1:42:18,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.50it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.43it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.31it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.8268121480941772, 'eval_acc': 0.7318681318681318, 'eval_f1': 0.5390063879661062, 'eval_precision': 0.6389286729795536, 'eval_recall': 0.524462022609742, 'eval_runtime': 5.9811, 'eval_samples_per_second': 76.072, 'eval_steps_per_second': 5.183, 'epoch': 7.0}\n",
            " 10% 749/7490 [12:29<1:42:18,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-749\n",
            "Configuration saved in results/sciie_3/checkpoint-749/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-749/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-749/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-749/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-749/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-749/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-749/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-749/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-749/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-749/pytorch_model.bin\n",
            " 11% 856/7490 [14:11<1:40:41,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.29it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.23it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 97% 30/31 [00:05<00:00,  5.23it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.761128306388855, 'eval_acc': 0.7472527472527473, 'eval_f1': 0.563853818034441, 'eval_precision': 0.6641268137136008, 'eval_recall': 0.5548849960052896, 'eval_runtime': 5.9845, 'eval_samples_per_second': 76.03, 'eval_steps_per_second': 5.18, 'epoch': 8.0}\n",
            " 11% 856/7490 [14:17<1:40:41,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-856\n",
            "Configuration saved in results/sciie_3/checkpoint-856/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-856/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-856/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-856/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-856/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-856/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-856/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-856/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-856/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-856/pytorch_model.bin\n",
            " 13% 963/7490 [15:58<1:38:55,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6699358820915222, 'eval_acc': 0.7714285714285715, 'eval_f1': 0.6061217871039186, 'eval_precision': 0.7953306887887316, 'eval_recall': 0.6002120230176322, 'eval_runtime': 5.9846, 'eval_samples_per_second': 76.028, 'eval_steps_per_second': 5.18, 'epoch': 9.0}\n",
            " 13% 963/7490 [16:04<1:38:55,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-963\n",
            "Configuration saved in results/sciie_3/checkpoint-963/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-963/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-963/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-963/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-963/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-963/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-963/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-963/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-963/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-963/pytorch_model.bin\n",
            "{'loss': 0.7691, 'learning_rate': 3.465954606141522e-05, 'epoch': 9.34}\n",
            " 14% 1070/7490 [17:45<1:37:14,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.42it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.03it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6175515651702881, 'eval_acc': 0.7846153846153846, 'eval_f1': 0.6383221219455371, 'eval_precision': 0.745139218478089, 'eval_recall': 0.6260490001476102, 'eval_runtime': 5.9801, 'eval_samples_per_second': 76.085, 'eval_steps_per_second': 5.184, 'epoch': 10.0}\n",
            " 14% 1070/7490 [17:51<1:37:14,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1070\n",
            "Configuration saved in results/sciie_3/checkpoint-1070/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1070/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1070/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1070/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1070/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1070/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1070/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1070/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1070/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1070/pytorch_model.bin\n",
            " 16% 1177/7490 [19:32<1:35:50,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.32it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6263189911842346, 'eval_acc': 0.7846153846153846, 'eval_f1': 0.6064758773519049, 'eval_precision': 0.6768278191931154, 'eval_recall': 0.601031541403516, 'eval_runtime': 5.9782, 'eval_samples_per_second': 76.11, 'eval_steps_per_second': 5.186, 'epoch': 11.0}\n",
            " 16% 1177/7490 [19:38<1:35:50,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1177\n",
            "Configuration saved in results/sciie_3/checkpoint-1177/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1177/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1177/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1177/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1177/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1177/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1177/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1177/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1177/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1177/pytorch_model.bin\n",
            " 17% 1284/7490 [21:19<1:34:06,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.21it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4961385726928711, 'eval_acc': 0.8263736263736263, 'eval_f1': 0.7356070991437518, 'eval_precision': 0.7579245025606937, 'eval_recall': 0.7357078657383858, 'eval_runtime': 5.9878, 'eval_samples_per_second': 75.988, 'eval_steps_per_second': 5.177, 'epoch': 12.0}\n",
            " 17% 1284/7490 [21:26<1:34:06,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1284\n",
            "Configuration saved in results/sciie_3/checkpoint-1284/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1284/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1284/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1284/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1284/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1284/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1284/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1284/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1284/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1284/pytorch_model.bin\n",
            " 19% 1391/7490 [23:07<1:32:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4783102571964264, 'eval_acc': 0.8395604395604396, 'eval_f1': 0.7827477259898066, 'eval_precision': 0.8271559831280005, 'eval_recall': 0.7718139390369928, 'eval_runtime': 5.9692, 'eval_samples_per_second': 76.225, 'eval_steps_per_second': 5.193, 'epoch': 13.0}\n",
            " 19% 1391/7490 [23:13<1:32:23,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1391\n",
            "Configuration saved in results/sciie_3/checkpoint-1391/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1391/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1391/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1391/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1391/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1391/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1391/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1391/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1391/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1391/pytorch_model.bin\n",
            " 20% 1498/7490 [24:54<1:30:52,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.44it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4604373276233673, 'eval_acc': 0.8461538461538461, 'eval_f1': 0.7813581128290121, 'eval_precision': 0.8206236761902276, 'eval_recall': 0.761238191721547, 'eval_runtime': 5.9773, 'eval_samples_per_second': 76.121, 'eval_steps_per_second': 5.186, 'epoch': 14.0}\n",
            " 20% 1498/7490 [25:00<1:30:52,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1498\n",
            "Configuration saved in results/sciie_3/checkpoint-1498/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1498/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1498/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1498/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1498/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1498/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1498/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1498/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1498/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1498/pytorch_model.bin\n",
            "{'loss': 0.5268, 'learning_rate': 3.1989319092122837e-05, 'epoch': 14.02}\n",
            " 21% 1605/7490 [26:42<1:29:15,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.45210930705070496, 'eval_acc': 0.865934065934066, 'eval_f1': 0.8107005567846542, 'eval_precision': 0.850843854162858, 'eval_recall': 0.783203203546174, 'eval_runtime': 5.9825, 'eval_samples_per_second': 76.055, 'eval_steps_per_second': 5.182, 'epoch': 15.0}\n",
            " 21% 1605/7490 [26:48<1:29:15,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1605\n",
            "Configuration saved in results/sciie_3/checkpoint-1605/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1605/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1605/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1605/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1605/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1605/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1605/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1605/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1605/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1605/pytorch_model.bin\n",
            " 23% 1712/7490 [28:29<1:27:32,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44162869453430176, 'eval_acc': 0.8461538461538461, 'eval_f1': 0.7871703684365592, 'eval_precision': 0.818273009025549, 'eval_recall': 0.7743065312638633, 'eval_runtime': 5.9685, 'eval_samples_per_second': 76.233, 'eval_steps_per_second': 5.194, 'epoch': 16.0}\n",
            " 23% 1712/7490 [28:35<1:27:32,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1712\n",
            "Configuration saved in results/sciie_3/checkpoint-1712/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1712/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1712/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1712/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1712/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1712/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1712/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1712/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1712/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1712/pytorch_model.bin\n",
            " 24% 1819/7490 [30:17<1:26:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4340340495109558, 'eval_acc': 0.8527472527472527, 'eval_f1': 0.795922111474115, 'eval_precision': 0.7995843782789993, 'eval_recall': 0.8011469289792755, 'eval_runtime': 5.9765, 'eval_samples_per_second': 76.131, 'eval_steps_per_second': 5.187, 'epoch': 17.0}\n",
            " 24% 1819/7490 [30:23<1:26:01,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1819\n",
            "Configuration saved in results/sciie_3/checkpoint-1819/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1819/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1819/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1819/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1819/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1819/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1819/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1819/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1819/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1819/pytorch_model.bin\n",
            " 26% 1926/7490 [32:05<1:24:18,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.52it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.36it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.42911458015441895, 'eval_acc': 0.8483516483516483, 'eval_f1': 0.8011222623668163, 'eval_precision': 0.8011781779318295, 'eval_recall': 0.8123746550874072, 'eval_runtime': 5.9687, 'eval_samples_per_second': 76.231, 'eval_steps_per_second': 5.194, 'epoch': 18.0}\n",
            " 26% 1926/7490 [32:11<1:24:18,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-1926\n",
            "Configuration saved in results/sciie_3/checkpoint-1926/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1926/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1926/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1926/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1926/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1926/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1926/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-1926/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-1926/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-1926/pytorch_model.bin\n",
            "{'loss': 0.3778, 'learning_rate': 2.9319092122830443e-05, 'epoch': 18.69}\n",
            " 27% 2033/7490 [33:52<1:22:52,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.42800360918045044, 'eval_acc': 0.8549450549450549, 'eval_f1': 0.806046223547828, 'eval_precision': 0.8181461225021989, 'eval_recall': 0.8008718765849508, 'eval_runtime': 5.982, 'eval_samples_per_second': 76.061, 'eval_steps_per_second': 5.182, 'epoch': 19.0}\n",
            " 27% 2033/7490 [33:58<1:22:52,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2033\n",
            "Configuration saved in results/sciie_3/checkpoint-2033/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2033/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2033/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2033/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2033/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2033/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2033/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2033/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2033/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2033/pytorch_model.bin\n",
            " 29% 2140/7490 [35:40<1:21:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4442509114742279, 'eval_acc': 0.8615384615384616, 'eval_f1': 0.8080348451736062, 'eval_precision': 0.8542766267992029, 'eval_recall': 0.7816125447527537, 'eval_runtime': 5.9705, 'eval_samples_per_second': 76.208, 'eval_steps_per_second': 5.192, 'epoch': 20.0}\n",
            " 29% 2140/7490 [35:46<1:21:01,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2140\n",
            "Configuration saved in results/sciie_3/checkpoint-2140/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2140/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2140/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2140/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2140/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2140/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2140/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2140/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2140/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2140/pytorch_model.bin\n",
            " 30% 2247/7490 [37:27<1:19:30,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4303729832172394, 'eval_acc': 0.8637362637362638, 'eval_f1': 0.8171528272370877, 'eval_precision': 0.8329607997440887, 'eval_recall': 0.808094613668457, 'eval_runtime': 5.9717, 'eval_samples_per_second': 76.193, 'eval_steps_per_second': 5.191, 'epoch': 21.0}\n",
            " 30% 2247/7490 [37:33<1:19:30,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2247\n",
            "Configuration saved in results/sciie_3/checkpoint-2247/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2247/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2247/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2247/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2247/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2247/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2247/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2247/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2247/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2247/pytorch_model.bin\n",
            " 31% 2354/7490 [39:14<1:17:54,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.45it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.03it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.40922415256500244, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8382253620966006, 'eval_precision': 0.8471516623202362, 'eval_recall': 0.8347441173048977, 'eval_runtime': 5.9744, 'eval_samples_per_second': 76.158, 'eval_steps_per_second': 5.189, 'epoch': 22.0}\n",
            " 31% 2354/7490 [39:21<1:17:54,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2354\n",
            "Configuration saved in results/sciie_3/checkpoint-2354/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2354/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2354/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2354/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2354/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2354/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2354/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2354/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2354/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2354/pytorch_model.bin\n",
            " 33% 2461/7490 [41:02<1:16:14,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.39552009105682373, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8349582219265318, 'eval_precision': 0.8539154260451299, 'eval_recall': 0.8256512314306713, 'eval_runtime': 5.9793, 'eval_samples_per_second': 76.095, 'eval_steps_per_second': 5.185, 'epoch': 23.0}\n",
            " 33% 2461/7490 [41:08<1:16:14,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2461\n",
            "Configuration saved in results/sciie_3/checkpoint-2461/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2461/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2461/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2461/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2461/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2461/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2461/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2461/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2461/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2461/pytorch_model.bin\n",
            "{'loss': 0.3044, 'learning_rate': 2.6648865153538053e-05, 'epoch': 23.36}\n",
            " 34% 2568/7490 [42:49<1:14:39,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.42436060309410095, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.8190165389910034, 'eval_precision': 0.8463148759571977, 'eval_recall': 0.8012996545303511, 'eval_runtime': 5.9789, 'eval_samples_per_second': 76.1, 'eval_steps_per_second': 5.185, 'epoch': 24.0}\n",
            " 34% 2568/7490 [42:55<1:14:39,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2568\n",
            "Configuration saved in results/sciie_3/checkpoint-2568/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2568/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2568/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2568/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2568/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2568/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2568/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2568/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2568/pytorch_model.bin\n",
            " 36% 2675/7490 [44:36<1:13:02,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.69it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.42947351932525635, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8137732409229116, 'eval_precision': 0.8386003333281019, 'eval_recall': 0.8098756090185789, 'eval_runtime': 5.9799, 'eval_samples_per_second': 76.088, 'eval_steps_per_second': 5.184, 'epoch': 25.0}\n",
            " 36% 2675/7490 [44:42<1:13:02,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2675\n",
            "Configuration saved in results/sciie_3/checkpoint-2675/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2675/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2675/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2675/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2675/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2675/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2675/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2675/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2675/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2675/pytorch_model.bin\n",
            " 37% 2782/7490 [46:23<1:11:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.43it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.03it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44281038641929626, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8153347589671592, 'eval_precision': 0.8610665768564034, 'eval_recall': 0.79116691168063, 'eval_runtime': 5.9765, 'eval_samples_per_second': 76.132, 'eval_steps_per_second': 5.187, 'epoch': 26.0}\n",
            " 37% 2782/7490 [46:29<1:11:29,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2782\n",
            "Configuration saved in results/sciie_3/checkpoint-2782/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2782/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2782/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2782/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2782/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2782/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2782/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2782/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2782/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2782/pytorch_model.bin\n",
            " 39% 2889/7490 [48:11<1:09:45,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.96it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.97it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4162651300430298, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.8263915570284761, 'eval_precision': 0.8260199919465764, 'eval_recall': 0.8302362954666699, 'eval_runtime': 5.9724, 'eval_samples_per_second': 76.184, 'eval_steps_per_second': 5.191, 'epoch': 27.0}\n",
            " 39% 2889/7490 [48:17<1:09:45,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2889\n",
            "Configuration saved in results/sciie_3/checkpoint-2889/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2889/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2889/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2889/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2889/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2889/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2889/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2889/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2889/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2889/pytorch_model.bin\n",
            " 40% 2996/7490 [49:58<1:08:12,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4433101713657379, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8288471141997038, 'eval_precision': 0.883517977555918, 'eval_recall': 0.7945374879725249, 'eval_runtime': 5.9794, 'eval_samples_per_second': 76.095, 'eval_steps_per_second': 5.184, 'epoch': 28.0}\n",
            " 40% 2996/7490 [50:04<1:08:12,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-2996\n",
            "Configuration saved in results/sciie_3/checkpoint-2996/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2996/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2996/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2996/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2996/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2996/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2996/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-2996/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-2996/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-2996/pytorch_model.bin\n",
            "{'loss': 0.2616, 'learning_rate': 2.3978638184245663e-05, 'epoch': 28.04}\n",
            " 41% 3103/7490 [51:45<1:06:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.50it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.37481844425201416, 'eval_acc': 0.8835164835164835, 'eval_f1': 0.8473898917633659, 'eval_precision': 0.8483281949765998, 'eval_recall': 0.8485357538983609, 'eval_runtime': 5.9719, 'eval_samples_per_second': 76.19, 'eval_steps_per_second': 5.191, 'epoch': 29.0}\n",
            " 41% 3103/7490 [51:51<1:06:29,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3103\n",
            "Configuration saved in results/sciie_3/checkpoint-3103/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3103/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3103/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3103/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3103/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3103/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3103/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3103/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3103/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3103/pytorch_model.bin\n",
            " 43% 3210/7490 [53:32<1:04:51,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.33it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.21it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3957216441631317, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8384918560421476, 'eval_precision': 0.8502441167854702, 'eval_recall': 0.8318558355274873, 'eval_runtime': 5.976, 'eval_samples_per_second': 76.137, 'eval_steps_per_second': 5.187, 'epoch': 30.0}\n",
            " 43% 3210/7490 [53:38<1:04:51,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3210\n",
            "Configuration saved in results/sciie_3/checkpoint-3210/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3210/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3210/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3210/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3210/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3210/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3210/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3210/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3210/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3210/pytorch_model.bin\n",
            " 44% 3317/7490 [55:19<1:03:16,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.45it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3743683993816376, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8371134856126973, 'eval_precision': 0.8367532865859522, 'eval_recall': 0.8399655991629518, 'eval_runtime': 5.9742, 'eval_samples_per_second': 76.161, 'eval_steps_per_second': 5.189, 'epoch': 31.0}\n",
            " 44% 3317/7490 [55:25<1:03:16,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3317\n",
            "Configuration saved in results/sciie_3/checkpoint-3317/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3317/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3317/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3317/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3317/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3317/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3317/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3317/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3317/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3317/pytorch_model.bin\n",
            " 46% 3424/7490 [57:07<1:01:43,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.40545809268951416, 'eval_acc': 0.8835164835164835, 'eval_f1': 0.8343946837266915, 'eval_precision': 0.846754970714264, 'eval_recall': 0.8284621493263348, 'eval_runtime': 5.9795, 'eval_samples_per_second': 76.093, 'eval_steps_per_second': 5.184, 'epoch': 32.0}\n",
            " 46% 3424/7490 [57:13<1:01:43,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3424\n",
            "Configuration saved in results/sciie_3/checkpoint-3424/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3424/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3424/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3424/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3424/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3424/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3424/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3424/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3424/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3424/pytorch_model.bin\n",
            "{'loss': 0.226, 'learning_rate': 2.130841121495327e-05, 'epoch': 32.71}\n",
            " 47% 3531/7490 [58:54<1:00:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.53it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4105958938598633, 'eval_acc': 0.8637362637362638, 'eval_f1': 0.8249000271006436, 'eval_precision': 0.8133674373062952, 'eval_recall': 0.8431361737841391, 'eval_runtime': 5.9776, 'eval_samples_per_second': 76.117, 'eval_steps_per_second': 5.186, 'epoch': 33.0}\n",
            " 47% 3531/7490 [59:00<1:00:01,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3531\n",
            "Configuration saved in results/sciie_3/checkpoint-3531/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3531/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3531/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3531/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3531/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3531/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3531/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3531/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3531/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3531/pytorch_model.bin\n",
            " 49% 3638/7490 [1:00:41<58:28,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4004649519920349, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8296434093113783, 'eval_precision': 0.8246414158675043, 'eval_recall': 0.8433401421219184, 'eval_runtime': 5.9805, 'eval_samples_per_second': 76.08, 'eval_steps_per_second': 5.183, 'epoch': 34.0}\n",
            " 49% 3638/7490 [1:00:47<58:28,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3638\n",
            "Configuration saved in results/sciie_3/checkpoint-3638/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3638/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3638/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3638/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3638/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3638/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3638/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3638/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3638/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3638/pytorch_model.bin\n",
            " 50% 3745/7490 [1:02:29<56:42,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.45it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.40073537826538086, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8450100481076132, 'eval_precision': 0.860838119510464, 'eval_recall': 0.8356957498804646, 'eval_runtime': 5.9794, 'eval_samples_per_second': 76.095, 'eval_steps_per_second': 5.184, 'epoch': 35.0}\n",
            " 50% 3745/7490 [1:02:35<56:42,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3745\n",
            "Configuration saved in results/sciie_3/checkpoint-3745/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3745/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3745/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3745/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3745/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3745/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3745/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3745/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3745/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3745/pytorch_model.bin\n",
            " 51% 3852/7490 [1:04:17<55:05,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4202623665332794, 'eval_acc': 0.8725274725274725, 'eval_f1': 0.824351136125382, 'eval_precision': 0.8278788996504117, 'eval_recall': 0.8286741750698037, 'eval_runtime': 5.9725, 'eval_samples_per_second': 76.182, 'eval_steps_per_second': 5.19, 'epoch': 36.0}\n",
            " 51% 3852/7490 [1:04:23<55:05,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3852\n",
            "Configuration saved in results/sciie_3/checkpoint-3852/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3852/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3852/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3852/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3852/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3852/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3852/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3852/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3852/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3852/pytorch_model.bin\n",
            " 53% 3959/7490 [1:06:04<53:31,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.54it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.10it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.97it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.97it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.58it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.48it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.41it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.36it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.33it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.26it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.26it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4088957607746124, 'eval_acc': 0.8835164835164835, 'eval_f1': 0.8458170641034515, 'eval_precision': 0.8549015230354067, 'eval_recall': 0.8376829877798733, 'eval_runtime': 5.9559, 'eval_samples_per_second': 76.395, 'eval_steps_per_second': 5.205, 'epoch': 37.0}\n",
            " 53% 3959/7490 [1:06:10<53:31,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-3959\n",
            "Configuration saved in results/sciie_3/checkpoint-3959/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3959/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3959/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3959/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3959/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3959/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3959/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-3959/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-3959/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-3959/pytorch_model.bin\n",
            "{'loss': 0.2044, 'learning_rate': 1.8638184245660883e-05, 'epoch': 37.38}\n",
            " 54% 4066/7490 [1:07:52<51:55,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.41229668259620667, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8242421429788579, 'eval_precision': 0.8400160863584508, 'eval_recall': 0.8110432788988875, 'eval_runtime': 5.9796, 'eval_samples_per_second': 76.092, 'eval_steps_per_second': 5.184, 'epoch': 38.0}\n",
            " 54% 4066/7490 [1:07:58<51:55,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4066\n",
            "Configuration saved in results/sciie_3/checkpoint-4066/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4066/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4066/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4066/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4066/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4066/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4066/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4066/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4066/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4066/pytorch_model.bin\n",
            " 56% 4173/7490 [1:09:39<50:20,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.30it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.25it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4357677400112152, 'eval_acc': 0.8549450549450549, 'eval_f1': 0.8074652665380482, 'eval_precision': 0.8231672729987869, 'eval_recall': 0.8131610666828636, 'eval_runtime': 5.9777, 'eval_samples_per_second': 76.117, 'eval_steps_per_second': 5.186, 'epoch': 39.0}\n",
            " 56% 4173/7490 [1:09:45<50:20,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4173\n",
            "Configuration saved in results/sciie_3/checkpoint-4173/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4173/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4173/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4173/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4173/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4173/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4173/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4173/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4173/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4173/pytorch_model.bin\n",
            " 57% 4280/7490 [1:11:26<48:35,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.34it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.73it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4464303255081177, 'eval_acc': 0.8615384615384616, 'eval_f1': 0.8076605682178617, 'eval_precision': 0.8037434665634031, 'eval_recall': 0.8183050676846525, 'eval_runtime': 5.969, 'eval_samples_per_second': 76.228, 'eval_steps_per_second': 5.194, 'epoch': 40.0}\n",
            " 57% 4280/7490 [1:11:32<48:35,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4280\n",
            "Configuration saved in results/sciie_3/checkpoint-4280/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4280/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4280/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4280/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4280/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4280/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4280/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4280/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4280/pytorch_model.bin\n",
            " 59% 4387/7490 [1:13:14<47:01,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4528147280216217, 'eval_acc': 0.865934065934066, 'eval_f1': 0.8243260528208356, 'eval_precision': 0.8223296277133013, 'eval_recall': 0.8386454088524188, 'eval_runtime': 5.9805, 'eval_samples_per_second': 76.08, 'eval_steps_per_second': 5.184, 'epoch': 41.0}\n",
            " 59% 4387/7490 [1:13:20<47:01,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.21it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4387\n",
            "Configuration saved in results/sciie_3/checkpoint-4387/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4387/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4387/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4387/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4387/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4387/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4387/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4387/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4387/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4387/pytorch_model.bin\n",
            " 60% 4494/7490 [1:15:01<45:23,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4395444393157959, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8463246365738352, 'eval_precision': 0.8763225773990061, 'eval_recall': 0.8280585433122823, 'eval_runtime': 5.9734, 'eval_samples_per_second': 76.171, 'eval_steps_per_second': 5.19, 'epoch': 42.0}\n",
            " 60% 4494/7490 [1:15:07<45:23,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4494\n",
            "Configuration saved in results/sciie_3/checkpoint-4494/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4494/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4494/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4494/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4494/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4494/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4494/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4494/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4494/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4494/pytorch_model.bin\n",
            "{'loss': 0.1854, 'learning_rate': 1.5967957276368493e-05, 'epoch': 42.06}\n",
            " 61% 4601/7490 [1:16:48<43:52,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4229712188243866, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8298464659164991, 'eval_precision': 0.8442301962526216, 'eval_recall': 0.8202474459908833, 'eval_runtime': 5.9818, 'eval_samples_per_second': 76.064, 'eval_steps_per_second': 5.182, 'epoch': 43.0}\n",
            " 61% 4601/7490 [1:16:54<43:52,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4601\n",
            "Configuration saved in results/sciie_3/checkpoint-4601/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4601/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4601/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4601/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4601/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4601/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4601/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4601/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4601/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4601/pytorch_model.bin\n",
            " 63% 4708/7490 [1:18:35<42:07,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4362955689430237, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8367925189579111, 'eval_precision': 0.8353049969376569, 'eval_recall': 0.8427589013654438, 'eval_runtime': 5.9739, 'eval_samples_per_second': 76.165, 'eval_steps_per_second': 5.189, 'epoch': 44.0}\n",
            " 63% 4708/7490 [1:18:41<42:07,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4708\n",
            "Configuration saved in results/sciie_3/checkpoint-4708/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4708/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4708/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4708/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4708/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4708/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4708/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4708/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4708/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4708/pytorch_model.bin\n",
            " 64% 4815/7490 [1:20:22<40:31,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.45it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3944613039493561, 'eval_acc': 0.8879120879120879, 'eval_f1': 0.8479534514029271, 'eval_precision': 0.8632449624599846, 'eval_recall': 0.8376812220354282, 'eval_runtime': 5.9715, 'eval_samples_per_second': 76.195, 'eval_steps_per_second': 5.191, 'epoch': 45.0}\n",
            " 64% 4815/7490 [1:20:28<40:31,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4815\n",
            "Configuration saved in results/sciie_3/checkpoint-4815/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4815/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4815/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4815/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4815/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4815/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4815/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4815/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4815/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4815/pytorch_model.bin\n",
            " 66% 4922/7490 [1:22:09<38:53,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.44it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.03it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.41614267230033875, 'eval_acc': 0.8945054945054945, 'eval_f1': 0.8557191869132524, 'eval_precision': 0.8822053908224365, 'eval_recall': 0.8397098795714065, 'eval_runtime': 5.9727, 'eval_samples_per_second': 76.18, 'eval_steps_per_second': 5.19, 'epoch': 46.0}\n",
            " 66% 4922/7490 [1:22:15<38:53,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-4922\n",
            "Configuration saved in results/sciie_3/checkpoint-4922/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4922/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4922/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4922/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4922/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4922/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4922/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-4922/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-4922/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-4922/pytorch_model.bin\n",
            "{'loss': 0.1658, 'learning_rate': 1.3297730307076101e-05, 'epoch': 46.73}\n",
            " 67% 5029/7490 [1:23:56<37:22,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.412774920463562, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8488730139572113, 'eval_precision': 0.853778278734234, 'eval_recall': 0.8451705313434396, 'eval_runtime': 5.9781, 'eval_samples_per_second': 76.111, 'eval_steps_per_second': 5.186, 'epoch': 47.0}\n",
            " 67% 5029/7490 [1:24:02<37:22,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5029\n",
            "Configuration saved in results/sciie_3/checkpoint-5029/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5029/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5029/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5029/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5029/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5029/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5029/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5029/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5029/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5029/pytorch_model.bin\n",
            " 69% 5136/7490 [1:25:44<35:42,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.50it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4327075183391571, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8395402921963543, 'eval_precision': 0.8452097780488578, 'eval_recall': 0.8390856703908112, 'eval_runtime': 5.9823, 'eval_samples_per_second': 76.057, 'eval_steps_per_second': 5.182, 'epoch': 48.0}\n",
            " 69% 5136/7490 [1:25:50<35:42,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5136\n",
            "Configuration saved in results/sciie_3/checkpoint-5136/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5136/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5136/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5136/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5136/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5136/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5136/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5136/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5136/pytorch_model.bin\n",
            " 70% 5243/7490 [1:27:31<34:02,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.50it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.96it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4030398726463318, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8427359692599907, 'eval_precision': 0.8552318175888729, 'eval_recall': 0.8325416255193412, 'eval_runtime': 5.9713, 'eval_samples_per_second': 76.198, 'eval_steps_per_second': 5.191, 'epoch': 49.0}\n",
            " 70% 5243/7490 [1:27:37<34:02,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5243\n",
            "Configuration saved in results/sciie_3/checkpoint-5243/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5243/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5243/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5243/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5243/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5243/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5243/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5243/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5243/pytorch_model.bin\n",
            " 71% 5350/7490 [1:29:19<32:25,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4100230038166046, 'eval_acc': 0.8901098901098901, 'eval_f1': 0.8553655209038594, 'eval_precision': 0.866413800251876, 'eval_recall': 0.8455773398050555, 'eval_runtime': 5.9677, 'eval_samples_per_second': 76.244, 'eval_steps_per_second': 5.195, 'epoch': 50.0}\n",
            " 71% 5350/7490 [1:29:25<32:25,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5350\n",
            "Configuration saved in results/sciie_3/checkpoint-5350/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5350/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5350/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5350/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5350/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5350/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5350/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5350/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5350/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5350/pytorch_model.bin\n",
            " 73% 5457/7490 [1:31:07<30:47,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4197971224784851, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8465154263428856, 'eval_precision': 0.8521390875909443, 'eval_recall': 0.8433151951333987, 'eval_runtime': 5.9694, 'eval_samples_per_second': 76.221, 'eval_steps_per_second': 5.193, 'epoch': 51.0}\n",
            " 73% 5457/7490 [1:31:13<30:47,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5457\n",
            "Configuration saved in results/sciie_3/checkpoint-5457/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5457/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5457/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5457/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5457/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5457/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5457/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5457/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5457/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5457/pytorch_model.bin\n",
            "{'loss': 0.157, 'learning_rate': 1.0627503337783713e-05, 'epoch': 51.4}\n",
            " 74% 5564/7490 [1:32:55<29:12,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4106495678424835, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8426676708323529, 'eval_precision': 0.8411031186390682, 'eval_recall': 0.8446608635994932, 'eval_runtime': 5.9835, 'eval_samples_per_second': 76.043, 'eval_steps_per_second': 5.181, 'epoch': 52.0}\n",
            " 74% 5564/7490 [1:33:01<29:12,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5564\n",
            "Configuration saved in results/sciie_3/checkpoint-5564/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5564/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5564/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5564/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5564/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5564/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5564/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5564/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5564/pytorch_model.bin\n",
            " 76% 5671/7490 [1:34:42<27:33,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.50it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.95it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.21it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.41357138752937317, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8477896247365856, 'eval_precision': 0.8552466705061654, 'eval_recall': 0.8412259206288699, 'eval_runtime': 5.9753, 'eval_samples_per_second': 76.147, 'eval_steps_per_second': 5.188, 'epoch': 53.0}\n",
            " 76% 5671/7490 [1:34:48<27:33,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5671\n",
            "Configuration saved in results/sciie_3/checkpoint-5671/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5671/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5671/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5671/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5671/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5671/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5671/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5671/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5671/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5671/pytorch_model.bin\n",
            " 77% 5778/7490 [1:36:29<25:55,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.42314448952674866, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8264117927283176, 'eval_precision': 0.832017103976186, 'eval_recall': 0.8237549475183437, 'eval_runtime': 5.9798, 'eval_samples_per_second': 76.089, 'eval_steps_per_second': 5.184, 'epoch': 54.0}\n",
            " 77% 5778/7490 [1:36:35<25:55,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5778\n",
            "Configuration saved in results/sciie_3/checkpoint-5778/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5778/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5778/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5778/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5778/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5778/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5778/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5778/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5778/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5778/pytorch_model.bin\n",
            " 79% 5885/7490 [1:38:17<24:18,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4211408793926239, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8545726610680141, 'eval_precision': 0.8624605136431862, 'eval_recall': 0.8480402291283916, 'eval_runtime': 5.9711, 'eval_samples_per_second': 76.2, 'eval_steps_per_second': 5.192, 'epoch': 55.0}\n",
            " 79% 5885/7490 [1:38:23<24:18,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5885\n",
            "Configuration saved in results/sciie_3/checkpoint-5885/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5885/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5885/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5885/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5885/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5885/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5885/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5885/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5885/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5885/pytorch_model.bin\n",
            " 80% 5992/7490 [1:40:04<22:43,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4657749533653259, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.8384207992835007, 'eval_precision': 0.8446254626494406, 'eval_recall': 0.8465024924768024, 'eval_runtime': 5.9741, 'eval_samples_per_second': 76.162, 'eval_steps_per_second': 5.189, 'epoch': 56.0}\n",
            " 80% 5992/7490 [1:40:10<22:43,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-5992\n",
            "Configuration saved in results/sciie_3/checkpoint-5992/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5992/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5992/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5992/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5992/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5992/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5992/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-5992/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-5992/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-5992/pytorch_model.bin\n",
            "{'loss': 0.1467, 'learning_rate': 7.957276368491323e-06, 'epoch': 56.07}\n",
            " 81% 6099/7490 [1:41:51<21:03,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.43076908588409424, 'eval_acc': 0.8901098901098901, 'eval_f1': 0.8524568262589051, 'eval_precision': 0.8632568978758693, 'eval_recall': 0.843143477371193, 'eval_runtime': 5.9789, 'eval_samples_per_second': 76.101, 'eval_steps_per_second': 5.185, 'epoch': 57.0}\n",
            " 81% 6099/7490 [1:41:57<21:03,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6099\n",
            "Configuration saved in results/sciie_3/checkpoint-6099/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6099/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6099/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6099/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6099/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6099/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6099/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6099/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6099/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6099/pytorch_model.bin\n",
            " 83% 6206/7490 [1:43:38<19:28,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.44it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.02it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4424910247325897, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8421288723276412, 'eval_precision': 0.8596690344703234, 'eval_recall': 0.8281594181658157, 'eval_runtime': 5.9786, 'eval_samples_per_second': 76.105, 'eval_steps_per_second': 5.185, 'epoch': 58.0}\n",
            " 83% 6206/7490 [1:43:44<19:28,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6206\n",
            "Configuration saved in results/sciie_3/checkpoint-6206/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6206/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6206/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6206/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6206/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6206/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6206/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6206/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6206/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6206/pytorch_model.bin\n",
            " 84% 6313/7490 [1:45:25<17:50,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.30it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.54it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.32it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.45746004581451416, 'eval_acc': 0.8681318681318682, 'eval_f1': 0.8245214015857033, 'eval_precision': 0.8135586360301168, 'eval_recall': 0.838830833947356, 'eval_runtime': 5.9834, 'eval_samples_per_second': 76.044, 'eval_steps_per_second': 5.181, 'epoch': 59.0}\n",
            " 84% 6313/7490 [1:45:31<17:50,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6313\n",
            "Configuration saved in results/sciie_3/checkpoint-6313/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6313/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6313/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6313/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6313/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6313/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6313/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6313/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6313/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6313/pytorch_model.bin\n",
            " 86% 6420/7490 [1:47:13<16:12,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44826531410217285, 'eval_acc': 0.8703296703296703, 'eval_f1': 0.8242146398775771, 'eval_precision': 0.841127506004377, 'eval_recall': 0.8213864165320454, 'eval_runtime': 5.977, 'eval_samples_per_second': 76.125, 'eval_steps_per_second': 5.187, 'epoch': 60.0}\n",
            " 86% 6420/7490 [1:47:19<16:12,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6420\n",
            "Configuration saved in results/sciie_3/checkpoint-6420/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6420/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6420/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6420/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6420/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6420/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6420/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6420/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6420/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6420/pytorch_model.bin\n",
            "{'loss': 0.137, 'learning_rate': 5.287049399198932e-06, 'epoch': 60.74}\n",
            " 87% 6527/7490 [1:49:00<14:37,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.51it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.07it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4310908317565918, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8397284653854553, 'eval_precision': 0.8404697633658842, 'eval_recall': 0.8408766895553398, 'eval_runtime': 5.9757, 'eval_samples_per_second': 76.142, 'eval_steps_per_second': 5.188, 'epoch': 61.0}\n",
            " 87% 6527/7490 [1:49:06<14:37,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6527\n",
            "Configuration saved in results/sciie_3/checkpoint-6527/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6527/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6527/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6527/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6527/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6527/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6527/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6527/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6527/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6527/pytorch_model.bin\n",
            " 89% 6634/7490 [1:50:47<12:58,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.30it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.23it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.45568183064460754, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.836456436511649, 'eval_precision': 0.8261047541994143, 'eval_recall': 0.8489816523224734, 'eval_runtime': 5.9881, 'eval_samples_per_second': 75.983, 'eval_steps_per_second': 5.177, 'epoch': 62.0}\n",
            " 89% 6634/7490 [1:50:53<12:58,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6634\n",
            "Configuration saved in results/sciie_3/checkpoint-6634/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6634/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6634/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6634/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6634/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6634/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6634/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6634/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6634/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6634/pytorch_model.bin\n",
            " 90% 6741/7490 [1:52:34<11:21,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.48it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.32it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.23it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.22it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44624921679496765, 'eval_acc': 0.8813186813186813, 'eval_f1': 0.8373198251589588, 'eval_precision': 0.8412163549763326, 'eval_recall': 0.8377475173257004, 'eval_runtime': 5.987, 'eval_samples_per_second': 75.998, 'eval_steps_per_second': 5.178, 'epoch': 63.0}\n",
            " 90% 6741/7490 [1:52:40<11:21,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6741\n",
            "Configuration saved in results/sciie_3/checkpoint-6741/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6741/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6741/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6741/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6741/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6741/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6741/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6741/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6741/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6741/pytorch_model.bin\n",
            " 91% 6848/7490 [1:54:23<09:43,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.44it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4393594264984131, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8446387064683997, 'eval_precision': 0.8382995256007746, 'eval_recall': 0.8530935960479472, 'eval_runtime': 5.9798, 'eval_samples_per_second': 76.089, 'eval_steps_per_second': 5.184, 'epoch': 64.0}\n",
            " 91% 6848/7490 [1:54:29<09:43,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6848\n",
            "Configuration saved in results/sciie_3/checkpoint-6848/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6848/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6848/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6848/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6848/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6848/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6848/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6848/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6848/pytorch_model.bin\n",
            " 93% 6955/7490 [1:56:11<08:05,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.30it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.24it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.45370540022850037, 'eval_acc': 0.8769230769230769, 'eval_f1': 0.8397591428884498, 'eval_precision': 0.8325179389864933, 'eval_recall': 0.8503830610973918, 'eval_runtime': 5.9821, 'eval_samples_per_second': 76.06, 'eval_steps_per_second': 5.182, 'epoch': 65.0}\n",
            " 93% 6955/7490 [1:56:17<08:05,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-6955\n",
            "Configuration saved in results/sciie_3/checkpoint-6955/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6955/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6955/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6955/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6955/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6955/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6955/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-6955/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-6955/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-6955/pytorch_model.bin\n",
            "{'loss': 0.13, 'learning_rate': 2.616822429906542e-06, 'epoch': 65.42}\n",
            " 94% 7062/7490 [1:57:58<06:29,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.46it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.04it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.91it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.29it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.93it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.70it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.30it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.24it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.23it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.22it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.21it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.22it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.22it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44538575410842896, 'eval_acc': 0.8857142857142857, 'eval_f1': 0.8511344813555872, 'eval_precision': 0.8551814753519719, 'eval_recall': 0.8502748377034879, 'eval_runtime': 5.9874, 'eval_samples_per_second': 75.993, 'eval_steps_per_second': 5.178, 'epoch': 66.0}\n",
            " 94% 7062/7490 [1:58:04<06:29,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-7062\n",
            "Configuration saved in results/sciie_3/checkpoint-7062/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7062/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7062/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7062/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7062/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7062/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7062/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7062/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7062/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-7062/pytorch_model.bin\n",
            " 96% 7169/7490 [1:59:45<04:51,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.49it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.37it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.32it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.25it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.24it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.22it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44818440079689026, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8415563379125549, 'eval_precision': 0.8412388952258959, 'eval_recall': 0.8455001381040219, 'eval_runtime': 5.9814, 'eval_samples_per_second': 76.069, 'eval_steps_per_second': 5.183, 'epoch': 67.0}\n",
            " 96% 7169/7490 [1:59:51<04:51,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-7169\n",
            "Configuration saved in results/sciie_3/checkpoint-7169/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7169/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7169/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7169/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7169/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7169/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7169/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7169/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7169/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-7169/pytorch_model.bin\n",
            " 97% 7276/7490 [2:01:32<03:14,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.92it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.94it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.56it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.46it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.39it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.34it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.25it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.23it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.24it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4501689076423645, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8429233644131929, 'eval_precision': 0.8388716178099631, 'eval_recall': 0.849443474982572, 'eval_runtime': 5.9764, 'eval_samples_per_second': 76.133, 'eval_steps_per_second': 5.187, 'epoch': 68.0}\n",
            " 97% 7276/7490 [2:01:38<03:14,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-7276\n",
            "Configuration saved in results/sciie_3/checkpoint-7276/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7276/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7276/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7276/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7276/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7276/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7276/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7276/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7276/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-7276/pytorch_model.bin\n",
            " 99% 7383/7490 [2:03:19<01:37,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.47it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.93it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.31it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.45it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.38it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.33it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.31it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.26it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.25it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.24it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.23it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.23it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.23it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44360774755477905, 'eval_acc': 0.8747252747252747, 'eval_f1': 0.838488393350355, 'eval_precision': 0.835443380871188, 'eval_recall': 0.8433383050426563, 'eval_runtime': 5.9788, 'eval_samples_per_second': 76.103, 'eval_steps_per_second': 5.185, 'epoch': 69.0}\n",
            " 99% 7383/7490 [2:03:25<01:37,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.23it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-7383\n",
            "Configuration saved in results/sciie_3/checkpoint-7383/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7383/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7383/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7383/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7383/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7383/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7383/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7383/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7383/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-7383/pytorch_model.bin\n",
            "100% 7490/7490 [2:05:07<00:00,  1.10it/s]***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 10.45it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:03,  8.05it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:03,  6.94it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:04,  6.32it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:04,  5.95it/s]\u001b[A\n",
            " 23% 7/31 [00:01<00:04,  5.72it/s]\u001b[A\n",
            " 26% 8/31 [00:01<00:04,  5.57it/s]\u001b[A\n",
            " 29% 9/31 [00:01<00:04,  5.47it/s]\u001b[A\n",
            " 32% 10/31 [00:01<00:03,  5.40it/s]\u001b[A\n",
            " 35% 11/31 [00:01<00:03,  5.35it/s]\u001b[A\n",
            " 39% 12/31 [00:02<00:03,  5.32it/s]\u001b[A\n",
            " 42% 13/31 [00:02<00:03,  5.29it/s]\u001b[A\n",
            " 45% 14/31 [00:02<00:03,  5.28it/s]\u001b[A\n",
            " 48% 15/31 [00:02<00:03,  5.27it/s]\u001b[A\n",
            " 52% 16/31 [00:02<00:02,  5.26it/s]\u001b[A\n",
            " 55% 17/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 58% 18/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 61% 19/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 65% 20/31 [00:03<00:02,  5.26it/s]\u001b[A\n",
            " 68% 21/31 [00:03<00:01,  5.25it/s]\u001b[A\n",
            " 71% 22/31 [00:04<00:01,  5.25it/s]\u001b[A\n",
            " 74% 23/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 77% 24/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 81% 25/31 [00:04<00:01,  5.24it/s]\u001b[A\n",
            " 84% 26/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 87% 27/31 [00:04<00:00,  5.24it/s]\u001b[A\n",
            " 90% 28/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            " 94% 29/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4442104399204254, 'eval_acc': 0.8791208791208791, 'eval_f1': 0.8442258495301517, 'eval_precision': 0.8430934774633302, 'eval_recall': 0.8468630046421222, 'eval_runtime': 5.7948, 'eval_samples_per_second': 78.519, 'eval_steps_per_second': 5.35, 'epoch': 70.0}\n",
            "100% 7490/7490 [2:05:13<00:00,  1.10it/s]\n",
            "100% 31/31 [00:05<00:00,  5.25it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to results/sciie_3/checkpoint-7490\n",
            "Configuration saved in results/sciie_3/checkpoint-7490/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7490/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7490/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7490/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7490/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7490/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7490/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/checkpoint-7490/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/checkpoint-7490/config.json\n",
            "Model weights saved in results/sciie_3/checkpoint-7490/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/sciie_3/checkpoint-3317 (score: 0.3743683993816376).\n",
            "Loading best adapter(s) from results/sciie_3/checkpoint-3317 (score: 0.3743683993816376).\n",
            "Loading module configuration from results/sciie_3/checkpoint-3317/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/sciie_3/checkpoint-3317/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/sciie_3/checkpoint-3317/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/sciie_3/checkpoint-3317/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/sciie_3/checkpoint-3317 (score: 0.3743683993816376).\n",
            "Loading module configuration from results/sciie_3/checkpoint-3317/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/sciie_3/checkpoint-3317/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 7531.3921, 'train_samples_per_second': 29.919, 'train_steps_per_second': 0.995, 'train_loss': 0.3356696181049016, 'epoch': 70.0}\n",
            "100% 7490/7490 [2:05:31<00:00,  1.01s/it]\n",
            "Saving model checkpoint to results/sciie_3/\n",
            "Configuration saved in results/sciie_3/mlm/adapter_config.json\n",
            "Module weights saved in results/sciie_3/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/sciie_3/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/mlm/head_config.json\n",
            "Module weights saved in results/sciie_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/sciie_3/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/sciie_3/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/sciie_3/config.json\n",
            "Model weights saved in results/sciie_3/pytorch_model.bin\n",
            "tokenizer config file saved in results/sciie_3/tokenizer_config.json\n",
            "Special tokens file saved in results/sciie_3/special_tokens_map.json\n",
            "08/03/2021 17:50:15 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 455\n",
            "  Batch size = 15\n",
            "100% 31/31 [00:05<00:00,  5.51it/s]\n",
            "08/03/2021 17:50:21 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_loss = 0.3743683993816376\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_acc = 0.8769230769230769\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_f1 = 0.8371134856126973\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_precision = 0.8367532865859522\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_recall = 0.8399655991629518\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_runtime = 5.8354\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_samples_per_second = 77.973\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   eval_steps_per_second = 5.312\n",
            "08/03/2021 17:50:21 - INFO - __main__ -   epoch = 70.0\n",
            "08/03/2021 17:50:21 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 974\n",
            "  Batch size = 15\n",
            "100% 65/65 [00:12<00:00,  5.31it/s]\n",
            "08/03/2021 17:50:33 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_loss = 0.41612428426742554\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_acc = 0.8737166324435318\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_f1 = 0.7920747329405894\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_precision = 0.7889610807381615\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_recall = 0.8014707408445977\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_runtime = 12.4447\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_samples_per_second = 78.266\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   eval_steps_per_second = 5.223\n",
            "08/03/2021 17:50:33 - INFO - __main__ -   epoch = 70.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLloPvfjmZp_",
        "outputId": "cd7945c1-87f6-4dcb-cfb0-c974f410459e"
      },
      "source": [
        "# Experiment 10 train_new_adapter_rct-sample\n",
        "!python3 run_mlm.py \\\n",
        "--train_file data/rct-sample_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/rct-sample_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/rct-sample \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 100 \\\n",
        "--learning_rate 1e-4 \\\n",
        "--logging_steps 50 \\\n",
        "--per_gpu_train_batch_size 16 \\\n",
        "--per_gpu_eval_batch_size 16 \\\n",
        "--gradient_accumulation_steps 16  \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 20:01:29.074340: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 20:01:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/03/2021 20:01:30 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=50,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=16,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/rct-sample/runs/Aug03_20-01-30_b79b4b505e94,\n",
            "logging_first_step=False,\n",
            "logging_steps=50,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "output_dir=results/adapters/rct-sample,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=rct-sample,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/rct-sample,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/03/2021 20:01:31 - WARNING - datasets.builder -   Using custom data configuration default-4ad4fc08ca8aea4b\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-4ad4fc08ca8aea4b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-4ad4fc08ca8aea4b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 20:01:33,721 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 20:01:33,722 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-03 20:01:34,069 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-03 20:01:34,416 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-03 20:01:34,417 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-03 20:01:36,486 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|modeling_utils.py:1163] 2021-08-03 20:01:36,921 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-03 20:01:45,347 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-03 20:01:45,347 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-03 20:01:45,356 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00,  3.44ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 31/31 [00:02<00:00, 11.96ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-03 20:01:52,189 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-03 20:01:52,192 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-03 20:01:52,192 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-03 20:01:52,200 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-03 20:01:52,201 >>   Num examples = 500\n",
            "[INFO|trainer.py:1201] 2021-08-03 20:01:52,201 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1202] 2021-08-03 20:01:52,201 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-03 20:01:52,201 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "[INFO|trainer.py:1204] 2021-08-03 20:01:52,201 >>   Gradient Accumulation steps = 16\n",
            "[INFO|trainer.py:1205] 2021-08-03 20:01:52,201 >>   Total optimization steps = 200\n",
            "[WARNING|training_args.py:760] 2021-08-03 20:01:52,214 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-03 20:01:52,215 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.2083, 'learning_rate': 7.500000000000001e-05, 'epoch': 25.0}\n",
            "{'loss': 2.0607, 'learning_rate': 5e-05, 'epoch': 50.0}\n",
            "{'loss': 1.9961, 'learning_rate': 2.5e-05, 'epoch': 75.0}\n",
            "{'loss': 1.9916, 'learning_rate': 0.0, 'epoch': 100.0}\n",
            "100% 200/200 [03:49<00:00,  1.14s/it][INFO|trainer.py:1403] 2021-08-03 20:05:41,434 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 229.2337, 'train_samples_per_second': 218.118, 'train_steps_per_second': 0.872, 'train_loss': 2.064178047180176, 'epoch': 100.0}\n",
            "100% 200/200 [03:49<00:00,  1.15s/it]\n",
            "[INFO|trainer.py:1989] 2021-08-03 20:05:41,438 >> Saving model checkpoint to results/adapters/rct-sample\n",
            "[INFO|loading.py:59] 2021-08-03 20:05:41,460 >> Configuration saved in results/adapters/rct-sample/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 20:05:41,486 >> Module weights saved in results/adapters/rct-sample/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-03 20:05:41,489 >> Configuration saved in results/adapters/rct-sample/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 20:05:42,093 >> Module weights saved in results/adapters/rct-sample/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-03 20:05:42,097 >> Configuration saved in results/adapters/rct-sample/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-03 20:05:42,728 >> Module weights saved in results/adapters/rct-sample/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-03 20:05:42,751 >> tokenizer config file saved in results/adapters/rct-sample/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-03 20:05:42,754 >> Special tokens file saved in results/adapters/rct-sample/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      100.0\n",
            "  train_loss               =     2.0642\n",
            "  train_runtime            = 0:03:49.23\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =    218.118\n",
            "  train_steps_per_second   =      0.872\n",
            "08/03/2021 20:05:42 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:547] 2021-08-03 20:05:42,890 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-03 20:05:42,892 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-03 20:05:42,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-03 20:05:42,893 >>   Num examples = 30212\n",
            "[INFO|trainer.py:2244] 2021-08-03 20:05:42,893 >>   Batch size = 16\n",
            "100% 1889/1889 [01:07<00:00, 29.93it/s][WARNING|training_args.py:774] 2021-08-03 20:06:50,231 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 1889/1889 [01:07<00:00, 28.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      100.0\n",
            "  eval_loss               =     1.9316\n",
            "  eval_runtime            = 0:01:07.33\n",
            "  eval_samples            =      30212\n",
            "  eval_samples_per_second =    448.659\n",
            "  eval_steps_per_second   =     28.052\n",
            "  perplexity              =     6.9007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYVzMmtRY4jw",
        "outputId": "dbaa35a4-2f63-4931-aeec-030e14e0f6e2"
      },
      "source": [
        "# Experiment 11\n",
        "!python3 run_multiple_choice_adapter_fusion_wo_pfeiffer.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/rct-sample_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 15 \\\n",
        "--gradient_accumulation_steps 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 40 \\\n",
        "--output_dir results/rct-sample_rct-sample_chemprot_rct_3/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/rct-sample/mlm \\\n",
        "--adapter_2 results/adapters/chemprot/mlm \\\n",
        "--per_device_eval_batch_size 15 \\\n",
        "--weight_decay 0.1 \\\n",
        "--adam_beta1 0.9 \\\n",
        "--adam_beta2 0.97 \\\n",
        "--adam_epsilon 5e-5 \\\n",
        "--evaluation_strategy epoch \\\n",
        "--seed 1 \\\n",
        "--avg_type micro \\\n",
        "--load_best_model_at_end \\\n",
        "# --overwrite_output_dir \\"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " 57% 1157/2015 [06:56<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1158/2015 [06:57<05:09,  2.77it/s]\u001b[A\n",
            " 58% 1159/2015 [06:57<05:08,  2.77it/s]\u001b[A\n",
            " 58% 1160/2015 [06:58<05:08,  2.77it/s]\u001b[A\n",
            " 58% 1161/2015 [06:58<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1162/2015 [06:58<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1163/2015 [06:59<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1164/2015 [06:59<05:06,  2.77it/s]\u001b[A\n",
            " 58% 1165/2015 [06:59<05:06,  2.77it/s]\u001b[A\n",
            " 58% 1166/2015 [07:00<05:06,  2.77it/s]\u001b[A\n",
            " 58% 1167/2015 [07:00<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1168/2015 [07:00<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1169/2015 [07:01<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1170/2015 [07:01<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1171/2015 [07:02<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1172/2015 [07:02<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1173/2015 [07:02<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1174/2015 [07:03<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1175/2015 [07:03<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1176/2015 [07:03<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1177/2015 [07:04<05:02,  2.77it/s]\u001b[A\n",
            " 58% 1178/2015 [07:04<05:02,  2.77it/s]\u001b[A\n",
            " 59% 1179/2015 [07:04<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1180/2015 [07:05<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1181/2015 [07:05<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1182/2015 [07:05<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1183/2015 [07:06<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1184/2015 [07:06<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1185/2015 [07:07<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1186/2015 [07:07<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1187/2015 [07:07<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1188/2015 [07:08<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1189/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1190/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1191/2015 [07:09<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1192/2015 [07:09<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1193/2015 [07:09<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1194/2015 [07:10<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1195/2015 [07:10<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1196/2015 [07:11<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1197/2015 [07:11<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1198/2015 [07:11<04:54,  2.77it/s]\u001b[A\n",
            " 60% 1199/2015 [07:12<04:54,  2.77it/s]\u001b[A\n",
            " 60% 1200/2015 [07:12<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1201/2015 [07:12<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1202/2015 [07:13<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1203/2015 [07:13<04:52,  2.77it/s]\u001b[A\n",
            " 60% 1204/2015 [07:13<04:52,  2.77it/s]\u001b[A\n",
            " 60% 1205/2015 [07:14<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1206/2015 [07:14<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1207/2015 [07:14<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1208/2015 [07:15<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1209/2015 [07:15<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1210/2015 [07:16<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1211/2015 [07:16<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1212/2015 [07:16<04:49,  2.77it/s]\u001b[A\n",
            " 60% 1213/2015 [07:17<04:49,  2.77it/s]\u001b[A\n",
            " 60% 1214/2015 [07:17<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1215/2015 [07:17<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1216/2015 [07:18<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1217/2015 [07:18<04:47,  2.77it/s]\u001b[A\n",
            " 60% 1218/2015 [07:18<04:47,  2.77it/s]\u001b[A\n",
            " 60% 1219/2015 [07:19<04:47,  2.77it/s]\u001b[A\n",
            " 61% 1220/2015 [07:19<04:46,  2.77it/s]\u001b[A\n",
            " 61% 1221/2015 [07:20<04:46,  2.77it/s]\u001b[A\n",
            " 61% 1222/2015 [07:20<04:45,  2.77it/s]\u001b[A\n",
            " 61% 1223/2015 [07:20<04:45,  2.77it/s]\u001b[A\n",
            " 61% 1224/2015 [07:21<04:45,  2.78it/s]\u001b[A\n",
            " 61% 1225/2015 [07:21<04:44,  2.77it/s]\u001b[A\n",
            " 61% 1226/2015 [07:21<04:44,  2.78it/s]\u001b[A\n",
            " 61% 1227/2015 [07:22<04:43,  2.78it/s]\u001b[A\n",
            " 61% 1228/2015 [07:22<04:43,  2.78it/s]\u001b[A\n",
            " 61% 1229/2015 [07:22<04:43,  2.78it/s]\u001b[A\n",
            " 61% 1230/2015 [07:23<04:42,  2.78it/s]\u001b[A\n",
            " 61% 1231/2015 [07:23<04:42,  2.78it/s]\u001b[A\n",
            " 61% 1232/2015 [07:24<04:42,  2.77it/s]\u001b[A\n",
            " 61% 1233/2015 [07:24<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1234/2015 [07:24<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1235/2015 [07:25<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1236/2015 [07:25<04:40,  2.77it/s]\u001b[A\n",
            " 61% 1237/2015 [07:25<04:40,  2.77it/s]\u001b[A\n",
            " 61% 1238/2015 [07:26<04:40,  2.77it/s]\u001b[A\n",
            " 61% 1239/2015 [07:26<04:39,  2.77it/s]\u001b[A\n",
            " 62% 1240/2015 [07:26<04:39,  2.77it/s]\u001b[A\n",
            " 62% 1241/2015 [07:27<04:38,  2.77it/s]\u001b[A\n",
            " 62% 1242/2015 [07:27<04:38,  2.77it/s]\u001b[A\n",
            " 62% 1243/2015 [07:27<04:38,  2.77it/s]\u001b[A\n",
            " 62% 1244/2015 [07:28<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1245/2015 [07:28<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1246/2015 [07:29<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1247/2015 [07:29<04:36,  2.77it/s]\u001b[A\n",
            " 62% 1248/2015 [07:29<04:36,  2.77it/s]\u001b[A\n",
            " 62% 1249/2015 [07:30<04:36,  2.77it/s]\u001b[A\n",
            " 62% 1250/2015 [07:30<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1251/2015 [07:30<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1252/2015 [07:31<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1253/2015 [07:31<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1254/2015 [07:31<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1255/2015 [07:32<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1256/2015 [07:32<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1257/2015 [07:33<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1258/2015 [07:33<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1259/2015 [07:33<04:32,  2.77it/s]\u001b[A\n",
            " 63% 1260/2015 [07:34<04:32,  2.77it/s]\u001b[A\n",
            " 63% 1261/2015 [07:34<04:32,  2.77it/s]\u001b[A\n",
            " 63% 1262/2015 [07:34<04:31,  2.77it/s]\u001b[A\n",
            " 63% 1263/2015 [07:35<04:31,  2.77it/s]\u001b[A\n",
            " 63% 1264/2015 [07:35<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1265/2015 [07:35<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1266/2015 [07:36<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1267/2015 [07:36<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1268/2015 [07:36<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1269/2015 [07:37<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1270/2015 [07:37<04:28,  2.77it/s]\u001b[A\n",
            " 63% 1271/2015 [07:38<04:28,  2.77it/s]\u001b[A\n",
            " 63% 1272/2015 [07:38<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1273/2015 [07:38<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1274/2015 [07:39<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1275/2015 [07:39<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1276/2015 [07:39<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1277/2015 [07:40<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1278/2015 [07:40<04:25,  2.77it/s]\u001b[A\n",
            " 63% 1279/2015 [07:40<04:25,  2.77it/s]\u001b[A\n",
            " 64% 1280/2015 [07:41<04:25,  2.77it/s]\u001b[A\n",
            " 64% 1281/2015 [07:41<04:24,  2.77it/s]\u001b[A\n",
            " 64% 1282/2015 [07:42<04:24,  2.77it/s]\u001b[A\n",
            " 64% 1283/2015 [07:42<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1284/2015 [07:42<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1285/2015 [07:43<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1286/2015 [07:43<04:22,  2.77it/s]\u001b[A\n",
            " 64% 1287/2015 [07:43<04:22,  2.77it/s]\u001b[A\n",
            " 64% 1288/2015 [07:44<04:22,  2.77it/s]\u001b[A\n",
            " 64% 1289/2015 [07:44<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1290/2015 [07:44<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1291/2015 [07:45<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1292/2015 [07:45<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1293/2015 [07:46<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1294/2015 [07:46<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1295/2015 [07:46<04:19,  2.77it/s]\u001b[A\n",
            " 64% 1296/2015 [07:47<04:19,  2.77it/s]\u001b[A\n",
            " 64% 1297/2015 [07:47<04:18,  2.77it/s]\u001b[A\n",
            " 64% 1298/2015 [07:47<04:18,  2.77it/s]\u001b[A\n",
            " 64% 1299/2015 [07:48<04:18,  2.77it/s]\u001b[A\n",
            " 65% 1300/2015 [07:48<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1301/2015 [07:48<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1302/2015 [07:49<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1303/2015 [07:49<04:16,  2.77it/s]\u001b[A\n",
            " 65% 1304/2015 [07:49<04:16,  2.77it/s]\u001b[A\n",
            " 65% 1305/2015 [07:50<04:15,  2.77it/s]\u001b[A\n",
            " 65% 1306/2015 [07:50<04:15,  2.77it/s]\u001b[A\n",
            " 65% 1307/2015 [07:51<04:15,  2.77it/s]\u001b[A\n",
            " 65% 1308/2015 [07:51<04:14,  2.77it/s]\u001b[A\n",
            " 65% 1309/2015 [07:51<04:14,  2.77it/s]\u001b[A\n",
            " 65% 1310/2015 [07:52<04:14,  2.77it/s]\u001b[A\n",
            " 65% 1311/2015 [07:52<04:13,  2.77it/s]\u001b[A\n",
            " 65% 1312/2015 [07:52<04:13,  2.77it/s]\u001b[A\n",
            " 65% 1313/2015 [07:53<04:12,  2.77it/s]\u001b[A\n",
            " 65% 1314/2015 [07:53<04:12,  2.77it/s]\u001b[A\n",
            " 65% 1315/2015 [07:53<04:12,  2.77it/s]\u001b[A\n",
            " 65% 1316/2015 [07:54<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1317/2015 [07:54<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1318/2015 [07:55<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1319/2015 [07:55<04:10,  2.77it/s]\u001b[A\n",
            " 66% 1320/2015 [07:55<04:10,  2.77it/s]\u001b[A\n",
            " 66% 1321/2015 [07:56<04:10,  2.77it/s]\u001b[A\n",
            " 66% 1322/2015 [07:56<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1323/2015 [07:56<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1324/2015 [07:57<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1325/2015 [07:57<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1326/2015 [07:57<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1327/2015 [07:58<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1328/2015 [07:58<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1329/2015 [07:58<04:07,  2.77it/s]\u001b[A\n",
            " 66% 1330/2015 [07:59<04:07,  2.77it/s]\u001b[A\n",
            " 66% 1331/2015 [07:59<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1332/2015 [08:00<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1333/2015 [08:00<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1334/2015 [08:00<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1335/2015 [08:01<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1336/2015 [08:01<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1337/2015 [08:01<04:04,  2.77it/s]\u001b[A\n",
            " 66% 1338/2015 [08:02<04:04,  2.77it/s]\u001b[A\n",
            " 66% 1339/2015 [08:02<04:03,  2.77it/s]\u001b[A\n",
            " 67% 1340/2015 [08:02<04:03,  2.77it/s]\u001b[A\n",
            " 67% 1341/2015 [08:03<04:03,  2.77it/s]\u001b[A\n",
            " 67% 1342/2015 [08:03<04:02,  2.77it/s]\u001b[A\n",
            " 67% 1343/2015 [08:04<04:02,  2.77it/s]\u001b[A\n",
            " 67% 1344/2015 [08:04<04:01,  2.77it/s]\u001b[A\n",
            " 67% 1345/2015 [08:04<04:01,  2.77it/s]\u001b[A\n",
            " 67% 1346/2015 [08:05<04:01,  2.77it/s]\u001b[A\n",
            " 67% 1347/2015 [08:05<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1348/2015 [08:05<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1349/2015 [08:06<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1350/2015 [08:06<03:59,  2.77it/s]\u001b[A\n",
            " 67% 1351/2015 [08:06<03:59,  2.77it/s]\u001b[A\n",
            " 67% 1352/2015 [08:07<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1353/2015 [08:07<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1354/2015 [08:08<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1355/2015 [08:08<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1356/2015 [08:08<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1357/2015 [08:09<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1358/2015 [08:09<03:56,  2.77it/s]\u001b[A\n",
            " 67% 1359/2015 [08:09<03:56,  2.77it/s]\u001b[A\n",
            " 67% 1360/2015 [08:10<03:56,  2.77it/s]\u001b[A\n",
            " 68% 1361/2015 [08:10<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1362/2015 [08:10<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1363/2015 [08:11<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1364/2015 [08:11<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1365/2015 [08:11<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1366/2015 [08:12<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1367/2015 [08:12<03:53,  2.77it/s]\u001b[A\n",
            " 68% 1368/2015 [08:13<03:53,  2.77it/s]\u001b[A\n",
            " 68% 1369/2015 [08:13<03:53,  2.77it/s]\u001b[A\n",
            " 68% 1370/2015 [08:13<03:52,  2.77it/s]\u001b[A\n",
            " 68% 1371/2015 [08:14<03:52,  2.77it/s]\u001b[A\n",
            " 68% 1372/2015 [08:14<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1373/2015 [08:14<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1374/2015 [08:15<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1375/2015 [08:15<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1376/2015 [08:15<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1377/2015 [08:16<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1378/2015 [08:16<03:49,  2.77it/s]\u001b[A\n",
            " 68% 1379/2015 [08:17<03:49,  2.77it/s]\u001b[A\n",
            " 68% 1380/2015 [08:17<03:48,  2.77it/s]\u001b[A\n",
            " 69% 1381/2015 [08:17<03:48,  2.77it/s]\u001b[A\n",
            " 69% 1382/2015 [08:18<03:48,  2.77it/s]\u001b[A\n",
            " 69% 1383/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1384/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1385/2015 [08:19<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1386/2015 [08:19<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1387/2015 [08:19<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1388/2015 [08:20<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1389/2015 [08:20<03:45,  2.77it/s]\u001b[A\n",
            " 69% 1390/2015 [08:20<03:45,  2.77it/s]\u001b[A\n",
            " 69% 1391/2015 [08:21<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1392/2015 [08:21<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1393/2015 [08:22<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1394/2015 [08:22<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1395/2015 [08:22<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1396/2015 [08:23<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1397/2015 [08:23<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1398/2015 [08:23<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1399/2015 [08:24<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1400/2015 [08:24<03:41,  2.77it/s]\u001b[A\n",
            " 70% 1401/2015 [08:24<03:41,  2.77it/s]\u001b[A\n",
            " 70% 1402/2015 [08:25<03:40,  2.77it/s]\u001b[A\n",
            " 70% 1403/2015 [08:25<03:40,  2.77it/s]\u001b[A\n",
            " 70% 1404/2015 [08:26<03:40,  2.77it/s]\u001b[A\n",
            " 70% 1405/2015 [08:26<03:39,  2.77it/s]\u001b[A\n",
            " 70% 1406/2015 [08:26<03:39,  2.77it/s]\u001b[A\n",
            " 70% 1407/2015 [08:27<03:39,  2.77it/s]\u001b[A\n",
            " 70% 1408/2015 [08:27<03:39,  2.77it/s]\u001b[A\n",
            " 70% 1409/2015 [08:27<03:38,  2.77it/s]\u001b[A\n",
            " 70% 1410/2015 [08:28<03:38,  2.77it/s]\u001b[A\n",
            " 70% 1411/2015 [08:28<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1412/2015 [08:28<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1413/2015 [08:29<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1414/2015 [08:29<03:36,  2.77it/s]\u001b[A\n",
            " 70% 1415/2015 [08:30<03:36,  2.77it/s]\u001b[A\n",
            " 70% 1416/2015 [08:30<03:36,  2.77it/s]\u001b[A\n",
            " 70% 1417/2015 [08:30<03:35,  2.77it/s]\u001b[A\n",
            " 70% 1418/2015 [08:31<03:35,  2.77it/s]\u001b[A\n",
            " 70% 1419/2015 [08:31<03:34,  2.77it/s]\u001b[A\n",
            " 70% 1420/2015 [08:31<03:34,  2.77it/s]\u001b[A\n",
            " 71% 1421/2015 [08:32<03:34,  2.77it/s]\u001b[A\n",
            " 71% 1422/2015 [08:32<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1423/2015 [08:32<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1424/2015 [08:33<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1425/2015 [08:33<03:32,  2.77it/s]\u001b[A\n",
            " 71% 1426/2015 [08:33<03:32,  2.77it/s]\u001b[A\n",
            " 71% 1427/2015 [08:34<03:31,  2.77it/s]\u001b[A\n",
            " 71% 1428/2015 [08:34<03:31,  2.78it/s]\u001b[A\n",
            " 71% 1429/2015 [08:35<03:31,  2.77it/s]\u001b[A\n",
            " 71% 1430/2015 [08:35<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1431/2015 [08:35<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1432/2015 [08:36<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1433/2015 [08:36<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1434/2015 [08:36<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1435/2015 [08:37<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1436/2015 [08:37<03:28,  2.77it/s]\u001b[A\n",
            " 71% 1437/2015 [08:37<03:28,  2.78it/s]\u001b[A\n",
            " 71% 1438/2015 [08:38<03:27,  2.77it/s]\u001b[A\n",
            " 71% 1439/2015 [08:38<03:27,  2.77it/s]\u001b[A\n",
            " 71% 1440/2015 [08:39<03:27,  2.77it/s]\u001b[A\n",
            " 72% 1441/2015 [08:39<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1442/2015 [08:39<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1443/2015 [08:40<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1444/2015 [08:40<03:25,  2.77it/s]\u001b[A\n",
            " 72% 1445/2015 [08:40<03:25,  2.77it/s]\u001b[A\n",
            " 72% 1446/2015 [08:41<03:25,  2.78it/s]\u001b[A\n",
            " 72% 1447/2015 [08:41<03:24,  2.78it/s]\u001b[A\n",
            " 72% 1448/2015 [08:41<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1449/2015 [08:42<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1450/2015 [08:42<03:23,  2.77it/s]\u001b[A\n",
            " 72% 1451/2015 [08:42<03:23,  2.77it/s]\u001b[A\n",
            " 72% 1452/2015 [08:43<03:23,  2.77it/s]\u001b[A\n",
            " 72% 1453/2015 [08:43<03:22,  2.77it/s]\u001b[A\n",
            " 72% 1454/2015 [08:44<03:22,  2.77it/s]\u001b[A\n",
            " 72% 1455/2015 [08:44<03:21,  2.77it/s]\u001b[A\n",
            " 72% 1456/2015 [08:44<03:21,  2.77it/s]\u001b[A\n",
            " 72% 1457/2015 [08:45<03:21,  2.77it/s]\u001b[A\n",
            " 72% 1458/2015 [08:45<03:20,  2.77it/s]\u001b[A\n",
            " 72% 1459/2015 [08:45<03:20,  2.77it/s]\u001b[A\n",
            " 72% 1460/2015 [08:46<03:20,  2.77it/s]\u001b[A\n",
            " 73% 1461/2015 [08:46<03:19,  2.77it/s]\u001b[A\n",
            " 73% 1462/2015 [08:46<03:19,  2.77it/s]\u001b[A\n",
            " 73% 1463/2015 [08:47<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1464/2015 [08:47<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1465/2015 [08:48<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1466/2015 [08:48<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1467/2015 [08:48<03:17,  2.77it/s]\u001b[A\n",
            " 73% 1468/2015 [08:49<03:17,  2.77it/s]\u001b[A\n",
            " 73% 1469/2015 [08:49<03:16,  2.77it/s]\u001b[A\n",
            " 73% 1470/2015 [08:49<03:16,  2.77it/s]\u001b[A\n",
            " 73% 1471/2015 [08:50<03:16,  2.78it/s]\u001b[A\n",
            " 73% 1472/2015 [08:50<03:15,  2.78it/s]\u001b[A\n",
            " 73% 1473/2015 [08:50<03:15,  2.77it/s]\u001b[A\n",
            " 73% 1474/2015 [08:51<03:14,  2.77it/s]\u001b[A\n",
            " 73% 1475/2015 [08:51<03:14,  2.78it/s]\u001b[A\n",
            " 73% 1476/2015 [08:51<03:14,  2.78it/s]\u001b[A\n",
            " 73% 1477/2015 [08:52<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1478/2015 [08:52<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1479/2015 [08:53<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1480/2015 [08:53<03:12,  2.78it/s]\u001b[A\n",
            " 73% 1481/2015 [08:53<03:12,  2.78it/s]\u001b[A\n",
            " 74% 1482/2015 [08:54<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1483/2015 [08:54<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1484/2015 [08:54<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1485/2015 [08:55<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1486/2015 [08:55<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1487/2015 [08:55<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1488/2015 [08:56<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1489/2015 [08:56<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1490/2015 [08:57<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1491/2015 [08:57<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1492/2015 [08:57<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1493/2015 [08:58<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1494/2015 [08:58<03:07,  2.78it/s]\u001b[A\n",
            " 74% 1495/2015 [08:58<03:07,  2.77it/s]\u001b[A\n",
            " 74% 1496/2015 [08:59<03:07,  2.78it/s]\u001b[A\n",
            " 74% 1497/2015 [08:59<03:06,  2.78it/s]\u001b[A\n",
            " 74% 1498/2015 [08:59<03:06,  2.78it/s]\u001b[A\n",
            " 74% 1499/2015 [09:00<03:05,  2.77it/s]\u001b[A\n",
            " 74% 1500/2015 [09:00<03:05,  2.77it/s]\u001b[A\n",
            " 74% 1501/2015 [09:01<03:05,  2.78it/s]\u001b[A\n",
            " 75% 1502/2015 [09:01<03:04,  2.77it/s]\u001b[A\n",
            " 75% 1503/2015 [09:01<03:04,  2.77it/s]\u001b[A\n",
            " 75% 1504/2015 [09:02<03:04,  2.78it/s]\u001b[A\n",
            " 75% 1505/2015 [09:02<03:03,  2.78it/s]\u001b[A\n",
            " 75% 1506/2015 [09:02<03:03,  2.78it/s]\u001b[A\n",
            " 75% 1507/2015 [09:03<03:03,  2.78it/s]\u001b[A\n",
            " 75% 1508/2015 [09:03<03:02,  2.78it/s]\u001b[A\n",
            " 75% 1509/2015 [09:03<03:02,  2.78it/s]\u001b[A\n",
            " 75% 1510/2015 [09:04<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1511/2015 [09:04<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1512/2015 [09:04<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1513/2015 [09:05<03:00,  2.78it/s]\u001b[A\n",
            " 75% 1514/2015 [09:05<03:00,  2.78it/s]\u001b[A\n",
            " 75% 1515/2015 [09:06<03:00,  2.78it/s]\u001b[A\n",
            " 75% 1516/2015 [09:06<02:59,  2.78it/s]\u001b[A\n",
            " 75% 1517/2015 [09:06<02:59,  2.78it/s]\u001b[A\n",
            " 75% 1518/2015 [09:07<02:58,  2.78it/s]\u001b[A\n",
            " 75% 1519/2015 [09:07<02:58,  2.78it/s]\u001b[A\n",
            " 75% 1520/2015 [09:07<02:58,  2.78it/s]\u001b[A\n",
            " 75% 1521/2015 [09:08<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1522/2015 [09:08<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1523/2015 [09:08<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1524/2015 [09:09<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1525/2015 [09:09<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1526/2015 [09:10<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1527/2015 [09:10<02:55,  2.78it/s]\u001b[A\n",
            " 76% 1528/2015 [09:10<02:55,  2.78it/s]\u001b[A\n",
            " 76% 1529/2015 [09:11<02:54,  2.78it/s]\u001b[A\n",
            " 76% 1530/2015 [09:11<02:54,  2.78it/s]\u001b[A\n",
            " 76% 1531/2015 [09:11<02:54,  2.78it/s]\u001b[A\n",
            " 76% 1532/2015 [09:12<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1533/2015 [09:12<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1534/2015 [09:12<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1535/2015 [09:13<02:52,  2.78it/s]\u001b[A\n",
            " 76% 1536/2015 [09:13<02:52,  2.78it/s]\u001b[A\n",
            " 76% 1537/2015 [09:13<02:52,  2.78it/s]\u001b[A\n",
            " 76% 1538/2015 [09:14<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1539/2015 [09:14<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1540/2015 [09:15<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1541/2015 [09:15<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1542/2015 [09:15<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1543/2015 [09:16<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1544/2015 [09:16<02:49,  2.78it/s]\u001b[A\n",
            " 77% 1545/2015 [09:16<02:49,  2.78it/s]\u001b[A\n",
            " 77% 1546/2015 [09:17<02:48,  2.78it/s]\u001b[A\n",
            " 77% 1547/2015 [09:17<02:48,  2.78it/s]\u001b[A\n",
            " 77% 1548/2015 [09:17<02:48,  2.78it/s]\u001b[A\n",
            " 77% 1549/2015 [09:18<02:47,  2.78it/s]\u001b[A\n",
            " 77% 1550/2015 [09:18<02:47,  2.77it/s]\u001b[A\n",
            " 77% 1551/2015 [09:19<02:47,  2.77it/s]\u001b[A\n",
            " 77% 1552/2015 [09:19<02:46,  2.78it/s]\u001b[A\n",
            " 77% 1553/2015 [09:19<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1554/2015 [09:20<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1555/2015 [09:20<02:45,  2.78it/s]\u001b[A\n",
            " 77% 1556/2015 [09:20<02:45,  2.78it/s]\u001b[A\n",
            " 77% 1557/2015 [09:21<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1558/2015 [09:21<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1559/2015 [09:21<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1560/2015 [09:22<02:43,  2.78it/s]\u001b[A\n",
            " 77% 1561/2015 [09:22<02:43,  2.78it/s]\u001b[A\n",
            " 78% 1562/2015 [09:22<02:43,  2.78it/s]\u001b[A\n",
            " 78% 1563/2015 [09:23<02:42,  2.78it/s]\u001b[A\n",
            " 78% 1564/2015 [09:23<02:42,  2.78it/s]\u001b[A\n",
            " 78% 1565/2015 [09:24<02:42,  2.77it/s]\u001b[A\n",
            " 78% 1566/2015 [09:24<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1567/2015 [09:24<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1568/2015 [09:25<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1569/2015 [09:25<02:40,  2.78it/s]\u001b[A\n",
            " 78% 1570/2015 [09:25<02:40,  2.78it/s]\u001b[A\n",
            " 78% 1571/2015 [09:26<02:39,  2.78it/s]\u001b[A\n",
            " 78% 1572/2015 [09:26<02:39,  2.78it/s]\u001b[A\n",
            " 78% 1573/2015 [09:26<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1574/2015 [09:27<02:38,  2.77it/s]\u001b[A\n",
            " 78% 1575/2015 [09:27<02:38,  2.78it/s]\u001b[A\n",
            " 78% 1576/2015 [09:28<02:38,  2.78it/s]\u001b[A\n",
            " 78% 1577/2015 [09:28<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1578/2015 [09:28<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1579/2015 [09:29<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1580/2015 [09:29<02:36,  2.78it/s]\u001b[A\n",
            " 78% 1581/2015 [09:29<02:36,  2.78it/s]\u001b[A\n",
            " 79% 1582/2015 [09:30<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1583/2015 [09:30<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1584/2015 [09:30<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1585/2015 [09:31<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1586/2015 [09:31<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1587/2015 [09:31<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1588/2015 [09:32<02:33,  2.78it/s]\u001b[A\n",
            " 79% 1589/2015 [09:32<02:33,  2.78it/s]\u001b[A\n",
            " 79% 1590/2015 [09:33<02:33,  2.77it/s]\u001b[A\n",
            " 79% 1591/2015 [09:33<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1592/2015 [09:33<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1593/2015 [09:34<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1594/2015 [09:34<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1595/2015 [09:34<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1596/2015 [09:35<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1597/2015 [09:35<02:30,  2.77it/s]\u001b[A\n",
            " 79% 1598/2015 [09:35<02:30,  2.77it/s]\u001b[A\n",
            " 79% 1599/2015 [09:36<02:29,  2.77it/s]\u001b[A\n",
            " 79% 1600/2015 [09:36<02:29,  2.77it/s]\u001b[A\n",
            " 79% 1601/2015 [09:37<02:29,  2.77it/s]\u001b[A\n",
            " 80% 1602/2015 [09:37<02:28,  2.77it/s]\u001b[A\n",
            " 80% 1603/2015 [09:37<02:28,  2.77it/s]\u001b[A\n",
            " 80% 1604/2015 [09:38<02:28,  2.77it/s]\u001b[A\n",
            " 80% 1605/2015 [09:38<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1606/2015 [09:38<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1607/2015 [09:39<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1608/2015 [09:39<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1609/2015 [09:39<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1610/2015 [09:40<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1611/2015 [09:40<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1612/2015 [09:40<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1613/2015 [09:41<02:24,  2.77it/s]\u001b[A\n",
            " 80% 1614/2015 [09:41<02:24,  2.77it/s]\u001b[A\n",
            " 80% 1615/2015 [09:42<02:24,  2.77it/s]\u001b[A\n",
            " 80% 1616/2015 [09:42<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1617/2015 [09:42<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1618/2015 [09:43<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1619/2015 [09:43<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1620/2015 [09:43<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1621/2015 [09:44<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1622/2015 [09:44<02:21,  2.77it/s]\u001b[A\n",
            " 81% 1623/2015 [09:44<02:21,  2.77it/s]\u001b[A\n",
            " 81% 1624/2015 [09:45<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1625/2015 [09:45<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1626/2015 [09:46<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1627/2015 [09:46<02:19,  2.77it/s]\u001b[A\n",
            " 81% 1628/2015 [09:46<02:19,  2.77it/s]\u001b[A\n",
            " 81% 1629/2015 [09:47<02:19,  2.77it/s]\u001b[A\n",
            " 81% 1630/2015 [09:47<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1631/2015 [09:47<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1632/2015 [09:48<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1633/2015 [09:48<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1634/2015 [09:48<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1635/2015 [09:49<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1636/2015 [09:49<02:16,  2.77it/s]\u001b[A\n",
            " 81% 1637/2015 [09:50<02:16,  2.77it/s]\u001b[A\n",
            " 81% 1638/2015 [09:50<02:15,  2.77it/s]\u001b[A\n",
            " 81% 1639/2015 [09:50<02:15,  2.77it/s]\u001b[A\n",
            " 81% 1640/2015 [09:51<02:15,  2.77it/s]\u001b[A\n",
            " 81% 1641/2015 [09:51<02:14,  2.77it/s]\u001b[A\n",
            " 81% 1642/2015 [09:51<02:14,  2.77it/s]\u001b[A\n",
            " 82% 1643/2015 [09:52<02:14,  2.77it/s]\u001b[A\n",
            " 82% 1644/2015 [09:52<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1645/2015 [09:52<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1646/2015 [09:53<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1647/2015 [09:53<02:12,  2.77it/s]\u001b[A\n",
            " 82% 1648/2015 [09:53<02:12,  2.77it/s]\u001b[A\n",
            " 82% 1649/2015 [09:54<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1650/2015 [09:54<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1651/2015 [09:55<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1652/2015 [09:55<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1653/2015 [09:55<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1654/2015 [09:56<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1655/2015 [09:56<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1656/2015 [09:56<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1657/2015 [09:57<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1658/2015 [09:57<02:08,  2.77it/s]\u001b[A\n",
            " 82% 1659/2015 [09:57<02:08,  2.78it/s]\u001b[A\n",
            " 82% 1660/2015 [09:58<02:07,  2.78it/s]\u001b[A\n",
            " 82% 1661/2015 [09:58<02:07,  2.78it/s]\u001b[A\n",
            " 82% 1662/2015 [09:59<02:07,  2.78it/s]\u001b[A\n",
            " 83% 1663/2015 [09:59<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1664/2015 [09:59<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1665/2015 [10:00<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1666/2015 [10:00<02:05,  2.78it/s]\u001b[A\n",
            " 83% 1667/2015 [10:00<02:05,  2.78it/s]\u001b[A\n",
            " 83% 1668/2015 [10:01<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1669/2015 [10:01<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1670/2015 [10:01<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1671/2015 [10:02<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1672/2015 [10:02<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1673/2015 [10:02<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1674/2015 [10:03<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1675/2015 [10:03<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1676/2015 [10:04<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1677/2015 [10:04<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1678/2015 [10:04<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1679/2015 [10:05<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1680/2015 [10:05<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1681/2015 [10:05<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1682/2015 [10:06<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1683/2015 [10:06<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1684/2015 [10:06<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1685/2015 [10:07<01:58,  2.78it/s]\u001b[A\n",
            " 84% 1686/2015 [10:07<01:58,  2.78it/s]\u001b[A\n",
            " 84% 1687/2015 [10:08<01:58,  2.78it/s]\u001b[A\n",
            " 84% 1688/2015 [10:08<01:57,  2.78it/s]\u001b[A\n",
            " 84% 1689/2015 [10:08<01:57,  2.78it/s]\u001b[A\n",
            " 84% 1690/2015 [10:09<01:57,  2.78it/s]\u001b[A\n",
            " 84% 1691/2015 [10:09<01:56,  2.78it/s]\u001b[A\n",
            " 84% 1692/2015 [10:09<01:56,  2.78it/s]\u001b[A\n",
            " 84% 1693/2015 [10:10<01:55,  2.78it/s]\u001b[A\n",
            " 84% 1694/2015 [10:10<01:55,  2.78it/s]\u001b[A\n",
            " 84% 1695/2015 [10:10<01:55,  2.78it/s]\u001b[A\n",
            " 84% 1696/2015 [10:11<01:54,  2.78it/s]\u001b[A\n",
            " 84% 1697/2015 [10:11<01:54,  2.78it/s]\u001b[A\n",
            " 84% 1698/2015 [10:11<01:54,  2.78it/s]\u001b[A\n",
            " 84% 1699/2015 [10:12<01:53,  2.78it/s]\u001b[A\n",
            " 84% 1700/2015 [10:12<01:53,  2.78it/s]\u001b[A\n",
            " 84% 1701/2015 [10:13<01:53,  2.78it/s]\u001b[A\n",
            " 84% 1702/2015 [10:13<01:52,  2.78it/s]\u001b[A\n",
            " 85% 1703/2015 [10:13<01:52,  2.78it/s]\u001b[A\n",
            " 85% 1704/2015 [10:14<01:51,  2.78it/s]\u001b[A\n",
            " 85% 1705/2015 [10:14<01:51,  2.78it/s]\u001b[A\n",
            " 85% 1706/2015 [10:14<01:51,  2.78it/s]\u001b[A\n",
            " 85% 1707/2015 [10:15<01:50,  2.78it/s]\u001b[A\n",
            " 85% 1708/2015 [10:15<01:50,  2.78it/s]\u001b[A\n",
            " 85% 1709/2015 [10:15<01:50,  2.78it/s]\u001b[A\n",
            " 85% 1710/2015 [10:16<01:49,  2.78it/s]\u001b[A\n",
            " 85% 1711/2015 [10:16<01:49,  2.78it/s]\u001b[A\n",
            " 85% 1712/2015 [10:17<01:49,  2.78it/s]\u001b[A\n",
            " 85% 1713/2015 [10:17<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1714/2015 [10:17<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1715/2015 [10:18<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1716/2015 [10:18<01:47,  2.78it/s]\u001b[A\n",
            " 85% 1717/2015 [10:18<01:47,  2.78it/s]\u001b[A\n",
            " 85% 1718/2015 [10:19<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1719/2015 [10:19<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1720/2015 [10:19<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1721/2015 [10:20<01:45,  2.78it/s]\u001b[A\n",
            " 85% 1722/2015 [10:20<01:45,  2.78it/s]\u001b[A\n",
            " 86% 1723/2015 [10:20<01:45,  2.78it/s]\u001b[A\n",
            " 86% 1724/2015 [10:21<01:44,  2.78it/s]\u001b[A\n",
            " 86% 1725/2015 [10:21<01:44,  2.78it/s]\u001b[A\n",
            " 86% 1726/2015 [10:22<01:44,  2.78it/s]\u001b[A\n",
            " 86% 1727/2015 [10:22<01:43,  2.78it/s]\u001b[A\n",
            " 86% 1728/2015 [10:22<01:43,  2.78it/s]\u001b[A\n",
            " 86% 1729/2015 [10:23<01:42,  2.78it/s]\u001b[A\n",
            " 86% 1730/2015 [10:23<01:42,  2.78it/s]\u001b[A\n",
            " 86% 1731/2015 [10:23<01:42,  2.78it/s]\u001b[A\n",
            " 86% 1732/2015 [10:24<01:41,  2.78it/s]\u001b[A\n",
            " 86% 1733/2015 [10:24<01:41,  2.78it/s]\u001b[A\n",
            " 86% 1734/2015 [10:24<01:41,  2.78it/s]\u001b[A\n",
            " 86% 1735/2015 [10:25<01:40,  2.78it/s]\u001b[A\n",
            " 86% 1736/2015 [10:25<01:40,  2.78it/s]\u001b[A\n",
            " 86% 1737/2015 [10:26<01:40,  2.78it/s]\u001b[A\n",
            " 86% 1738/2015 [10:26<01:39,  2.78it/s]\u001b[A\n",
            " 86% 1739/2015 [10:26<01:39,  2.78it/s]\u001b[A\n",
            " 86% 1740/2015 [10:27<01:39,  2.78it/s]\u001b[A\n",
            " 86% 1741/2015 [10:27<01:38,  2.78it/s]\u001b[A\n",
            " 86% 1742/2015 [10:27<01:38,  2.78it/s]\u001b[A\n",
            " 87% 1743/2015 [10:28<01:37,  2.78it/s]\u001b[A\n",
            " 87% 1744/2015 [10:28<01:37,  2.78it/s]\u001b[A\n",
            " 87% 1745/2015 [10:28<01:37,  2.78it/s]\u001b[A\n",
            " 87% 1746/2015 [10:29<01:36,  2.78it/s]\u001b[A\n",
            " 87% 1747/2015 [10:29<01:36,  2.78it/s]\u001b[A\n",
            " 87% 1748/2015 [10:29<01:36,  2.78it/s]\u001b[A\n",
            " 87% 1749/2015 [10:30<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1750/2015 [10:30<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1751/2015 [10:31<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1752/2015 [10:31<01:34,  2.78it/s]\u001b[A\n",
            " 87% 1753/2015 [10:31<01:34,  2.78it/s]\u001b[A\n",
            " 87% 1754/2015 [10:32<01:34,  2.78it/s]\u001b[A\n",
            " 87% 1755/2015 [10:32<01:33,  2.78it/s]\u001b[A\n",
            " 87% 1756/2015 [10:32<01:33,  2.78it/s]\u001b[A\n",
            " 87% 1757/2015 [10:33<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1758/2015 [10:33<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1759/2015 [10:33<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1760/2015 [10:34<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1761/2015 [10:34<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1762/2015 [10:35<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1763/2015 [10:35<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1764/2015 [10:35<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1765/2015 [10:36<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1766/2015 [10:36<01:29,  2.78it/s]\u001b[A\n",
            " 88% 1767/2015 [10:36<01:29,  2.78it/s]\u001b[A\n",
            " 88% 1768/2015 [10:37<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1769/2015 [10:37<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1770/2015 [10:37<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1771/2015 [10:38<01:27,  2.78it/s]\u001b[A\n",
            " 88% 1772/2015 [10:38<01:27,  2.78it/s]\u001b[A\n",
            " 88% 1773/2015 [10:38<01:27,  2.77it/s]\u001b[A\n",
            " 88% 1774/2015 [10:39<01:26,  2.78it/s]\u001b[A\n",
            " 88% 1775/2015 [10:39<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1776/2015 [10:40<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1777/2015 [10:40<01:25,  2.78it/s]\u001b[A\n",
            " 88% 1778/2015 [10:40<01:25,  2.78it/s]\u001b[A\n",
            " 88% 1779/2015 [10:41<01:25,  2.78it/s]\u001b[A\n",
            " 88% 1780/2015 [10:41<01:24,  2.78it/s]\u001b[A\n",
            " 88% 1781/2015 [10:41<01:24,  2.78it/s]\u001b[A\n",
            " 88% 1782/2015 [10:42<01:23,  2.78it/s]\u001b[A\n",
            " 88% 1783/2015 [10:42<01:23,  2.77it/s]\u001b[A\n",
            " 89% 1784/2015 [10:42<01:23,  2.78it/s]\u001b[A\n",
            " 89% 1785/2015 [10:43<01:22,  2.78it/s]\u001b[A\n",
            " 89% 1786/2015 [10:43<01:22,  2.77it/s]\u001b[A\n",
            " 89% 1787/2015 [10:44<01:22,  2.77it/s]\u001b[A\n",
            " 89% 1788/2015 [10:44<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1789/2015 [10:44<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1790/2015 [10:45<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1791/2015 [10:45<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1792/2015 [10:45<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1793/2015 [10:46<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1794/2015 [10:46<01:19,  2.77it/s]\u001b[A\n",
            " 89% 1795/2015 [10:46<01:19,  2.77it/s]\u001b[A\n",
            " 89% 1796/2015 [10:47<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1797/2015 [10:47<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1798/2015 [10:48<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1799/2015 [10:48<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1800/2015 [10:48<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1801/2015 [10:49<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1802/2015 [10:49<01:16,  2.78it/s]\u001b[A\n",
            " 89% 1803/2015 [10:49<01:16,  2.78it/s]\u001b[A\n",
            " 90% 1804/2015 [10:50<01:16,  2.78it/s]\u001b[A\n",
            " 90% 1805/2015 [10:50<01:15,  2.78it/s]\u001b[A\n",
            " 90% 1806/2015 [10:50<01:15,  2.78it/s]\u001b[A\n",
            " 90% 1807/2015 [10:51<01:14,  2.78it/s]\u001b[A\n",
            " 90% 1808/2015 [10:51<01:14,  2.77it/s]\u001b[A\n",
            " 90% 1809/2015 [10:51<01:14,  2.77it/s]\u001b[A\n",
            " 90% 1810/2015 [10:52<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1811/2015 [10:52<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1812/2015 [10:53<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1813/2015 [10:53<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1814/2015 [10:53<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1815/2015 [10:54<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1816/2015 [10:54<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1817/2015 [10:54<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1818/2015 [10:55<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1819/2015 [10:55<01:10,  2.77it/s]\u001b[A\n",
            " 90% 1820/2015 [10:55<01:10,  2.77it/s]\u001b[A\n",
            " 90% 1821/2015 [10:56<01:09,  2.77it/s]\u001b[A\n",
            " 90% 1822/2015 [10:56<01:09,  2.77it/s]\u001b[A\n",
            " 90% 1823/2015 [10:57<01:09,  2.77it/s]\u001b[A\n",
            " 91% 1824/2015 [10:57<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1825/2015 [10:57<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1826/2015 [10:58<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1827/2015 [10:58<01:07,  2.77it/s]\u001b[A\n",
            " 91% 1828/2015 [10:58<01:07,  2.77it/s]\u001b[A\n",
            " 91% 1829/2015 [10:59<01:07,  2.78it/s]\u001b[A\n",
            " 91% 1830/2015 [10:59<01:06,  2.78it/s]\u001b[A\n",
            " 91% 1831/2015 [10:59<01:06,  2.78it/s]\u001b[A\n",
            " 91% 1832/2015 [11:00<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1833/2015 [11:00<01:05,  2.78it/s]\u001b[A\n",
            " 91% 1834/2015 [11:00<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1835/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1836/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1837/2015 [11:02<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1838/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1839/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1840/2015 [11:03<01:03,  2.78it/s]\u001b[A\n",
            " 91% 1841/2015 [11:03<01:02,  2.78it/s]\u001b[A\n",
            " 91% 1842/2015 [11:03<01:02,  2.77it/s]\u001b[A\n",
            " 91% 1843/2015 [11:04<01:01,  2.78it/s]\u001b[A\n",
            " 92% 1844/2015 [11:04<01:01,  2.78it/s]\u001b[A\n",
            " 92% 1845/2015 [11:04<01:01,  2.78it/s]\u001b[A\n",
            " 92% 1846/2015 [11:05<01:00,  2.78it/s]\u001b[A\n",
            " 92% 1847/2015 [11:05<01:00,  2.78it/s]\u001b[A\n",
            " 92% 1848/2015 [11:06<01:00,  2.78it/s]\u001b[A\n",
            " 92% 1849/2015 [11:06<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1850/2015 [11:06<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1851/2015 [11:07<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1852/2015 [11:07<00:58,  2.78it/s]\u001b[A\n",
            " 92% 1853/2015 [11:07<00:58,  2.78it/s]\u001b[A\n",
            " 92% 1854/2015 [11:08<00:57,  2.78it/s]\u001b[A\n",
            " 92% 1855/2015 [11:08<00:57,  2.78it/s]\u001b[A\n",
            " 92% 1856/2015 [11:08<00:57,  2.78it/s]\u001b[A\n",
            " 92% 1857/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1858/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1859/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1860/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1861/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1862/2015 [11:11<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1863/2015 [11:11<00:54,  2.78it/s]\u001b[A\n",
            " 93% 1864/2015 [11:11<00:54,  2.78it/s]\u001b[A\n",
            " 93% 1865/2015 [11:12<00:54,  2.78it/s]\u001b[A\n",
            " 93% 1866/2015 [11:12<00:53,  2.78it/s]\u001b[A\n",
            " 93% 1867/2015 [11:12<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1868/2015 [11:13<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1869/2015 [11:13<00:52,  2.78it/s]\u001b[A\n",
            " 93% 1870/2015 [11:13<00:52,  2.78it/s]\u001b[A\n",
            " 93% 1871/2015 [11:14<00:51,  2.78it/s]\u001b[A\n",
            " 93% 1872/2015 [11:14<00:51,  2.78it/s]\u001b[A\n",
            " 93% 1873/2015 [11:15<00:51,  2.78it/s]\u001b[A\n",
            " 93% 1874/2015 [11:15<00:50,  2.78it/s]\u001b[A\n",
            " 93% 1875/2015 [11:15<00:50,  2.78it/s]\u001b[A\n",
            " 93% 1876/2015 [11:16<00:50,  2.78it/s]\u001b[A\n",
            " 93% 1877/2015 [11:16<00:49,  2.78it/s]\u001b[A\n",
            " 93% 1878/2015 [11:16<00:49,  2.78it/s]\u001b[A\n",
            " 93% 1879/2015 [11:17<00:48,  2.78it/s]\u001b[A\n",
            " 93% 1880/2015 [11:17<00:48,  2.78it/s]\u001b[A\n",
            " 93% 1881/2015 [11:17<00:48,  2.78it/s]\u001b[A\n",
            " 93% 1882/2015 [11:18<00:47,  2.78it/s]\u001b[A\n",
            " 93% 1883/2015 [11:18<00:47,  2.78it/s]\u001b[A\n",
            " 93% 1884/2015 [11:18<00:47,  2.78it/s]\u001b[A\n",
            " 94% 1885/2015 [11:19<00:46,  2.78it/s]\u001b[A\n",
            " 94% 1886/2015 [11:19<00:46,  2.78it/s]\u001b[A\n",
            " 94% 1887/2015 [11:20<00:46,  2.78it/s]\u001b[A\n",
            " 94% 1888/2015 [11:20<00:45,  2.78it/s]\u001b[A\n",
            " 94% 1889/2015 [11:20<00:45,  2.78it/s]\u001b[A\n",
            " 94% 1890/2015 [11:21<00:45,  2.78it/s]\u001b[A\n",
            " 94% 1891/2015 [11:21<00:44,  2.78it/s]\u001b[A\n",
            " 94% 1892/2015 [11:21<00:44,  2.78it/s]\u001b[A\n",
            " 94% 1893/2015 [11:22<00:43,  2.78it/s]\u001b[A\n",
            " 94% 1894/2015 [11:22<00:43,  2.78it/s]\u001b[A\n",
            " 94% 1895/2015 [11:22<00:43,  2.78it/s]\u001b[A\n",
            " 94% 1896/2015 [11:23<00:42,  2.78it/s]\u001b[A\n",
            " 94% 1897/2015 [11:23<00:42,  2.78it/s]\u001b[A\n",
            " 94% 1898/2015 [11:24<00:42,  2.78it/s]\u001b[A\n",
            " 94% 1899/2015 [11:24<00:41,  2.78it/s]\u001b[A\n",
            " 94% 1900/2015 [11:24<00:41,  2.78it/s]\u001b[A\n",
            " 94% 1901/2015 [11:25<00:41,  2.78it/s]\u001b[A\n",
            " 94% 1902/2015 [11:25<00:40,  2.78it/s]\u001b[A\n",
            " 94% 1903/2015 [11:25<00:40,  2.78it/s]\u001b[A\n",
            " 94% 1904/2015 [11:26<00:39,  2.78it/s]\u001b[A\n",
            " 95% 1905/2015 [11:26<00:39,  2.78it/s]\u001b[A\n",
            " 95% 1906/2015 [11:26<00:39,  2.78it/s]\u001b[A\n",
            " 95% 1907/2015 [11:27<00:38,  2.78it/s]\u001b[A\n",
            " 95% 1908/2015 [11:27<00:38,  2.78it/s]\u001b[A\n",
            " 95% 1909/2015 [11:27<00:38,  2.78it/s]\u001b[A\n",
            " 95% 1910/2015 [11:28<00:37,  2.78it/s]\u001b[A\n",
            " 95% 1911/2015 [11:28<00:37,  2.78it/s]\u001b[A\n",
            " 95% 1912/2015 [11:29<00:37,  2.78it/s]\u001b[A\n",
            " 95% 1913/2015 [11:29<00:36,  2.78it/s]\u001b[A\n",
            " 95% 1914/2015 [11:29<00:36,  2.78it/s]\u001b[A\n",
            " 95% 1915/2015 [11:30<00:36,  2.78it/s]\u001b[A\n",
            " 95% 1916/2015 [11:30<00:35,  2.78it/s]\u001b[A\n",
            " 95% 1917/2015 [11:30<00:35,  2.78it/s]\u001b[A\n",
            " 95% 1918/2015 [11:31<00:34,  2.78it/s]\u001b[A\n",
            " 95% 1919/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1920/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1921/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1922/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1923/2015 [11:33<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1924/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1925/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1926/2015 [11:34<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1927/2015 [11:34<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1928/2015 [11:34<00:31,  2.78it/s]\u001b[A\n",
            " 96% 1929/2015 [11:35<00:30,  2.78it/s]\u001b[A\n",
            " 96% 1930/2015 [11:35<00:30,  2.78it/s]\u001b[A\n",
            " 96% 1931/2015 [11:35<00:30,  2.78it/s]\u001b[A\n",
            " 96% 1932/2015 [11:36<00:29,  2.78it/s]\u001b[A\n",
            " 96% 1933/2015 [11:36<00:29,  2.78it/s]\u001b[A\n",
            " 96% 1934/2015 [11:37<00:29,  2.78it/s]\u001b[A\n",
            " 96% 1935/2015 [11:37<00:28,  2.78it/s]\u001b[A\n",
            " 96% 1936/2015 [11:37<00:28,  2.78it/s]\u001b[A\n",
            " 96% 1937/2015 [11:38<00:28,  2.78it/s]\u001b[A\n",
            " 96% 1938/2015 [11:38<00:27,  2.78it/s]\u001b[A\n",
            " 96% 1939/2015 [11:38<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1940/2015 [11:39<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1941/2015 [11:39<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1942/2015 [11:39<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1943/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 96% 1944/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1945/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1946/2015 [11:41<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1947/2015 [11:41<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1948/2015 [11:42<00:24,  2.78it/s]\u001b[A\n",
            " 97% 1949/2015 [11:42<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1950/2015 [11:42<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1951/2015 [11:43<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1952/2015 [11:43<00:22,  2.78it/s]\u001b[A\n",
            " 97% 1953/2015 [11:43<00:22,  2.78it/s]\u001b[A\n",
            " 97% 1954/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1955/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1956/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1957/2015 [11:45<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1958/2015 [11:45<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1959/2015 [11:46<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1960/2015 [11:46<00:19,  2.78it/s]\u001b[A\n",
            " 97% 1961/2015 [11:46<00:19,  2.78it/s]\u001b[A\n",
            " 97% 1962/2015 [11:47<00:19,  2.77it/s]\u001b[A\n",
            " 97% 1963/2015 [11:47<00:18,  2.78it/s]\u001b[A\n",
            " 97% 1964/2015 [11:47<00:18,  2.78it/s]\u001b[A\n",
            " 98% 1965/2015 [11:48<00:18,  2.78it/s]\u001b[A\n",
            " 98% 1966/2015 [11:48<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1967/2015 [11:48<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1968/2015 [11:49<00:16,  2.78it/s]\u001b[A\n",
            " 98% 1969/2015 [11:49<00:16,  2.78it/s]\u001b[A\n",
            " 98% 1970/2015 [11:49<00:16,  2.78it/s]\u001b[A\n",
            " 98% 1971/2015 [11:50<00:15,  2.78it/s]\u001b[A\n",
            " 98% 1972/2015 [11:50<00:15,  2.78it/s]\u001b[A\n",
            " 98% 1973/2015 [11:51<00:15,  2.78it/s]\u001b[A\n",
            " 98% 1974/2015 [11:51<00:14,  2.78it/s]\u001b[A\n",
            " 98% 1975/2015 [11:51<00:14,  2.78it/s]\u001b[A\n",
            " 98% 1976/2015 [11:52<00:14,  2.78it/s]\u001b[A\n",
            " 98% 1977/2015 [11:52<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1978/2015 [11:52<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1979/2015 [11:53<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1980/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1981/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1982/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1983/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1984/2015 [11:55<00:11,  2.77it/s]\u001b[A\n",
            " 99% 1985/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1986/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1987/2015 [11:56<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1988/2015 [11:56<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1989/2015 [11:56<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1990/2015 [11:57<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1991/2015 [11:57<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1992/2015 [11:57<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1993/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1994/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1995/2015 [11:58<00:07,  2.78it/s]\u001b[A\n",
            " 99% 1996/2015 [11:59<00:06,  2.78it/s]\u001b[A\n",
            " 99% 1997/2015 [11:59<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1998/2015 [12:00<00:06,  2.78it/s]\u001b[A\n",
            " 99% 1999/2015 [12:00<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2000/2015 [12:00<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2001/2015 [12:01<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2002/2015 [12:01<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2003/2015 [12:01<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2004/2015 [12:02<00:03,  2.78it/s]\u001b[A\n",
            "100% 2005/2015 [12:02<00:03,  2.78it/s]\u001b[A\n",
            "100% 2006/2015 [12:02<00:03,  2.77it/s]\u001b[A\n",
            "100% 2007/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2008/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2009/2015 [12:04<00:02,  2.77it/s]\u001b[A\n",
            "100% 2010/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2011/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2012/2015 [12:05<00:01,  2.77it/s]\u001b[A\n",
            "100% 2013/2015 [12:05<00:00,  2.77it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6638731360435486, 'eval_acc': 0.7763140473983847, 'eval_f1': 0.7763140473983846, 'eval_precision': 0.7763140473983847, 'eval_recall': 0.7763140473983847, 'eval_runtime': 726.272, 'eval_samples_per_second': 41.599, 'eval_steps_per_second': 2.774, 'epoch': 38.0}\n",
            " 95% 646/680 [8:00:18<01:16,  2.24s/it]\n",
            "100% 2015/2015 [12:05<00:00,  2.77it/s]\u001b[A\n",
            "                                       \u001b[ASaving model checkpoint to results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/adapter_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/config.json\n",
            "Model weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-646/pytorch_model.bin\n",
            " 98% 663/680 [8:00:49<00:38,  2.24s/it]***** Running Evaluation *****\n",
            "  Num examples = 30212\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/2015 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 2/2015 [00:00<06:02,  5.55it/s]\u001b[A\n",
            "  0% 3/2015 [00:00<07:51,  4.27it/s]\u001b[A\n",
            "  0% 4/2015 [00:01<09:06,  3.68it/s]\u001b[A\n",
            "  0% 5/2015 [00:01<09:59,  3.35it/s]\u001b[A\n",
            "  0% 6/2015 [00:01<10:36,  3.16it/s]\u001b[A\n",
            "  0% 7/2015 [00:02<11:02,  3.03it/s]\u001b[A\n",
            "  0% 8/2015 [00:02<11:20,  2.95it/s]\u001b[A\n",
            "  0% 9/2015 [00:02<11:32,  2.90it/s]\u001b[A\n",
            "  0% 10/2015 [00:03<11:41,  2.86it/s]\u001b[A\n",
            "  1% 11/2015 [00:03<11:47,  2.83it/s]\u001b[A\n",
            "  1% 12/2015 [00:03<11:51,  2.82it/s]\u001b[A\n",
            "  1% 13/2015 [00:04<11:54,  2.80it/s]\u001b[A\n",
            "  1% 14/2015 [00:04<11:55,  2.79it/s]\u001b[A\n",
            "  1% 15/2015 [00:05<11:57,  2.79it/s]\u001b[A\n",
            "  1% 16/2015 [00:05<11:57,  2.79it/s]\u001b[A\n",
            "  1% 17/2015 [00:05<11:58,  2.78it/s]\u001b[A\n",
            "  1% 18/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 19/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 20/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 21/2015 [00:07<11:57,  2.78it/s]\u001b[A\n",
            "  1% 22/2015 [00:07<11:57,  2.78it/s]\u001b[A\n",
            "  1% 23/2015 [00:07<11:57,  2.78it/s]\u001b[A\n",
            "  1% 24/2015 [00:08<11:57,  2.78it/s]\u001b[A\n",
            "  1% 25/2015 [00:08<11:56,  2.78it/s]\u001b[A\n",
            "  1% 26/2015 [00:09<11:56,  2.78it/s]\u001b[A\n",
            "  1% 27/2015 [00:09<11:56,  2.78it/s]\u001b[A\n",
            "  1% 28/2015 [00:09<11:55,  2.78it/s]\u001b[A\n",
            "  1% 29/2015 [00:10<11:55,  2.78it/s]\u001b[A\n",
            "  1% 30/2015 [00:10<11:54,  2.78it/s]\u001b[A\n",
            "  2% 31/2015 [00:10<11:54,  2.78it/s]\u001b[A\n",
            "  2% 32/2015 [00:11<11:54,  2.78it/s]\u001b[A\n",
            "  2% 33/2015 [00:11<11:53,  2.78it/s]\u001b[A\n",
            "  2% 34/2015 [00:11<11:53,  2.78it/s]\u001b[A\n",
            "  2% 35/2015 [00:12<11:53,  2.77it/s]\u001b[A\n",
            "  2% 36/2015 [00:12<11:52,  2.78it/s]\u001b[A\n",
            "  2% 37/2015 [00:12<11:52,  2.78it/s]\u001b[A\n",
            "  2% 38/2015 [00:13<11:52,  2.77it/s]\u001b[A\n",
            "  2% 39/2015 [00:13<11:52,  2.77it/s]\u001b[A\n",
            "  2% 40/2015 [00:14<11:51,  2.77it/s]\u001b[A\n",
            "  2% 41/2015 [00:14<11:51,  2.77it/s]\u001b[A\n",
            "  2% 42/2015 [00:14<11:51,  2.77it/s]\u001b[A\n",
            "  2% 43/2015 [00:15<11:50,  2.78it/s]\u001b[A\n",
            "  2% 44/2015 [00:15<11:50,  2.78it/s]\u001b[A\n",
            "  2% 45/2015 [00:15<11:49,  2.78it/s]\u001b[A\n",
            "  2% 46/2015 [00:16<11:49,  2.77it/s]\u001b[A\n",
            "  2% 47/2015 [00:16<11:49,  2.77it/s]\u001b[A\n",
            "  2% 48/2015 [00:16<11:48,  2.77it/s]\u001b[A\n",
            "  2% 49/2015 [00:17<11:48,  2.77it/s]\u001b[A\n",
            "  2% 50/2015 [00:17<11:48,  2.77it/s]\u001b[A\n",
            "  3% 51/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 52/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 53/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 54/2015 [00:19<11:46,  2.77it/s]\u001b[A\n",
            "  3% 55/2015 [00:19<11:46,  2.77it/s]\u001b[A\n",
            "  3% 56/2015 [00:19<11:46,  2.77it/s]\u001b[A\n",
            "  3% 57/2015 [00:20<11:45,  2.77it/s]\u001b[A\n",
            "  3% 58/2015 [00:20<11:45,  2.77it/s]\u001b[A\n",
            "  3% 59/2015 [00:20<11:45,  2.77it/s]\u001b[A\n",
            "  3% 60/2015 [00:21<11:44,  2.77it/s]\u001b[A\n",
            "  3% 61/2015 [00:21<11:44,  2.77it/s]\u001b[A\n",
            "  3% 62/2015 [00:21<11:44,  2.77it/s]\u001b[A\n",
            "  3% 63/2015 [00:22<11:43,  2.77it/s]\u001b[A\n",
            "  3% 64/2015 [00:22<11:43,  2.77it/s]\u001b[A\n",
            "  3% 65/2015 [00:23<11:43,  2.77it/s]\u001b[A\n",
            "  3% 66/2015 [00:23<11:42,  2.77it/s]\u001b[A\n",
            "  3% 67/2015 [00:23<11:42,  2.77it/s]\u001b[A\n",
            "  3% 68/2015 [00:24<11:42,  2.77it/s]\u001b[A\n",
            "  3% 69/2015 [00:24<11:41,  2.77it/s]\u001b[A\n",
            "  3% 70/2015 [00:24<11:41,  2.77it/s]\u001b[A\n",
            "  4% 71/2015 [00:25<11:40,  2.77it/s]\u001b[A\n",
            "  4% 72/2015 [00:25<11:40,  2.77it/s]\u001b[A\n",
            "  4% 73/2015 [00:25<11:40,  2.77it/s]\u001b[A\n",
            "  4% 74/2015 [00:26<11:39,  2.77it/s]\u001b[A\n",
            "  4% 75/2015 [00:26<11:39,  2.77it/s]\u001b[A\n",
            "  4% 76/2015 [00:27<11:38,  2.77it/s]\u001b[A\n",
            "  4% 77/2015 [00:27<11:38,  2.77it/s]\u001b[A\n",
            "  4% 78/2015 [00:27<11:38,  2.77it/s]\u001b[A\n",
            "  4% 79/2015 [00:28<11:37,  2.77it/s]\u001b[A\n",
            "  4% 80/2015 [00:28<11:37,  2.77it/s]\u001b[A\n",
            "  4% 81/2015 [00:28<11:37,  2.77it/s]\u001b[A\n",
            "  4% 82/2015 [00:29<11:36,  2.77it/s]\u001b[A\n",
            "  4% 83/2015 [00:29<11:36,  2.77it/s]\u001b[A\n",
            "  4% 84/2015 [00:29<11:35,  2.77it/s]\u001b[A\n",
            "  4% 85/2015 [00:30<11:35,  2.77it/s]\u001b[A\n",
            "  4% 86/2015 [00:30<11:35,  2.78it/s]\u001b[A\n",
            "  4% 87/2015 [00:30<11:34,  2.78it/s]\u001b[A\n",
            "  4% 88/2015 [00:31<11:34,  2.78it/s]\u001b[A\n",
            "  4% 89/2015 [00:31<11:33,  2.78it/s]\u001b[A\n",
            "  4% 90/2015 [00:32<11:33,  2.78it/s]\u001b[A\n",
            "  5% 91/2015 [00:32<11:33,  2.77it/s]\u001b[A\n",
            "  5% 92/2015 [00:32<11:33,  2.77it/s]\u001b[A\n",
            "  5% 93/2015 [00:33<11:32,  2.78it/s]\u001b[A\n",
            "  5% 94/2015 [00:33<11:32,  2.77it/s]\u001b[A\n",
            "  5% 95/2015 [00:33<11:32,  2.77it/s]\u001b[A\n",
            "  5% 96/2015 [00:34<11:31,  2.77it/s]\u001b[A\n",
            "  5% 97/2015 [00:34<11:31,  2.78it/s]\u001b[A\n",
            "  5% 98/2015 [00:34<11:31,  2.77it/s]\u001b[A\n",
            "  5% 99/2015 [00:35<11:30,  2.77it/s]\u001b[A\n",
            "  5% 100/2015 [00:35<11:30,  2.77it/s]\u001b[A\n",
            "  5% 101/2015 [00:36<11:30,  2.77it/s]\u001b[A\n",
            "  5% 102/2015 [00:36<11:29,  2.77it/s]\u001b[A\n",
            "  5% 103/2015 [00:36<11:29,  2.77it/s]\u001b[A\n",
            "  5% 104/2015 [00:37<11:28,  2.77it/s]\u001b[A\n",
            "  5% 105/2015 [00:37<11:28,  2.77it/s]\u001b[A\n",
            "  5% 106/2015 [00:37<11:28,  2.77it/s]\u001b[A\n",
            "  5% 107/2015 [00:38<11:27,  2.77it/s]\u001b[A\n",
            "  5% 108/2015 [00:38<11:27,  2.77it/s]\u001b[A\n",
            "  5% 109/2015 [00:38<11:26,  2.78it/s]\u001b[A\n",
            "  5% 110/2015 [00:39<11:26,  2.77it/s]\u001b[A\n",
            "  6% 111/2015 [00:39<11:25,  2.78it/s]\u001b[A\n",
            "  6% 112/2015 [00:40<11:25,  2.78it/s]\u001b[A\n",
            "  6% 113/2015 [00:40<11:25,  2.78it/s]\u001b[A\n",
            "  6% 114/2015 [00:40<11:24,  2.78it/s]\u001b[A\n",
            "  6% 115/2015 [00:41<11:24,  2.78it/s]\u001b[A\n",
            "  6% 116/2015 [00:41<11:24,  2.78it/s]\u001b[A\n",
            "  6% 117/2015 [00:41<11:23,  2.78it/s]\u001b[A\n",
            "  6% 118/2015 [00:42<11:23,  2.78it/s]\u001b[A\n",
            "  6% 119/2015 [00:42<11:22,  2.78it/s]\u001b[A\n",
            "  6% 120/2015 [00:42<11:22,  2.78it/s]\u001b[A\n",
            "  6% 121/2015 [00:43<11:22,  2.78it/s]\u001b[A\n",
            "  6% 122/2015 [00:43<11:22,  2.78it/s]\u001b[A\n",
            "  6% 123/2015 [00:43<11:21,  2.78it/s]\u001b[A\n",
            "  6% 124/2015 [00:44<11:21,  2.78it/s]\u001b[A\n",
            "  6% 125/2015 [00:44<11:20,  2.78it/s]\u001b[A\n",
            "  6% 126/2015 [00:45<11:20,  2.78it/s]\u001b[A\n",
            "  6% 127/2015 [00:45<11:20,  2.78it/s]\u001b[A\n",
            "  6% 128/2015 [00:45<11:19,  2.78it/s]\u001b[A\n",
            "  6% 129/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  6% 130/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  7% 131/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  7% 132/2015 [00:47<11:18,  2.77it/s]\u001b[A\n",
            "  7% 133/2015 [00:47<11:18,  2.77it/s]\u001b[A\n",
            "  7% 134/2015 [00:47<11:17,  2.77it/s]\u001b[A\n",
            "  7% 135/2015 [00:48<11:17,  2.77it/s]\u001b[A\n",
            "  7% 136/2015 [00:48<11:17,  2.77it/s]\u001b[A\n",
            "  7% 137/2015 [00:49<11:16,  2.77it/s]\u001b[A\n",
            "  7% 138/2015 [00:49<11:16,  2.77it/s]\u001b[A\n",
            "  7% 139/2015 [00:49<11:16,  2.77it/s]\u001b[A\n",
            "  7% 140/2015 [00:50<11:15,  2.77it/s]\u001b[A\n",
            "  7% 141/2015 [00:50<11:15,  2.77it/s]\u001b[A\n",
            "  7% 142/2015 [00:50<11:14,  2.77it/s]\u001b[A\n",
            "  7% 143/2015 [00:51<11:14,  2.77it/s]\u001b[A\n",
            "  7% 144/2015 [00:51<11:14,  2.78it/s]\u001b[A\n",
            "  7% 145/2015 [00:51<11:13,  2.78it/s]\u001b[A\n",
            "  7% 146/2015 [00:52<11:13,  2.78it/s]\u001b[A\n",
            "  7% 147/2015 [00:52<11:12,  2.78it/s]\u001b[A\n",
            "  7% 148/2015 [00:52<11:12,  2.78it/s]\u001b[A\n",
            "  7% 149/2015 [00:53<11:12,  2.78it/s]\u001b[A\n",
            "  7% 150/2015 [00:53<11:11,  2.78it/s]\u001b[A\n",
            "  7% 151/2015 [00:54<11:11,  2.78it/s]\u001b[A\n",
            "  8% 152/2015 [00:54<11:11,  2.78it/s]\u001b[A\n",
            "  8% 153/2015 [00:54<11:10,  2.78it/s]\u001b[A\n",
            "  8% 154/2015 [00:55<11:10,  2.78it/s]\u001b[A\n",
            "  8% 155/2015 [00:55<11:10,  2.77it/s]\u001b[A\n",
            "  8% 156/2015 [00:55<11:09,  2.78it/s]\u001b[A\n",
            "  8% 157/2015 [00:56<11:09,  2.78it/s]\u001b[A\n",
            "  8% 158/2015 [00:56<11:09,  2.78it/s]\u001b[A\n",
            "  8% 159/2015 [00:56<11:08,  2.78it/s]\u001b[A\n",
            "  8% 160/2015 [00:57<11:08,  2.78it/s]\u001b[A\n",
            "  8% 161/2015 [00:57<11:07,  2.78it/s]\u001b[A\n",
            "  8% 162/2015 [00:58<11:07,  2.78it/s]\u001b[A\n",
            "  8% 163/2015 [00:58<11:07,  2.78it/s]\u001b[A\n",
            "  8% 164/2015 [00:58<11:06,  2.78it/s]\u001b[A\n",
            "  8% 165/2015 [00:59<11:06,  2.78it/s]\u001b[A\n",
            "  8% 166/2015 [00:59<11:06,  2.78it/s]\u001b[A\n",
            "  8% 167/2015 [00:59<11:06,  2.77it/s]\u001b[A\n",
            "  8% 168/2015 [01:00<11:05,  2.77it/s]\u001b[A\n",
            "  8% 169/2015 [01:00<11:05,  2.77it/s]\u001b[A\n",
            "  8% 170/2015 [01:00<11:04,  2.77it/s]\u001b[A\n",
            "  8% 171/2015 [01:01<11:04,  2.77it/s]\u001b[A\n",
            "  9% 172/2015 [01:01<11:04,  2.78it/s]\u001b[A\n",
            "  9% 173/2015 [01:01<11:03,  2.78it/s]\u001b[A\n",
            "  9% 174/2015 [01:02<11:03,  2.78it/s]\u001b[A\n",
            "  9% 175/2015 [01:02<11:02,  2.78it/s]\u001b[A\n",
            "  9% 176/2015 [01:03<11:02,  2.78it/s]\u001b[A\n",
            "  9% 177/2015 [01:03<11:02,  2.78it/s]\u001b[A\n",
            "  9% 178/2015 [01:03<11:01,  2.78it/s]\u001b[A\n",
            "  9% 179/2015 [01:04<11:01,  2.77it/s]\u001b[A\n",
            "  9% 180/2015 [01:04<11:01,  2.77it/s]\u001b[A\n",
            "  9% 181/2015 [01:04<11:01,  2.77it/s]\u001b[A\n",
            "  9% 182/2015 [01:05<11:00,  2.77it/s]\u001b[A\n",
            "  9% 183/2015 [01:05<11:00,  2.77it/s]\u001b[A\n",
            "  9% 184/2015 [01:05<11:00,  2.77it/s]\u001b[A\n",
            "  9% 185/2015 [01:06<10:59,  2.77it/s]\u001b[A\n",
            "  9% 186/2015 [01:06<10:59,  2.77it/s]\u001b[A\n",
            "  9% 187/2015 [01:07<10:59,  2.77it/s]\u001b[A\n",
            "  9% 188/2015 [01:07<10:58,  2.77it/s]\u001b[A\n",
            "  9% 189/2015 [01:07<10:58,  2.77it/s]\u001b[A\n",
            "  9% 190/2015 [01:08<10:57,  2.77it/s]\u001b[A\n",
            "  9% 191/2015 [01:08<10:57,  2.77it/s]\u001b[A\n",
            " 10% 192/2015 [01:08<10:57,  2.77it/s]\u001b[A\n",
            " 10% 193/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 194/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 195/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 196/2015 [01:10<10:55,  2.77it/s]\u001b[A\n",
            " 10% 197/2015 [01:10<10:55,  2.78it/s]\u001b[A\n",
            " 10% 198/2015 [01:10<10:54,  2.78it/s]\u001b[A\n",
            " 10% 199/2015 [01:11<10:54,  2.78it/s]\u001b[A\n",
            " 10% 200/2015 [01:11<10:53,  2.78it/s]\u001b[A\n",
            " 10% 201/2015 [01:12<10:53,  2.78it/s]\u001b[A\n",
            " 10% 202/2015 [01:12<10:52,  2.78it/s]\u001b[A\n",
            " 10% 203/2015 [01:12<10:52,  2.78it/s]\u001b[A\n",
            " 10% 204/2015 [01:13<10:52,  2.78it/s]\u001b[A\n",
            " 10% 205/2015 [01:13<10:51,  2.78it/s]\u001b[A\n",
            " 10% 206/2015 [01:13<10:51,  2.78it/s]\u001b[A\n",
            " 10% 207/2015 [01:14<10:51,  2.78it/s]\u001b[A\n",
            " 10% 208/2015 [01:14<10:50,  2.78it/s]\u001b[A\n",
            " 10% 209/2015 [01:14<10:50,  2.78it/s]\u001b[A\n",
            " 10% 210/2015 [01:15<10:50,  2.78it/s]\u001b[A\n",
            " 10% 211/2015 [01:15<10:49,  2.78it/s]\u001b[A\n",
            " 11% 212/2015 [01:16<10:49,  2.78it/s]\u001b[A\n",
            " 11% 213/2015 [01:16<10:49,  2.78it/s]\u001b[A\n",
            " 11% 214/2015 [01:16<10:48,  2.78it/s]\u001b[A\n",
            " 11% 215/2015 [01:17<10:48,  2.78it/s]\u001b[A\n",
            " 11% 216/2015 [01:17<10:48,  2.78it/s]\u001b[A\n",
            " 11% 217/2015 [01:17<10:47,  2.78it/s]\u001b[A\n",
            " 11% 218/2015 [01:18<10:47,  2.78it/s]\u001b[A\n",
            " 11% 219/2015 [01:18<10:46,  2.78it/s]\u001b[A\n",
            " 11% 220/2015 [01:18<10:46,  2.78it/s]\u001b[A\n",
            " 11% 221/2015 [01:19<10:46,  2.78it/s]\u001b[A\n",
            " 11% 222/2015 [01:19<10:45,  2.78it/s]\u001b[A\n",
            " 11% 223/2015 [01:19<10:45,  2.78it/s]\u001b[A\n",
            " 11% 224/2015 [01:20<10:44,  2.78it/s]\u001b[A\n",
            " 11% 225/2015 [01:20<10:44,  2.78it/s]\u001b[A\n",
            " 11% 226/2015 [01:21<10:44,  2.78it/s]\u001b[A\n",
            " 11% 227/2015 [01:21<10:43,  2.78it/s]\u001b[A\n",
            " 11% 228/2015 [01:21<10:43,  2.78it/s]\u001b[A\n",
            " 11% 229/2015 [01:22<10:43,  2.78it/s]\u001b[A\n",
            " 11% 230/2015 [01:22<10:42,  2.78it/s]\u001b[A\n",
            " 11% 231/2015 [01:22<10:42,  2.78it/s]\u001b[A\n",
            " 12% 232/2015 [01:23<10:42,  2.78it/s]\u001b[A\n",
            " 12% 233/2015 [01:23<10:41,  2.78it/s]\u001b[A\n",
            " 12% 234/2015 [01:23<10:41,  2.78it/s]\u001b[A\n",
            " 12% 235/2015 [01:24<10:41,  2.78it/s]\u001b[A\n",
            " 12% 236/2015 [01:24<10:40,  2.78it/s]\u001b[A\n",
            " 12% 237/2015 [01:25<10:40,  2.78it/s]\u001b[A\n",
            " 12% 238/2015 [01:25<10:39,  2.78it/s]\u001b[A\n",
            " 12% 239/2015 [01:25<10:39,  2.78it/s]\u001b[A\n",
            " 12% 240/2015 [01:26<10:39,  2.78it/s]\u001b[A\n",
            " 12% 241/2015 [01:26<10:39,  2.78it/s]\u001b[A\n",
            " 12% 242/2015 [01:26<10:38,  2.78it/s]\u001b[A\n",
            " 12% 243/2015 [01:27<10:38,  2.78it/s]\u001b[A\n",
            " 12% 244/2015 [01:27<10:38,  2.78it/s]\u001b[A\n",
            " 12% 245/2015 [01:27<10:37,  2.78it/s]\u001b[A\n",
            " 12% 246/2015 [01:28<10:37,  2.78it/s]\u001b[A\n",
            " 12% 247/2015 [01:28<10:36,  2.78it/s]\u001b[A\n",
            " 12% 248/2015 [01:29<10:36,  2.78it/s]\u001b[A\n",
            " 12% 249/2015 [01:29<10:36,  2.78it/s]\u001b[A\n",
            " 12% 250/2015 [01:29<10:35,  2.78it/s]\u001b[A\n",
            " 12% 251/2015 [01:30<10:35,  2.78it/s]\u001b[A\n",
            " 13% 252/2015 [01:30<10:35,  2.78it/s]\u001b[A\n",
            " 13% 253/2015 [01:30<10:34,  2.78it/s]\u001b[A\n",
            " 13% 254/2015 [01:31<10:34,  2.78it/s]\u001b[A\n",
            " 13% 255/2015 [01:31<10:34,  2.77it/s]\u001b[A\n",
            " 13% 256/2015 [01:31<10:33,  2.78it/s]\u001b[A\n",
            " 13% 257/2015 [01:32<10:33,  2.78it/s]\u001b[A\n",
            " 13% 258/2015 [01:32<10:33,  2.78it/s]\u001b[A\n",
            " 13% 259/2015 [01:32<10:32,  2.78it/s]\u001b[A\n",
            " 13% 260/2015 [01:33<10:32,  2.78it/s]\u001b[A\n",
            " 13% 261/2015 [01:33<10:32,  2.77it/s]\u001b[A\n",
            " 13% 262/2015 [01:34<10:31,  2.78it/s]\u001b[A\n",
            " 13% 263/2015 [01:34<10:31,  2.78it/s]\u001b[A\n",
            " 13% 264/2015 [01:34<10:30,  2.78it/s]\u001b[A\n",
            " 13% 265/2015 [01:35<10:30,  2.78it/s]\u001b[A\n",
            " 13% 266/2015 [01:35<10:30,  2.78it/s]\u001b[A\n",
            " 13% 267/2015 [01:35<10:29,  2.78it/s]\u001b[A\n",
            " 13% 268/2015 [01:36<10:29,  2.78it/s]\u001b[A\n",
            " 13% 269/2015 [01:36<10:29,  2.77it/s]\u001b[A\n",
            " 13% 270/2015 [01:36<10:28,  2.77it/s]\u001b[A\n",
            " 13% 271/2015 [01:37<10:28,  2.77it/s]\u001b[A\n",
            " 13% 272/2015 [01:37<10:28,  2.77it/s]\u001b[A\n",
            " 14% 273/2015 [01:38<10:28,  2.77it/s]\u001b[A\n",
            " 14% 274/2015 [01:38<10:27,  2.77it/s]\u001b[A\n",
            " 14% 275/2015 [01:38<10:27,  2.77it/s]\u001b[A\n",
            " 14% 276/2015 [01:39<10:26,  2.77it/s]\u001b[A\n",
            " 14% 277/2015 [01:39<10:26,  2.78it/s]\u001b[A\n",
            " 14% 278/2015 [01:39<10:25,  2.78it/s]\u001b[A\n",
            " 14% 279/2015 [01:40<10:25,  2.78it/s]\u001b[A\n",
            " 14% 280/2015 [01:40<10:24,  2.78it/s]\u001b[A\n",
            " 14% 281/2015 [01:40<10:24,  2.78it/s]\u001b[A\n",
            " 14% 282/2015 [01:41<10:24,  2.78it/s]\u001b[A\n",
            " 14% 283/2015 [01:41<10:23,  2.78it/s]\u001b[A\n",
            " 14% 284/2015 [01:41<10:23,  2.78it/s]\u001b[A\n",
            " 14% 285/2015 [01:42<10:22,  2.78it/s]\u001b[A\n",
            " 14% 286/2015 [01:42<10:22,  2.78it/s]\u001b[A\n",
            " 14% 287/2015 [01:43<10:22,  2.78it/s]\u001b[A\n",
            " 14% 288/2015 [01:43<10:22,  2.78it/s]\u001b[A\n",
            " 14% 289/2015 [01:43<10:21,  2.78it/s]\u001b[A\n",
            " 14% 290/2015 [01:44<10:21,  2.78it/s]\u001b[A\n",
            " 14% 291/2015 [01:44<10:20,  2.78it/s]\u001b[A\n",
            " 14% 292/2015 [01:44<10:20,  2.78it/s]\u001b[A\n",
            " 15% 293/2015 [01:45<10:20,  2.78it/s]\u001b[A\n",
            " 15% 294/2015 [01:45<10:19,  2.78it/s]\u001b[A\n",
            " 15% 295/2015 [01:45<10:19,  2.78it/s]\u001b[A\n",
            " 15% 296/2015 [01:46<10:19,  2.78it/s]\u001b[A\n",
            " 15% 297/2015 [01:46<10:18,  2.78it/s]\u001b[A\n",
            " 15% 298/2015 [01:47<10:18,  2.78it/s]\u001b[A\n",
            " 15% 299/2015 [01:47<10:17,  2.78it/s]\u001b[A\n",
            " 15% 300/2015 [01:47<10:17,  2.78it/s]\u001b[A\n",
            " 15% 301/2015 [01:48<10:17,  2.78it/s]\u001b[A\n",
            " 15% 302/2015 [01:48<10:16,  2.78it/s]\u001b[A\n",
            " 15% 303/2015 [01:48<10:16,  2.78it/s]\u001b[A\n",
            " 15% 304/2015 [01:49<10:15,  2.78it/s]\u001b[A\n",
            " 15% 305/2015 [01:49<10:15,  2.78it/s]\u001b[A\n",
            " 15% 306/2015 [01:49<10:15,  2.78it/s]\u001b[A\n",
            " 15% 307/2015 [01:50<10:14,  2.78it/s]\u001b[A\n",
            " 15% 308/2015 [01:50<10:14,  2.78it/s]\u001b[A\n",
            " 15% 309/2015 [01:50<10:14,  2.78it/s]\u001b[A\n",
            " 15% 310/2015 [01:51<10:14,  2.78it/s]\u001b[A\n",
            " 15% 311/2015 [01:51<10:13,  2.78it/s]\u001b[A\n",
            " 15% 312/2015 [01:52<10:13,  2.78it/s]\u001b[A\n",
            " 16% 313/2015 [01:52<10:13,  2.78it/s]\u001b[A\n",
            " 16% 314/2015 [01:52<10:12,  2.78it/s]\u001b[A\n",
            " 16% 315/2015 [01:53<10:12,  2.78it/s]\u001b[A\n",
            " 16% 316/2015 [01:53<10:12,  2.77it/s]\u001b[A\n",
            " 16% 317/2015 [01:53<10:11,  2.77it/s]\u001b[A\n",
            " 16% 318/2015 [01:54<10:11,  2.78it/s]\u001b[A\n",
            " 16% 319/2015 [01:54<10:11,  2.77it/s]\u001b[A\n",
            " 16% 320/2015 [01:54<10:10,  2.77it/s]\u001b[A\n",
            " 16% 321/2015 [01:55<10:10,  2.77it/s]\u001b[A\n",
            " 16% 322/2015 [01:55<10:10,  2.77it/s]\u001b[A\n",
            " 16% 323/2015 [01:56<10:09,  2.77it/s]\u001b[A\n",
            " 16% 324/2015 [01:56<10:09,  2.77it/s]\u001b[A\n",
            " 16% 325/2015 [01:56<10:09,  2.77it/s]\u001b[A\n",
            " 16% 326/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 327/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 328/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 329/2015 [01:58<10:08,  2.77it/s]\u001b[A\n",
            " 16% 330/2015 [01:58<10:07,  2.77it/s]\u001b[A\n",
            " 16% 331/2015 [01:58<10:07,  2.77it/s]\u001b[A\n",
            " 16% 332/2015 [01:59<10:06,  2.77it/s]\u001b[A\n",
            " 17% 333/2015 [01:59<10:06,  2.77it/s]\u001b[A\n",
            " 17% 334/2015 [01:59<10:05,  2.77it/s]\u001b[A\n",
            " 17% 335/2015 [02:00<10:05,  2.77it/s]\u001b[A\n",
            " 17% 336/2015 [02:00<10:05,  2.77it/s]\u001b[A\n",
            " 17% 337/2015 [02:01<10:04,  2.77it/s]\u001b[A\n",
            " 17% 338/2015 [02:01<10:04,  2.77it/s]\u001b[A\n",
            " 17% 339/2015 [02:01<10:04,  2.77it/s]\u001b[A\n",
            " 17% 340/2015 [02:02<10:03,  2.77it/s]\u001b[A\n",
            " 17% 341/2015 [02:02<10:03,  2.78it/s]\u001b[A\n",
            " 17% 342/2015 [02:02<10:02,  2.77it/s]\u001b[A\n",
            " 17% 343/2015 [02:03<10:02,  2.77it/s]\u001b[A\n",
            " 17% 344/2015 [02:03<10:02,  2.77it/s]\u001b[A\n",
            " 17% 345/2015 [02:03<10:01,  2.78it/s]\u001b[A\n",
            " 17% 346/2015 [02:04<10:01,  2.77it/s]\u001b[A\n",
            " 17% 347/2015 [02:04<10:01,  2.77it/s]\u001b[A\n",
            " 17% 348/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 349/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 350/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 351/2015 [02:06<09:59,  2.77it/s]\u001b[A\n",
            " 17% 352/2015 [02:06<09:59,  2.77it/s]\u001b[A\n",
            " 18% 353/2015 [02:06<09:59,  2.77it/s]\u001b[A\n",
            " 18% 354/2015 [02:07<09:58,  2.77it/s]\u001b[A\n",
            " 18% 355/2015 [02:07<09:58,  2.77it/s]\u001b[A\n",
            " 18% 356/2015 [02:07<09:58,  2.77it/s]\u001b[A\n",
            " 18% 357/2015 [02:08<09:57,  2.77it/s]\u001b[A\n",
            " 18% 358/2015 [02:08<09:57,  2.77it/s]\u001b[A\n",
            " 18% 359/2015 [02:08<09:56,  2.77it/s]\u001b[A\n",
            " 18% 360/2015 [02:09<09:56,  2.78it/s]\u001b[A\n",
            " 18% 361/2015 [02:09<09:55,  2.78it/s]\u001b[A\n",
            " 18% 362/2015 [02:10<09:55,  2.77it/s]\u001b[A\n",
            " 18% 363/2015 [02:10<09:55,  2.77it/s]\u001b[A\n",
            " 18% 364/2015 [02:10<09:55,  2.77it/s]\u001b[A\n",
            " 18% 365/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 366/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 367/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 368/2015 [02:12<09:53,  2.77it/s]\u001b[A\n",
            " 18% 369/2015 [02:12<09:53,  2.77it/s]\u001b[A\n",
            " 18% 370/2015 [02:12<09:52,  2.77it/s]\u001b[A\n",
            " 18% 371/2015 [02:13<09:52,  2.77it/s]\u001b[A\n",
            " 18% 372/2015 [02:13<09:52,  2.77it/s]\u001b[A\n",
            " 19% 373/2015 [02:14<09:51,  2.77it/s]\u001b[A\n",
            " 19% 374/2015 [02:14<09:51,  2.78it/s]\u001b[A\n",
            " 19% 375/2015 [02:14<09:50,  2.78it/s]\u001b[A\n",
            " 19% 376/2015 [02:15<09:50,  2.78it/s]\u001b[A\n",
            " 19% 377/2015 [02:15<09:50,  2.77it/s]\u001b[A\n",
            " 19% 378/2015 [02:15<09:50,  2.77it/s]\u001b[A\n",
            " 19% 379/2015 [02:16<09:49,  2.77it/s]\u001b[A\n",
            " 19% 380/2015 [02:16<09:48,  2.78it/s]\u001b[A\n",
            " 19% 381/2015 [02:16<09:48,  2.78it/s]\u001b[A\n",
            " 19% 382/2015 [02:17<09:48,  2.78it/s]\u001b[A\n",
            " 19% 383/2015 [02:17<09:47,  2.78it/s]\u001b[A\n",
            " 19% 384/2015 [02:18<09:47,  2.78it/s]\u001b[A\n",
            " 19% 385/2015 [02:18<09:46,  2.78it/s]\u001b[A\n",
            " 19% 386/2015 [02:18<09:46,  2.78it/s]\u001b[A\n",
            " 19% 387/2015 [02:19<09:46,  2.78it/s]\u001b[A\n",
            " 19% 388/2015 [02:19<09:45,  2.78it/s]\u001b[A\n",
            " 19% 389/2015 [02:19<09:45,  2.78it/s]\u001b[A\n",
            " 19% 390/2015 [02:20<09:45,  2.78it/s]\u001b[A\n",
            " 19% 391/2015 [02:20<09:44,  2.78it/s]\u001b[A\n",
            " 19% 392/2015 [02:20<09:44,  2.78it/s]\u001b[A\n",
            " 20% 393/2015 [02:21<09:44,  2.78it/s]\u001b[A\n",
            " 20% 394/2015 [02:21<09:43,  2.78it/s]\u001b[A\n",
            " 20% 395/2015 [02:21<09:43,  2.78it/s]\u001b[A\n",
            " 20% 396/2015 [02:22<09:43,  2.78it/s]\u001b[A\n",
            " 20% 397/2015 [02:22<09:43,  2.78it/s]\u001b[A\n",
            " 20% 398/2015 [02:23<09:42,  2.78it/s]\u001b[A\n",
            " 20% 399/2015 [02:23<09:42,  2.78it/s]\u001b[A\n",
            " 20% 400/2015 [02:23<09:41,  2.78it/s]\u001b[A\n",
            " 20% 401/2015 [02:24<09:41,  2.78it/s]\u001b[A\n",
            " 20% 402/2015 [02:24<09:41,  2.78it/s]\u001b[A\n",
            " 20% 403/2015 [02:24<09:40,  2.78it/s]\u001b[A\n",
            " 20% 404/2015 [02:25<09:40,  2.78it/s]\u001b[A\n",
            " 20% 405/2015 [02:25<09:40,  2.78it/s]\u001b[A\n",
            " 20% 406/2015 [02:25<09:39,  2.77it/s]\u001b[A\n",
            " 20% 407/2015 [02:26<09:39,  2.77it/s]\u001b[A\n",
            " 20% 408/2015 [02:26<09:39,  2.77it/s]\u001b[A\n",
            " 20% 409/2015 [02:27<09:38,  2.78it/s]\u001b[A\n",
            " 20% 410/2015 [02:27<09:38,  2.78it/s]\u001b[A\n",
            " 20% 411/2015 [02:27<09:37,  2.78it/s]\u001b[A\n",
            " 20% 412/2015 [02:28<09:37,  2.78it/s]\u001b[A\n",
            " 20% 413/2015 [02:28<09:37,  2.78it/s]\u001b[A\n",
            " 21% 414/2015 [02:28<09:36,  2.78it/s]\u001b[A\n",
            " 21% 415/2015 [02:29<09:36,  2.78it/s]\u001b[A\n",
            " 21% 416/2015 [02:29<09:35,  2.78it/s]\u001b[A\n",
            " 21% 417/2015 [02:29<09:35,  2.78it/s]\u001b[A\n",
            " 21% 418/2015 [02:30<09:35,  2.78it/s]\u001b[A\n",
            " 21% 419/2015 [02:30<09:34,  2.78it/s]\u001b[A\n",
            " 21% 420/2015 [02:30<09:34,  2.78it/s]\u001b[A\n",
            " 21% 421/2015 [02:31<09:34,  2.78it/s]\u001b[A\n",
            " 21% 422/2015 [02:31<09:33,  2.78it/s]\u001b[A\n",
            " 21% 423/2015 [02:32<09:33,  2.78it/s]\u001b[A\n",
            " 21% 424/2015 [02:32<09:32,  2.78it/s]\u001b[A\n",
            " 21% 425/2015 [02:32<09:32,  2.78it/s]\u001b[A\n",
            " 21% 426/2015 [02:33<09:32,  2.78it/s]\u001b[A\n",
            " 21% 427/2015 [02:33<09:31,  2.78it/s]\u001b[A\n",
            " 21% 428/2015 [02:33<09:31,  2.78it/s]\u001b[A\n",
            " 21% 429/2015 [02:34<09:31,  2.78it/s]\u001b[A\n",
            " 21% 430/2015 [02:34<09:30,  2.78it/s]\u001b[A\n",
            " 21% 431/2015 [02:34<09:30,  2.78it/s]\u001b[A\n",
            " 21% 432/2015 [02:35<09:30,  2.78it/s]\u001b[A\n",
            " 21% 433/2015 [02:35<09:29,  2.78it/s]\u001b[A\n",
            " 22% 434/2015 [02:36<09:29,  2.78it/s]\u001b[A\n",
            " 22% 435/2015 [02:36<09:28,  2.78it/s]\u001b[A\n",
            " 22% 436/2015 [02:36<09:28,  2.78it/s]\u001b[A\n",
            " 22% 437/2015 [02:37<09:28,  2.78it/s]\u001b[A\n",
            " 22% 438/2015 [02:37<09:28,  2.78it/s]\u001b[A\n",
            " 22% 439/2015 [02:37<09:27,  2.77it/s]\u001b[A\n",
            " 22% 440/2015 [02:38<09:27,  2.78it/s]\u001b[A\n",
            " 22% 441/2015 [02:38<09:27,  2.78it/s]\u001b[A\n",
            " 22% 442/2015 [02:38<09:26,  2.78it/s]\u001b[A\n",
            " 22% 443/2015 [02:39<09:26,  2.78it/s]\u001b[A\n",
            " 22% 444/2015 [02:39<09:25,  2.78it/s]\u001b[A\n",
            " 22% 445/2015 [02:39<09:25,  2.78it/s]\u001b[A\n",
            " 22% 446/2015 [02:40<09:25,  2.77it/s]\u001b[A\n",
            " 22% 447/2015 [02:40<09:25,  2.78it/s]\u001b[A\n",
            " 22% 448/2015 [02:41<09:24,  2.77it/s]\u001b[A\n",
            " 22% 449/2015 [02:41<09:24,  2.77it/s]\u001b[A\n",
            " 22% 450/2015 [02:41<09:23,  2.78it/s]\u001b[A\n",
            " 22% 451/2015 [02:42<09:23,  2.77it/s]\u001b[A\n",
            " 22% 452/2015 [02:42<09:23,  2.77it/s]\u001b[A\n",
            " 22% 453/2015 [02:42<09:22,  2.78it/s]\u001b[A\n",
            " 23% 454/2015 [02:43<09:22,  2.78it/s]\u001b[A\n",
            " 23% 455/2015 [02:43<09:21,  2.78it/s]\u001b[A\n",
            " 23% 456/2015 [02:43<09:21,  2.78it/s]\u001b[A\n",
            " 23% 457/2015 [02:44<09:21,  2.78it/s]\u001b[A\n",
            " 23% 458/2015 [02:44<09:20,  2.78it/s]\u001b[A\n",
            " 23% 459/2015 [02:45<09:20,  2.78it/s]\u001b[A\n",
            " 23% 460/2015 [02:45<09:20,  2.78it/s]\u001b[A\n",
            " 23% 461/2015 [02:45<09:19,  2.78it/s]\u001b[A\n",
            " 23% 462/2015 [02:46<09:19,  2.78it/s]\u001b[A\n",
            " 23% 463/2015 [02:46<09:18,  2.78it/s]\u001b[A\n",
            " 23% 464/2015 [02:46<09:18,  2.78it/s]\u001b[A\n",
            " 23% 465/2015 [02:47<09:18,  2.78it/s]\u001b[A\n",
            " 23% 466/2015 [02:47<09:17,  2.78it/s]\u001b[A\n",
            " 23% 467/2015 [02:47<09:17,  2.77it/s]\u001b[A\n",
            " 23% 468/2015 [02:48<09:17,  2.78it/s]\u001b[A\n",
            " 23% 469/2015 [02:48<09:17,  2.78it/s]\u001b[A\n",
            " 23% 470/2015 [02:48<09:16,  2.78it/s]\u001b[A\n",
            " 23% 471/2015 [02:49<09:16,  2.78it/s]\u001b[A\n",
            " 23% 472/2015 [02:49<09:16,  2.77it/s]\u001b[A\n",
            " 23% 473/2015 [02:50<09:15,  2.78it/s]\u001b[A\n",
            " 24% 474/2015 [02:50<09:15,  2.78it/s]\u001b[A\n",
            " 24% 475/2015 [02:50<09:14,  2.78it/s]\u001b[A\n",
            " 24% 476/2015 [02:51<09:14,  2.78it/s]\u001b[A\n",
            " 24% 477/2015 [02:51<09:14,  2.78it/s]\u001b[A\n",
            " 24% 478/2015 [02:51<09:13,  2.78it/s]\u001b[A\n",
            " 24% 479/2015 [02:52<09:13,  2.78it/s]\u001b[A\n",
            " 24% 480/2015 [02:52<09:12,  2.78it/s]\u001b[A\n",
            " 24% 481/2015 [02:52<09:12,  2.78it/s]\u001b[A\n",
            " 24% 482/2015 [02:53<09:12,  2.78it/s]\u001b[A\n",
            " 24% 483/2015 [02:53<09:11,  2.78it/s]\u001b[A\n",
            " 24% 484/2015 [02:54<09:11,  2.78it/s]\u001b[A\n",
            " 24% 485/2015 [02:54<09:11,  2.78it/s]\u001b[A\n",
            " 24% 486/2015 [02:54<09:10,  2.78it/s]\u001b[A\n",
            " 24% 487/2015 [02:55<09:10,  2.78it/s]\u001b[A\n",
            " 24% 488/2015 [02:55<09:10,  2.78it/s]\u001b[A\n",
            " 24% 489/2015 [02:55<09:09,  2.78it/s]\u001b[A\n",
            " 24% 490/2015 [02:56<09:09,  2.78it/s]\u001b[A\n",
            " 24% 491/2015 [02:56<09:08,  2.78it/s]\u001b[A\n",
            " 24% 492/2015 [02:56<09:08,  2.78it/s]\u001b[A\n",
            " 24% 493/2015 [02:57<09:08,  2.78it/s]\u001b[A\n",
            " 25% 494/2015 [02:57<09:07,  2.78it/s]\u001b[A\n",
            " 25% 495/2015 [02:57<09:07,  2.78it/s]\u001b[A\n",
            " 25% 496/2015 [02:58<09:07,  2.77it/s]\u001b[A\n",
            " 25% 497/2015 [02:58<09:07,  2.77it/s]\u001b[A\n",
            " 25% 498/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 499/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 500/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 501/2015 [03:00<09:05,  2.77it/s]\u001b[A\n",
            " 25% 502/2015 [03:00<09:05,  2.77it/s]\u001b[A\n",
            " 25% 503/2015 [03:00<09:05,  2.77it/s]\u001b[A\n",
            " 25% 504/2015 [03:01<09:04,  2.77it/s]\u001b[A\n",
            " 25% 505/2015 [03:01<09:04,  2.77it/s]\u001b[A\n",
            " 25% 506/2015 [03:01<09:04,  2.77it/s]\u001b[A\n",
            " 25% 507/2015 [03:02<09:03,  2.77it/s]\u001b[A\n",
            " 25% 508/2015 [03:02<09:03,  2.77it/s]\u001b[A\n",
            " 25% 509/2015 [03:03<09:02,  2.77it/s]\u001b[A\n",
            " 25% 510/2015 [03:03<09:02,  2.77it/s]\u001b[A\n",
            " 25% 511/2015 [03:03<09:02,  2.77it/s]\u001b[A\n",
            " 25% 512/2015 [03:04<09:01,  2.77it/s]\u001b[A\n",
            " 25% 513/2015 [03:04<09:01,  2.77it/s]\u001b[A\n",
            " 26% 514/2015 [03:04<09:01,  2.77it/s]\u001b[A\n",
            " 26% 515/2015 [03:05<09:00,  2.77it/s]\u001b[A\n",
            " 26% 516/2015 [03:05<09:00,  2.78it/s]\u001b[A\n",
            " 26% 517/2015 [03:05<08:59,  2.78it/s]\u001b[A\n",
            " 26% 518/2015 [03:06<08:59,  2.78it/s]\u001b[A\n",
            " 26% 519/2015 [03:06<08:59,  2.77it/s]\u001b[A\n",
            " 26% 520/2015 [03:07<08:58,  2.78it/s]\u001b[A\n",
            " 26% 521/2015 [03:07<08:58,  2.78it/s]\u001b[A\n",
            " 26% 522/2015 [03:07<08:58,  2.78it/s]\u001b[A\n",
            " 26% 523/2015 [03:08<08:57,  2.77it/s]\u001b[A\n",
            " 26% 524/2015 [03:08<08:57,  2.77it/s]\u001b[A\n",
            " 26% 525/2015 [03:08<08:57,  2.77it/s]\u001b[A\n",
            " 26% 526/2015 [03:09<08:56,  2.77it/s]\u001b[A\n",
            " 26% 527/2015 [03:09<08:56,  2.77it/s]\u001b[A\n",
            " 26% 528/2015 [03:09<08:55,  2.78it/s]\u001b[A\n",
            " 26% 529/2015 [03:10<08:55,  2.78it/s]\u001b[A\n",
            " 26% 530/2015 [03:10<08:55,  2.78it/s]\u001b[A\n",
            " 26% 531/2015 [03:10<08:54,  2.78it/s]\u001b[A\n",
            " 26% 532/2015 [03:11<08:54,  2.78it/s]\u001b[A\n",
            " 26% 533/2015 [03:11<08:53,  2.78it/s]\u001b[A\n",
            " 27% 534/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 535/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 536/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 537/2015 [03:13<08:52,  2.77it/s]\u001b[A\n",
            " 27% 538/2015 [03:13<08:52,  2.77it/s]\u001b[A\n",
            " 27% 539/2015 [03:13<08:52,  2.77it/s]\u001b[A\n",
            " 27% 540/2015 [03:14<08:51,  2.77it/s]\u001b[A\n",
            " 27% 541/2015 [03:14<08:51,  2.77it/s]\u001b[A\n",
            " 27% 542/2015 [03:14<08:51,  2.77it/s]\u001b[A\n",
            " 27% 543/2015 [03:15<08:50,  2.77it/s]\u001b[A\n",
            " 27% 544/2015 [03:15<08:50,  2.77it/s]\u001b[A\n",
            " 27% 545/2015 [03:16<08:49,  2.77it/s]\u001b[A\n",
            " 27% 546/2015 [03:16<08:49,  2.77it/s]\u001b[A\n",
            " 27% 547/2015 [03:16<08:49,  2.77it/s]\u001b[A\n",
            " 27% 548/2015 [03:17<08:48,  2.77it/s]\u001b[A\n",
            " 27% 549/2015 [03:17<08:48,  2.77it/s]\u001b[A\n",
            " 27% 550/2015 [03:17<08:47,  2.77it/s]\u001b[A\n",
            " 27% 551/2015 [03:18<08:47,  2.77it/s]\u001b[A\n",
            " 27% 552/2015 [03:18<08:47,  2.77it/s]\u001b[A\n",
            " 27% 553/2015 [03:18<08:46,  2.77it/s]\u001b[A\n",
            " 27% 554/2015 [03:19<08:46,  2.77it/s]\u001b[A\n",
            " 28% 555/2015 [03:19<08:46,  2.77it/s]\u001b[A\n",
            " 28% 556/2015 [03:19<08:45,  2.78it/s]\u001b[A\n",
            " 28% 557/2015 [03:20<08:45,  2.78it/s]\u001b[A\n",
            " 28% 558/2015 [03:20<08:44,  2.78it/s]\u001b[A\n",
            " 28% 559/2015 [03:21<08:44,  2.78it/s]\u001b[A\n",
            " 28% 560/2015 [03:21<08:44,  2.78it/s]\u001b[A\n",
            " 28% 561/2015 [03:21<08:43,  2.78it/s]\u001b[A\n",
            " 28% 562/2015 [03:22<08:43,  2.78it/s]\u001b[A\n",
            " 28% 563/2015 [03:22<08:42,  2.78it/s]\u001b[A\n",
            " 28% 564/2015 [03:22<08:42,  2.78it/s]\u001b[A\n",
            " 28% 565/2015 [03:23<08:42,  2.78it/s]\u001b[A\n",
            " 28% 566/2015 [03:23<08:41,  2.78it/s]\u001b[A\n",
            " 28% 567/2015 [03:23<08:41,  2.78it/s]\u001b[A\n",
            " 28% 568/2015 [03:24<08:41,  2.78it/s]\u001b[A\n",
            " 28% 569/2015 [03:24<08:40,  2.78it/s]\u001b[A\n",
            " 28% 570/2015 [03:25<08:40,  2.78it/s]\u001b[A\n",
            " 28% 571/2015 [03:25<08:40,  2.78it/s]\u001b[A\n",
            " 28% 572/2015 [03:25<08:39,  2.78it/s]\u001b[A\n",
            " 28% 573/2015 [03:26<08:39,  2.78it/s]\u001b[A\n",
            " 28% 574/2015 [03:26<08:39,  2.78it/s]\u001b[A\n",
            " 29% 575/2015 [03:26<08:38,  2.78it/s]\u001b[A\n",
            " 29% 576/2015 [03:27<08:38,  2.77it/s]\u001b[A\n",
            " 29% 577/2015 [03:27<08:38,  2.77it/s]\u001b[A\n",
            " 29% 578/2015 [03:27<08:37,  2.77it/s]\u001b[A\n",
            " 29% 579/2015 [03:28<08:37,  2.78it/s]\u001b[A\n",
            " 29% 580/2015 [03:28<08:36,  2.78it/s]\u001b[A\n",
            " 29% 581/2015 [03:28<08:36,  2.78it/s]\u001b[A\n",
            " 29% 582/2015 [03:29<08:36,  2.78it/s]\u001b[A\n",
            " 29% 583/2015 [03:29<08:36,  2.78it/s]\u001b[A\n",
            " 29% 584/2015 [03:30<08:35,  2.77it/s]\u001b[A\n",
            " 29% 585/2015 [03:30<08:35,  2.78it/s]\u001b[A\n",
            " 29% 586/2015 [03:30<08:34,  2.78it/s]\u001b[A\n",
            " 29% 587/2015 [03:31<08:34,  2.78it/s]\u001b[A\n",
            " 29% 588/2015 [03:31<08:34,  2.78it/s]\u001b[A\n",
            " 29% 589/2015 [03:31<08:33,  2.78it/s]\u001b[A\n",
            " 29% 590/2015 [03:32<08:33,  2.78it/s]\u001b[A\n",
            " 29% 591/2015 [03:32<08:33,  2.78it/s]\u001b[A\n",
            " 29% 592/2015 [03:32<08:32,  2.78it/s]\u001b[A\n",
            " 29% 593/2015 [03:33<08:32,  2.78it/s]\u001b[A\n",
            " 29% 594/2015 [03:33<08:31,  2.78it/s]\u001b[A\n",
            " 30% 595/2015 [03:34<08:31,  2.78it/s]\u001b[A\n",
            " 30% 596/2015 [03:34<08:31,  2.77it/s]\u001b[A\n",
            " 30% 597/2015 [03:34<08:30,  2.78it/s]\u001b[A\n",
            " 30% 598/2015 [03:35<08:30,  2.78it/s]\u001b[A\n",
            " 30% 599/2015 [03:35<08:30,  2.78it/s]\u001b[A\n",
            " 30% 600/2015 [03:35<08:29,  2.78it/s]\u001b[A\n",
            " 30% 601/2015 [03:36<08:29,  2.78it/s]\u001b[A\n",
            " 30% 602/2015 [03:36<08:29,  2.77it/s]\u001b[A\n",
            " 30% 603/2015 [03:36<08:28,  2.77it/s]\u001b[A\n",
            " 30% 604/2015 [03:37<08:28,  2.78it/s]\u001b[A\n",
            " 30% 605/2015 [03:37<08:28,  2.78it/s]\u001b[A\n",
            " 30% 606/2015 [03:37<08:27,  2.78it/s]\u001b[A\n",
            " 30% 607/2015 [03:38<08:27,  2.78it/s]\u001b[A\n",
            " 30% 608/2015 [03:38<08:26,  2.78it/s]\u001b[A\n",
            " 30% 609/2015 [03:39<08:26,  2.78it/s]\u001b[A\n",
            " 30% 610/2015 [03:39<08:26,  2.78it/s]\u001b[A\n",
            " 30% 611/2015 [03:39<08:25,  2.78it/s]\u001b[A\n",
            " 30% 612/2015 [03:40<08:25,  2.78it/s]\u001b[A\n",
            " 30% 613/2015 [03:40<08:25,  2.78it/s]\u001b[A\n",
            " 30% 614/2015 [03:40<08:24,  2.78it/s]\u001b[A\n",
            " 31% 615/2015 [03:41<08:24,  2.78it/s]\u001b[A\n",
            " 31% 616/2015 [03:41<08:24,  2.78it/s]\u001b[A\n",
            " 31% 617/2015 [03:41<08:23,  2.78it/s]\u001b[A\n",
            " 31% 618/2015 [03:42<08:23,  2.77it/s]\u001b[A\n",
            " 31% 619/2015 [03:42<08:23,  2.77it/s]\u001b[A\n",
            " 31% 620/2015 [03:43<08:22,  2.77it/s]\u001b[A\n",
            " 31% 621/2015 [03:43<08:22,  2.77it/s]\u001b[A\n",
            " 31% 622/2015 [03:43<08:22,  2.77it/s]\u001b[A\n",
            " 31% 623/2015 [03:44<08:21,  2.78it/s]\u001b[A\n",
            " 31% 624/2015 [03:44<08:21,  2.78it/s]\u001b[A\n",
            " 31% 625/2015 [03:44<08:20,  2.78it/s]\u001b[A\n",
            " 31% 626/2015 [03:45<08:20,  2.78it/s]\u001b[A\n",
            " 31% 627/2015 [03:45<08:20,  2.78it/s]\u001b[A\n",
            " 31% 628/2015 [03:45<08:19,  2.78it/s]\u001b[A\n",
            " 31% 629/2015 [03:46<08:19,  2.78it/s]\u001b[A\n",
            " 31% 630/2015 [03:46<08:18,  2.78it/s]\u001b[A\n",
            " 31% 631/2015 [03:47<08:18,  2.78it/s]\u001b[A\n",
            " 31% 632/2015 [03:47<08:18,  2.78it/s]\u001b[A\n",
            " 31% 633/2015 [03:47<08:17,  2.78it/s]\u001b[A\n",
            " 31% 634/2015 [03:48<08:17,  2.78it/s]\u001b[A\n",
            " 32% 635/2015 [03:48<08:17,  2.78it/s]\u001b[A\n",
            " 32% 636/2015 [03:48<08:16,  2.78it/s]\u001b[A\n",
            " 32% 637/2015 [03:49<08:16,  2.78it/s]\u001b[A\n",
            " 32% 638/2015 [03:49<08:16,  2.78it/s]\u001b[A\n",
            " 32% 639/2015 [03:49<08:15,  2.78it/s]\u001b[A\n",
            " 32% 640/2015 [03:50<08:15,  2.78it/s]\u001b[A\n",
            " 32% 641/2015 [03:50<08:14,  2.78it/s]\u001b[A\n",
            " 32% 642/2015 [03:50<08:14,  2.78it/s]\u001b[A\n",
            " 32% 643/2015 [03:51<08:14,  2.78it/s]\u001b[A\n",
            " 32% 644/2015 [03:51<08:13,  2.78it/s]\u001b[A\n",
            " 32% 645/2015 [03:52<08:13,  2.78it/s]\u001b[A\n",
            " 32% 646/2015 [03:52<08:13,  2.78it/s]\u001b[A\n",
            " 32% 647/2015 [03:52<08:12,  2.78it/s]\u001b[A\n",
            " 32% 648/2015 [03:53<08:12,  2.78it/s]\u001b[A\n",
            " 32% 649/2015 [03:53<08:12,  2.78it/s]\u001b[A\n",
            " 32% 650/2015 [03:53<08:11,  2.78it/s]\u001b[A\n",
            " 32% 651/2015 [03:54<08:11,  2.78it/s]\u001b[A\n",
            " 32% 652/2015 [03:54<08:11,  2.78it/s]\u001b[A\n",
            " 32% 653/2015 [03:54<08:10,  2.78it/s]\u001b[A\n",
            " 32% 654/2015 [03:55<08:10,  2.78it/s]\u001b[A\n",
            " 33% 655/2015 [03:55<08:09,  2.78it/s]\u001b[A\n",
            " 33% 656/2015 [03:56<08:09,  2.78it/s]\u001b[A\n",
            " 33% 657/2015 [03:56<08:09,  2.78it/s]\u001b[A\n",
            " 33% 658/2015 [03:56<08:08,  2.78it/s]\u001b[A\n",
            " 33% 659/2015 [03:57<08:08,  2.78it/s]\u001b[A\n",
            " 33% 660/2015 [03:57<08:07,  2.78it/s]\u001b[A\n",
            " 33% 661/2015 [03:57<08:07,  2.78it/s]\u001b[A\n",
            " 33% 662/2015 [03:58<08:07,  2.78it/s]\u001b[A\n",
            " 33% 663/2015 [03:58<08:06,  2.78it/s]\u001b[A\n",
            " 33% 664/2015 [03:58<08:06,  2.78it/s]\u001b[A\n",
            " 33% 665/2015 [03:59<08:06,  2.78it/s]\u001b[A\n",
            " 33% 666/2015 [03:59<08:05,  2.78it/s]\u001b[A\n",
            " 33% 667/2015 [03:59<08:05,  2.78it/s]\u001b[A\n",
            " 33% 668/2015 [04:00<08:05,  2.78it/s]\u001b[A\n",
            " 33% 669/2015 [04:00<08:04,  2.78it/s]\u001b[A\n",
            " 33% 670/2015 [04:01<08:04,  2.78it/s]\u001b[A\n",
            " 33% 671/2015 [04:01<08:04,  2.78it/s]\u001b[A\n",
            " 33% 672/2015 [04:01<08:03,  2.78it/s]\u001b[A\n",
            " 33% 673/2015 [04:02<08:03,  2.77it/s]\u001b[A\n",
            " 33% 674/2015 [04:02<08:03,  2.77it/s]\u001b[A\n",
            " 33% 675/2015 [04:02<08:02,  2.78it/s]\u001b[A\n",
            " 34% 676/2015 [04:03<08:02,  2.78it/s]\u001b[A\n",
            " 34% 677/2015 [04:03<08:02,  2.78it/s]\u001b[A\n",
            " 34% 678/2015 [04:03<08:01,  2.78it/s]\u001b[A\n",
            " 34% 679/2015 [04:04<08:01,  2.78it/s]\u001b[A\n",
            " 34% 680/2015 [04:04<08:00,  2.78it/s]\u001b[A\n",
            " 34% 681/2015 [04:05<08:00,  2.78it/s]\u001b[A\n",
            " 34% 682/2015 [04:05<08:00,  2.77it/s]\u001b[A\n",
            " 34% 683/2015 [04:05<08:00,  2.77it/s]\u001b[A\n",
            " 34% 684/2015 [04:06<07:59,  2.77it/s]\u001b[A\n",
            " 34% 685/2015 [04:06<07:59,  2.77it/s]\u001b[A\n",
            " 34% 686/2015 [04:06<07:59,  2.77it/s]\u001b[A\n",
            " 34% 687/2015 [04:07<07:59,  2.77it/s]\u001b[A\n",
            " 34% 688/2015 [04:07<07:58,  2.77it/s]\u001b[A\n",
            " 34% 689/2015 [04:07<07:58,  2.77it/s]\u001b[A\n",
            " 34% 690/2015 [04:08<07:57,  2.77it/s]\u001b[A\n",
            " 34% 691/2015 [04:08<07:57,  2.77it/s]\u001b[A\n",
            " 34% 692/2015 [04:08<07:57,  2.77it/s]\u001b[A\n",
            " 34% 693/2015 [04:09<07:56,  2.77it/s]\u001b[A\n",
            " 34% 694/2015 [04:09<07:56,  2.77it/s]\u001b[A\n",
            " 34% 695/2015 [04:10<07:55,  2.77it/s]\u001b[A\n",
            " 35% 696/2015 [04:10<07:55,  2.77it/s]\u001b[A\n",
            " 35% 697/2015 [04:10<07:54,  2.77it/s]\u001b[A\n",
            " 35% 698/2015 [04:11<07:54,  2.77it/s]\u001b[A\n",
            " 35% 699/2015 [04:11<07:54,  2.77it/s]\u001b[A\n",
            " 35% 700/2015 [04:11<07:53,  2.77it/s]\u001b[A\n",
            " 35% 701/2015 [04:12<07:53,  2.77it/s]\u001b[A\n",
            " 35% 702/2015 [04:12<07:53,  2.77it/s]\u001b[A\n",
            " 35% 703/2015 [04:12<07:52,  2.77it/s]\u001b[A\n",
            " 35% 704/2015 [04:13<07:52,  2.77it/s]\u001b[A\n",
            " 35% 705/2015 [04:13<07:52,  2.77it/s]\u001b[A\n",
            " 35% 706/2015 [04:14<07:51,  2.78it/s]\u001b[A\n",
            " 35% 707/2015 [04:14<07:51,  2.78it/s]\u001b[A\n",
            " 35% 708/2015 [04:14<07:51,  2.77it/s]\u001b[A\n",
            " 35% 709/2015 [04:15<07:51,  2.77it/s]\u001b[A\n",
            " 35% 710/2015 [04:15<07:50,  2.77it/s]\u001b[A\n",
            " 35% 711/2015 [04:15<07:50,  2.77it/s]\u001b[A\n",
            " 35% 712/2015 [04:16<07:50,  2.77it/s]\u001b[A\n",
            " 35% 713/2015 [04:16<07:49,  2.77it/s]\u001b[A\n",
            " 35% 714/2015 [04:16<07:49,  2.77it/s]\u001b[A\n",
            " 35% 715/2015 [04:17<07:48,  2.77it/s]\u001b[A\n",
            " 36% 716/2015 [04:17<07:48,  2.77it/s]\u001b[A\n",
            " 36% 717/2015 [04:17<07:47,  2.77it/s]\u001b[A\n",
            " 36% 718/2015 [04:18<07:47,  2.77it/s]\u001b[A\n",
            " 36% 719/2015 [04:18<07:47,  2.77it/s]\u001b[A\n",
            " 36% 720/2015 [04:19<07:47,  2.77it/s]\u001b[A\n",
            " 36% 721/2015 [04:19<07:46,  2.77it/s]\u001b[A\n",
            " 36% 722/2015 [04:19<07:46,  2.77it/s]\u001b[A\n",
            " 36% 723/2015 [04:20<07:45,  2.77it/s]\u001b[A\n",
            " 36% 724/2015 [04:20<07:45,  2.77it/s]\u001b[A\n",
            " 36% 725/2015 [04:20<07:45,  2.77it/s]\u001b[A\n",
            " 36% 726/2015 [04:21<07:44,  2.77it/s]\u001b[A\n",
            " 36% 727/2015 [04:21<07:44,  2.77it/s]\u001b[A\n",
            " 36% 728/2015 [04:21<07:44,  2.77it/s]\u001b[A\n",
            " 36% 729/2015 [04:22<07:43,  2.77it/s]\u001b[A\n",
            " 36% 730/2015 [04:22<07:43,  2.77it/s]\u001b[A\n",
            " 36% 731/2015 [04:23<07:42,  2.77it/s]\u001b[A\n",
            " 36% 732/2015 [04:23<07:42,  2.77it/s]\u001b[A\n",
            " 36% 733/2015 [04:23<07:42,  2.77it/s]\u001b[A\n",
            " 36% 734/2015 [04:24<07:41,  2.77it/s]\u001b[A\n",
            " 36% 735/2015 [04:24<07:41,  2.77it/s]\u001b[A\n",
            " 37% 736/2015 [04:24<07:40,  2.77it/s]\u001b[A\n",
            " 37% 737/2015 [04:25<07:40,  2.77it/s]\u001b[A\n",
            " 37% 738/2015 [04:25<07:40,  2.77it/s]\u001b[A\n",
            " 37% 739/2015 [04:25<07:39,  2.77it/s]\u001b[A\n",
            " 37% 740/2015 [04:26<07:39,  2.77it/s]\u001b[A\n",
            " 37% 741/2015 [04:26<07:39,  2.77it/s]\u001b[A\n",
            " 37% 742/2015 [04:27<07:38,  2.77it/s]\u001b[A\n",
            " 37% 743/2015 [04:27<07:38,  2.77it/s]\u001b[A\n",
            " 37% 744/2015 [04:27<07:38,  2.77it/s]\u001b[A\n",
            " 37% 745/2015 [04:28<07:37,  2.77it/s]\u001b[A\n",
            " 37% 746/2015 [04:28<07:37,  2.77it/s]\u001b[A\n",
            " 37% 747/2015 [04:28<07:36,  2.78it/s]\u001b[A\n",
            " 37% 748/2015 [04:29<07:36,  2.78it/s]\u001b[A\n",
            " 37% 749/2015 [04:29<07:36,  2.78it/s]\u001b[A\n",
            " 37% 750/2015 [04:29<07:35,  2.77it/s]\u001b[A\n",
            " 37% 751/2015 [04:30<07:35,  2.78it/s]\u001b[A\n",
            " 37% 752/2015 [04:30<07:35,  2.78it/s]\u001b[A\n",
            " 37% 753/2015 [04:30<07:34,  2.78it/s]\u001b[A\n",
            " 37% 754/2015 [04:31<07:34,  2.78it/s]\u001b[A\n",
            " 37% 755/2015 [04:31<07:33,  2.78it/s]\u001b[A\n",
            " 38% 756/2015 [04:32<07:33,  2.78it/s]\u001b[A\n",
            " 38% 757/2015 [04:32<07:33,  2.77it/s]\u001b[A\n",
            " 38% 758/2015 [04:32<07:32,  2.78it/s]\u001b[A\n",
            " 38% 759/2015 [04:33<07:32,  2.78it/s]\u001b[A\n",
            " 38% 760/2015 [04:33<07:32,  2.78it/s]\u001b[A\n",
            " 38% 761/2015 [04:33<07:31,  2.78it/s]\u001b[A\n",
            " 38% 762/2015 [04:34<07:31,  2.78it/s]\u001b[A\n",
            " 38% 763/2015 [04:34<07:31,  2.78it/s]\u001b[A\n",
            " 38% 764/2015 [04:34<07:30,  2.78it/s]\u001b[A\n",
            " 38% 765/2015 [04:35<07:30,  2.78it/s]\u001b[A\n",
            " 38% 766/2015 [04:35<07:29,  2.78it/s]\u001b[A\n",
            " 38% 767/2015 [04:36<07:29,  2.78it/s]\u001b[A\n",
            " 38% 768/2015 [04:36<07:29,  2.78it/s]\u001b[A\n",
            " 38% 769/2015 [04:36<07:28,  2.78it/s]\u001b[A\n",
            " 38% 770/2015 [04:37<07:28,  2.78it/s]\u001b[A\n",
            " 38% 771/2015 [04:37<07:28,  2.78it/s]\u001b[A\n",
            " 38% 772/2015 [04:37<07:27,  2.78it/s]\u001b[A\n",
            " 38% 773/2015 [04:38<07:27,  2.78it/s]\u001b[A\n",
            " 38% 774/2015 [04:38<07:26,  2.78it/s]\u001b[A\n",
            " 38% 775/2015 [04:38<07:26,  2.78it/s]\u001b[A\n",
            " 39% 776/2015 [04:39<07:26,  2.78it/s]\u001b[A\n",
            " 39% 777/2015 [04:39<07:25,  2.78it/s]\u001b[A\n",
            " 39% 778/2015 [04:39<07:25,  2.78it/s]\u001b[A\n",
            " 39% 779/2015 [04:40<07:25,  2.78it/s]\u001b[A\n",
            " 39% 780/2015 [04:40<07:24,  2.78it/s]\u001b[A\n",
            " 39% 781/2015 [04:41<07:24,  2.78it/s]\u001b[A\n",
            " 39% 782/2015 [04:41<07:24,  2.78it/s]\u001b[A\n",
            " 39% 783/2015 [04:41<07:24,  2.77it/s]\u001b[A\n",
            " 39% 784/2015 [04:42<07:23,  2.77it/s]\u001b[A\n",
            " 39% 785/2015 [04:42<07:23,  2.78it/s]\u001b[A\n",
            " 39% 786/2015 [04:42<07:22,  2.78it/s]\u001b[A\n",
            " 39% 787/2015 [04:43<07:22,  2.78it/s]\u001b[A\n",
            " 39% 788/2015 [04:43<07:21,  2.78it/s]\u001b[A\n",
            " 39% 789/2015 [04:43<07:21,  2.78it/s]\u001b[A\n",
            " 39% 790/2015 [04:44<07:21,  2.78it/s]\u001b[A\n",
            " 39% 791/2015 [04:44<07:20,  2.78it/s]\u001b[A\n",
            " 39% 792/2015 [04:45<07:20,  2.78it/s]\u001b[A\n",
            " 39% 793/2015 [04:45<07:20,  2.78it/s]\u001b[A\n",
            " 39% 794/2015 [04:45<07:19,  2.78it/s]\u001b[A\n",
            " 39% 795/2015 [04:46<07:19,  2.78it/s]\u001b[A\n",
            " 40% 796/2015 [04:46<07:19,  2.78it/s]\u001b[A\n",
            " 40% 797/2015 [04:46<07:18,  2.78it/s]\u001b[A\n",
            " 40% 798/2015 [04:47<07:18,  2.78it/s]\u001b[A\n",
            " 40% 799/2015 [04:47<07:18,  2.78it/s]\u001b[A\n",
            " 40% 800/2015 [04:47<07:17,  2.78it/s]\u001b[A\n",
            " 40% 801/2015 [04:48<07:17,  2.78it/s]\u001b[A\n",
            " 40% 802/2015 [04:48<07:16,  2.78it/s]\u001b[A\n",
            " 40% 803/2015 [04:48<07:16,  2.78it/s]\u001b[A\n",
            " 40% 804/2015 [04:49<07:16,  2.78it/s]\u001b[A\n",
            " 40% 805/2015 [04:49<07:15,  2.78it/s]\u001b[A\n",
            " 40% 806/2015 [04:50<07:15,  2.78it/s]\u001b[A\n",
            " 40% 807/2015 [04:50<07:15,  2.78it/s]\u001b[A\n",
            " 40% 808/2015 [04:50<07:14,  2.78it/s]\u001b[A\n",
            " 40% 809/2015 [04:51<07:14,  2.78it/s]\u001b[A\n",
            " 40% 810/2015 [04:51<07:14,  2.78it/s]\u001b[A\n",
            " 40% 811/2015 [04:51<07:13,  2.78it/s]\u001b[A\n",
            " 40% 812/2015 [04:52<07:13,  2.78it/s]\u001b[A\n",
            " 40% 813/2015 [04:52<07:12,  2.78it/s]\u001b[A\n",
            " 40% 814/2015 [04:52<07:12,  2.78it/s]\u001b[A\n",
            " 40% 815/2015 [04:53<07:12,  2.78it/s]\u001b[A\n",
            " 40% 816/2015 [04:53<07:11,  2.78it/s]\u001b[A\n",
            " 41% 817/2015 [04:54<07:11,  2.78it/s]\u001b[A\n",
            " 41% 818/2015 [04:54<07:11,  2.78it/s]\u001b[A\n",
            " 41% 819/2015 [04:54<07:10,  2.78it/s]\u001b[A\n",
            " 41% 820/2015 [04:55<07:10,  2.78it/s]\u001b[A\n",
            " 41% 821/2015 [04:55<07:09,  2.78it/s]\u001b[A\n",
            " 41% 822/2015 [04:55<07:09,  2.78it/s]\u001b[A\n",
            " 41% 823/2015 [04:56<07:09,  2.78it/s]\u001b[A\n",
            " 41% 824/2015 [04:56<07:08,  2.78it/s]\u001b[A\n",
            " 41% 825/2015 [04:56<07:08,  2.78it/s]\u001b[A\n",
            " 41% 826/2015 [04:57<07:08,  2.78it/s]\u001b[A\n",
            " 41% 827/2015 [04:57<07:07,  2.78it/s]\u001b[A\n",
            " 41% 828/2015 [04:57<07:07,  2.78it/s]\u001b[A\n",
            " 41% 829/2015 [04:58<07:06,  2.78it/s]\u001b[A\n",
            " 41% 830/2015 [04:58<07:06,  2.78it/s]\u001b[A\n",
            " 41% 831/2015 [04:59<07:06,  2.78it/s]\u001b[A\n",
            " 41% 832/2015 [04:59<07:05,  2.78it/s]\u001b[A\n",
            " 41% 833/2015 [04:59<07:05,  2.78it/s]\u001b[A\n",
            " 41% 834/2015 [05:00<07:05,  2.78it/s]\u001b[A\n",
            " 41% 835/2015 [05:00<07:04,  2.78it/s]\u001b[A\n",
            " 41% 836/2015 [05:00<07:04,  2.78it/s]\u001b[A\n",
            " 42% 837/2015 [05:01<07:04,  2.78it/s]\u001b[A\n",
            " 42% 838/2015 [05:01<07:03,  2.78it/s]\u001b[A\n",
            " 42% 839/2015 [05:01<07:03,  2.78it/s]\u001b[A\n",
            " 42% 840/2015 [05:02<07:03,  2.78it/s]\u001b[A\n",
            " 42% 841/2015 [05:02<07:02,  2.78it/s]\u001b[A\n",
            " 42% 842/2015 [05:03<07:02,  2.78it/s]\u001b[A\n",
            " 42% 843/2015 [05:03<07:02,  2.78it/s]\u001b[A\n",
            " 42% 844/2015 [05:03<07:01,  2.78it/s]\u001b[A\n",
            " 42% 845/2015 [05:04<07:01,  2.78it/s]\u001b[A\n",
            " 42% 846/2015 [05:04<07:01,  2.78it/s]\u001b[A\n",
            " 42% 847/2015 [05:04<07:00,  2.78it/s]\u001b[A\n",
            " 42% 848/2015 [05:05<07:00,  2.78it/s]\u001b[A\n",
            " 42% 849/2015 [05:05<06:59,  2.78it/s]\u001b[A\n",
            " 42% 850/2015 [05:05<06:59,  2.78it/s]\u001b[A\n",
            " 42% 851/2015 [05:06<06:59,  2.78it/s]\u001b[A\n",
            " 42% 852/2015 [05:06<06:58,  2.78it/s]\u001b[A\n",
            " 42% 853/2015 [05:06<06:58,  2.78it/s]\u001b[A\n",
            " 42% 854/2015 [05:07<06:57,  2.78it/s]\u001b[A\n",
            " 42% 855/2015 [05:07<06:57,  2.78it/s]\u001b[A\n",
            " 42% 856/2015 [05:08<06:57,  2.78it/s]\u001b[A\n",
            " 43% 857/2015 [05:08<06:56,  2.78it/s]\u001b[A\n",
            " 43% 858/2015 [05:08<06:56,  2.78it/s]\u001b[A\n",
            " 43% 859/2015 [05:09<06:56,  2.78it/s]\u001b[A\n",
            " 43% 860/2015 [05:09<06:55,  2.78it/s]\u001b[A\n",
            " 43% 861/2015 [05:09<06:55,  2.78it/s]\u001b[A\n",
            " 43% 862/2015 [05:10<06:55,  2.77it/s]\u001b[A\n",
            " 43% 863/2015 [05:10<06:55,  2.77it/s]\u001b[A\n",
            " 43% 864/2015 [05:10<06:55,  2.77it/s]\u001b[A\n",
            " 43% 865/2015 [05:11<06:54,  2.77it/s]\u001b[A\n",
            " 43% 866/2015 [05:11<06:54,  2.77it/s]\u001b[A\n",
            " 43% 867/2015 [05:12<06:53,  2.77it/s]\u001b[A\n",
            " 43% 868/2015 [05:12<06:53,  2.77it/s]\u001b[A\n",
            " 43% 869/2015 [05:12<06:53,  2.77it/s]\u001b[A\n",
            " 43% 870/2015 [05:13<06:52,  2.77it/s]\u001b[A\n",
            " 43% 871/2015 [05:13<06:52,  2.77it/s]\u001b[A\n",
            " 43% 872/2015 [05:13<06:52,  2.77it/s]\u001b[A\n",
            " 43% 873/2015 [05:14<06:51,  2.77it/s]\u001b[A\n",
            " 43% 874/2015 [05:14<06:51,  2.77it/s]\u001b[A\n",
            " 43% 875/2015 [05:14<06:50,  2.77it/s]\u001b[A\n",
            " 43% 876/2015 [05:15<06:50,  2.78it/s]\u001b[A\n",
            " 44% 877/2015 [05:15<06:50,  2.77it/s]\u001b[A\n",
            " 44% 878/2015 [05:15<06:49,  2.77it/s]\u001b[A\n",
            " 44% 879/2015 [05:16<06:49,  2.77it/s]\u001b[A\n",
            " 44% 880/2015 [05:16<06:49,  2.77it/s]\u001b[A\n",
            " 44% 881/2015 [05:17<06:48,  2.77it/s]\u001b[A\n",
            " 44% 882/2015 [05:17<06:48,  2.78it/s]\u001b[A\n",
            " 44% 883/2015 [05:17<06:47,  2.78it/s]\u001b[A\n",
            " 44% 884/2015 [05:18<06:47,  2.78it/s]\u001b[A\n",
            " 44% 885/2015 [05:18<06:47,  2.78it/s]\u001b[A\n",
            " 44% 886/2015 [05:18<06:46,  2.78it/s]\u001b[A\n",
            " 44% 887/2015 [05:19<06:46,  2.78it/s]\u001b[A\n",
            " 44% 888/2015 [05:19<06:46,  2.78it/s]\u001b[A\n",
            " 44% 889/2015 [05:19<06:45,  2.78it/s]\u001b[A\n",
            " 44% 890/2015 [05:20<06:45,  2.78it/s]\u001b[A\n",
            " 44% 891/2015 [05:20<06:44,  2.78it/s]\u001b[A\n",
            " 44% 892/2015 [05:21<06:44,  2.78it/s]\u001b[A\n",
            " 44% 893/2015 [05:21<06:44,  2.78it/s]\u001b[A\n",
            " 44% 894/2015 [05:21<06:43,  2.78it/s]\u001b[A\n",
            " 44% 895/2015 [05:22<06:43,  2.78it/s]\u001b[A\n",
            " 44% 896/2015 [05:22<06:43,  2.78it/s]\u001b[A\n",
            " 45% 897/2015 [05:22<06:42,  2.78it/s]\u001b[A\n",
            " 45% 898/2015 [05:23<06:42,  2.78it/s]\u001b[A\n",
            " 45% 899/2015 [05:23<06:42,  2.78it/s]\u001b[A\n",
            " 45% 900/2015 [05:23<06:41,  2.78it/s]\u001b[A\n",
            " 45% 901/2015 [05:24<06:41,  2.78it/s]\u001b[A\n",
            " 45% 902/2015 [05:24<06:40,  2.78it/s]\u001b[A\n",
            " 45% 903/2015 [05:25<06:40,  2.78it/s]\u001b[A\n",
            " 45% 904/2015 [05:25<06:40,  2.78it/s]\u001b[A\n",
            " 45% 905/2015 [05:25<06:40,  2.77it/s]\u001b[A\n",
            " 45% 906/2015 [05:26<06:39,  2.77it/s]\u001b[A\n",
            " 45% 907/2015 [05:26<06:39,  2.77it/s]\u001b[A\n",
            " 45% 908/2015 [05:26<06:39,  2.77it/s]\u001b[A\n",
            " 45% 909/2015 [05:27<06:38,  2.77it/s]\u001b[A\n",
            " 45% 910/2015 [05:27<06:38,  2.77it/s]\u001b[A\n",
            " 45% 911/2015 [05:27<06:37,  2.77it/s]\u001b[A\n",
            " 45% 912/2015 [05:28<06:37,  2.77it/s]\u001b[A\n",
            " 45% 913/2015 [05:28<06:37,  2.78it/s]\u001b[A\n",
            " 45% 914/2015 [05:28<06:36,  2.78it/s]\u001b[A\n",
            " 45% 915/2015 [05:29<06:36,  2.78it/s]\u001b[A\n",
            " 45% 916/2015 [05:29<06:36,  2.78it/s]\u001b[A\n",
            " 46% 917/2015 [05:30<06:35,  2.78it/s]\u001b[A\n",
            " 46% 918/2015 [05:30<06:35,  2.77it/s]\u001b[A\n",
            " 46% 919/2015 [05:30<06:35,  2.77it/s]\u001b[A\n",
            " 46% 920/2015 [05:31<06:34,  2.77it/s]\u001b[A\n",
            " 46% 921/2015 [05:31<06:34,  2.77it/s]\u001b[A\n",
            " 46% 922/2015 [05:31<06:33,  2.77it/s]\u001b[A\n",
            " 46% 923/2015 [05:32<06:33,  2.77it/s]\u001b[A\n",
            " 46% 924/2015 [05:32<06:33,  2.77it/s]\u001b[A\n",
            " 46% 925/2015 [05:32<06:32,  2.77it/s]\u001b[A\n",
            " 46% 926/2015 [05:33<06:32,  2.77it/s]\u001b[A\n",
            " 46% 927/2015 [05:33<06:32,  2.77it/s]\u001b[A\n",
            " 46% 928/2015 [05:34<06:31,  2.77it/s]\u001b[A\n",
            " 46% 929/2015 [05:34<06:31,  2.77it/s]\u001b[A\n",
            " 46% 930/2015 [05:34<06:31,  2.77it/s]\u001b[A\n",
            " 46% 931/2015 [05:35<06:30,  2.77it/s]\u001b[A\n",
            " 46% 932/2015 [05:35<06:30,  2.77it/s]\u001b[A\n",
            " 46% 933/2015 [05:35<06:30,  2.77it/s]\u001b[A\n",
            " 46% 934/2015 [05:36<06:29,  2.77it/s]\u001b[A\n",
            " 46% 935/2015 [05:36<06:29,  2.77it/s]\u001b[A\n",
            " 46% 936/2015 [05:36<06:28,  2.78it/s]\u001b[A\n",
            " 47% 937/2015 [05:37<06:28,  2.77it/s]\u001b[A\n",
            " 47% 938/2015 [05:37<06:28,  2.77it/s]\u001b[A\n",
            " 47% 939/2015 [05:37<06:27,  2.77it/s]\u001b[A\n",
            " 47% 940/2015 [05:38<06:27,  2.77it/s]\u001b[A\n",
            " 47% 941/2015 [05:38<06:27,  2.78it/s]\u001b[A\n",
            " 47% 942/2015 [05:39<06:26,  2.78it/s]\u001b[A\n",
            " 47% 943/2015 [05:39<06:26,  2.78it/s]\u001b[A\n",
            " 47% 944/2015 [05:39<06:25,  2.78it/s]\u001b[A\n",
            " 47% 945/2015 [05:40<06:25,  2.78it/s]\u001b[A\n",
            " 47% 946/2015 [05:40<06:25,  2.78it/s]\u001b[A\n",
            " 47% 947/2015 [05:40<06:24,  2.78it/s]\u001b[A\n",
            " 47% 948/2015 [05:41<06:24,  2.78it/s]\u001b[A\n",
            " 47% 949/2015 [05:41<06:23,  2.78it/s]\u001b[A\n",
            " 47% 950/2015 [05:41<06:23,  2.78it/s]\u001b[A\n",
            " 47% 951/2015 [05:42<06:23,  2.78it/s]\u001b[A\n",
            " 47% 952/2015 [05:42<06:22,  2.78it/s]\u001b[A\n",
            " 47% 953/2015 [05:43<06:22,  2.78it/s]\u001b[A\n",
            " 47% 954/2015 [05:43<06:22,  2.78it/s]\u001b[A\n",
            " 47% 955/2015 [05:43<06:21,  2.78it/s]\u001b[A\n",
            " 47% 956/2015 [05:44<06:21,  2.78it/s]\u001b[A\n",
            " 47% 957/2015 [05:44<06:20,  2.78it/s]\u001b[A\n",
            " 48% 958/2015 [05:44<06:20,  2.78it/s]\u001b[A\n",
            " 48% 959/2015 [05:45<06:20,  2.78it/s]\u001b[A\n",
            " 48% 960/2015 [05:45<06:20,  2.78it/s]\u001b[A\n",
            " 48% 961/2015 [05:45<06:19,  2.78it/s]\u001b[A\n",
            " 48% 962/2015 [05:46<06:19,  2.78it/s]\u001b[A\n",
            " 48% 963/2015 [05:46<06:18,  2.78it/s]\u001b[A\n",
            " 48% 964/2015 [05:46<06:18,  2.78it/s]\u001b[A\n",
            " 48% 965/2015 [05:47<06:18,  2.78it/s]\u001b[A\n",
            " 48% 966/2015 [05:47<06:17,  2.78it/s]\u001b[A\n",
            " 48% 967/2015 [05:48<06:17,  2.78it/s]\u001b[A\n",
            " 48% 968/2015 [05:48<06:17,  2.78it/s]\u001b[A\n",
            " 48% 969/2015 [05:48<06:16,  2.78it/s]\u001b[A\n",
            " 48% 970/2015 [05:49<06:16,  2.78it/s]\u001b[A\n",
            " 48% 971/2015 [05:49<06:15,  2.78it/s]\u001b[A\n",
            " 48% 972/2015 [05:49<06:15,  2.78it/s]\u001b[A\n",
            " 48% 973/2015 [05:50<06:15,  2.78it/s]\u001b[A\n",
            " 48% 974/2015 [05:50<06:15,  2.78it/s]\u001b[A\n",
            " 48% 975/2015 [05:50<06:14,  2.78it/s]\u001b[A\n",
            " 48% 976/2015 [05:51<06:14,  2.78it/s]\u001b[A\n",
            " 48% 977/2015 [05:51<06:13,  2.78it/s]\u001b[A\n",
            " 49% 978/2015 [05:52<06:13,  2.78it/s]\u001b[A\n",
            " 49% 979/2015 [05:52<06:13,  2.78it/s]\u001b[A\n",
            " 49% 980/2015 [05:52<06:12,  2.78it/s]\u001b[A\n",
            " 49% 981/2015 [05:53<06:12,  2.78it/s]\u001b[A\n",
            " 49% 982/2015 [05:53<06:12,  2.78it/s]\u001b[A\n",
            " 49% 983/2015 [05:53<06:11,  2.78it/s]\u001b[A\n",
            " 49% 984/2015 [05:54<06:11,  2.78it/s]\u001b[A\n",
            " 49% 985/2015 [05:54<06:11,  2.78it/s]\u001b[A\n",
            " 49% 986/2015 [05:54<06:10,  2.78it/s]\u001b[A\n",
            " 49% 987/2015 [05:55<06:10,  2.78it/s]\u001b[A\n",
            " 49% 988/2015 [05:55<06:09,  2.78it/s]\u001b[A\n",
            " 49% 989/2015 [05:55<06:09,  2.78it/s]\u001b[A\n",
            " 49% 990/2015 [05:56<06:09,  2.78it/s]\u001b[A\n",
            " 49% 991/2015 [05:56<06:08,  2.78it/s]\u001b[A\n",
            " 49% 992/2015 [05:57<06:08,  2.78it/s]\u001b[A\n",
            " 49% 993/2015 [05:57<06:07,  2.78it/s]\u001b[A\n",
            " 49% 994/2015 [05:57<06:07,  2.78it/s]\u001b[A\n",
            " 49% 995/2015 [05:58<06:07,  2.78it/s]\u001b[A\n",
            " 49% 996/2015 [05:58<06:06,  2.78it/s]\u001b[A\n",
            " 49% 997/2015 [05:58<06:06,  2.78it/s]\u001b[A\n",
            " 50% 998/2015 [05:59<06:06,  2.78it/s]\u001b[A\n",
            " 50% 999/2015 [05:59<06:05,  2.78it/s]\u001b[A\n",
            " 50% 1000/2015 [05:59<06:05,  2.78it/s]\u001b[A\n",
            " 50% 1001/2015 [06:00<06:05,  2.78it/s]\u001b[A\n",
            " 50% 1002/2015 [06:00<06:04,  2.78it/s]\u001b[A\n",
            " 50% 1003/2015 [06:01<06:04,  2.78it/s]\u001b[A\n",
            " 50% 1004/2015 [06:01<06:03,  2.78it/s]\u001b[A\n",
            " 50% 1005/2015 [06:01<06:03,  2.78it/s]\u001b[A\n",
            " 50% 1006/2015 [06:02<06:03,  2.78it/s]\u001b[A\n",
            " 50% 1007/2015 [06:02<06:02,  2.78it/s]\u001b[A\n",
            " 50% 1008/2015 [06:02<06:02,  2.78it/s]\u001b[A\n",
            " 50% 1009/2015 [06:03<06:02,  2.78it/s]\u001b[A\n",
            " 50% 1010/2015 [06:03<06:01,  2.78it/s]\u001b[A\n",
            " 50% 1011/2015 [06:03<06:01,  2.78it/s]\u001b[A\n",
            " 50% 1012/2015 [06:04<06:01,  2.78it/s]\u001b[A\n",
            " 50% 1013/2015 [06:04<06:00,  2.78it/s]\u001b[A\n",
            " 50% 1014/2015 [06:04<06:00,  2.78it/s]\u001b[A\n",
            " 50% 1015/2015 [06:05<06:00,  2.78it/s]\u001b[A\n",
            " 50% 1016/2015 [06:05<05:59,  2.78it/s]\u001b[A\n",
            " 50% 1017/2015 [06:06<05:59,  2.78it/s]\u001b[A\n",
            " 51% 1018/2015 [06:06<05:59,  2.78it/s]\u001b[A\n",
            " 51% 1019/2015 [06:06<05:58,  2.78it/s]\u001b[A\n",
            " 51% 1020/2015 [06:07<05:58,  2.78it/s]\u001b[A\n",
            " 51% 1021/2015 [06:07<05:58,  2.78it/s]\u001b[A\n",
            " 51% 1022/2015 [06:07<05:57,  2.78it/s]\u001b[A\n",
            " 51% 1023/2015 [06:08<05:57,  2.78it/s]\u001b[A\n",
            " 51% 1024/2015 [06:08<05:57,  2.78it/s]\u001b[A\n",
            " 51% 1025/2015 [06:08<05:56,  2.78it/s]\u001b[A\n",
            " 51% 1026/2015 [06:09<05:56,  2.78it/s]\u001b[A\n",
            " 51% 1027/2015 [06:09<05:55,  2.78it/s]\u001b[A\n",
            " 51% 1028/2015 [06:10<05:55,  2.78it/s]\u001b[A\n",
            " 51% 1029/2015 [06:10<05:55,  2.78it/s]\u001b[A\n",
            " 51% 1030/2015 [06:10<05:54,  2.78it/s]\u001b[A\n",
            " 51% 1031/2015 [06:11<05:54,  2.78it/s]\u001b[A\n",
            " 51% 1032/2015 [06:11<05:54,  2.78it/s]\u001b[A\n",
            " 51% 1033/2015 [06:11<05:53,  2.78it/s]\u001b[A\n",
            " 51% 1034/2015 [06:12<05:53,  2.78it/s]\u001b[A\n",
            " 51% 1035/2015 [06:12<05:52,  2.78it/s]\u001b[A\n",
            " 51% 1036/2015 [06:12<05:52,  2.78it/s]\u001b[A\n",
            " 51% 1037/2015 [06:13<05:52,  2.78it/s]\u001b[A\n",
            " 52% 1038/2015 [06:13<05:51,  2.78it/s]\u001b[A\n",
            " 52% 1039/2015 [06:13<05:51,  2.78it/s]\u001b[A\n",
            " 52% 1040/2015 [06:14<05:51,  2.78it/s]\u001b[A\n",
            " 52% 1041/2015 [06:14<05:50,  2.78it/s]\u001b[A\n",
            " 52% 1042/2015 [06:15<05:50,  2.78it/s]\u001b[A\n",
            " 52% 1043/2015 [06:15<05:50,  2.78it/s]\u001b[A\n",
            " 52% 1044/2015 [06:15<05:49,  2.78it/s]\u001b[A\n",
            " 52% 1045/2015 [06:16<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1046/2015 [06:16<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1047/2015 [06:16<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1048/2015 [06:17<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1049/2015 [06:17<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1050/2015 [06:17<05:47,  2.77it/s]\u001b[A\n",
            " 52% 1051/2015 [06:18<05:47,  2.77it/s]\u001b[A\n",
            " 52% 1052/2015 [06:18<05:47,  2.77it/s]\u001b[A\n",
            " 52% 1053/2015 [06:19<05:46,  2.78it/s]\u001b[A\n",
            " 52% 1054/2015 [06:19<05:46,  2.78it/s]\u001b[A\n",
            " 52% 1055/2015 [06:19<05:45,  2.78it/s]\u001b[A\n",
            " 52% 1056/2015 [06:20<05:45,  2.77it/s]\u001b[A\n",
            " 52% 1057/2015 [06:20<05:45,  2.78it/s]\u001b[A\n",
            " 53% 1058/2015 [06:20<05:44,  2.78it/s]\u001b[A\n",
            " 53% 1059/2015 [06:21<05:44,  2.78it/s]\u001b[A\n",
            " 53% 1060/2015 [06:21<05:44,  2.78it/s]\u001b[A\n",
            " 53% 1061/2015 [06:21<05:43,  2.78it/s]\u001b[A\n",
            " 53% 1062/2015 [06:22<05:43,  2.78it/s]\u001b[A\n",
            " 53% 1063/2015 [06:22<05:42,  2.78it/s]\u001b[A\n",
            " 53% 1064/2015 [06:23<05:42,  2.78it/s]\u001b[A\n",
            " 53% 1065/2015 [06:23<05:42,  2.78it/s]\u001b[A\n",
            " 53% 1066/2015 [06:23<05:41,  2.78it/s]\u001b[A\n",
            " 53% 1067/2015 [06:24<05:41,  2.78it/s]\u001b[A\n",
            " 53% 1068/2015 [06:24<05:41,  2.78it/s]\u001b[A\n",
            " 53% 1069/2015 [06:24<05:40,  2.78it/s]\u001b[A\n",
            " 53% 1070/2015 [06:25<05:40,  2.78it/s]\u001b[A\n",
            " 53% 1071/2015 [06:25<05:40,  2.78it/s]\u001b[A\n",
            " 53% 1072/2015 [06:25<05:39,  2.78it/s]\u001b[A\n",
            " 53% 1073/2015 [06:26<05:39,  2.78it/s]\u001b[A\n",
            " 53% 1074/2015 [06:26<05:39,  2.77it/s]\u001b[A\n",
            " 53% 1075/2015 [06:26<05:38,  2.77it/s]\u001b[A\n",
            " 53% 1076/2015 [06:27<05:38,  2.77it/s]\u001b[A\n",
            " 53% 1077/2015 [06:27<05:38,  2.77it/s]\u001b[A\n",
            " 53% 1078/2015 [06:28<05:37,  2.77it/s]\u001b[A\n",
            " 54% 1079/2015 [06:28<05:37,  2.77it/s]\u001b[A\n",
            " 54% 1080/2015 [06:28<05:36,  2.77it/s]\u001b[A\n",
            " 54% 1081/2015 [06:29<05:36,  2.78it/s]\u001b[A\n",
            " 54% 1082/2015 [06:29<05:36,  2.78it/s]\u001b[A\n",
            " 54% 1083/2015 [06:29<05:35,  2.78it/s]\u001b[A\n",
            " 54% 1084/2015 [06:30<05:35,  2.78it/s]\u001b[A\n",
            " 54% 1085/2015 [06:30<05:35,  2.78it/s]\u001b[A\n",
            " 54% 1086/2015 [06:30<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1087/2015 [06:31<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1088/2015 [06:31<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1089/2015 [06:32<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1090/2015 [06:32<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1091/2015 [06:32<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1092/2015 [06:33<05:32,  2.77it/s]\u001b[A\n",
            " 54% 1093/2015 [06:33<05:32,  2.78it/s]\u001b[A\n",
            " 54% 1094/2015 [06:33<05:31,  2.78it/s]\u001b[A\n",
            " 54% 1095/2015 [06:34<05:31,  2.78it/s]\u001b[A\n",
            " 54% 1096/2015 [06:34<05:31,  2.77it/s]\u001b[A\n",
            " 54% 1097/2015 [06:34<05:30,  2.78it/s]\u001b[A\n",
            " 54% 1098/2015 [06:35<05:30,  2.78it/s]\u001b[A\n",
            " 55% 1099/2015 [06:35<05:30,  2.78it/s]\u001b[A\n",
            " 55% 1100/2015 [06:35<05:29,  2.77it/s]\u001b[A\n",
            " 55% 1101/2015 [06:36<05:29,  2.78it/s]\u001b[A\n",
            " 55% 1102/2015 [06:36<05:28,  2.78it/s]\u001b[A\n",
            " 55% 1103/2015 [06:37<05:28,  2.78it/s]\u001b[A\n",
            " 55% 1104/2015 [06:37<05:28,  2.78it/s]\u001b[A\n",
            " 55% 1105/2015 [06:37<05:27,  2.78it/s]\u001b[A\n",
            " 55% 1106/2015 [06:38<05:27,  2.78it/s]\u001b[A\n",
            " 55% 1107/2015 [06:38<05:27,  2.78it/s]\u001b[A\n",
            " 55% 1108/2015 [06:38<05:26,  2.78it/s]\u001b[A\n",
            " 55% 1109/2015 [06:39<05:26,  2.78it/s]\u001b[A\n",
            " 55% 1110/2015 [06:39<05:26,  2.78it/s]\u001b[A\n",
            " 55% 1111/2015 [06:39<05:25,  2.77it/s]\u001b[A\n",
            " 55% 1112/2015 [06:40<05:25,  2.77it/s]\u001b[A\n",
            " 55% 1113/2015 [06:40<05:24,  2.78it/s]\u001b[A\n",
            " 55% 1114/2015 [06:41<05:24,  2.78it/s]\u001b[A\n",
            " 55% 1115/2015 [06:41<05:24,  2.78it/s]\u001b[A\n",
            " 55% 1116/2015 [06:41<05:23,  2.78it/s]\u001b[A\n",
            " 55% 1117/2015 [06:42<05:23,  2.78it/s]\u001b[A\n",
            " 55% 1118/2015 [06:42<05:23,  2.78it/s]\u001b[A\n",
            " 56% 1119/2015 [06:42<05:22,  2.78it/s]\u001b[A\n",
            " 56% 1120/2015 [06:43<05:22,  2.78it/s]\u001b[A\n",
            " 56% 1121/2015 [06:43<05:21,  2.78it/s]\u001b[A\n",
            " 56% 1122/2015 [06:43<05:21,  2.78it/s]\u001b[A\n",
            " 56% 1123/2015 [06:44<05:21,  2.78it/s]\u001b[A\n",
            " 56% 1124/2015 [06:44<05:20,  2.78it/s]\u001b[A\n",
            " 56% 1125/2015 [06:44<05:20,  2.78it/s]\u001b[A\n",
            " 56% 1126/2015 [06:45<05:20,  2.78it/s]\u001b[A\n",
            " 56% 1127/2015 [06:45<05:19,  2.78it/s]\u001b[A\n",
            " 56% 1128/2015 [06:46<05:19,  2.78it/s]\u001b[A\n",
            " 56% 1129/2015 [06:46<05:19,  2.78it/s]\u001b[A\n",
            " 56% 1130/2015 [06:46<05:18,  2.78it/s]\u001b[A\n",
            " 56% 1131/2015 [06:47<05:18,  2.78it/s]\u001b[A\n",
            " 56% 1132/2015 [06:47<05:17,  2.78it/s]\u001b[A\n",
            " 56% 1133/2015 [06:47<05:17,  2.78it/s]\u001b[A\n",
            " 56% 1134/2015 [06:48<05:17,  2.78it/s]\u001b[A\n",
            " 56% 1135/2015 [06:48<05:17,  2.78it/s]\u001b[A\n",
            " 56% 1136/2015 [06:48<05:16,  2.78it/s]\u001b[A\n",
            " 56% 1137/2015 [06:49<05:16,  2.78it/s]\u001b[A\n",
            " 56% 1138/2015 [06:49<05:16,  2.77it/s]\u001b[A\n",
            " 57% 1139/2015 [06:50<05:15,  2.77it/s]\u001b[A\n",
            " 57% 1140/2015 [06:50<05:15,  2.77it/s]\u001b[A\n",
            " 57% 1141/2015 [06:50<05:14,  2.78it/s]\u001b[A\n",
            " 57% 1142/2015 [06:51<05:14,  2.78it/s]\u001b[A\n",
            " 57% 1143/2015 [06:51<05:14,  2.78it/s]\u001b[A\n",
            " 57% 1144/2015 [06:51<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1145/2015 [06:52<05:13,  2.78it/s]\u001b[A\n",
            " 57% 1146/2015 [06:52<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1147/2015 [06:52<05:12,  2.77it/s]\u001b[A\n",
            " 57% 1148/2015 [06:53<05:12,  2.77it/s]\u001b[A\n",
            " 57% 1149/2015 [06:53<05:12,  2.77it/s]\u001b[A\n",
            " 57% 1150/2015 [06:53<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1151/2015 [06:54<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1152/2015 [06:54<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1153/2015 [06:55<05:10,  2.77it/s]\u001b[A\n",
            " 57% 1154/2015 [06:55<05:10,  2.78it/s]\u001b[A\n",
            " 57% 1155/2015 [06:55<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1156/2015 [06:56<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1157/2015 [06:56<05:09,  2.78it/s]\u001b[A\n",
            " 57% 1158/2015 [06:56<05:08,  2.78it/s]\u001b[A\n",
            " 58% 1159/2015 [06:57<05:08,  2.78it/s]\u001b[A\n",
            " 58% 1160/2015 [06:57<05:07,  2.78it/s]\u001b[A\n",
            " 58% 1161/2015 [06:57<05:07,  2.78it/s]\u001b[A\n",
            " 58% 1162/2015 [06:58<05:07,  2.78it/s]\u001b[A\n",
            " 58% 1163/2015 [06:58<05:06,  2.78it/s]\u001b[A\n",
            " 58% 1164/2015 [06:59<05:06,  2.78it/s]\u001b[A\n",
            " 58% 1165/2015 [06:59<05:06,  2.78it/s]\u001b[A\n",
            " 58% 1166/2015 [06:59<05:05,  2.78it/s]\u001b[A\n",
            " 58% 1167/2015 [07:00<05:05,  2.78it/s]\u001b[A\n",
            " 58% 1168/2015 [07:00<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1169/2015 [07:00<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1170/2015 [07:01<05:04,  2.78it/s]\u001b[A\n",
            " 58% 1171/2015 [07:01<05:04,  2.78it/s]\u001b[A\n",
            " 58% 1172/2015 [07:01<05:03,  2.78it/s]\u001b[A\n",
            " 58% 1173/2015 [07:02<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1174/2015 [07:02<05:03,  2.78it/s]\u001b[A\n",
            " 58% 1175/2015 [07:02<05:02,  2.77it/s]\u001b[A\n",
            " 58% 1176/2015 [07:03<05:02,  2.78it/s]\u001b[A\n",
            " 58% 1177/2015 [07:03<05:01,  2.78it/s]\u001b[A\n",
            " 58% 1178/2015 [07:04<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1179/2015 [07:04<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1180/2015 [07:04<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1181/2015 [07:05<05:00,  2.78it/s]\u001b[A\n",
            " 59% 1182/2015 [07:05<05:00,  2.78it/s]\u001b[A\n",
            " 59% 1183/2015 [07:05<04:59,  2.78it/s]\u001b[A\n",
            " 59% 1184/2015 [07:06<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1185/2015 [07:06<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1186/2015 [07:06<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1187/2015 [07:07<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1188/2015 [07:07<04:58,  2.78it/s]\u001b[A\n",
            " 59% 1189/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1190/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1191/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1192/2015 [07:09<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1193/2015 [07:09<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1194/2015 [07:09<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1195/2015 [07:10<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1196/2015 [07:10<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1197/2015 [07:10<04:54,  2.77it/s]\u001b[A\n",
            " 59% 1198/2015 [07:11<04:54,  2.78it/s]\u001b[A\n",
            " 60% 1199/2015 [07:11<04:54,  2.77it/s]\u001b[A\n",
            " 60% 1200/2015 [07:12<04:53,  2.78it/s]\u001b[A\n",
            " 60% 1201/2015 [07:12<04:53,  2.78it/s]\u001b[A\n",
            " 60% 1202/2015 [07:12<04:52,  2.78it/s]\u001b[A\n",
            " 60% 1203/2015 [07:13<04:52,  2.78it/s]\u001b[A\n",
            " 60% 1204/2015 [07:13<04:52,  2.78it/s]\u001b[A\n",
            " 60% 1205/2015 [07:13<04:51,  2.78it/s]\u001b[A\n",
            " 60% 1206/2015 [07:14<04:51,  2.78it/s]\u001b[A\n",
            " 60% 1207/2015 [07:14<04:50,  2.78it/s]\u001b[A\n",
            " 60% 1208/2015 [07:14<04:50,  2.78it/s]\u001b[A\n",
            " 60% 1209/2015 [07:15<04:50,  2.78it/s]\u001b[A\n",
            " 60% 1210/2015 [07:15<04:49,  2.78it/s]\u001b[A\n",
            " 60% 1211/2015 [07:15<04:49,  2.78it/s]\u001b[A\n",
            " 60% 1212/2015 [07:16<04:49,  2.78it/s]\u001b[A\n",
            " 60% 1213/2015 [07:16<04:48,  2.78it/s]\u001b[A\n",
            " 60% 1214/2015 [07:17<04:48,  2.78it/s]\u001b[A\n",
            " 60% 1215/2015 [07:17<04:48,  2.78it/s]\u001b[A\n",
            " 60% 1216/2015 [07:17<04:47,  2.78it/s]\u001b[A\n",
            " 60% 1217/2015 [07:18<04:47,  2.78it/s]\u001b[A\n",
            " 60% 1218/2015 [07:18<04:47,  2.78it/s]\u001b[A\n",
            " 60% 1219/2015 [07:18<04:46,  2.78it/s]\u001b[A\n",
            " 61% 1220/2015 [07:19<04:46,  2.78it/s]\u001b[A\n",
            " 61% 1221/2015 [07:19<04:45,  2.78it/s]\u001b[A\n",
            " 61% 1222/2015 [07:19<04:45,  2.78it/s]\u001b[A\n",
            " 61% 1223/2015 [07:20<04:45,  2.78it/s]\u001b[A\n",
            " 61% 1224/2015 [07:20<04:44,  2.78it/s]\u001b[A\n",
            " 61% 1225/2015 [07:21<04:44,  2.78it/s]\u001b[A\n",
            " 61% 1226/2015 [07:21<04:44,  2.78it/s]\u001b[A\n",
            " 61% 1227/2015 [07:21<04:43,  2.78it/s]\u001b[A\n",
            " 61% 1228/2015 [07:22<04:43,  2.78it/s]\u001b[A\n",
            " 61% 1229/2015 [07:22<04:42,  2.78it/s]\u001b[A\n",
            " 61% 1230/2015 [07:22<04:42,  2.78it/s]\u001b[A\n",
            " 61% 1231/2015 [07:23<04:42,  2.78it/s]\u001b[A\n",
            " 61% 1232/2015 [07:23<04:41,  2.78it/s]\u001b[A\n",
            " 61% 1233/2015 [07:23<04:41,  2.78it/s]\u001b[A\n",
            " 61% 1234/2015 [07:24<04:41,  2.78it/s]\u001b[A\n",
            " 61% 1235/2015 [07:24<04:40,  2.78it/s]\u001b[A\n",
            " 61% 1236/2015 [07:24<04:40,  2.78it/s]\u001b[A\n",
            " 61% 1237/2015 [07:25<04:40,  2.78it/s]\u001b[A\n",
            " 61% 1238/2015 [07:25<04:39,  2.78it/s]\u001b[A\n",
            " 61% 1239/2015 [07:26<04:39,  2.78it/s]\u001b[A\n",
            " 62% 1240/2015 [07:26<04:39,  2.78it/s]\u001b[A\n",
            " 62% 1241/2015 [07:26<04:38,  2.78it/s]\u001b[A\n",
            " 62% 1242/2015 [07:27<04:38,  2.78it/s]\u001b[A\n",
            " 62% 1243/2015 [07:27<04:37,  2.78it/s]\u001b[A\n",
            " 62% 1244/2015 [07:27<04:37,  2.78it/s]\u001b[A\n",
            " 62% 1245/2015 [07:28<04:37,  2.78it/s]\u001b[A\n",
            " 62% 1246/2015 [07:28<04:36,  2.78it/s]\u001b[A\n",
            " 62% 1247/2015 [07:28<04:36,  2.78it/s]\u001b[A\n",
            " 62% 1248/2015 [07:29<04:36,  2.78it/s]\u001b[A\n",
            " 62% 1249/2015 [07:29<04:35,  2.78it/s]\u001b[A\n",
            " 62% 1250/2015 [07:30<04:35,  2.78it/s]\u001b[A\n",
            " 62% 1251/2015 [07:30<04:35,  2.78it/s]\u001b[A\n",
            " 62% 1252/2015 [07:30<04:34,  2.78it/s]\u001b[A\n",
            " 62% 1253/2015 [07:31<04:34,  2.78it/s]\u001b[A\n",
            " 62% 1254/2015 [07:31<04:34,  2.78it/s]\u001b[A\n",
            " 62% 1255/2015 [07:31<04:33,  2.78it/s]\u001b[A\n",
            " 62% 1256/2015 [07:32<04:33,  2.78it/s]\u001b[A\n",
            " 62% 1257/2015 [07:32<04:33,  2.78it/s]\u001b[A\n",
            " 62% 1258/2015 [07:32<04:32,  2.78it/s]\u001b[A\n",
            " 62% 1259/2015 [07:33<04:32,  2.78it/s]\u001b[A\n",
            " 63% 1260/2015 [07:33<04:32,  2.78it/s]\u001b[A\n",
            " 63% 1261/2015 [07:33<04:31,  2.78it/s]\u001b[A\n",
            " 63% 1262/2015 [07:34<04:31,  2.78it/s]\u001b[A\n",
            " 63% 1263/2015 [07:34<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1264/2015 [07:35<04:30,  2.78it/s]\u001b[A\n",
            " 63% 1265/2015 [07:35<04:30,  2.78it/s]\u001b[A\n",
            " 63% 1266/2015 [07:35<04:29,  2.78it/s]\u001b[A\n",
            " 63% 1267/2015 [07:36<04:29,  2.78it/s]\u001b[A\n",
            " 63% 1268/2015 [07:36<04:29,  2.78it/s]\u001b[A\n",
            " 63% 1269/2015 [07:36<04:28,  2.78it/s]\u001b[A\n",
            " 63% 1270/2015 [07:37<04:28,  2.78it/s]\u001b[A\n",
            " 63% 1271/2015 [07:37<04:27,  2.78it/s]\u001b[A\n",
            " 63% 1272/2015 [07:37<04:27,  2.78it/s]\u001b[A\n",
            " 63% 1273/2015 [07:38<04:27,  2.78it/s]\u001b[A\n",
            " 63% 1274/2015 [07:38<04:26,  2.78it/s]\u001b[A\n",
            " 63% 1275/2015 [07:39<04:26,  2.78it/s]\u001b[A\n",
            " 63% 1276/2015 [07:39<04:26,  2.78it/s]\u001b[A\n",
            " 63% 1277/2015 [07:39<04:25,  2.78it/s]\u001b[A\n",
            " 63% 1278/2015 [07:40<04:25,  2.78it/s]\u001b[A\n",
            " 63% 1279/2015 [07:40<04:25,  2.78it/s]\u001b[A\n",
            " 64% 1280/2015 [07:40<04:24,  2.78it/s]\u001b[A\n",
            " 64% 1281/2015 [07:41<04:24,  2.78it/s]\u001b[A\n",
            " 64% 1282/2015 [07:41<04:24,  2.78it/s]\u001b[A\n",
            " 64% 1283/2015 [07:41<04:23,  2.78it/s]\u001b[A\n",
            " 64% 1284/2015 [07:42<04:23,  2.78it/s]\u001b[A\n",
            " 64% 1285/2015 [07:42<04:22,  2.78it/s]\u001b[A\n",
            " 64% 1286/2015 [07:42<04:22,  2.78it/s]\u001b[A\n",
            " 64% 1287/2015 [07:43<04:22,  2.78it/s]\u001b[A\n",
            " 64% 1288/2015 [07:43<04:21,  2.78it/s]\u001b[A\n",
            " 64% 1289/2015 [07:44<04:21,  2.78it/s]\u001b[A\n",
            " 64% 1290/2015 [07:44<04:21,  2.78it/s]\u001b[A\n",
            " 64% 1291/2015 [07:44<04:20,  2.78it/s]\u001b[A\n",
            " 64% 1292/2015 [07:45<04:20,  2.78it/s]\u001b[A\n",
            " 64% 1293/2015 [07:45<04:20,  2.78it/s]\u001b[A\n",
            " 64% 1294/2015 [07:45<04:19,  2.78it/s]\u001b[A\n",
            " 64% 1295/2015 [07:46<04:19,  2.78it/s]\u001b[A\n",
            " 64% 1296/2015 [07:46<04:18,  2.78it/s]\u001b[A\n",
            " 64% 1297/2015 [07:46<04:18,  2.78it/s]\u001b[A\n",
            " 64% 1298/2015 [07:47<04:18,  2.78it/s]\u001b[A\n",
            " 64% 1299/2015 [07:47<04:17,  2.78it/s]\u001b[A\n",
            " 65% 1300/2015 [07:48<04:17,  2.78it/s]\u001b[A\n",
            " 65% 1301/2015 [07:48<04:17,  2.78it/s]\u001b[A\n",
            " 65% 1302/2015 [07:48<04:16,  2.78it/s]\u001b[A\n",
            " 65% 1303/2015 [07:49<04:16,  2.78it/s]\u001b[A\n",
            " 65% 1304/2015 [07:49<04:16,  2.78it/s]\u001b[A\n",
            " 65% 1305/2015 [07:49<04:15,  2.78it/s]\u001b[A\n",
            " 65% 1306/2015 [07:50<04:15,  2.78it/s]\u001b[A\n",
            " 65% 1307/2015 [07:50<04:14,  2.78it/s]\u001b[A\n",
            " 65% 1308/2015 [07:50<04:14,  2.78it/s]\u001b[A\n",
            " 65% 1309/2015 [07:51<04:14,  2.78it/s]\u001b[A\n",
            " 65% 1310/2015 [07:51<04:13,  2.78it/s]\u001b[A\n",
            " 65% 1311/2015 [07:51<04:13,  2.78it/s]\u001b[A\n",
            " 65% 1312/2015 [07:52<04:13,  2.78it/s]\u001b[A\n",
            " 65% 1313/2015 [07:52<04:12,  2.78it/s]\u001b[A\n",
            " 65% 1314/2015 [07:53<04:12,  2.78it/s]\u001b[A\n",
            " 65% 1315/2015 [07:53<04:12,  2.78it/s]\u001b[A\n",
            " 65% 1316/2015 [07:53<04:11,  2.78it/s]\u001b[A\n",
            " 65% 1317/2015 [07:54<04:11,  2.78it/s]\u001b[A\n",
            " 65% 1318/2015 [07:54<04:10,  2.78it/s]\u001b[A\n",
            " 65% 1319/2015 [07:54<04:10,  2.78it/s]\u001b[A\n",
            " 66% 1320/2015 [07:55<04:10,  2.78it/s]\u001b[A\n",
            " 66% 1321/2015 [07:55<04:09,  2.78it/s]\u001b[A\n",
            " 66% 1322/2015 [07:55<04:09,  2.78it/s]\u001b[A\n",
            " 66% 1323/2015 [07:56<04:09,  2.78it/s]\u001b[A\n",
            " 66% 1324/2015 [07:56<04:08,  2.78it/s]\u001b[A\n",
            " 66% 1325/2015 [07:57<04:08,  2.78it/s]\u001b[A\n",
            " 66% 1326/2015 [07:57<04:08,  2.78it/s]\u001b[A\n",
            " 66% 1327/2015 [07:57<04:07,  2.78it/s]\u001b[A\n",
            " 66% 1328/2015 [07:58<04:07,  2.78it/s]\u001b[A\n",
            " 66% 1329/2015 [07:58<04:07,  2.78it/s]\u001b[A\n",
            " 66% 1330/2015 [07:58<04:06,  2.78it/s]\u001b[A\n",
            " 66% 1331/2015 [07:59<04:06,  2.78it/s]\u001b[A\n",
            " 66% 1332/2015 [07:59<04:06,  2.78it/s]\u001b[A\n",
            " 66% 1333/2015 [07:59<04:05,  2.78it/s]\u001b[A\n",
            " 66% 1334/2015 [08:00<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1335/2015 [08:00<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1336/2015 [08:00<04:04,  2.78it/s]\u001b[A\n",
            " 66% 1337/2015 [08:01<04:04,  2.78it/s]\u001b[A\n",
            " 66% 1338/2015 [08:01<04:03,  2.78it/s]\u001b[A\n",
            " 66% 1339/2015 [08:02<04:03,  2.78it/s]\u001b[A\n",
            " 67% 1340/2015 [08:02<04:03,  2.78it/s]\u001b[A\n",
            " 67% 1341/2015 [08:02<04:02,  2.78it/s]\u001b[A\n",
            " 67% 1342/2015 [08:03<04:02,  2.78it/s]\u001b[A\n",
            " 67% 1343/2015 [08:03<04:02,  2.78it/s]\u001b[A\n",
            " 67% 1344/2015 [08:03<04:01,  2.78it/s]\u001b[A\n",
            " 67% 1345/2015 [08:04<04:01,  2.78it/s]\u001b[A\n",
            " 67% 1346/2015 [08:04<04:00,  2.78it/s]\u001b[A\n",
            " 67% 1347/2015 [08:04<04:00,  2.78it/s]\u001b[A\n",
            " 67% 1348/2015 [08:05<04:00,  2.78it/s]\u001b[A\n",
            " 67% 1349/2015 [08:05<03:59,  2.78it/s]\u001b[A\n",
            " 67% 1350/2015 [08:06<03:59,  2.78it/s]\u001b[A\n",
            " 67% 1351/2015 [08:06<03:59,  2.78it/s]\u001b[A\n",
            " 67% 1352/2015 [08:06<03:58,  2.78it/s]\u001b[A\n",
            " 67% 1353/2015 [08:07<03:58,  2.78it/s]\u001b[A\n",
            " 67% 1354/2015 [08:07<03:58,  2.78it/s]\u001b[A\n",
            " 67% 1355/2015 [08:07<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1356/2015 [08:08<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1357/2015 [08:08<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1358/2015 [08:08<03:56,  2.78it/s]\u001b[A\n",
            " 67% 1359/2015 [08:09<03:56,  2.78it/s]\u001b[A\n",
            " 67% 1360/2015 [08:09<03:55,  2.78it/s]\u001b[A\n",
            " 68% 1361/2015 [08:09<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1362/2015 [08:10<03:55,  2.78it/s]\u001b[A\n",
            " 68% 1363/2015 [08:10<03:54,  2.78it/s]\u001b[A\n",
            " 68% 1364/2015 [08:11<03:54,  2.78it/s]\u001b[A\n",
            " 68% 1365/2015 [08:11<03:54,  2.78it/s]\u001b[A\n",
            " 68% 1366/2015 [08:11<03:53,  2.78it/s]\u001b[A\n",
            " 68% 1367/2015 [08:12<03:53,  2.78it/s]\u001b[A\n",
            " 68% 1368/2015 [08:12<03:53,  2.78it/s]\u001b[A\n",
            " 68% 1369/2015 [08:12<03:52,  2.78it/s]\u001b[A\n",
            " 68% 1370/2015 [08:13<03:52,  2.78it/s]\u001b[A\n",
            " 68% 1371/2015 [08:13<03:51,  2.78it/s]\u001b[A\n",
            " 68% 1372/2015 [08:13<03:51,  2.78it/s]\u001b[A\n",
            " 68% 1373/2015 [08:14<03:51,  2.78it/s]\u001b[A\n",
            " 68% 1374/2015 [08:14<03:50,  2.78it/s]\u001b[A\n",
            " 68% 1375/2015 [08:15<03:50,  2.78it/s]\u001b[A\n",
            " 68% 1376/2015 [08:15<03:50,  2.78it/s]\u001b[A\n",
            " 68% 1377/2015 [08:15<03:49,  2.78it/s]\u001b[A\n",
            " 68% 1378/2015 [08:16<03:49,  2.78it/s]\u001b[A\n",
            " 68% 1379/2015 [08:16<03:49,  2.78it/s]\u001b[A\n",
            " 68% 1380/2015 [08:16<03:48,  2.78it/s]\u001b[A\n",
            " 69% 1381/2015 [08:17<03:48,  2.78it/s]\u001b[A\n",
            " 69% 1382/2015 [08:17<03:47,  2.78it/s]\u001b[A\n",
            " 69% 1383/2015 [08:17<03:47,  2.78it/s]\u001b[A\n",
            " 69% 1384/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1385/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1386/2015 [08:18<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1387/2015 [08:19<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1388/2015 [08:19<03:45,  2.78it/s]\u001b[A\n",
            " 69% 1389/2015 [08:20<03:45,  2.78it/s]\u001b[A\n",
            " 69% 1390/2015 [08:20<03:45,  2.78it/s]\u001b[A\n",
            " 69% 1391/2015 [08:20<03:44,  2.78it/s]\u001b[A\n",
            " 69% 1392/2015 [08:21<03:44,  2.78it/s]\u001b[A\n",
            " 69% 1393/2015 [08:21<03:43,  2.78it/s]\u001b[A\n",
            " 69% 1394/2015 [08:21<03:43,  2.78it/s]\u001b[A\n",
            " 69% 1395/2015 [08:22<03:43,  2.78it/s]\u001b[A\n",
            " 69% 1396/2015 [08:22<03:42,  2.78it/s]\u001b[A\n",
            " 69% 1397/2015 [08:22<03:42,  2.78it/s]\u001b[A\n",
            " 69% 1398/2015 [08:23<03:42,  2.78it/s]\u001b[A\n",
            " 69% 1399/2015 [08:23<03:41,  2.78it/s]\u001b[A\n",
            " 69% 1400/2015 [08:24<03:41,  2.78it/s]\u001b[A\n",
            " 70% 1401/2015 [08:24<03:41,  2.78it/s]\u001b[A\n",
            " 70% 1402/2015 [08:24<03:40,  2.78it/s]\u001b[A\n",
            " 70% 1403/2015 [08:25<03:40,  2.78it/s]\u001b[A\n",
            " 70% 1404/2015 [08:25<03:40,  2.78it/s]\u001b[A\n",
            " 70% 1405/2015 [08:25<03:39,  2.78it/s]\u001b[A\n",
            " 70% 1406/2015 [08:26<03:39,  2.78it/s]\u001b[A\n",
            " 70% 1407/2015 [08:26<03:38,  2.78it/s]\u001b[A\n",
            " 70% 1408/2015 [08:26<03:38,  2.78it/s]\u001b[A\n",
            " 70% 1409/2015 [08:27<03:38,  2.78it/s]\u001b[A\n",
            " 70% 1410/2015 [08:27<03:37,  2.78it/s]\u001b[A\n",
            " 70% 1411/2015 [08:28<03:37,  2.78it/s]\u001b[A\n",
            " 70% 1412/2015 [08:28<03:37,  2.78it/s]\u001b[A\n",
            " 70% 1413/2015 [08:28<03:36,  2.78it/s]\u001b[A\n",
            " 70% 1414/2015 [08:29<03:36,  2.78it/s]\u001b[A\n",
            " 70% 1415/2015 [08:29<03:36,  2.78it/s]\u001b[A\n",
            " 70% 1416/2015 [08:29<03:35,  2.78it/s]\u001b[A\n",
            " 70% 1417/2015 [08:30<03:35,  2.78it/s]\u001b[A\n",
            " 70% 1418/2015 [08:30<03:34,  2.78it/s]\u001b[A\n",
            " 70% 1419/2015 [08:30<03:34,  2.78it/s]\u001b[A\n",
            " 70% 1420/2015 [08:31<03:34,  2.78it/s]\u001b[A\n",
            " 71% 1421/2015 [08:31<03:33,  2.78it/s]\u001b[A\n",
            " 71% 1422/2015 [08:31<03:33,  2.78it/s]\u001b[A\n",
            " 71% 1423/2015 [08:32<03:33,  2.78it/s]\u001b[A\n",
            " 71% 1424/2015 [08:32<03:32,  2.78it/s]\u001b[A\n",
            " 71% 1425/2015 [08:33<03:32,  2.78it/s]\u001b[A\n",
            " 71% 1426/2015 [08:33<03:32,  2.78it/s]\u001b[A\n",
            " 71% 1427/2015 [08:33<03:31,  2.78it/s]\u001b[A\n",
            " 71% 1428/2015 [08:34<03:31,  2.78it/s]\u001b[A\n",
            " 71% 1429/2015 [08:34<03:30,  2.78it/s]\u001b[A\n",
            " 71% 1430/2015 [08:34<03:30,  2.78it/s]\u001b[A\n",
            " 71% 1431/2015 [08:35<03:30,  2.78it/s]\u001b[A\n",
            " 71% 1432/2015 [08:35<03:30,  2.78it/s]\u001b[A\n",
            " 71% 1433/2015 [08:35<03:29,  2.78it/s]\u001b[A\n",
            " 71% 1434/2015 [08:36<03:29,  2.78it/s]\u001b[A\n",
            " 71% 1435/2015 [08:36<03:28,  2.78it/s]\u001b[A\n",
            " 71% 1436/2015 [08:37<03:28,  2.78it/s]\u001b[A\n",
            " 71% 1437/2015 [08:37<03:28,  2.78it/s]\u001b[A\n",
            " 71% 1438/2015 [08:37<03:27,  2.78it/s]\u001b[A\n",
            " 71% 1439/2015 [08:38<03:27,  2.78it/s]\u001b[A\n",
            " 71% 1440/2015 [08:38<03:27,  2.78it/s]\u001b[A\n",
            " 72% 1441/2015 [08:38<03:26,  2.78it/s]\u001b[A\n",
            " 72% 1442/2015 [08:39<03:26,  2.78it/s]\u001b[A\n",
            " 72% 1443/2015 [08:39<03:26,  2.78it/s]\u001b[A\n",
            " 72% 1444/2015 [08:39<03:25,  2.78it/s]\u001b[A\n",
            " 72% 1445/2015 [08:40<03:25,  2.78it/s]\u001b[A\n",
            " 72% 1446/2015 [08:40<03:24,  2.78it/s]\u001b[A\n",
            " 72% 1447/2015 [08:40<03:24,  2.78it/s]\u001b[A\n",
            " 72% 1448/2015 [08:41<03:24,  2.78it/s]\u001b[A\n",
            " 72% 1449/2015 [08:41<03:23,  2.78it/s]\u001b[A\n",
            " 72% 1450/2015 [08:42<03:23,  2.78it/s]\u001b[A\n",
            " 72% 1451/2015 [08:42<03:23,  2.78it/s]\u001b[A\n",
            " 72% 1452/2015 [08:42<03:22,  2.78it/s]\u001b[A\n",
            " 72% 1453/2015 [08:43<03:22,  2.78it/s]\u001b[A\n",
            " 72% 1454/2015 [08:43<03:22,  2.78it/s]\u001b[A\n",
            " 72% 1455/2015 [08:43<03:21,  2.78it/s]\u001b[A\n",
            " 72% 1456/2015 [08:44<03:21,  2.78it/s]\u001b[A\n",
            " 72% 1457/2015 [08:44<03:21,  2.78it/s]\u001b[A\n",
            " 72% 1458/2015 [08:44<03:20,  2.78it/s]\u001b[A\n",
            " 72% 1459/2015 [08:45<03:20,  2.78it/s]\u001b[A\n",
            " 72% 1460/2015 [08:45<03:19,  2.78it/s]\u001b[A\n",
            " 73% 1461/2015 [08:46<03:19,  2.78it/s]\u001b[A\n",
            " 73% 1462/2015 [08:46<03:19,  2.78it/s]\u001b[A\n",
            " 73% 1463/2015 [08:46<03:18,  2.78it/s]\u001b[A\n",
            " 73% 1464/2015 [08:47<03:18,  2.78it/s]\u001b[A\n",
            " 73% 1465/2015 [08:47<03:18,  2.78it/s]\u001b[A\n",
            " 73% 1466/2015 [08:47<03:17,  2.78it/s]\u001b[A\n",
            " 73% 1467/2015 [08:48<03:17,  2.78it/s]\u001b[A\n",
            " 73% 1468/2015 [08:48<03:17,  2.78it/s]\u001b[A\n",
            " 73% 1469/2015 [08:48<03:16,  2.78it/s]\u001b[A\n",
            " 73% 1470/2015 [08:49<03:16,  2.78it/s]\u001b[A\n",
            " 73% 1471/2015 [08:49<03:15,  2.78it/s]\u001b[A\n",
            " 73% 1472/2015 [08:49<03:15,  2.78it/s]\u001b[A\n",
            " 73% 1473/2015 [08:50<03:15,  2.78it/s]\u001b[A\n",
            " 73% 1474/2015 [08:50<03:14,  2.78it/s]\u001b[A\n",
            " 73% 1475/2015 [08:51<03:14,  2.78it/s]\u001b[A\n",
            " 73% 1476/2015 [08:51<03:14,  2.78it/s]\u001b[A\n",
            " 73% 1477/2015 [08:51<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1478/2015 [08:52<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1479/2015 [08:52<03:13,  2.78it/s]\u001b[A\n",
            " 73% 1480/2015 [08:52<03:12,  2.78it/s]\u001b[A\n",
            " 73% 1481/2015 [08:53<03:12,  2.78it/s]\u001b[A\n",
            " 74% 1482/2015 [08:53<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1483/2015 [08:53<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1484/2015 [08:54<03:11,  2.78it/s]\u001b[A\n",
            " 74% 1485/2015 [08:54<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1486/2015 [08:55<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1487/2015 [08:55<03:10,  2.78it/s]\u001b[A\n",
            " 74% 1488/2015 [08:55<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1489/2015 [08:56<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1490/2015 [08:56<03:09,  2.78it/s]\u001b[A\n",
            " 74% 1491/2015 [08:56<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1492/2015 [08:57<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1493/2015 [08:57<03:08,  2.78it/s]\u001b[A\n",
            " 74% 1494/2015 [08:57<03:07,  2.78it/s]\u001b[A\n",
            " 74% 1495/2015 [08:58<03:07,  2.78it/s]\u001b[A\n",
            " 74% 1496/2015 [08:58<03:06,  2.78it/s]\u001b[A\n",
            " 74% 1497/2015 [08:58<03:06,  2.78it/s]\u001b[A\n",
            " 74% 1498/2015 [08:59<03:06,  2.78it/s]\u001b[A\n",
            " 74% 1499/2015 [08:59<03:05,  2.78it/s]\u001b[A\n",
            " 74% 1500/2015 [09:00<03:05,  2.78it/s]\u001b[A\n",
            " 74% 1501/2015 [09:00<03:05,  2.78it/s]\u001b[A\n",
            " 75% 1502/2015 [09:00<03:04,  2.78it/s]\u001b[A\n",
            " 75% 1503/2015 [09:01<03:04,  2.78it/s]\u001b[A\n",
            " 75% 1504/2015 [09:01<03:04,  2.78it/s]\u001b[A\n",
            " 75% 1505/2015 [09:01<03:03,  2.78it/s]\u001b[A\n",
            " 75% 1506/2015 [09:02<03:03,  2.78it/s]\u001b[A\n",
            " 75% 1507/2015 [09:02<03:02,  2.78it/s]\u001b[A\n",
            " 75% 1508/2015 [09:02<03:02,  2.78it/s]\u001b[A\n",
            " 75% 1509/2015 [09:03<03:02,  2.78it/s]\u001b[A\n",
            " 75% 1510/2015 [09:03<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1511/2015 [09:04<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1512/2015 [09:04<03:01,  2.78it/s]\u001b[A\n",
            " 75% 1513/2015 [09:04<03:00,  2.77it/s]\u001b[A\n",
            " 75% 1514/2015 [09:05<03:00,  2.78it/s]\u001b[A\n",
            " 75% 1515/2015 [09:05<03:00,  2.78it/s]\u001b[A\n",
            " 75% 1516/2015 [09:05<02:59,  2.78it/s]\u001b[A\n",
            " 75% 1517/2015 [09:06<02:59,  2.78it/s]\u001b[A\n",
            " 75% 1518/2015 [09:06<02:59,  2.78it/s]\u001b[A\n",
            " 75% 1519/2015 [09:06<02:58,  2.78it/s]\u001b[A\n",
            " 75% 1520/2015 [09:07<02:58,  2.78it/s]\u001b[A\n",
            " 75% 1521/2015 [09:07<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1522/2015 [09:07<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1523/2015 [09:08<02:57,  2.78it/s]\u001b[A\n",
            " 76% 1524/2015 [09:08<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1525/2015 [09:09<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1526/2015 [09:09<02:56,  2.78it/s]\u001b[A\n",
            " 76% 1527/2015 [09:09<02:55,  2.78it/s]\u001b[A\n",
            " 76% 1528/2015 [09:10<02:55,  2.78it/s]\u001b[A\n",
            " 76% 1529/2015 [09:10<02:55,  2.78it/s]\u001b[A\n",
            " 76% 1530/2015 [09:10<02:54,  2.78it/s]\u001b[A\n",
            " 76% 1531/2015 [09:11<02:54,  2.78it/s]\u001b[A\n",
            " 76% 1532/2015 [09:11<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1533/2015 [09:11<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1534/2015 [09:12<02:53,  2.78it/s]\u001b[A\n",
            " 76% 1535/2015 [09:12<02:53,  2.77it/s]\u001b[A\n",
            " 76% 1536/2015 [09:13<02:52,  2.77it/s]\u001b[A\n",
            " 76% 1537/2015 [09:13<02:52,  2.77it/s]\u001b[A\n",
            " 76% 1538/2015 [09:13<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1539/2015 [09:14<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1540/2015 [09:14<02:51,  2.78it/s]\u001b[A\n",
            " 76% 1541/2015 [09:14<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1542/2015 [09:15<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1543/2015 [09:15<02:50,  2.78it/s]\u001b[A\n",
            " 77% 1544/2015 [09:15<02:49,  2.77it/s]\u001b[A\n",
            " 77% 1545/2015 [09:16<02:49,  2.78it/s]\u001b[A\n",
            " 77% 1546/2015 [09:16<02:49,  2.77it/s]\u001b[A\n",
            " 77% 1547/2015 [09:16<02:48,  2.78it/s]\u001b[A\n",
            " 77% 1548/2015 [09:17<02:48,  2.78it/s]\u001b[A\n",
            " 77% 1549/2015 [09:17<02:47,  2.77it/s]\u001b[A\n",
            " 77% 1550/2015 [09:18<02:47,  2.78it/s]\u001b[A\n",
            " 77% 1551/2015 [09:18<02:47,  2.78it/s]\u001b[A\n",
            " 77% 1552/2015 [09:18<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1553/2015 [09:19<02:46,  2.78it/s]\u001b[A\n",
            " 77% 1554/2015 [09:19<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1555/2015 [09:19<02:45,  2.77it/s]\u001b[A\n",
            " 77% 1556/2015 [09:20<02:45,  2.78it/s]\u001b[A\n",
            " 77% 1557/2015 [09:20<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1558/2015 [09:20<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1559/2015 [09:21<02:44,  2.78it/s]\u001b[A\n",
            " 77% 1560/2015 [09:21<02:43,  2.78it/s]\u001b[A\n",
            " 77% 1561/2015 [09:22<02:43,  2.78it/s]\u001b[A\n",
            " 78% 1562/2015 [09:22<02:43,  2.78it/s]\u001b[A\n",
            " 78% 1563/2015 [09:22<02:42,  2.78it/s]\u001b[A\n",
            " 78% 1564/2015 [09:23<02:42,  2.78it/s]\u001b[A\n",
            " 78% 1565/2015 [09:23<02:42,  2.78it/s]\u001b[A\n",
            " 78% 1566/2015 [09:23<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1567/2015 [09:24<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1568/2015 [09:24<02:41,  2.78it/s]\u001b[A\n",
            " 78% 1569/2015 [09:24<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1570/2015 [09:25<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1571/2015 [09:25<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1572/2015 [09:25<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1573/2015 [09:26<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1574/2015 [09:26<02:38,  2.78it/s]\u001b[A\n",
            " 78% 1575/2015 [09:27<02:38,  2.78it/s]\u001b[A\n",
            " 78% 1576/2015 [09:27<02:38,  2.78it/s]\u001b[A\n",
            " 78% 1577/2015 [09:27<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1578/2015 [09:28<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1579/2015 [09:28<02:37,  2.78it/s]\u001b[A\n",
            " 78% 1580/2015 [09:28<02:36,  2.78it/s]\u001b[A\n",
            " 78% 1581/2015 [09:29<02:36,  2.78it/s]\u001b[A\n",
            " 79% 1582/2015 [09:29<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1583/2015 [09:29<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1584/2015 [09:30<02:35,  2.78it/s]\u001b[A\n",
            " 79% 1585/2015 [09:30<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1586/2015 [09:31<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1587/2015 [09:31<02:34,  2.78it/s]\u001b[A\n",
            " 79% 1588/2015 [09:31<02:33,  2.78it/s]\u001b[A\n",
            " 79% 1589/2015 [09:32<02:33,  2.78it/s]\u001b[A\n",
            " 79% 1590/2015 [09:32<02:33,  2.78it/s]\u001b[A\n",
            " 79% 1591/2015 [09:32<02:32,  2.78it/s]\u001b[A\n",
            " 79% 1592/2015 [09:33<02:32,  2.78it/s]\u001b[A\n",
            " 79% 1593/2015 [09:33<02:31,  2.78it/s]\u001b[A\n",
            " 79% 1594/2015 [09:33<02:31,  2.78it/s]\u001b[A\n",
            " 79% 1595/2015 [09:34<02:31,  2.78it/s]\u001b[A\n",
            " 79% 1596/2015 [09:34<02:30,  2.78it/s]\u001b[A\n",
            " 79% 1597/2015 [09:34<02:30,  2.78it/s]\u001b[A\n",
            " 79% 1598/2015 [09:35<02:30,  2.78it/s]\u001b[A\n",
            " 79% 1599/2015 [09:35<02:29,  2.78it/s]\u001b[A\n",
            " 79% 1600/2015 [09:36<02:29,  2.78it/s]\u001b[A\n",
            " 79% 1601/2015 [09:36<02:29,  2.78it/s]\u001b[A\n",
            " 80% 1602/2015 [09:36<02:28,  2.78it/s]\u001b[A\n",
            " 80% 1603/2015 [09:37<02:28,  2.78it/s]\u001b[A\n",
            " 80% 1604/2015 [09:37<02:28,  2.78it/s]\u001b[A\n",
            " 80% 1605/2015 [09:37<02:27,  2.78it/s]\u001b[A\n",
            " 80% 1606/2015 [09:38<02:27,  2.78it/s]\u001b[A\n",
            " 80% 1607/2015 [09:38<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1608/2015 [09:38<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1609/2015 [09:39<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1610/2015 [09:39<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1611/2015 [09:40<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1612/2015 [09:40<02:25,  2.78it/s]\u001b[A\n",
            " 80% 1613/2015 [09:40<02:24,  2.78it/s]\u001b[A\n",
            " 80% 1614/2015 [09:41<02:24,  2.78it/s]\u001b[A\n",
            " 80% 1615/2015 [09:41<02:24,  2.78it/s]\u001b[A\n",
            " 80% 1616/2015 [09:41<02:23,  2.78it/s]\u001b[A\n",
            " 80% 1617/2015 [09:42<02:23,  2.78it/s]\u001b[A\n",
            " 80% 1618/2015 [09:42<02:22,  2.78it/s]\u001b[A\n",
            " 80% 1619/2015 [09:42<02:22,  2.78it/s]\u001b[A\n",
            " 80% 1620/2015 [09:43<02:22,  2.78it/s]\u001b[A\n",
            " 80% 1621/2015 [09:43<02:21,  2.78it/s]\u001b[A\n",
            " 80% 1622/2015 [09:43<02:21,  2.78it/s]\u001b[A\n",
            " 81% 1623/2015 [09:44<02:21,  2.78it/s]\u001b[A\n",
            " 81% 1624/2015 [09:44<02:20,  2.78it/s]\u001b[A\n",
            " 81% 1625/2015 [09:45<02:20,  2.78it/s]\u001b[A\n",
            " 81% 1626/2015 [09:45<02:20,  2.78it/s]\u001b[A\n",
            " 81% 1627/2015 [09:45<02:19,  2.78it/s]\u001b[A\n",
            " 81% 1628/2015 [09:46<02:19,  2.78it/s]\u001b[A\n",
            " 81% 1629/2015 [09:46<02:19,  2.78it/s]\u001b[A\n",
            " 81% 1630/2015 [09:46<02:18,  2.78it/s]\u001b[A\n",
            " 81% 1631/2015 [09:47<02:18,  2.78it/s]\u001b[A\n",
            " 81% 1632/2015 [09:47<02:17,  2.78it/s]\u001b[A\n",
            " 81% 1633/2015 [09:47<02:17,  2.78it/s]\u001b[A\n",
            " 81% 1634/2015 [09:48<02:17,  2.78it/s]\u001b[A\n",
            " 81% 1635/2015 [09:48<02:16,  2.78it/s]\u001b[A\n",
            " 81% 1636/2015 [09:49<02:16,  2.78it/s]\u001b[A\n",
            " 81% 1637/2015 [09:49<02:16,  2.78it/s]\u001b[A\n",
            " 81% 1638/2015 [09:49<02:15,  2.78it/s]\u001b[A\n",
            " 81% 1639/2015 [09:50<02:15,  2.78it/s]\u001b[A\n",
            " 81% 1640/2015 [09:50<02:15,  2.78it/s]\u001b[A\n",
            " 81% 1641/2015 [09:50<02:14,  2.78it/s]\u001b[A\n",
            " 81% 1642/2015 [09:51<02:14,  2.78it/s]\u001b[A\n",
            " 82% 1643/2015 [09:51<02:14,  2.78it/s]\u001b[A\n",
            " 82% 1644/2015 [09:51<02:13,  2.78it/s]\u001b[A\n",
            " 82% 1645/2015 [09:52<02:13,  2.78it/s]\u001b[A\n",
            " 82% 1646/2015 [09:52<02:12,  2.78it/s]\u001b[A\n",
            " 82% 1647/2015 [09:53<02:12,  2.78it/s]\u001b[A\n",
            " 82% 1648/2015 [09:53<02:12,  2.78it/s]\u001b[A\n",
            " 82% 1649/2015 [09:53<02:11,  2.78it/s]\u001b[A\n",
            " 82% 1650/2015 [09:54<02:11,  2.78it/s]\u001b[A\n",
            " 82% 1651/2015 [09:54<02:11,  2.78it/s]\u001b[A\n",
            " 82% 1652/2015 [09:54<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1653/2015 [09:55<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1654/2015 [09:55<02:10,  2.78it/s]\u001b[A\n",
            " 82% 1655/2015 [09:55<02:09,  2.78it/s]\u001b[A\n",
            " 82% 1656/2015 [09:56<02:09,  2.78it/s]\u001b[A\n",
            " 82% 1657/2015 [09:56<02:08,  2.78it/s]\u001b[A\n",
            " 82% 1658/2015 [09:56<02:08,  2.78it/s]\u001b[A\n",
            " 82% 1659/2015 [09:57<02:08,  2.78it/s]\u001b[A\n",
            " 82% 1660/2015 [09:57<02:07,  2.78it/s]\u001b[A\n",
            " 82% 1661/2015 [09:58<02:07,  2.78it/s]\u001b[A\n",
            " 82% 1662/2015 [09:58<02:07,  2.78it/s]\u001b[A\n",
            " 83% 1663/2015 [09:58<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1664/2015 [09:59<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1665/2015 [09:59<02:06,  2.78it/s]\u001b[A\n",
            " 83% 1666/2015 [09:59<02:05,  2.78it/s]\u001b[A\n",
            " 83% 1667/2015 [10:00<02:05,  2.78it/s]\u001b[A\n",
            " 83% 1668/2015 [10:00<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1669/2015 [10:00<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1670/2015 [10:01<02:04,  2.78it/s]\u001b[A\n",
            " 83% 1671/2015 [10:01<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1672/2015 [10:02<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1673/2015 [10:02<02:03,  2.78it/s]\u001b[A\n",
            " 83% 1674/2015 [10:02<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1675/2015 [10:03<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1676/2015 [10:03<02:02,  2.78it/s]\u001b[A\n",
            " 83% 1677/2015 [10:03<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1678/2015 [10:04<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1679/2015 [10:04<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1680/2015 [10:04<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1681/2015 [10:05<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1682/2015 [10:05<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1683/2015 [10:05<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1684/2015 [10:06<01:59,  2.78it/s]\u001b[A\n",
            " 84% 1685/2015 [10:06<01:58,  2.78it/s]\u001b[A\n",
            " 84% 1686/2015 [10:07<01:58,  2.78it/s]\u001b[A\n",
            " 84% 1687/2015 [10:07<01:58,  2.77it/s]\u001b[A\n",
            " 84% 1688/2015 [10:07<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1689/2015 [10:08<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1690/2015 [10:08<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1691/2015 [10:08<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1692/2015 [10:09<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1693/2015 [10:09<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1694/2015 [10:09<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1695/2015 [10:10<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1696/2015 [10:10<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1697/2015 [10:11<01:54,  2.77it/s]\u001b[A\n",
            " 84% 1698/2015 [10:11<01:54,  2.77it/s]\u001b[A\n",
            " 84% 1699/2015 [10:11<01:53,  2.77it/s]\u001b[A\n",
            " 84% 1700/2015 [10:12<01:53,  2.77it/s]\u001b[A\n",
            " 84% 1701/2015 [10:12<01:53,  2.77it/s]\u001b[A\n",
            " 84% 1702/2015 [10:12<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1703/2015 [10:13<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1704/2015 [10:13<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1705/2015 [10:13<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1706/2015 [10:14<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1707/2015 [10:14<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1708/2015 [10:14<01:50,  2.77it/s]\u001b[A\n",
            " 85% 1709/2015 [10:15<01:50,  2.77it/s]\u001b[A\n",
            " 85% 1710/2015 [10:15<01:49,  2.77it/s]\u001b[A\n",
            " 85% 1711/2015 [10:16<01:49,  2.77it/s]\u001b[A\n",
            " 85% 1712/2015 [10:16<01:49,  2.78it/s]\u001b[A\n",
            " 85% 1713/2015 [10:16<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1714/2015 [10:17<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1715/2015 [10:17<01:48,  2.78it/s]\u001b[A\n",
            " 85% 1716/2015 [10:17<01:47,  2.78it/s]\u001b[A\n",
            " 85% 1717/2015 [10:18<01:47,  2.78it/s]\u001b[A\n",
            " 85% 1718/2015 [10:18<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1719/2015 [10:18<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1720/2015 [10:19<01:46,  2.78it/s]\u001b[A\n",
            " 85% 1721/2015 [10:19<01:45,  2.77it/s]\u001b[A\n",
            " 85% 1722/2015 [10:20<01:45,  2.77it/s]\u001b[A\n",
            " 86% 1723/2015 [10:20<01:45,  2.77it/s]\u001b[A\n",
            " 86% 1724/2015 [10:20<01:44,  2.77it/s]\u001b[A\n",
            " 86% 1725/2015 [10:21<01:44,  2.77it/s]\u001b[A\n",
            " 86% 1726/2015 [10:21<01:44,  2.77it/s]\u001b[A\n",
            " 86% 1727/2015 [10:21<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1728/2015 [10:22<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1729/2015 [10:22<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1730/2015 [10:22<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1731/2015 [10:23<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1732/2015 [10:23<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1733/2015 [10:23<01:41,  2.77it/s]\u001b[A\n",
            " 86% 1734/2015 [10:24<01:41,  2.77it/s]\u001b[A\n",
            " 86% 1735/2015 [10:24<01:40,  2.77it/s]\u001b[A\n",
            " 86% 1736/2015 [10:25<01:40,  2.77it/s]\u001b[A\n",
            " 86% 1737/2015 [10:25<01:40,  2.77it/s]\u001b[A\n",
            " 86% 1738/2015 [10:25<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1739/2015 [10:26<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1740/2015 [10:26<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1741/2015 [10:26<01:38,  2.78it/s]\u001b[A\n",
            " 86% 1742/2015 [10:27<01:38,  2.77it/s]\u001b[A\n",
            " 87% 1743/2015 [10:27<01:38,  2.77it/s]\u001b[A\n",
            " 87% 1744/2015 [10:27<01:37,  2.78it/s]\u001b[A\n",
            " 87% 1745/2015 [10:28<01:37,  2.78it/s]\u001b[A\n",
            " 87% 1746/2015 [10:28<01:36,  2.78it/s]\u001b[A\n",
            " 87% 1747/2015 [10:29<01:36,  2.78it/s]\u001b[A\n",
            " 87% 1748/2015 [10:29<01:36,  2.77it/s]\u001b[A\n",
            " 87% 1749/2015 [10:29<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1750/2015 [10:30<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1751/2015 [10:30<01:35,  2.78it/s]\u001b[A\n",
            " 87% 1752/2015 [10:30<01:34,  2.78it/s]\u001b[A\n",
            " 87% 1753/2015 [10:31<01:34,  2.78it/s]\u001b[A\n",
            " 87% 1754/2015 [10:31<01:34,  2.77it/s]\u001b[A\n",
            " 87% 1755/2015 [10:31<01:33,  2.77it/s]\u001b[A\n",
            " 87% 1756/2015 [10:32<01:33,  2.78it/s]\u001b[A\n",
            " 87% 1757/2015 [10:32<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1758/2015 [10:33<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1759/2015 [10:33<01:32,  2.78it/s]\u001b[A\n",
            " 87% 1760/2015 [10:33<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1761/2015 [10:34<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1762/2015 [10:34<01:31,  2.78it/s]\u001b[A\n",
            " 87% 1763/2015 [10:34<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1764/2015 [10:35<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1765/2015 [10:35<01:30,  2.78it/s]\u001b[A\n",
            " 88% 1766/2015 [10:35<01:29,  2.78it/s]\u001b[A\n",
            " 88% 1767/2015 [10:36<01:29,  2.78it/s]\u001b[A\n",
            " 88% 1768/2015 [10:36<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1769/2015 [10:36<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1770/2015 [10:37<01:28,  2.78it/s]\u001b[A\n",
            " 88% 1771/2015 [10:37<01:27,  2.78it/s]\u001b[A\n",
            " 88% 1772/2015 [10:38<01:27,  2.78it/s]\u001b[A\n",
            " 88% 1773/2015 [10:38<01:27,  2.77it/s]\u001b[A\n",
            " 88% 1774/2015 [10:38<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1775/2015 [10:39<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1776/2015 [10:39<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1777/2015 [10:39<01:25,  2.77it/s]\u001b[A\n",
            " 88% 1778/2015 [10:40<01:25,  2.78it/s]\u001b[A\n",
            " 88% 1779/2015 [10:40<01:25,  2.78it/s]\u001b[A\n",
            " 88% 1780/2015 [10:40<01:24,  2.78it/s]\u001b[A\n",
            " 88% 1781/2015 [10:41<01:24,  2.78it/s]\u001b[A\n",
            " 88% 1782/2015 [10:41<01:23,  2.78it/s]\u001b[A\n",
            " 88% 1783/2015 [10:42<01:23,  2.78it/s]\u001b[A\n",
            " 89% 1784/2015 [10:42<01:23,  2.78it/s]\u001b[A\n",
            " 89% 1785/2015 [10:42<01:22,  2.78it/s]\u001b[A\n",
            " 89% 1786/2015 [10:43<01:22,  2.78it/s]\u001b[A\n",
            " 89% 1787/2015 [10:43<01:22,  2.78it/s]\u001b[A\n",
            " 89% 1788/2015 [10:43<01:21,  2.78it/s]\u001b[A\n",
            " 89% 1789/2015 [10:44<01:21,  2.78it/s]\u001b[A\n",
            " 89% 1790/2015 [10:44<01:21,  2.78it/s]\u001b[A\n",
            " 89% 1791/2015 [10:44<01:20,  2.78it/s]\u001b[A\n",
            " 89% 1792/2015 [10:45<01:20,  2.78it/s]\u001b[A\n",
            " 89% 1793/2015 [10:45<01:19,  2.78it/s]\u001b[A\n",
            " 89% 1794/2015 [10:45<01:19,  2.78it/s]\u001b[A\n",
            " 89% 1795/2015 [10:46<01:19,  2.78it/s]\u001b[A\n",
            " 89% 1796/2015 [10:46<01:18,  2.78it/s]\u001b[A\n",
            " 89% 1797/2015 [10:47<01:18,  2.78it/s]\u001b[A\n",
            " 89% 1798/2015 [10:47<01:18,  2.78it/s]\u001b[A\n",
            " 89% 1799/2015 [10:47<01:17,  2.78it/s]\u001b[A\n",
            " 89% 1800/2015 [10:48<01:17,  2.78it/s]\u001b[A\n",
            " 89% 1801/2015 [10:48<01:17,  2.78it/s]\u001b[A\n",
            " 89% 1802/2015 [10:48<01:16,  2.78it/s]\u001b[A\n",
            " 89% 1803/2015 [10:49<01:16,  2.78it/s]\u001b[A\n",
            " 90% 1804/2015 [10:49<01:16,  2.78it/s]\u001b[A\n",
            " 90% 1805/2015 [10:49<01:15,  2.78it/s]\u001b[A\n",
            " 90% 1806/2015 [10:50<01:15,  2.78it/s]\u001b[A\n",
            " 90% 1807/2015 [10:50<01:14,  2.78it/s]\u001b[A\n",
            " 90% 1808/2015 [10:51<01:14,  2.78it/s]\u001b[A\n",
            " 90% 1809/2015 [10:51<01:14,  2.78it/s]\u001b[A\n",
            " 90% 1810/2015 [10:51<01:13,  2.78it/s]\u001b[A\n",
            " 90% 1811/2015 [10:52<01:13,  2.78it/s]\u001b[A\n",
            " 90% 1812/2015 [10:52<01:13,  2.78it/s]\u001b[A\n",
            " 90% 1813/2015 [10:52<01:12,  2.78it/s]\u001b[A\n",
            " 90% 1814/2015 [10:53<01:12,  2.78it/s]\u001b[A\n",
            " 90% 1815/2015 [10:53<01:11,  2.78it/s]\u001b[A\n",
            " 90% 1816/2015 [10:53<01:11,  2.78it/s]\u001b[A\n",
            " 90% 1817/2015 [10:54<01:11,  2.78it/s]\u001b[A\n",
            " 90% 1818/2015 [10:54<01:10,  2.78it/s]\u001b[A\n",
            " 90% 1819/2015 [10:54<01:10,  2.78it/s]\u001b[A\n",
            " 90% 1820/2015 [10:55<01:10,  2.78it/s]\u001b[A\n",
            " 90% 1821/2015 [10:55<01:09,  2.78it/s]\u001b[A\n",
            " 90% 1822/2015 [10:56<01:09,  2.78it/s]\u001b[A\n",
            " 90% 1823/2015 [10:56<01:09,  2.78it/s]\u001b[A\n",
            " 91% 1824/2015 [10:56<01:08,  2.78it/s]\u001b[A\n",
            " 91% 1825/2015 [10:57<01:08,  2.78it/s]\u001b[A\n",
            " 91% 1826/2015 [10:57<01:08,  2.78it/s]\u001b[A\n",
            " 91% 1827/2015 [10:57<01:07,  2.78it/s]\u001b[A\n",
            " 91% 1828/2015 [10:58<01:07,  2.78it/s]\u001b[A\n",
            " 91% 1829/2015 [10:58<01:06,  2.78it/s]\u001b[A\n",
            " 91% 1830/2015 [10:58<01:06,  2.78it/s]\u001b[A\n",
            " 91% 1831/2015 [10:59<01:06,  2.77it/s]\u001b[A\n",
            " 91% 1832/2015 [10:59<01:05,  2.78it/s]\u001b[A\n",
            " 91% 1833/2015 [11:00<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1834/2015 [11:00<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1835/2015 [11:00<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1836/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1837/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1838/2015 [11:01<01:03,  2.78it/s]\u001b[A\n",
            " 91% 1839/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1840/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1841/2015 [11:02<01:02,  2.77it/s]\u001b[A\n",
            " 91% 1842/2015 [11:03<01:02,  2.77it/s]\u001b[A\n",
            " 91% 1843/2015 [11:03<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1844/2015 [11:03<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1845/2015 [11:04<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1846/2015 [11:04<01:00,  2.77it/s]\u001b[A\n",
            " 92% 1847/2015 [11:05<01:00,  2.77it/s]\u001b[A\n",
            " 92% 1848/2015 [11:05<01:00,  2.78it/s]\u001b[A\n",
            " 92% 1849/2015 [11:05<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1850/2015 [11:06<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1851/2015 [11:06<00:59,  2.78it/s]\u001b[A\n",
            " 92% 1852/2015 [11:06<00:58,  2.78it/s]\u001b[A\n",
            " 92% 1853/2015 [11:07<00:58,  2.78it/s]\u001b[A\n",
            " 92% 1854/2015 [11:07<00:58,  2.78it/s]\u001b[A\n",
            " 92% 1855/2015 [11:07<00:57,  2.77it/s]\u001b[A\n",
            " 92% 1856/2015 [11:08<00:57,  2.77it/s]\u001b[A\n",
            " 92% 1857/2015 [11:08<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1858/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1859/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1860/2015 [11:09<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1861/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1862/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1863/2015 [11:10<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1864/2015 [11:11<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1865/2015 [11:11<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1866/2015 [11:11<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1867/2015 [11:12<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1868/2015 [11:12<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1869/2015 [11:12<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1870/2015 [11:13<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1871/2015 [11:13<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1872/2015 [11:14<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1873/2015 [11:14<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1874/2015 [11:14<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1875/2015 [11:15<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1876/2015 [11:15<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1877/2015 [11:15<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1878/2015 [11:16<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1879/2015 [11:16<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1880/2015 [11:16<00:48,  2.77it/s]\u001b[A\n",
            " 93% 1881/2015 [11:17<00:48,  2.77it/s]\u001b[A\n",
            " 93% 1882/2015 [11:17<00:47,  2.77it/s]\u001b[A\n",
            " 93% 1883/2015 [11:18<00:47,  2.77it/s]\u001b[A\n",
            " 93% 1884/2015 [11:18<00:47,  2.77it/s]\u001b[A\n",
            " 94% 1885/2015 [11:18<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1886/2015 [11:19<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1887/2015 [11:19<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1888/2015 [11:19<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1889/2015 [11:20<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1890/2015 [11:20<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1891/2015 [11:20<00:44,  2.77it/s]\u001b[A\n",
            " 94% 1892/2015 [11:21<00:44,  2.77it/s]\u001b[A\n",
            " 94% 1893/2015 [11:21<00:43,  2.77it/s]\u001b[A\n",
            " 94% 1894/2015 [11:22<00:43,  2.77it/s]\u001b[A\n",
            " 94% 1895/2015 [11:22<00:43,  2.77it/s]\u001b[A\n",
            " 94% 1896/2015 [11:22<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1897/2015 [11:23<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1898/2015 [11:23<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1899/2015 [11:23<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1900/2015 [11:24<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1901/2015 [11:24<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1902/2015 [11:24<00:40,  2.77it/s]\u001b[A\n",
            " 94% 1903/2015 [11:25<00:40,  2.77it/s]\u001b[A\n",
            " 94% 1904/2015 [11:25<00:40,  2.77it/s]\u001b[A\n",
            " 95% 1905/2015 [11:25<00:39,  2.77it/s]\u001b[A\n",
            " 95% 1906/2015 [11:26<00:39,  2.77it/s]\u001b[A\n",
            " 95% 1907/2015 [11:26<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1908/2015 [11:27<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1909/2015 [11:27<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1910/2015 [11:27<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1911/2015 [11:28<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1912/2015 [11:28<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1913/2015 [11:28<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1914/2015 [11:29<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1915/2015 [11:29<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1916/2015 [11:29<00:35,  2.77it/s]\u001b[A\n",
            " 95% 1917/2015 [11:30<00:35,  2.77it/s]\u001b[A\n",
            " 95% 1918/2015 [11:30<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1919/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1920/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1921/2015 [11:31<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1922/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1923/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1924/2015 [11:32<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1925/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1926/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1927/2015 [11:33<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1928/2015 [11:34<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1929/2015 [11:34<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1930/2015 [11:34<00:30,  2.77it/s]\u001b[A\n",
            " 96% 1931/2015 [11:35<00:30,  2.77it/s]\u001b[A\n",
            " 96% 1932/2015 [11:35<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1933/2015 [11:36<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1934/2015 [11:36<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1935/2015 [11:36<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1936/2015 [11:37<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1937/2015 [11:37<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1938/2015 [11:37<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1939/2015 [11:38<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1940/2015 [11:38<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1941/2015 [11:38<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1942/2015 [11:39<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1943/2015 [11:39<00:25,  2.77it/s]\u001b[A\n",
            " 96% 1944/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1945/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1946/2015 [11:40<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1947/2015 [11:41<00:24,  2.78it/s]\u001b[A\n",
            " 97% 1948/2015 [11:41<00:24,  2.78it/s]\u001b[A\n",
            " 97% 1949/2015 [11:41<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1950/2015 [11:42<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1951/2015 [11:42<00:23,  2.78it/s]\u001b[A\n",
            " 97% 1952/2015 [11:42<00:22,  2.78it/s]\u001b[A\n",
            " 97% 1953/2015 [11:43<00:22,  2.78it/s]\u001b[A\n",
            " 97% 1954/2015 [11:43<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1955/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1956/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1957/2015 [11:44<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1958/2015 [11:45<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1959/2015 [11:45<00:20,  2.78it/s]\u001b[A\n",
            " 97% 1960/2015 [11:45<00:19,  2.78it/s]\u001b[A\n",
            " 97% 1961/2015 [11:46<00:19,  2.77it/s]\u001b[A\n",
            " 97% 1962/2015 [11:46<00:19,  2.77it/s]\u001b[A\n",
            " 97% 1963/2015 [11:46<00:18,  2.77it/s]\u001b[A\n",
            " 97% 1964/2015 [11:47<00:18,  2.77it/s]\u001b[A\n",
            " 98% 1965/2015 [11:47<00:18,  2.77it/s]\u001b[A\n",
            " 98% 1966/2015 [11:47<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1967/2015 [11:48<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1968/2015 [11:48<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1969/2015 [11:49<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1970/2015 [11:49<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1971/2015 [11:49<00:15,  2.78it/s]\u001b[A\n",
            " 98% 1972/2015 [11:50<00:15,  2.77it/s]\u001b[A\n",
            " 98% 1973/2015 [11:50<00:15,  2.77it/s]\u001b[A\n",
            " 98% 1974/2015 [11:50<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1975/2015 [11:51<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1976/2015 [11:51<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1977/2015 [11:51<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1978/2015 [11:52<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1979/2015 [11:52<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1980/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1981/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1982/2015 [11:53<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1983/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1984/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 99% 1985/2015 [11:54<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1986/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1987/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1988/2015 [11:55<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1989/2015 [11:56<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1990/2015 [11:56<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1991/2015 [11:56<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1992/2015 [11:57<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1993/2015 [11:57<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1994/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1995/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1996/2015 [11:58<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1997/2015 [11:59<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1998/2015 [11:59<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1999/2015 [11:59<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2000/2015 [12:00<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2001/2015 [12:00<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2002/2015 [12:00<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2003/2015 [12:01<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2004/2015 [12:01<00:03,  2.77it/s]\u001b[A\n",
            "100% 2005/2015 [12:02<00:03,  2.77it/s]\u001b[A\n",
            "100% 2006/2015 [12:02<00:03,  2.77it/s]\u001b[A\n",
            "100% 2007/2015 [12:02<00:02,  2.77it/s]\u001b[A\n",
            "100% 2008/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2009/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2010/2015 [12:03<00:01,  2.77it/s]\u001b[A\n",
            "100% 2011/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2012/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2013/2015 [12:04<00:00,  2.77it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.663286566734314, 'eval_acc': 0.7764464451211439, 'eval_f1': 0.7764464451211439, 'eval_precision': 0.7764464451211439, 'eval_recall': 0.7764464451211439, 'eval_runtime': 725.7134, 'eval_samples_per_second': 41.631, 'eval_steps_per_second': 2.777, 'epoch': 39.0}\n",
            " 98% 663/680 [8:12:55<00:38,  2.24s/it]\n",
            "100% 2015/2015 [12:05<00:00,  2.77it/s]\u001b[A\n",
            "                                       \u001b[ASaving model checkpoint to results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/adapter_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/config.json\n",
            "Model weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-663/pytorch_model.bin\n",
            "100% 680/680 [8:13:27<00:00,  2.24s/it]***** Running Evaluation *****\n",
            "  Num examples = 30212\n",
            "  Batch size = 15\n",
            "\n",
            "  0% 0/2015 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 2/2015 [00:00<06:02,  5.55it/s]\u001b[A\n",
            "  0% 3/2015 [00:00<07:51,  4.27it/s]\u001b[A\n",
            "  0% 4/2015 [00:01<09:07,  3.68it/s]\u001b[A\n",
            "  0% 5/2015 [00:01<10:00,  3.35it/s]\u001b[A\n",
            "  0% 6/2015 [00:01<10:36,  3.15it/s]\u001b[A\n",
            "  0% 7/2015 [00:02<11:02,  3.03it/s]\u001b[A\n",
            "  0% 8/2015 [00:02<11:20,  2.95it/s]\u001b[A\n",
            "  0% 9/2015 [00:02<11:33,  2.89it/s]\u001b[A\n",
            "  0% 10/2015 [00:03<11:42,  2.86it/s]\u001b[A\n",
            "  1% 11/2015 [00:03<11:48,  2.83it/s]\u001b[A\n",
            "  1% 12/2015 [00:03<11:51,  2.81it/s]\u001b[A\n",
            "  1% 13/2015 [00:04<11:54,  2.80it/s]\u001b[A\n",
            "  1% 14/2015 [00:04<11:55,  2.80it/s]\u001b[A\n",
            "  1% 15/2015 [00:05<11:57,  2.79it/s]\u001b[A\n",
            "  1% 16/2015 [00:05<11:57,  2.78it/s]\u001b[A\n",
            "  1% 17/2015 [00:05<11:58,  2.78it/s]\u001b[A\n",
            "  1% 18/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 19/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 20/2015 [00:06<11:58,  2.78it/s]\u001b[A\n",
            "  1% 21/2015 [00:07<11:58,  2.78it/s]\u001b[A\n",
            "  1% 22/2015 [00:07<11:58,  2.77it/s]\u001b[A\n",
            "  1% 23/2015 [00:07<11:58,  2.77it/s]\u001b[A\n",
            "  1% 24/2015 [00:08<11:57,  2.77it/s]\u001b[A\n",
            "  1% 25/2015 [00:08<11:57,  2.77it/s]\u001b[A\n",
            "  1% 26/2015 [00:09<11:56,  2.77it/s]\u001b[A\n",
            "  1% 27/2015 [00:09<11:56,  2.77it/s]\u001b[A\n",
            "  1% 28/2015 [00:09<11:56,  2.77it/s]\u001b[A\n",
            "  1% 29/2015 [00:10<11:56,  2.77it/s]\u001b[A\n",
            "  1% 30/2015 [00:10<11:55,  2.77it/s]\u001b[A\n",
            "  2% 31/2015 [00:10<11:55,  2.77it/s]\u001b[A\n",
            "  2% 32/2015 [00:11<11:54,  2.77it/s]\u001b[A\n",
            "  2% 33/2015 [00:11<11:54,  2.78it/s]\u001b[A\n",
            "  2% 34/2015 [00:11<11:53,  2.78it/s]\u001b[A\n",
            "  2% 35/2015 [00:12<11:53,  2.77it/s]\u001b[A\n",
            "  2% 36/2015 [00:12<11:53,  2.77it/s]\u001b[A\n",
            "  2% 37/2015 [00:12<11:53,  2.77it/s]\u001b[A\n",
            "  2% 38/2015 [00:13<11:52,  2.77it/s]\u001b[A\n",
            "  2% 39/2015 [00:13<11:52,  2.77it/s]\u001b[A\n",
            "  2% 40/2015 [00:14<11:51,  2.77it/s]\u001b[A\n",
            "  2% 41/2015 [00:14<11:51,  2.78it/s]\u001b[A\n",
            "  2% 42/2015 [00:14<11:51,  2.77it/s]\u001b[A\n",
            "  2% 43/2015 [00:15<11:50,  2.78it/s]\u001b[A\n",
            "  2% 44/2015 [00:15<11:50,  2.78it/s]\u001b[A\n",
            "  2% 45/2015 [00:15<11:49,  2.78it/s]\u001b[A\n",
            "  2% 46/2015 [00:16<11:49,  2.77it/s]\u001b[A\n",
            "  2% 47/2015 [00:16<11:49,  2.77it/s]\u001b[A\n",
            "  2% 48/2015 [00:16<11:48,  2.77it/s]\u001b[A\n",
            "  2% 49/2015 [00:17<11:48,  2.78it/s]\u001b[A\n",
            "  2% 50/2015 [00:17<11:48,  2.78it/s]\u001b[A\n",
            "  3% 51/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 52/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 53/2015 [00:18<11:47,  2.77it/s]\u001b[A\n",
            "  3% 54/2015 [00:19<11:47,  2.77it/s]\u001b[A\n",
            "  3% 55/2015 [00:19<11:46,  2.77it/s]\u001b[A\n",
            "  3% 56/2015 [00:19<11:46,  2.77it/s]\u001b[A\n",
            "  3% 57/2015 [00:20<11:46,  2.77it/s]\u001b[A\n",
            "  3% 58/2015 [00:20<11:45,  2.77it/s]\u001b[A\n",
            "  3% 59/2015 [00:20<11:45,  2.77it/s]\u001b[A\n",
            "  3% 60/2015 [00:21<11:45,  2.77it/s]\u001b[A\n",
            "  3% 61/2015 [00:21<11:44,  2.77it/s]\u001b[A\n",
            "  3% 62/2015 [00:21<11:44,  2.77it/s]\u001b[A\n",
            "  3% 63/2015 [00:22<11:44,  2.77it/s]\u001b[A\n",
            "  3% 64/2015 [00:22<11:43,  2.77it/s]\u001b[A\n",
            "  3% 65/2015 [00:23<11:43,  2.77it/s]\u001b[A\n",
            "  3% 66/2015 [00:23<11:42,  2.77it/s]\u001b[A\n",
            "  3% 67/2015 [00:23<11:42,  2.77it/s]\u001b[A\n",
            "  3% 68/2015 [00:24<11:41,  2.77it/s]\u001b[A\n",
            "  3% 69/2015 [00:24<11:41,  2.77it/s]\u001b[A\n",
            "  3% 70/2015 [00:24<11:41,  2.77it/s]\u001b[A\n",
            "  4% 71/2015 [00:25<11:40,  2.77it/s]\u001b[A\n",
            "  4% 72/2015 [00:25<11:40,  2.77it/s]\u001b[A\n",
            "  4% 73/2015 [00:25<11:41,  2.77it/s]\u001b[A\n",
            "  4% 74/2015 [00:26<11:40,  2.77it/s]\u001b[A\n",
            "  4% 75/2015 [00:26<11:40,  2.77it/s]\u001b[A\n",
            "  4% 76/2015 [00:27<11:39,  2.77it/s]\u001b[A\n",
            "  4% 77/2015 [00:27<11:39,  2.77it/s]\u001b[A\n",
            "  4% 78/2015 [00:27<11:38,  2.77it/s]\u001b[A\n",
            "  4% 79/2015 [00:28<11:38,  2.77it/s]\u001b[A\n",
            "  4% 80/2015 [00:28<11:38,  2.77it/s]\u001b[A\n",
            "  4% 81/2015 [00:28<11:37,  2.77it/s]\u001b[A\n",
            "  4% 82/2015 [00:29<11:37,  2.77it/s]\u001b[A\n",
            "  4% 83/2015 [00:29<11:37,  2.77it/s]\u001b[A\n",
            "  4% 84/2015 [00:29<11:37,  2.77it/s]\u001b[A\n",
            "  4% 85/2015 [00:30<11:36,  2.77it/s]\u001b[A\n",
            "  4% 86/2015 [00:30<11:36,  2.77it/s]\u001b[A\n",
            "  4% 87/2015 [00:31<11:35,  2.77it/s]\u001b[A\n",
            "  4% 88/2015 [00:31<11:35,  2.77it/s]\u001b[A\n",
            "  4% 89/2015 [00:31<11:35,  2.77it/s]\u001b[A\n",
            "  4% 90/2015 [00:32<11:34,  2.77it/s]\u001b[A\n",
            "  5% 91/2015 [00:32<11:34,  2.77it/s]\u001b[A\n",
            "  5% 92/2015 [00:32<11:33,  2.77it/s]\u001b[A\n",
            "  5% 93/2015 [00:33<11:33,  2.77it/s]\u001b[A\n",
            "  5% 94/2015 [00:33<11:32,  2.77it/s]\u001b[A\n",
            "  5% 95/2015 [00:33<11:32,  2.77it/s]\u001b[A\n",
            "  5% 96/2015 [00:34<11:32,  2.77it/s]\u001b[A\n",
            "  5% 97/2015 [00:34<11:31,  2.77it/s]\u001b[A\n",
            "  5% 98/2015 [00:34<11:31,  2.77it/s]\u001b[A\n",
            "  5% 99/2015 [00:35<11:31,  2.77it/s]\u001b[A\n",
            "  5% 100/2015 [00:35<11:30,  2.77it/s]\u001b[A\n",
            "  5% 101/2015 [00:36<11:30,  2.77it/s]\u001b[A\n",
            "  5% 102/2015 [00:36<11:30,  2.77it/s]\u001b[A\n",
            "  5% 103/2015 [00:36<11:29,  2.77it/s]\u001b[A\n",
            "  5% 104/2015 [00:37<11:29,  2.77it/s]\u001b[A\n",
            "  5% 105/2015 [00:37<11:28,  2.77it/s]\u001b[A\n",
            "  5% 106/2015 [00:37<11:28,  2.77it/s]\u001b[A\n",
            "  5% 107/2015 [00:38<11:28,  2.77it/s]\u001b[A\n",
            "  5% 108/2015 [00:38<11:27,  2.77it/s]\u001b[A\n",
            "  5% 109/2015 [00:38<11:27,  2.77it/s]\u001b[A\n",
            "  5% 110/2015 [00:39<11:26,  2.77it/s]\u001b[A\n",
            "  6% 111/2015 [00:39<11:26,  2.77it/s]\u001b[A\n",
            "  6% 112/2015 [00:40<11:25,  2.77it/s]\u001b[A\n",
            "  6% 113/2015 [00:40<11:25,  2.78it/s]\u001b[A\n",
            "  6% 114/2015 [00:40<11:24,  2.78it/s]\u001b[A\n",
            "  6% 115/2015 [00:41<11:24,  2.78it/s]\u001b[A\n",
            "  6% 116/2015 [00:41<11:24,  2.78it/s]\u001b[A\n",
            "  6% 117/2015 [00:41<11:23,  2.78it/s]\u001b[A\n",
            "  6% 118/2015 [00:42<11:23,  2.78it/s]\u001b[A\n",
            "  6% 119/2015 [00:42<11:23,  2.77it/s]\u001b[A\n",
            "  6% 120/2015 [00:42<11:23,  2.77it/s]\u001b[A\n",
            "  6% 121/2015 [00:43<11:22,  2.77it/s]\u001b[A\n",
            "  6% 122/2015 [00:43<11:22,  2.77it/s]\u001b[A\n",
            "  6% 123/2015 [00:43<11:21,  2.78it/s]\u001b[A\n",
            "  6% 124/2015 [00:44<11:21,  2.78it/s]\u001b[A\n",
            "  6% 125/2015 [00:44<11:21,  2.77it/s]\u001b[A\n",
            "  6% 126/2015 [00:45<11:20,  2.77it/s]\u001b[A\n",
            "  6% 127/2015 [00:45<11:20,  2.77it/s]\u001b[A\n",
            "  6% 128/2015 [00:45<11:20,  2.77it/s]\u001b[A\n",
            "  6% 129/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  6% 130/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  7% 131/2015 [00:46<11:19,  2.77it/s]\u001b[A\n",
            "  7% 132/2015 [00:47<11:19,  2.77it/s]\u001b[A\n",
            "  7% 133/2015 [00:47<11:18,  2.77it/s]\u001b[A\n",
            "  7% 134/2015 [00:47<11:18,  2.77it/s]\u001b[A\n",
            "  7% 135/2015 [00:48<11:17,  2.77it/s]\u001b[A\n",
            "  7% 136/2015 [00:48<11:17,  2.77it/s]\u001b[A\n",
            "  7% 137/2015 [00:49<11:17,  2.77it/s]\u001b[A\n",
            "  7% 138/2015 [00:49<11:16,  2.77it/s]\u001b[A\n",
            "  7% 139/2015 [00:49<11:16,  2.77it/s]\u001b[A\n",
            "  7% 140/2015 [00:50<11:15,  2.78it/s]\u001b[A\n",
            "  7% 141/2015 [00:50<11:15,  2.78it/s]\u001b[A\n",
            "  7% 142/2015 [00:50<11:14,  2.78it/s]\u001b[A\n",
            "  7% 143/2015 [00:51<11:14,  2.78it/s]\u001b[A\n",
            "  7% 144/2015 [00:51<11:14,  2.78it/s]\u001b[A\n",
            "  7% 145/2015 [00:51<11:13,  2.77it/s]\u001b[A\n",
            "  7% 146/2015 [00:52<11:13,  2.77it/s]\u001b[A\n",
            "  7% 147/2015 [00:52<11:13,  2.78it/s]\u001b[A\n",
            "  7% 148/2015 [00:53<11:13,  2.77it/s]\u001b[A\n",
            "  7% 149/2015 [00:53<11:12,  2.77it/s]\u001b[A\n",
            "  7% 150/2015 [00:53<11:12,  2.77it/s]\u001b[A\n",
            "  7% 151/2015 [00:54<11:12,  2.77it/s]\u001b[A\n",
            "  8% 152/2015 [00:54<11:11,  2.77it/s]\u001b[A\n",
            "  8% 153/2015 [00:54<11:11,  2.77it/s]\u001b[A\n",
            "  8% 154/2015 [00:55<11:10,  2.77it/s]\u001b[A\n",
            "  8% 155/2015 [00:55<11:10,  2.77it/s]\u001b[A\n",
            "  8% 156/2015 [00:55<11:10,  2.77it/s]\u001b[A\n",
            "  8% 157/2015 [00:56<11:09,  2.77it/s]\u001b[A\n",
            "  8% 158/2015 [00:56<11:09,  2.77it/s]\u001b[A\n",
            "  8% 159/2015 [00:56<11:08,  2.77it/s]\u001b[A\n",
            "  8% 160/2015 [00:57<11:08,  2.77it/s]\u001b[A\n",
            "  8% 161/2015 [00:57<11:08,  2.77it/s]\u001b[A\n",
            "  8% 162/2015 [00:58<11:08,  2.77it/s]\u001b[A\n",
            "  8% 163/2015 [00:58<11:08,  2.77it/s]\u001b[A\n",
            "  8% 164/2015 [00:58<11:07,  2.77it/s]\u001b[A\n",
            "  8% 165/2015 [00:59<11:07,  2.77it/s]\u001b[A\n",
            "  8% 166/2015 [00:59<11:06,  2.77it/s]\u001b[A\n",
            "  8% 167/2015 [00:59<11:06,  2.77it/s]\u001b[A\n",
            "  8% 168/2015 [01:00<11:06,  2.77it/s]\u001b[A\n",
            "  8% 169/2015 [01:00<11:05,  2.77it/s]\u001b[A\n",
            "  8% 170/2015 [01:00<11:05,  2.77it/s]\u001b[A\n",
            "  8% 171/2015 [01:01<11:05,  2.77it/s]\u001b[A\n",
            "  9% 172/2015 [01:01<11:04,  2.77it/s]\u001b[A\n",
            "  9% 173/2015 [01:02<11:04,  2.77it/s]\u001b[A\n",
            "  9% 174/2015 [01:02<11:04,  2.77it/s]\u001b[A\n",
            "  9% 175/2015 [01:02<11:04,  2.77it/s]\u001b[A\n",
            "  9% 176/2015 [01:03<11:03,  2.77it/s]\u001b[A\n",
            "  9% 177/2015 [01:03<11:03,  2.77it/s]\u001b[A\n",
            "  9% 178/2015 [01:03<11:02,  2.77it/s]\u001b[A\n",
            "  9% 179/2015 [01:04<11:02,  2.77it/s]\u001b[A\n",
            "  9% 180/2015 [01:04<11:02,  2.77it/s]\u001b[A\n",
            "  9% 181/2015 [01:04<11:01,  2.77it/s]\u001b[A\n",
            "  9% 182/2015 [01:05<11:01,  2.77it/s]\u001b[A\n",
            "  9% 183/2015 [01:05<11:00,  2.77it/s]\u001b[A\n",
            "  9% 184/2015 [01:05<11:00,  2.77it/s]\u001b[A\n",
            "  9% 185/2015 [01:06<10:59,  2.77it/s]\u001b[A\n",
            "  9% 186/2015 [01:06<10:59,  2.77it/s]\u001b[A\n",
            "  9% 187/2015 [01:07<10:59,  2.77it/s]\u001b[A\n",
            "  9% 188/2015 [01:07<10:58,  2.77it/s]\u001b[A\n",
            "  9% 189/2015 [01:07<10:58,  2.77it/s]\u001b[A\n",
            "  9% 190/2015 [01:08<10:58,  2.77it/s]\u001b[A\n",
            "  9% 191/2015 [01:08<10:57,  2.77it/s]\u001b[A\n",
            " 10% 192/2015 [01:08<10:57,  2.77it/s]\u001b[A\n",
            " 10% 193/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 194/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 195/2015 [01:09<10:56,  2.77it/s]\u001b[A\n",
            " 10% 196/2015 [01:10<10:55,  2.77it/s]\u001b[A\n",
            " 10% 197/2015 [01:10<10:55,  2.77it/s]\u001b[A\n",
            " 10% 198/2015 [01:11<10:55,  2.77it/s]\u001b[A\n",
            " 10% 199/2015 [01:11<10:54,  2.77it/s]\u001b[A\n",
            " 10% 200/2015 [01:11<10:54,  2.77it/s]\u001b[A\n",
            " 10% 201/2015 [01:12<10:53,  2.77it/s]\u001b[A\n",
            " 10% 202/2015 [01:12<10:53,  2.77it/s]\u001b[A\n",
            " 10% 203/2015 [01:12<10:53,  2.77it/s]\u001b[A\n",
            " 10% 204/2015 [01:13<10:52,  2.77it/s]\u001b[A\n",
            " 10% 205/2015 [01:13<10:52,  2.77it/s]\u001b[A\n",
            " 10% 206/2015 [01:13<10:51,  2.78it/s]\u001b[A\n",
            " 10% 207/2015 [01:14<10:51,  2.78it/s]\u001b[A\n",
            " 10% 208/2015 [01:14<10:51,  2.77it/s]\u001b[A\n",
            " 10% 209/2015 [01:14<10:51,  2.77it/s]\u001b[A\n",
            " 10% 210/2015 [01:15<10:50,  2.77it/s]\u001b[A\n",
            " 10% 211/2015 [01:15<10:50,  2.77it/s]\u001b[A\n",
            " 11% 212/2015 [01:16<10:50,  2.77it/s]\u001b[A\n",
            " 11% 213/2015 [01:16<10:49,  2.77it/s]\u001b[A\n",
            " 11% 214/2015 [01:16<10:49,  2.77it/s]\u001b[A\n",
            " 11% 215/2015 [01:17<10:48,  2.77it/s]\u001b[A\n",
            " 11% 216/2015 [01:17<10:48,  2.78it/s]\u001b[A\n",
            " 11% 217/2015 [01:17<10:47,  2.78it/s]\u001b[A\n",
            " 11% 218/2015 [01:18<10:47,  2.78it/s]\u001b[A\n",
            " 11% 219/2015 [01:18<10:47,  2.78it/s]\u001b[A\n",
            " 11% 220/2015 [01:18<10:46,  2.78it/s]\u001b[A\n",
            " 11% 221/2015 [01:19<10:46,  2.78it/s]\u001b[A\n",
            " 11% 222/2015 [01:19<10:46,  2.78it/s]\u001b[A\n",
            " 11% 223/2015 [01:20<10:45,  2.77it/s]\u001b[A\n",
            " 11% 224/2015 [01:20<10:45,  2.77it/s]\u001b[A\n",
            " 11% 225/2015 [01:20<10:45,  2.77it/s]\u001b[A\n",
            " 11% 226/2015 [01:21<10:44,  2.77it/s]\u001b[A\n",
            " 11% 227/2015 [01:21<10:44,  2.77it/s]\u001b[A\n",
            " 11% 228/2015 [01:21<10:44,  2.77it/s]\u001b[A\n",
            " 11% 229/2015 [01:22<10:44,  2.77it/s]\u001b[A\n",
            " 11% 230/2015 [01:22<10:43,  2.77it/s]\u001b[A\n",
            " 11% 231/2015 [01:22<10:43,  2.77it/s]\u001b[A\n",
            " 12% 232/2015 [01:23<10:43,  2.77it/s]\u001b[A\n",
            " 12% 233/2015 [01:23<10:42,  2.77it/s]\u001b[A\n",
            " 12% 234/2015 [01:24<10:42,  2.77it/s]\u001b[A\n",
            " 12% 235/2015 [01:24<10:41,  2.77it/s]\u001b[A\n",
            " 12% 236/2015 [01:24<10:41,  2.77it/s]\u001b[A\n",
            " 12% 237/2015 [01:25<10:41,  2.77it/s]\u001b[A\n",
            " 12% 238/2015 [01:25<10:41,  2.77it/s]\u001b[A\n",
            " 12% 239/2015 [01:25<10:40,  2.77it/s]\u001b[A\n",
            " 12% 240/2015 [01:26<10:40,  2.77it/s]\u001b[A\n",
            " 12% 241/2015 [01:26<10:40,  2.77it/s]\u001b[A\n",
            " 12% 242/2015 [01:26<10:40,  2.77it/s]\u001b[A\n",
            " 12% 243/2015 [01:27<10:39,  2.77it/s]\u001b[A\n",
            " 12% 244/2015 [01:27<10:39,  2.77it/s]\u001b[A\n",
            " 12% 245/2015 [01:27<10:38,  2.77it/s]\u001b[A\n",
            " 12% 246/2015 [01:28<10:38,  2.77it/s]\u001b[A\n",
            " 12% 247/2015 [01:28<10:37,  2.77it/s]\u001b[A\n",
            " 12% 248/2015 [01:29<10:37,  2.77it/s]\u001b[A\n",
            " 12% 249/2015 [01:29<10:36,  2.77it/s]\u001b[A\n",
            " 12% 250/2015 [01:29<10:36,  2.77it/s]\u001b[A\n",
            " 12% 251/2015 [01:30<10:36,  2.77it/s]\u001b[A\n",
            " 13% 252/2015 [01:30<10:35,  2.77it/s]\u001b[A\n",
            " 13% 253/2015 [01:30<10:35,  2.77it/s]\u001b[A\n",
            " 13% 254/2015 [01:31<10:35,  2.77it/s]\u001b[A\n",
            " 13% 255/2015 [01:31<10:34,  2.77it/s]\u001b[A\n",
            " 13% 256/2015 [01:31<10:34,  2.77it/s]\u001b[A\n",
            " 13% 257/2015 [01:32<10:33,  2.77it/s]\u001b[A\n",
            " 13% 258/2015 [01:32<10:33,  2.77it/s]\u001b[A\n",
            " 13% 259/2015 [01:33<10:33,  2.77it/s]\u001b[A\n",
            " 13% 260/2015 [01:33<10:32,  2.77it/s]\u001b[A\n",
            " 13% 261/2015 [01:33<10:32,  2.77it/s]\u001b[A\n",
            " 13% 262/2015 [01:34<10:32,  2.77it/s]\u001b[A\n",
            " 13% 263/2015 [01:34<10:31,  2.77it/s]\u001b[A\n",
            " 13% 264/2015 [01:34<10:31,  2.77it/s]\u001b[A\n",
            " 13% 265/2015 [01:35<10:31,  2.77it/s]\u001b[A\n",
            " 13% 266/2015 [01:35<10:31,  2.77it/s]\u001b[A\n",
            " 13% 267/2015 [01:35<10:30,  2.77it/s]\u001b[A\n",
            " 13% 268/2015 [01:36<10:30,  2.77it/s]\u001b[A\n",
            " 13% 269/2015 [01:36<10:29,  2.77it/s]\u001b[A\n",
            " 13% 270/2015 [01:36<10:29,  2.77it/s]\u001b[A\n",
            " 13% 271/2015 [01:37<10:28,  2.77it/s]\u001b[A\n",
            " 13% 272/2015 [01:37<10:28,  2.77it/s]\u001b[A\n",
            " 14% 273/2015 [01:38<10:27,  2.77it/s]\u001b[A\n",
            " 14% 274/2015 [01:38<10:27,  2.77it/s]\u001b[A\n",
            " 14% 275/2015 [01:38<10:27,  2.77it/s]\u001b[A\n",
            " 14% 276/2015 [01:39<10:27,  2.77it/s]\u001b[A\n",
            " 14% 277/2015 [01:39<10:26,  2.77it/s]\u001b[A\n",
            " 14% 278/2015 [01:39<10:26,  2.77it/s]\u001b[A\n",
            " 14% 279/2015 [01:40<10:25,  2.77it/s]\u001b[A\n",
            " 14% 280/2015 [01:40<10:25,  2.77it/s]\u001b[A\n",
            " 14% 281/2015 [01:40<10:25,  2.77it/s]\u001b[A\n",
            " 14% 282/2015 [01:41<10:24,  2.77it/s]\u001b[A\n",
            " 14% 283/2015 [01:41<10:24,  2.77it/s]\u001b[A\n",
            " 14% 284/2015 [01:42<10:24,  2.77it/s]\u001b[A\n",
            " 14% 285/2015 [01:42<10:23,  2.77it/s]\u001b[A\n",
            " 14% 286/2015 [01:42<10:23,  2.77it/s]\u001b[A\n",
            " 14% 287/2015 [01:43<10:23,  2.77it/s]\u001b[A\n",
            " 14% 288/2015 [01:43<10:23,  2.77it/s]\u001b[A\n",
            " 14% 289/2015 [01:43<10:22,  2.77it/s]\u001b[A\n",
            " 14% 290/2015 [01:44<10:22,  2.77it/s]\u001b[A\n",
            " 14% 291/2015 [01:44<10:21,  2.77it/s]\u001b[A\n",
            " 14% 292/2015 [01:44<10:21,  2.77it/s]\u001b[A\n",
            " 15% 293/2015 [01:45<10:21,  2.77it/s]\u001b[A\n",
            " 15% 294/2015 [01:45<10:21,  2.77it/s]\u001b[A\n",
            " 15% 295/2015 [01:46<10:20,  2.77it/s]\u001b[A\n",
            " 15% 296/2015 [01:46<10:20,  2.77it/s]\u001b[A\n",
            " 15% 297/2015 [01:46<10:20,  2.77it/s]\u001b[A\n",
            " 15% 298/2015 [01:47<10:19,  2.77it/s]\u001b[A\n",
            " 15% 299/2015 [01:47<10:18,  2.77it/s]\u001b[A\n",
            " 15% 300/2015 [01:47<10:18,  2.77it/s]\u001b[A\n",
            " 15% 301/2015 [01:48<10:18,  2.77it/s]\u001b[A\n",
            " 15% 302/2015 [01:48<10:17,  2.77it/s]\u001b[A\n",
            " 15% 303/2015 [01:48<10:17,  2.77it/s]\u001b[A\n",
            " 15% 304/2015 [01:49<10:16,  2.77it/s]\u001b[A\n",
            " 15% 305/2015 [01:49<10:16,  2.77it/s]\u001b[A\n",
            " 15% 306/2015 [01:49<10:15,  2.77it/s]\u001b[A\n",
            " 15% 307/2015 [01:50<10:15,  2.77it/s]\u001b[A\n",
            " 15% 308/2015 [01:50<10:15,  2.77it/s]\u001b[A\n",
            " 15% 309/2015 [01:51<10:14,  2.77it/s]\u001b[A\n",
            " 15% 310/2015 [01:51<10:14,  2.77it/s]\u001b[A\n",
            " 15% 311/2015 [01:51<10:14,  2.77it/s]\u001b[A\n",
            " 15% 312/2015 [01:52<10:13,  2.77it/s]\u001b[A\n",
            " 16% 313/2015 [01:52<10:13,  2.78it/s]\u001b[A\n",
            " 16% 314/2015 [01:52<10:13,  2.77it/s]\u001b[A\n",
            " 16% 315/2015 [01:53<10:12,  2.78it/s]\u001b[A\n",
            " 16% 316/2015 [01:53<10:12,  2.77it/s]\u001b[A\n",
            " 16% 317/2015 [01:53<10:11,  2.77it/s]\u001b[A\n",
            " 16% 318/2015 [01:54<10:11,  2.77it/s]\u001b[A\n",
            " 16% 319/2015 [01:54<10:11,  2.77it/s]\u001b[A\n",
            " 16% 320/2015 [01:55<10:10,  2.77it/s]\u001b[A\n",
            " 16% 321/2015 [01:55<10:10,  2.77it/s]\u001b[A\n",
            " 16% 322/2015 [01:55<10:10,  2.78it/s]\u001b[A\n",
            " 16% 323/2015 [01:56<10:09,  2.78it/s]\u001b[A\n",
            " 16% 324/2015 [01:56<10:09,  2.78it/s]\u001b[A\n",
            " 16% 325/2015 [01:56<10:08,  2.78it/s]\u001b[A\n",
            " 16% 326/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 327/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 328/2015 [01:57<10:08,  2.77it/s]\u001b[A\n",
            " 16% 329/2015 [01:58<10:07,  2.77it/s]\u001b[A\n",
            " 16% 330/2015 [01:58<10:07,  2.77it/s]\u001b[A\n",
            " 16% 331/2015 [01:58<10:07,  2.77it/s]\u001b[A\n",
            " 16% 332/2015 [01:59<10:07,  2.77it/s]\u001b[A\n",
            " 17% 333/2015 [01:59<10:06,  2.77it/s]\u001b[A\n",
            " 17% 334/2015 [02:00<10:06,  2.77it/s]\u001b[A\n",
            " 17% 335/2015 [02:00<10:06,  2.77it/s]\u001b[A\n",
            " 17% 336/2015 [02:00<10:05,  2.77it/s]\u001b[A\n",
            " 17% 337/2015 [02:01<10:05,  2.77it/s]\u001b[A\n",
            " 17% 338/2015 [02:01<10:04,  2.77it/s]\u001b[A\n",
            " 17% 339/2015 [02:01<10:04,  2.77it/s]\u001b[A\n",
            " 17% 340/2015 [02:02<10:03,  2.77it/s]\u001b[A\n",
            " 17% 341/2015 [02:02<10:03,  2.77it/s]\u001b[A\n",
            " 17% 342/2015 [02:02<10:03,  2.77it/s]\u001b[A\n",
            " 17% 343/2015 [02:03<10:02,  2.77it/s]\u001b[A\n",
            " 17% 344/2015 [02:03<10:02,  2.77it/s]\u001b[A\n",
            " 17% 345/2015 [02:04<10:02,  2.77it/s]\u001b[A\n",
            " 17% 346/2015 [02:04<10:01,  2.77it/s]\u001b[A\n",
            " 17% 347/2015 [02:04<10:01,  2.77it/s]\u001b[A\n",
            " 17% 348/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 349/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 350/2015 [02:05<10:00,  2.77it/s]\u001b[A\n",
            " 17% 351/2015 [02:06<09:59,  2.77it/s]\u001b[A\n",
            " 17% 352/2015 [02:06<09:59,  2.78it/s]\u001b[A\n",
            " 18% 353/2015 [02:06<09:58,  2.78it/s]\u001b[A\n",
            " 18% 354/2015 [02:07<09:58,  2.78it/s]\u001b[A\n",
            " 18% 355/2015 [02:07<09:58,  2.77it/s]\u001b[A\n",
            " 18% 356/2015 [02:08<09:58,  2.77it/s]\u001b[A\n",
            " 18% 357/2015 [02:08<09:57,  2.77it/s]\u001b[A\n",
            " 18% 358/2015 [02:08<09:57,  2.77it/s]\u001b[A\n",
            " 18% 359/2015 [02:09<09:57,  2.77it/s]\u001b[A\n",
            " 18% 360/2015 [02:09<09:56,  2.77it/s]\u001b[A\n",
            " 18% 361/2015 [02:09<09:56,  2.77it/s]\u001b[A\n",
            " 18% 362/2015 [02:10<09:55,  2.78it/s]\u001b[A\n",
            " 18% 363/2015 [02:10<09:55,  2.78it/s]\u001b[A\n",
            " 18% 364/2015 [02:10<09:55,  2.77it/s]\u001b[A\n",
            " 18% 365/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 366/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 367/2015 [02:11<09:54,  2.77it/s]\u001b[A\n",
            " 18% 368/2015 [02:12<09:53,  2.77it/s]\u001b[A\n",
            " 18% 369/2015 [02:12<09:53,  2.77it/s]\u001b[A\n",
            " 18% 370/2015 [02:13<09:53,  2.77it/s]\u001b[A\n",
            " 18% 371/2015 [02:13<09:52,  2.77it/s]\u001b[A\n",
            " 18% 372/2015 [02:13<09:52,  2.77it/s]\u001b[A\n",
            " 19% 373/2015 [02:14<09:52,  2.77it/s]\u001b[A\n",
            " 19% 374/2015 [02:14<09:51,  2.77it/s]\u001b[A\n",
            " 19% 375/2015 [02:14<09:51,  2.77it/s]\u001b[A\n",
            " 19% 376/2015 [02:15<09:51,  2.77it/s]\u001b[A\n",
            " 19% 377/2015 [02:15<09:50,  2.77it/s]\u001b[A\n",
            " 19% 378/2015 [02:15<09:50,  2.77it/s]\u001b[A\n",
            " 19% 379/2015 [02:16<09:49,  2.77it/s]\u001b[A\n",
            " 19% 380/2015 [02:16<09:49,  2.77it/s]\u001b[A\n",
            " 19% 381/2015 [02:17<09:49,  2.77it/s]\u001b[A\n",
            " 19% 382/2015 [02:17<09:48,  2.77it/s]\u001b[A\n",
            " 19% 383/2015 [02:17<09:48,  2.77it/s]\u001b[A\n",
            " 19% 384/2015 [02:18<09:48,  2.77it/s]\u001b[A\n",
            " 19% 385/2015 [02:18<09:47,  2.77it/s]\u001b[A\n",
            " 19% 386/2015 [02:18<09:47,  2.77it/s]\u001b[A\n",
            " 19% 387/2015 [02:19<09:46,  2.77it/s]\u001b[A\n",
            " 19% 388/2015 [02:19<09:46,  2.77it/s]\u001b[A\n",
            " 19% 389/2015 [02:19<09:46,  2.77it/s]\u001b[A\n",
            " 19% 390/2015 [02:20<09:45,  2.77it/s]\u001b[A\n",
            " 19% 391/2015 [02:20<09:45,  2.77it/s]\u001b[A\n",
            " 19% 392/2015 [02:20<09:44,  2.77it/s]\u001b[A\n",
            " 20% 393/2015 [02:21<09:44,  2.77it/s]\u001b[A\n",
            " 20% 394/2015 [02:21<09:44,  2.78it/s]\u001b[A\n",
            " 20% 395/2015 [02:22<09:44,  2.77it/s]\u001b[A\n",
            " 20% 396/2015 [02:22<09:43,  2.77it/s]\u001b[A\n",
            " 20% 397/2015 [02:22<09:43,  2.77it/s]\u001b[A\n",
            " 20% 398/2015 [02:23<09:42,  2.77it/s]\u001b[A\n",
            " 20% 399/2015 [02:23<09:42,  2.77it/s]\u001b[A\n",
            " 20% 400/2015 [02:23<09:42,  2.77it/s]\u001b[A\n",
            " 20% 401/2015 [02:24<09:41,  2.77it/s]\u001b[A\n",
            " 20% 402/2015 [02:24<09:41,  2.77it/s]\u001b[A\n",
            " 20% 403/2015 [02:24<09:41,  2.77it/s]\u001b[A\n",
            " 20% 404/2015 [02:25<09:40,  2.77it/s]\u001b[A\n",
            " 20% 405/2015 [02:25<09:40,  2.77it/s]\u001b[A\n",
            " 20% 406/2015 [02:26<09:40,  2.77it/s]\u001b[A\n",
            " 20% 407/2015 [02:26<09:39,  2.77it/s]\u001b[A\n",
            " 20% 408/2015 [02:26<09:39,  2.77it/s]\u001b[A\n",
            " 20% 409/2015 [02:27<09:39,  2.77it/s]\u001b[A\n",
            " 20% 410/2015 [02:27<09:38,  2.77it/s]\u001b[A\n",
            " 20% 411/2015 [02:27<09:38,  2.77it/s]\u001b[A\n",
            " 20% 412/2015 [02:28<09:37,  2.77it/s]\u001b[A\n",
            " 20% 413/2015 [02:28<09:37,  2.77it/s]\u001b[A\n",
            " 21% 414/2015 [02:28<09:37,  2.77it/s]\u001b[A\n",
            " 21% 415/2015 [02:29<09:36,  2.77it/s]\u001b[A\n",
            " 21% 416/2015 [02:29<09:36,  2.77it/s]\u001b[A\n",
            " 21% 417/2015 [02:29<09:35,  2.77it/s]\u001b[A\n",
            " 21% 418/2015 [02:30<09:35,  2.77it/s]\u001b[A\n",
            " 21% 419/2015 [02:30<09:35,  2.77it/s]\u001b[A\n",
            " 21% 420/2015 [02:31<09:35,  2.77it/s]\u001b[A\n",
            " 21% 421/2015 [02:31<09:34,  2.77it/s]\u001b[A\n",
            " 21% 422/2015 [02:31<09:34,  2.77it/s]\u001b[A\n",
            " 21% 423/2015 [02:32<09:34,  2.77it/s]\u001b[A\n",
            " 21% 424/2015 [02:32<09:33,  2.77it/s]\u001b[A\n",
            " 21% 425/2015 [02:32<09:33,  2.77it/s]\u001b[A\n",
            " 21% 426/2015 [02:33<09:33,  2.77it/s]\u001b[A\n",
            " 21% 427/2015 [02:33<09:32,  2.77it/s]\u001b[A\n",
            " 21% 428/2015 [02:33<09:32,  2.77it/s]\u001b[A\n",
            " 21% 429/2015 [02:34<09:31,  2.77it/s]\u001b[A\n",
            " 21% 430/2015 [02:34<09:31,  2.77it/s]\u001b[A\n",
            " 21% 431/2015 [02:35<09:30,  2.77it/s]\u001b[A\n",
            " 21% 432/2015 [02:35<09:30,  2.77it/s]\u001b[A\n",
            " 21% 433/2015 [02:35<09:30,  2.77it/s]\u001b[A\n",
            " 22% 434/2015 [02:36<09:29,  2.77it/s]\u001b[A\n",
            " 22% 435/2015 [02:36<09:29,  2.77it/s]\u001b[A\n",
            " 22% 436/2015 [02:36<09:29,  2.77it/s]\u001b[A\n",
            " 22% 437/2015 [02:37<09:28,  2.77it/s]\u001b[A\n",
            " 22% 438/2015 [02:37<09:28,  2.77it/s]\u001b[A\n",
            " 22% 439/2015 [02:37<09:28,  2.77it/s]\u001b[A\n",
            " 22% 440/2015 [02:38<09:27,  2.77it/s]\u001b[A\n",
            " 22% 441/2015 [02:38<09:27,  2.77it/s]\u001b[A\n",
            " 22% 442/2015 [02:39<09:27,  2.77it/s]\u001b[A\n",
            " 22% 443/2015 [02:39<09:26,  2.77it/s]\u001b[A\n",
            " 22% 444/2015 [02:39<09:26,  2.77it/s]\u001b[A\n",
            " 22% 445/2015 [02:40<09:26,  2.77it/s]\u001b[A\n",
            " 22% 446/2015 [02:40<09:25,  2.77it/s]\u001b[A\n",
            " 22% 447/2015 [02:40<09:25,  2.77it/s]\u001b[A\n",
            " 22% 448/2015 [02:41<09:24,  2.77it/s]\u001b[A\n",
            " 22% 449/2015 [02:41<09:25,  2.77it/s]\u001b[A\n",
            " 22% 450/2015 [02:41<09:24,  2.77it/s]\u001b[A\n",
            " 22% 451/2015 [02:42<09:24,  2.77it/s]\u001b[A\n",
            " 22% 452/2015 [02:42<09:24,  2.77it/s]\u001b[A\n",
            " 22% 453/2015 [02:42<09:23,  2.77it/s]\u001b[A\n",
            " 23% 454/2015 [02:43<09:23,  2.77it/s]\u001b[A\n",
            " 23% 455/2015 [02:43<09:22,  2.77it/s]\u001b[A\n",
            " 23% 456/2015 [02:44<09:22,  2.77it/s]\u001b[A\n",
            " 23% 457/2015 [02:44<09:22,  2.77it/s]\u001b[A\n",
            " 23% 458/2015 [02:44<09:21,  2.77it/s]\u001b[A\n",
            " 23% 459/2015 [02:45<09:21,  2.77it/s]\u001b[A\n",
            " 23% 460/2015 [02:45<09:21,  2.77it/s]\u001b[A\n",
            " 23% 461/2015 [02:45<09:20,  2.77it/s]\u001b[A\n",
            " 23% 462/2015 [02:46<09:20,  2.77it/s]\u001b[A\n",
            " 23% 463/2015 [02:46<09:19,  2.77it/s]\u001b[A\n",
            " 23% 464/2015 [02:46<09:19,  2.77it/s]\u001b[A\n",
            " 23% 465/2015 [02:47<09:19,  2.77it/s]\u001b[A\n",
            " 23% 466/2015 [02:47<09:19,  2.77it/s]\u001b[A\n",
            " 23% 467/2015 [02:48<09:18,  2.77it/s]\u001b[A\n",
            " 23% 468/2015 [02:48<09:18,  2.77it/s]\u001b[A\n",
            " 23% 469/2015 [02:48<09:18,  2.77it/s]\u001b[A\n",
            " 23% 470/2015 [02:49<09:17,  2.77it/s]\u001b[A\n",
            " 23% 471/2015 [02:49<09:17,  2.77it/s]\u001b[A\n",
            " 23% 472/2015 [02:49<09:16,  2.77it/s]\u001b[A\n",
            " 23% 473/2015 [02:50<09:16,  2.77it/s]\u001b[A\n",
            " 24% 474/2015 [02:50<09:16,  2.77it/s]\u001b[A\n",
            " 24% 475/2015 [02:50<09:15,  2.77it/s]\u001b[A\n",
            " 24% 476/2015 [02:51<09:15,  2.77it/s]\u001b[A\n",
            " 24% 477/2015 [02:51<09:14,  2.77it/s]\u001b[A\n",
            " 24% 478/2015 [02:52<09:14,  2.77it/s]\u001b[A\n",
            " 24% 479/2015 [02:52<09:13,  2.77it/s]\u001b[A\n",
            " 24% 480/2015 [02:52<09:13,  2.78it/s]\u001b[A\n",
            " 24% 481/2015 [02:53<09:12,  2.78it/s]\u001b[A\n",
            " 24% 482/2015 [02:53<09:12,  2.78it/s]\u001b[A\n",
            " 24% 483/2015 [02:53<09:12,  2.77it/s]\u001b[A\n",
            " 24% 484/2015 [02:54<09:11,  2.77it/s]\u001b[A\n",
            " 24% 485/2015 [02:54<09:11,  2.77it/s]\u001b[A\n",
            " 24% 486/2015 [02:54<09:10,  2.77it/s]\u001b[A\n",
            " 24% 487/2015 [02:55<09:10,  2.77it/s]\u001b[A\n",
            " 24% 488/2015 [02:55<09:10,  2.77it/s]\u001b[A\n",
            " 24% 489/2015 [02:55<09:10,  2.77it/s]\u001b[A\n",
            " 24% 490/2015 [02:56<09:09,  2.77it/s]\u001b[A\n",
            " 24% 491/2015 [02:56<09:09,  2.77it/s]\u001b[A\n",
            " 24% 492/2015 [02:57<09:08,  2.78it/s]\u001b[A\n",
            " 24% 493/2015 [02:57<09:08,  2.78it/s]\u001b[A\n",
            " 25% 494/2015 [02:57<09:07,  2.78it/s]\u001b[A\n",
            " 25% 495/2015 [02:58<09:07,  2.77it/s]\u001b[A\n",
            " 25% 496/2015 [02:58<09:07,  2.77it/s]\u001b[A\n",
            " 25% 497/2015 [02:58<09:07,  2.77it/s]\u001b[A\n",
            " 25% 498/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 499/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 500/2015 [02:59<09:06,  2.77it/s]\u001b[A\n",
            " 25% 501/2015 [03:00<09:06,  2.77it/s]\u001b[A\n",
            " 25% 502/2015 [03:00<09:05,  2.77it/s]\u001b[A\n",
            " 25% 503/2015 [03:01<09:05,  2.77it/s]\u001b[A\n",
            " 25% 504/2015 [03:01<09:05,  2.77it/s]\u001b[A\n",
            " 25% 505/2015 [03:01<09:04,  2.77it/s]\u001b[A\n",
            " 25% 506/2015 [03:02<09:04,  2.77it/s]\u001b[A\n",
            " 25% 507/2015 [03:02<09:04,  2.77it/s]\u001b[A\n",
            " 25% 508/2015 [03:02<09:03,  2.77it/s]\u001b[A\n",
            " 25% 509/2015 [03:03<09:03,  2.77it/s]\u001b[A\n",
            " 25% 510/2015 [03:03<09:03,  2.77it/s]\u001b[A\n",
            " 25% 511/2015 [03:03<09:02,  2.77it/s]\u001b[A\n",
            " 25% 512/2015 [03:04<09:02,  2.77it/s]\u001b[A\n",
            " 25% 513/2015 [03:04<09:01,  2.77it/s]\u001b[A\n",
            " 26% 514/2015 [03:04<09:01,  2.77it/s]\u001b[A\n",
            " 26% 515/2015 [03:05<09:01,  2.77it/s]\u001b[A\n",
            " 26% 516/2015 [03:05<09:00,  2.77it/s]\u001b[A\n",
            " 26% 517/2015 [03:06<09:00,  2.77it/s]\u001b[A\n",
            " 26% 518/2015 [03:06<08:59,  2.77it/s]\u001b[A\n",
            " 26% 519/2015 [03:06<09:00,  2.77it/s]\u001b[A\n",
            " 26% 520/2015 [03:07<08:59,  2.77it/s]\u001b[A\n",
            " 26% 521/2015 [03:07<08:58,  2.77it/s]\u001b[A\n",
            " 26% 522/2015 [03:07<08:58,  2.77it/s]\u001b[A\n",
            " 26% 523/2015 [03:08<08:58,  2.77it/s]\u001b[A\n",
            " 26% 524/2015 [03:08<08:57,  2.77it/s]\u001b[A\n",
            " 26% 525/2015 [03:08<08:57,  2.77it/s]\u001b[A\n",
            " 26% 526/2015 [03:09<08:56,  2.77it/s]\u001b[A\n",
            " 26% 527/2015 [03:09<08:56,  2.77it/s]\u001b[A\n",
            " 26% 528/2015 [03:10<08:56,  2.77it/s]\u001b[A\n",
            " 26% 529/2015 [03:10<08:55,  2.77it/s]\u001b[A\n",
            " 26% 530/2015 [03:10<08:55,  2.77it/s]\u001b[A\n",
            " 26% 531/2015 [03:11<08:55,  2.77it/s]\u001b[A\n",
            " 26% 532/2015 [03:11<08:54,  2.77it/s]\u001b[A\n",
            " 26% 533/2015 [03:11<08:54,  2.77it/s]\u001b[A\n",
            " 27% 534/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 535/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 536/2015 [03:12<08:53,  2.77it/s]\u001b[A\n",
            " 27% 537/2015 [03:13<08:52,  2.77it/s]\u001b[A\n",
            " 27% 538/2015 [03:13<08:53,  2.77it/s]\u001b[A\n",
            " 27% 539/2015 [03:13<08:52,  2.77it/s]\u001b[A\n",
            " 27% 540/2015 [03:14<08:51,  2.77it/s]\u001b[A\n",
            " 27% 541/2015 [03:14<08:51,  2.77it/s]\u001b[A\n",
            " 27% 542/2015 [03:15<08:51,  2.77it/s]\u001b[A\n",
            " 27% 543/2015 [03:15<08:50,  2.77it/s]\u001b[A\n",
            " 27% 544/2015 [03:15<08:50,  2.77it/s]\u001b[A\n",
            " 27% 545/2015 [03:16<08:50,  2.77it/s]\u001b[A\n",
            " 27% 546/2015 [03:16<08:49,  2.77it/s]\u001b[A\n",
            " 27% 547/2015 [03:16<08:49,  2.77it/s]\u001b[A\n",
            " 27% 548/2015 [03:17<08:49,  2.77it/s]\u001b[A\n",
            " 27% 549/2015 [03:17<08:48,  2.77it/s]\u001b[A\n",
            " 27% 550/2015 [03:17<08:48,  2.77it/s]\u001b[A\n",
            " 27% 551/2015 [03:18<08:47,  2.77it/s]\u001b[A\n",
            " 27% 552/2015 [03:18<08:47,  2.77it/s]\u001b[A\n",
            " 27% 553/2015 [03:19<08:47,  2.77it/s]\u001b[A\n",
            " 27% 554/2015 [03:19<08:47,  2.77it/s]\u001b[A\n",
            " 28% 555/2015 [03:19<08:46,  2.77it/s]\u001b[A\n",
            " 28% 556/2015 [03:20<08:46,  2.77it/s]\u001b[A\n",
            " 28% 557/2015 [03:20<08:45,  2.77it/s]\u001b[A\n",
            " 28% 558/2015 [03:20<08:45,  2.77it/s]\u001b[A\n",
            " 28% 559/2015 [03:21<08:45,  2.77it/s]\u001b[A\n",
            " 28% 560/2015 [03:21<08:44,  2.77it/s]\u001b[A\n",
            " 28% 561/2015 [03:21<08:44,  2.77it/s]\u001b[A\n",
            " 28% 562/2015 [03:22<08:44,  2.77it/s]\u001b[A\n",
            " 28% 563/2015 [03:22<08:43,  2.77it/s]\u001b[A\n",
            " 28% 564/2015 [03:23<08:43,  2.77it/s]\u001b[A\n",
            " 28% 565/2015 [03:23<08:43,  2.77it/s]\u001b[A\n",
            " 28% 566/2015 [03:23<08:42,  2.77it/s]\u001b[A\n",
            " 28% 567/2015 [03:24<08:42,  2.77it/s]\u001b[A\n",
            " 28% 568/2015 [03:24<08:41,  2.77it/s]\u001b[A\n",
            " 28% 569/2015 [03:24<08:41,  2.77it/s]\u001b[A\n",
            " 28% 570/2015 [03:25<08:41,  2.77it/s]\u001b[A\n",
            " 28% 571/2015 [03:25<08:40,  2.77it/s]\u001b[A\n",
            " 28% 572/2015 [03:25<08:40,  2.77it/s]\u001b[A\n",
            " 28% 573/2015 [03:26<08:40,  2.77it/s]\u001b[A\n",
            " 28% 574/2015 [03:26<08:39,  2.77it/s]\u001b[A\n",
            " 29% 575/2015 [03:26<08:39,  2.77it/s]\u001b[A\n",
            " 29% 576/2015 [03:27<08:39,  2.77it/s]\u001b[A\n",
            " 29% 577/2015 [03:27<08:38,  2.77it/s]\u001b[A\n",
            " 29% 578/2015 [03:28<08:38,  2.77it/s]\u001b[A\n",
            " 29% 579/2015 [03:28<08:37,  2.77it/s]\u001b[A\n",
            " 29% 580/2015 [03:28<08:37,  2.77it/s]\u001b[A\n",
            " 29% 581/2015 [03:29<08:37,  2.77it/s]\u001b[A\n",
            " 29% 582/2015 [03:29<08:36,  2.77it/s]\u001b[A\n",
            " 29% 583/2015 [03:29<08:36,  2.77it/s]\u001b[A\n",
            " 29% 584/2015 [03:30<08:35,  2.77it/s]\u001b[A\n",
            " 29% 585/2015 [03:30<08:35,  2.77it/s]\u001b[A\n",
            " 29% 586/2015 [03:30<08:35,  2.77it/s]\u001b[A\n",
            " 29% 587/2015 [03:31<08:35,  2.77it/s]\u001b[A\n",
            " 29% 588/2015 [03:31<08:34,  2.77it/s]\u001b[A\n",
            " 29% 589/2015 [03:32<08:34,  2.77it/s]\u001b[A\n",
            " 29% 590/2015 [03:32<08:34,  2.77it/s]\u001b[A\n",
            " 29% 591/2015 [03:32<08:33,  2.77it/s]\u001b[A\n",
            " 29% 592/2015 [03:33<08:33,  2.77it/s]\u001b[A\n",
            " 29% 593/2015 [03:33<08:33,  2.77it/s]\u001b[A\n",
            " 29% 594/2015 [03:33<08:32,  2.77it/s]\u001b[A\n",
            " 30% 595/2015 [03:34<08:32,  2.77it/s]\u001b[A\n",
            " 30% 596/2015 [03:34<08:32,  2.77it/s]\u001b[A\n",
            " 30% 597/2015 [03:34<08:31,  2.77it/s]\u001b[A\n",
            " 30% 598/2015 [03:35<08:31,  2.77it/s]\u001b[A\n",
            " 30% 599/2015 [03:35<08:31,  2.77it/s]\u001b[A\n",
            " 30% 600/2015 [03:36<08:30,  2.77it/s]\u001b[A\n",
            " 30% 601/2015 [03:36<08:30,  2.77it/s]\u001b[A\n",
            " 30% 602/2015 [03:36<08:30,  2.77it/s]\u001b[A\n",
            " 30% 603/2015 [03:37<08:29,  2.77it/s]\u001b[A\n",
            " 30% 604/2015 [03:37<08:29,  2.77it/s]\u001b[A\n",
            " 30% 605/2015 [03:37<08:28,  2.77it/s]\u001b[A\n",
            " 30% 606/2015 [03:38<08:28,  2.77it/s]\u001b[A\n",
            " 30% 607/2015 [03:38<08:28,  2.77it/s]\u001b[A\n",
            " 30% 608/2015 [03:38<08:27,  2.77it/s]\u001b[A\n",
            " 30% 609/2015 [03:39<08:27,  2.77it/s]\u001b[A\n",
            " 30% 610/2015 [03:39<08:27,  2.77it/s]\u001b[A\n",
            " 30% 611/2015 [03:39<08:26,  2.77it/s]\u001b[A\n",
            " 30% 612/2015 [03:40<08:26,  2.77it/s]\u001b[A\n",
            " 30% 613/2015 [03:40<08:26,  2.77it/s]\u001b[A\n",
            " 30% 614/2015 [03:41<08:25,  2.77it/s]\u001b[A\n",
            " 31% 615/2015 [03:41<08:25,  2.77it/s]\u001b[A\n",
            " 31% 616/2015 [03:41<08:24,  2.77it/s]\u001b[A\n",
            " 31% 617/2015 [03:42<08:24,  2.77it/s]\u001b[A\n",
            " 31% 618/2015 [03:42<08:23,  2.77it/s]\u001b[A\n",
            " 31% 619/2015 [03:42<08:23,  2.77it/s]\u001b[A\n",
            " 31% 620/2015 [03:43<08:23,  2.77it/s]\u001b[A\n",
            " 31% 621/2015 [03:43<08:23,  2.77it/s]\u001b[A\n",
            " 31% 622/2015 [03:43<08:22,  2.77it/s]\u001b[A\n",
            " 31% 623/2015 [03:44<08:22,  2.77it/s]\u001b[A\n",
            " 31% 624/2015 [03:44<08:21,  2.77it/s]\u001b[A\n",
            " 31% 625/2015 [03:45<08:21,  2.77it/s]\u001b[A\n",
            " 31% 626/2015 [03:45<08:21,  2.77it/s]\u001b[A\n",
            " 31% 627/2015 [03:45<08:21,  2.77it/s]\u001b[A\n",
            " 31% 628/2015 [03:46<08:20,  2.77it/s]\u001b[A\n",
            " 31% 629/2015 [03:46<08:19,  2.77it/s]\u001b[A\n",
            " 31% 630/2015 [03:46<08:19,  2.77it/s]\u001b[A\n",
            " 31% 631/2015 [03:47<08:19,  2.77it/s]\u001b[A\n",
            " 31% 632/2015 [03:47<08:18,  2.77it/s]\u001b[A\n",
            " 31% 633/2015 [03:47<08:18,  2.77it/s]\u001b[A\n",
            " 31% 634/2015 [03:48<08:18,  2.77it/s]\u001b[A\n",
            " 32% 635/2015 [03:48<08:18,  2.77it/s]\u001b[A\n",
            " 32% 636/2015 [03:48<08:17,  2.77it/s]\u001b[A\n",
            " 32% 637/2015 [03:49<08:17,  2.77it/s]\u001b[A\n",
            " 32% 638/2015 [03:49<08:17,  2.77it/s]\u001b[A\n",
            " 32% 639/2015 [03:50<08:16,  2.77it/s]\u001b[A\n",
            " 32% 640/2015 [03:50<08:16,  2.77it/s]\u001b[A\n",
            " 32% 641/2015 [03:50<08:16,  2.77it/s]\u001b[A\n",
            " 32% 642/2015 [03:51<08:15,  2.77it/s]\u001b[A\n",
            " 32% 643/2015 [03:51<08:15,  2.77it/s]\u001b[A\n",
            " 32% 644/2015 [03:51<08:15,  2.77it/s]\u001b[A\n",
            " 32% 645/2015 [03:52<08:14,  2.77it/s]\u001b[A\n",
            " 32% 646/2015 [03:52<08:14,  2.77it/s]\u001b[A\n",
            " 32% 647/2015 [03:52<08:13,  2.77it/s]\u001b[A\n",
            " 32% 648/2015 [03:53<08:13,  2.77it/s]\u001b[A\n",
            " 32% 649/2015 [03:53<08:12,  2.77it/s]\u001b[A\n",
            " 32% 650/2015 [03:54<08:12,  2.77it/s]\u001b[A\n",
            " 32% 651/2015 [03:54<08:12,  2.77it/s]\u001b[A\n",
            " 32% 652/2015 [03:54<08:11,  2.77it/s]\u001b[A\n",
            " 32% 653/2015 [03:55<08:11,  2.77it/s]\u001b[A\n",
            " 32% 654/2015 [03:55<08:11,  2.77it/s]\u001b[A\n",
            " 33% 655/2015 [03:55<08:10,  2.77it/s]\u001b[A\n",
            " 33% 656/2015 [03:56<08:10,  2.77it/s]\u001b[A\n",
            " 33% 657/2015 [03:56<08:10,  2.77it/s]\u001b[A\n",
            " 33% 658/2015 [03:56<08:09,  2.77it/s]\u001b[A\n",
            " 33% 659/2015 [03:57<08:09,  2.77it/s]\u001b[A\n",
            " 33% 660/2015 [03:57<08:08,  2.77it/s]\u001b[A\n",
            " 33% 661/2015 [03:58<08:08,  2.77it/s]\u001b[A\n",
            " 33% 662/2015 [03:58<08:08,  2.77it/s]\u001b[A\n",
            " 33% 663/2015 [03:58<08:07,  2.77it/s]\u001b[A\n",
            " 33% 664/2015 [03:59<08:07,  2.77it/s]\u001b[A\n",
            " 33% 665/2015 [03:59<08:06,  2.77it/s]\u001b[A\n",
            " 33% 666/2015 [03:59<08:06,  2.77it/s]\u001b[A\n",
            " 33% 667/2015 [04:00<08:06,  2.77it/s]\u001b[A\n",
            " 33% 668/2015 [04:00<08:05,  2.77it/s]\u001b[A\n",
            " 33% 669/2015 [04:00<08:05,  2.77it/s]\u001b[A\n",
            " 33% 670/2015 [04:01<08:05,  2.77it/s]\u001b[A\n",
            " 33% 671/2015 [04:01<08:04,  2.77it/s]\u001b[A\n",
            " 33% 672/2015 [04:01<08:04,  2.77it/s]\u001b[A\n",
            " 33% 673/2015 [04:02<08:04,  2.77it/s]\u001b[A\n",
            " 33% 674/2015 [04:02<08:03,  2.77it/s]\u001b[A\n",
            " 33% 675/2015 [04:03<08:03,  2.77it/s]\u001b[A\n",
            " 34% 676/2015 [04:03<08:02,  2.77it/s]\u001b[A\n",
            " 34% 677/2015 [04:03<08:02,  2.77it/s]\u001b[A\n",
            " 34% 678/2015 [04:04<08:01,  2.77it/s]\u001b[A\n",
            " 34% 679/2015 [04:04<08:01,  2.78it/s]\u001b[A\n",
            " 34% 680/2015 [04:04<08:01,  2.77it/s]\u001b[A\n",
            " 34% 681/2015 [04:05<08:00,  2.77it/s]\u001b[A\n",
            " 34% 682/2015 [04:05<08:00,  2.77it/s]\u001b[A\n",
            " 34% 683/2015 [04:05<08:00,  2.77it/s]\u001b[A\n",
            " 34% 684/2015 [04:06<07:59,  2.77it/s]\u001b[A\n",
            " 34% 685/2015 [04:06<07:59,  2.77it/s]\u001b[A\n",
            " 34% 686/2015 [04:07<07:59,  2.77it/s]\u001b[A\n",
            " 34% 687/2015 [04:07<07:58,  2.77it/s]\u001b[A\n",
            " 34% 688/2015 [04:07<07:58,  2.77it/s]\u001b[A\n",
            " 34% 689/2015 [04:08<07:58,  2.77it/s]\u001b[A\n",
            " 34% 690/2015 [04:08<07:58,  2.77it/s]\u001b[A\n",
            " 34% 691/2015 [04:08<07:57,  2.77it/s]\u001b[A\n",
            " 34% 692/2015 [04:09<07:57,  2.77it/s]\u001b[A\n",
            " 34% 693/2015 [04:09<07:57,  2.77it/s]\u001b[A\n",
            " 34% 694/2015 [04:09<07:56,  2.77it/s]\u001b[A\n",
            " 34% 695/2015 [04:10<07:56,  2.77it/s]\u001b[A\n",
            " 35% 696/2015 [04:10<07:56,  2.77it/s]\u001b[A\n",
            " 35% 697/2015 [04:11<07:55,  2.77it/s]\u001b[A\n",
            " 35% 698/2015 [04:11<07:55,  2.77it/s]\u001b[A\n",
            " 35% 699/2015 [04:11<07:54,  2.77it/s]\u001b[A\n",
            " 35% 700/2015 [04:12<07:54,  2.77it/s]\u001b[A\n",
            " 35% 701/2015 [04:12<07:53,  2.77it/s]\u001b[A\n",
            " 35% 702/2015 [04:12<07:53,  2.77it/s]\u001b[A\n",
            " 35% 703/2015 [04:13<07:53,  2.77it/s]\u001b[A\n",
            " 35% 704/2015 [04:13<07:52,  2.77it/s]\u001b[A\n",
            " 35% 705/2015 [04:13<07:52,  2.77it/s]\u001b[A\n",
            " 35% 706/2015 [04:14<07:52,  2.77it/s]\u001b[A\n",
            " 35% 707/2015 [04:14<07:51,  2.77it/s]\u001b[A\n",
            " 35% 708/2015 [04:14<07:51,  2.77it/s]\u001b[A\n",
            " 35% 709/2015 [04:15<07:51,  2.77it/s]\u001b[A\n",
            " 35% 710/2015 [04:15<07:50,  2.77it/s]\u001b[A\n",
            " 35% 711/2015 [04:16<07:50,  2.77it/s]\u001b[A\n",
            " 35% 712/2015 [04:16<07:50,  2.77it/s]\u001b[A\n",
            " 35% 713/2015 [04:16<07:49,  2.77it/s]\u001b[A\n",
            " 35% 714/2015 [04:17<07:49,  2.77it/s]\u001b[A\n",
            " 35% 715/2015 [04:17<07:49,  2.77it/s]\u001b[A\n",
            " 36% 716/2015 [04:17<07:48,  2.77it/s]\u001b[A\n",
            " 36% 717/2015 [04:18<07:48,  2.77it/s]\u001b[A\n",
            " 36% 718/2015 [04:18<07:48,  2.77it/s]\u001b[A\n",
            " 36% 719/2015 [04:18<07:47,  2.77it/s]\u001b[A\n",
            " 36% 720/2015 [04:19<07:47,  2.77it/s]\u001b[A\n",
            " 36% 721/2015 [04:19<07:47,  2.77it/s]\u001b[A\n",
            " 36% 722/2015 [04:20<07:46,  2.77it/s]\u001b[A\n",
            " 36% 723/2015 [04:20<07:46,  2.77it/s]\u001b[A\n",
            " 36% 724/2015 [04:20<07:45,  2.77it/s]\u001b[A\n",
            " 36% 725/2015 [04:21<07:45,  2.77it/s]\u001b[A\n",
            " 36% 726/2015 [04:21<07:45,  2.77it/s]\u001b[A\n",
            " 36% 727/2015 [04:21<07:44,  2.77it/s]\u001b[A\n",
            " 36% 728/2015 [04:22<07:44,  2.77it/s]\u001b[A\n",
            " 36% 729/2015 [04:22<07:44,  2.77it/s]\u001b[A\n",
            " 36% 730/2015 [04:22<07:43,  2.77it/s]\u001b[A\n",
            " 36% 731/2015 [04:23<07:43,  2.77it/s]\u001b[A\n",
            " 36% 732/2015 [04:23<07:43,  2.77it/s]\u001b[A\n",
            " 36% 733/2015 [04:23<07:42,  2.77it/s]\u001b[A\n",
            " 36% 734/2015 [04:24<07:41,  2.77it/s]\u001b[A\n",
            " 36% 735/2015 [04:24<07:41,  2.77it/s]\u001b[A\n",
            " 37% 736/2015 [04:25<07:41,  2.77it/s]\u001b[A\n",
            " 37% 737/2015 [04:25<07:40,  2.77it/s]\u001b[A\n",
            " 37% 738/2015 [04:25<07:40,  2.77it/s]\u001b[A\n",
            " 37% 739/2015 [04:26<07:40,  2.77it/s]\u001b[A\n",
            " 37% 740/2015 [04:26<07:39,  2.77it/s]\u001b[A\n",
            " 37% 741/2015 [04:26<07:39,  2.77it/s]\u001b[A\n",
            " 37% 742/2015 [04:27<07:39,  2.77it/s]\u001b[A\n",
            " 37% 743/2015 [04:27<07:39,  2.77it/s]\u001b[A\n",
            " 37% 744/2015 [04:27<07:38,  2.77it/s]\u001b[A\n",
            " 37% 745/2015 [04:28<07:38,  2.77it/s]\u001b[A\n",
            " 37% 746/2015 [04:28<07:37,  2.77it/s]\u001b[A\n",
            " 37% 747/2015 [04:29<07:37,  2.77it/s]\u001b[A\n",
            " 37% 748/2015 [04:29<07:37,  2.77it/s]\u001b[A\n",
            " 37% 749/2015 [04:29<07:36,  2.77it/s]\u001b[A\n",
            " 37% 750/2015 [04:30<07:36,  2.77it/s]\u001b[A\n",
            " 37% 751/2015 [04:30<07:36,  2.77it/s]\u001b[A\n",
            " 37% 752/2015 [04:30<07:35,  2.77it/s]\u001b[A\n",
            " 37% 753/2015 [04:31<07:35,  2.77it/s]\u001b[A\n",
            " 37% 754/2015 [04:31<07:34,  2.77it/s]\u001b[A\n",
            " 37% 755/2015 [04:31<07:34,  2.77it/s]\u001b[A\n",
            " 38% 756/2015 [04:32<07:33,  2.77it/s]\u001b[A\n",
            " 38% 757/2015 [04:32<07:33,  2.77it/s]\u001b[A\n",
            " 38% 758/2015 [04:33<07:33,  2.77it/s]\u001b[A\n",
            " 38% 759/2015 [04:33<07:32,  2.77it/s]\u001b[A\n",
            " 38% 760/2015 [04:33<07:32,  2.77it/s]\u001b[A\n",
            " 38% 761/2015 [04:34<07:32,  2.77it/s]\u001b[A\n",
            " 38% 762/2015 [04:34<07:31,  2.77it/s]\u001b[A\n",
            " 38% 763/2015 [04:34<07:31,  2.77it/s]\u001b[A\n",
            " 38% 764/2015 [04:35<07:30,  2.77it/s]\u001b[A\n",
            " 38% 765/2015 [04:35<07:30,  2.77it/s]\u001b[A\n",
            " 38% 766/2015 [04:35<07:30,  2.77it/s]\u001b[A\n",
            " 38% 767/2015 [04:36<07:29,  2.77it/s]\u001b[A\n",
            " 38% 768/2015 [04:36<07:29,  2.77it/s]\u001b[A\n",
            " 38% 769/2015 [04:36<07:29,  2.77it/s]\u001b[A\n",
            " 38% 770/2015 [04:37<07:29,  2.77it/s]\u001b[A\n",
            " 38% 771/2015 [04:37<07:28,  2.77it/s]\u001b[A\n",
            " 38% 772/2015 [04:38<07:28,  2.77it/s]\u001b[A\n",
            " 38% 773/2015 [04:38<07:28,  2.77it/s]\u001b[A\n",
            " 38% 774/2015 [04:38<07:27,  2.77it/s]\u001b[A\n",
            " 38% 775/2015 [04:39<07:27,  2.77it/s]\u001b[A\n",
            " 39% 776/2015 [04:39<07:26,  2.77it/s]\u001b[A\n",
            " 39% 777/2015 [04:39<07:26,  2.77it/s]\u001b[A\n",
            " 39% 778/2015 [04:40<07:25,  2.77it/s]\u001b[A\n",
            " 39% 779/2015 [04:40<07:25,  2.77it/s]\u001b[A\n",
            " 39% 780/2015 [04:40<07:25,  2.77it/s]\u001b[A\n",
            " 39% 781/2015 [04:41<07:24,  2.77it/s]\u001b[A\n",
            " 39% 782/2015 [04:41<07:24,  2.77it/s]\u001b[A\n",
            " 39% 783/2015 [04:42<07:24,  2.77it/s]\u001b[A\n",
            " 39% 784/2015 [04:42<07:24,  2.77it/s]\u001b[A\n",
            " 39% 785/2015 [04:42<07:23,  2.77it/s]\u001b[A\n",
            " 39% 786/2015 [04:43<07:23,  2.77it/s]\u001b[A\n",
            " 39% 787/2015 [04:43<07:22,  2.77it/s]\u001b[A\n",
            " 39% 788/2015 [04:43<07:22,  2.77it/s]\u001b[A\n",
            " 39% 789/2015 [04:44<07:22,  2.77it/s]\u001b[A\n",
            " 39% 790/2015 [04:44<07:21,  2.77it/s]\u001b[A\n",
            " 39% 791/2015 [04:44<07:21,  2.77it/s]\u001b[A\n",
            " 39% 792/2015 [04:45<07:21,  2.77it/s]\u001b[A\n",
            " 39% 793/2015 [04:45<07:20,  2.77it/s]\u001b[A\n",
            " 39% 794/2015 [04:45<07:20,  2.77it/s]\u001b[A\n",
            " 39% 795/2015 [04:46<07:19,  2.77it/s]\u001b[A\n",
            " 40% 796/2015 [04:46<07:19,  2.77it/s]\u001b[A\n",
            " 40% 797/2015 [04:47<07:19,  2.77it/s]\u001b[A\n",
            " 40% 798/2015 [04:47<07:18,  2.77it/s]\u001b[A\n",
            " 40% 799/2015 [04:47<07:18,  2.77it/s]\u001b[A\n",
            " 40% 800/2015 [04:48<07:18,  2.77it/s]\u001b[A\n",
            " 40% 801/2015 [04:48<07:17,  2.77it/s]\u001b[A\n",
            " 40% 802/2015 [04:48<07:17,  2.77it/s]\u001b[A\n",
            " 40% 803/2015 [04:49<07:17,  2.77it/s]\u001b[A\n",
            " 40% 804/2015 [04:49<07:16,  2.77it/s]\u001b[A\n",
            " 40% 805/2015 [04:49<07:16,  2.77it/s]\u001b[A\n",
            " 40% 806/2015 [04:50<07:16,  2.77it/s]\u001b[A\n",
            " 40% 807/2015 [04:50<07:15,  2.77it/s]\u001b[A\n",
            " 40% 808/2015 [04:51<07:15,  2.77it/s]\u001b[A\n",
            " 40% 809/2015 [04:51<07:15,  2.77it/s]\u001b[A\n",
            " 40% 810/2015 [04:51<07:14,  2.77it/s]\u001b[A\n",
            " 40% 811/2015 [04:52<07:14,  2.77it/s]\u001b[A\n",
            " 40% 812/2015 [04:52<07:13,  2.77it/s]\u001b[A\n",
            " 40% 813/2015 [04:52<07:13,  2.77it/s]\u001b[A\n",
            " 40% 814/2015 [04:53<07:13,  2.77it/s]\u001b[A\n",
            " 40% 815/2015 [04:53<07:12,  2.77it/s]\u001b[A\n",
            " 40% 816/2015 [04:53<07:12,  2.77it/s]\u001b[A\n",
            " 41% 817/2015 [04:54<07:11,  2.77it/s]\u001b[A\n",
            " 41% 818/2015 [04:54<07:11,  2.77it/s]\u001b[A\n",
            " 41% 819/2015 [04:55<07:11,  2.77it/s]\u001b[A\n",
            " 41% 820/2015 [04:55<07:10,  2.77it/s]\u001b[A\n",
            " 41% 821/2015 [04:55<07:10,  2.77it/s]\u001b[A\n",
            " 41% 822/2015 [04:56<07:10,  2.77it/s]\u001b[A\n",
            " 41% 823/2015 [04:56<07:09,  2.77it/s]\u001b[A\n",
            " 41% 824/2015 [04:56<07:09,  2.77it/s]\u001b[A\n",
            " 41% 825/2015 [04:57<07:09,  2.77it/s]\u001b[A\n",
            " 41% 826/2015 [04:57<07:08,  2.77it/s]\u001b[A\n",
            " 41% 827/2015 [04:57<07:08,  2.77it/s]\u001b[A\n",
            " 41% 828/2015 [04:58<07:08,  2.77it/s]\u001b[A\n",
            " 41% 829/2015 [04:58<07:07,  2.77it/s]\u001b[A\n",
            " 41% 830/2015 [04:58<07:07,  2.77it/s]\u001b[A\n",
            " 41% 831/2015 [04:59<07:06,  2.77it/s]\u001b[A\n",
            " 41% 832/2015 [04:59<07:06,  2.77it/s]\u001b[A\n",
            " 41% 833/2015 [05:00<07:06,  2.77it/s]\u001b[A\n",
            " 41% 834/2015 [05:00<07:05,  2.77it/s]\u001b[A\n",
            " 41% 835/2015 [05:00<07:05,  2.77it/s]\u001b[A\n",
            " 41% 836/2015 [05:01<07:05,  2.77it/s]\u001b[A\n",
            " 42% 837/2015 [05:01<07:04,  2.77it/s]\u001b[A\n",
            " 42% 838/2015 [05:01<07:04,  2.77it/s]\u001b[A\n",
            " 42% 839/2015 [05:02<07:04,  2.77it/s]\u001b[A\n",
            " 42% 840/2015 [05:02<07:03,  2.77it/s]\u001b[A\n",
            " 42% 841/2015 [05:02<07:03,  2.77it/s]\u001b[A\n",
            " 42% 842/2015 [05:03<07:02,  2.77it/s]\u001b[A\n",
            " 42% 843/2015 [05:03<07:02,  2.77it/s]\u001b[A\n",
            " 42% 844/2015 [05:04<07:02,  2.77it/s]\u001b[A\n",
            " 42% 845/2015 [05:04<07:01,  2.77it/s]\u001b[A\n",
            " 42% 846/2015 [05:04<07:01,  2.77it/s]\u001b[A\n",
            " 42% 847/2015 [05:05<07:01,  2.77it/s]\u001b[A\n",
            " 42% 848/2015 [05:05<07:00,  2.77it/s]\u001b[A\n",
            " 42% 849/2015 [05:05<07:00,  2.77it/s]\u001b[A\n",
            " 42% 850/2015 [05:06<07:00,  2.77it/s]\u001b[A\n",
            " 42% 851/2015 [05:06<06:59,  2.77it/s]\u001b[A\n",
            " 42% 852/2015 [05:06<06:59,  2.77it/s]\u001b[A\n",
            " 42% 853/2015 [05:07<06:59,  2.77it/s]\u001b[A\n",
            " 42% 854/2015 [05:07<06:58,  2.77it/s]\u001b[A\n",
            " 42% 855/2015 [05:07<06:58,  2.77it/s]\u001b[A\n",
            " 42% 856/2015 [05:08<06:58,  2.77it/s]\u001b[A\n",
            " 43% 857/2015 [05:08<06:57,  2.77it/s]\u001b[A\n",
            " 43% 858/2015 [05:09<06:57,  2.77it/s]\u001b[A\n",
            " 43% 859/2015 [05:09<06:56,  2.77it/s]\u001b[A\n",
            " 43% 860/2015 [05:09<06:56,  2.77it/s]\u001b[A\n",
            " 43% 861/2015 [05:10<06:56,  2.77it/s]\u001b[A\n",
            " 43% 862/2015 [05:10<06:55,  2.77it/s]\u001b[A\n",
            " 43% 863/2015 [05:10<06:55,  2.77it/s]\u001b[A\n",
            " 43% 864/2015 [05:11<06:55,  2.77it/s]\u001b[A\n",
            " 43% 865/2015 [05:11<06:54,  2.77it/s]\u001b[A\n",
            " 43% 866/2015 [05:11<06:54,  2.77it/s]\u001b[A\n",
            " 43% 867/2015 [05:12<06:54,  2.77it/s]\u001b[A\n",
            " 43% 868/2015 [05:12<06:53,  2.77it/s]\u001b[A\n",
            " 43% 869/2015 [05:13<06:53,  2.77it/s]\u001b[A\n",
            " 43% 870/2015 [05:13<06:53,  2.77it/s]\u001b[A\n",
            " 43% 871/2015 [05:13<06:53,  2.77it/s]\u001b[A\n",
            " 43% 872/2015 [05:14<06:52,  2.77it/s]\u001b[A\n",
            " 43% 873/2015 [05:14<06:52,  2.77it/s]\u001b[A\n",
            " 43% 874/2015 [05:14<06:51,  2.77it/s]\u001b[A\n",
            " 43% 875/2015 [05:15<06:51,  2.77it/s]\u001b[A\n",
            " 43% 876/2015 [05:15<06:51,  2.77it/s]\u001b[A\n",
            " 44% 877/2015 [05:15<06:50,  2.77it/s]\u001b[A\n",
            " 44% 878/2015 [05:16<06:50,  2.77it/s]\u001b[A\n",
            " 44% 879/2015 [05:16<06:49,  2.77it/s]\u001b[A\n",
            " 44% 880/2015 [05:17<06:49,  2.77it/s]\u001b[A\n",
            " 44% 881/2015 [05:17<06:49,  2.77it/s]\u001b[A\n",
            " 44% 882/2015 [05:17<06:49,  2.77it/s]\u001b[A\n",
            " 44% 883/2015 [05:18<06:48,  2.77it/s]\u001b[A\n",
            " 44% 884/2015 [05:18<06:48,  2.77it/s]\u001b[A\n",
            " 44% 885/2015 [05:18<06:47,  2.77it/s]\u001b[A\n",
            " 44% 886/2015 [05:19<06:47,  2.77it/s]\u001b[A\n",
            " 44% 887/2015 [05:19<06:47,  2.77it/s]\u001b[A\n",
            " 44% 888/2015 [05:19<06:47,  2.77it/s]\u001b[A\n",
            " 44% 889/2015 [05:20<06:46,  2.77it/s]\u001b[A\n",
            " 44% 890/2015 [05:20<06:46,  2.77it/s]\u001b[A\n",
            " 44% 891/2015 [05:20<06:46,  2.77it/s]\u001b[A\n",
            " 44% 892/2015 [05:21<06:45,  2.77it/s]\u001b[A\n",
            " 44% 893/2015 [05:21<06:45,  2.77it/s]\u001b[A\n",
            " 44% 894/2015 [05:22<06:44,  2.77it/s]\u001b[A\n",
            " 44% 895/2015 [05:22<06:44,  2.77it/s]\u001b[A\n",
            " 44% 896/2015 [05:22<06:43,  2.77it/s]\u001b[A\n",
            " 45% 897/2015 [05:23<06:43,  2.77it/s]\u001b[A\n",
            " 45% 898/2015 [05:23<06:42,  2.77it/s]\u001b[A\n",
            " 45% 899/2015 [05:23<06:42,  2.77it/s]\u001b[A\n",
            " 45% 900/2015 [05:24<06:42,  2.77it/s]\u001b[A\n",
            " 45% 901/2015 [05:24<06:41,  2.77it/s]\u001b[A\n",
            " 45% 902/2015 [05:24<06:41,  2.77it/s]\u001b[A\n",
            " 45% 903/2015 [05:25<06:40,  2.77it/s]\u001b[A\n",
            " 45% 904/2015 [05:25<06:40,  2.77it/s]\u001b[A\n",
            " 45% 905/2015 [05:26<06:40,  2.77it/s]\u001b[A\n",
            " 45% 906/2015 [05:26<06:40,  2.77it/s]\u001b[A\n",
            " 45% 907/2015 [05:26<06:39,  2.77it/s]\u001b[A\n",
            " 45% 908/2015 [05:27<06:39,  2.77it/s]\u001b[A\n",
            " 45% 909/2015 [05:27<06:39,  2.77it/s]\u001b[A\n",
            " 45% 910/2015 [05:27<06:38,  2.77it/s]\u001b[A\n",
            " 45% 911/2015 [05:28<06:38,  2.77it/s]\u001b[A\n",
            " 45% 912/2015 [05:28<06:38,  2.77it/s]\u001b[A\n",
            " 45% 913/2015 [05:28<06:37,  2.77it/s]\u001b[A\n",
            " 45% 914/2015 [05:29<06:37,  2.77it/s]\u001b[A\n",
            " 45% 915/2015 [05:29<06:37,  2.77it/s]\u001b[A\n",
            " 45% 916/2015 [05:30<06:36,  2.77it/s]\u001b[A\n",
            " 46% 917/2015 [05:30<06:35,  2.77it/s]\u001b[A\n",
            " 46% 918/2015 [05:30<06:35,  2.77it/s]\u001b[A\n",
            " 46% 919/2015 [05:31<06:35,  2.77it/s]\u001b[A\n",
            " 46% 920/2015 [05:31<06:34,  2.77it/s]\u001b[A\n",
            " 46% 921/2015 [05:31<06:34,  2.77it/s]\u001b[A\n",
            " 46% 922/2015 [05:32<06:34,  2.77it/s]\u001b[A\n",
            " 46% 923/2015 [05:32<06:33,  2.77it/s]\u001b[A\n",
            " 46% 924/2015 [05:32<06:33,  2.77it/s]\u001b[A\n",
            " 46% 925/2015 [05:33<06:33,  2.77it/s]\u001b[A\n",
            " 46% 926/2015 [05:33<06:32,  2.77it/s]\u001b[A\n",
            " 46% 927/2015 [05:33<06:32,  2.77it/s]\u001b[A\n",
            " 46% 928/2015 [05:34<06:32,  2.77it/s]\u001b[A\n",
            " 46% 929/2015 [05:34<06:31,  2.77it/s]\u001b[A\n",
            " 46% 930/2015 [05:35<06:31,  2.77it/s]\u001b[A\n",
            " 46% 931/2015 [05:35<06:31,  2.77it/s]\u001b[A\n",
            " 46% 932/2015 [05:35<06:30,  2.77it/s]\u001b[A\n",
            " 46% 933/2015 [05:36<06:30,  2.77it/s]\u001b[A\n",
            " 46% 934/2015 [05:36<06:29,  2.77it/s]\u001b[A\n",
            " 46% 935/2015 [05:36<06:29,  2.77it/s]\u001b[A\n",
            " 46% 936/2015 [05:37<06:28,  2.77it/s]\u001b[A\n",
            " 47% 937/2015 [05:37<06:28,  2.77it/s]\u001b[A\n",
            " 47% 938/2015 [05:37<06:28,  2.77it/s]\u001b[A\n",
            " 47% 939/2015 [05:38<06:27,  2.77it/s]\u001b[A\n",
            " 47% 940/2015 [05:38<06:27,  2.77it/s]\u001b[A\n",
            " 47% 941/2015 [05:39<06:27,  2.77it/s]\u001b[A\n",
            " 47% 942/2015 [05:39<06:26,  2.77it/s]\u001b[A\n",
            " 47% 943/2015 [05:39<06:26,  2.77it/s]\u001b[A\n",
            " 47% 944/2015 [05:40<06:26,  2.77it/s]\u001b[A\n",
            " 47% 945/2015 [05:40<06:25,  2.77it/s]\u001b[A\n",
            " 47% 946/2015 [05:40<06:25,  2.77it/s]\u001b[A\n",
            " 47% 947/2015 [05:41<06:25,  2.77it/s]\u001b[A\n",
            " 47% 948/2015 [05:41<06:24,  2.77it/s]\u001b[A\n",
            " 47% 949/2015 [05:41<06:24,  2.77it/s]\u001b[A\n",
            " 47% 950/2015 [05:42<06:24,  2.77it/s]\u001b[A\n",
            " 47% 951/2015 [05:42<06:23,  2.77it/s]\u001b[A\n",
            " 47% 952/2015 [05:42<06:23,  2.77it/s]\u001b[A\n",
            " 47% 953/2015 [05:43<06:22,  2.77it/s]\u001b[A\n",
            " 47% 954/2015 [05:43<06:22,  2.77it/s]\u001b[A\n",
            " 47% 955/2015 [05:44<06:22,  2.77it/s]\u001b[A\n",
            " 47% 956/2015 [05:44<06:21,  2.77it/s]\u001b[A\n",
            " 47% 957/2015 [05:44<06:21,  2.77it/s]\u001b[A\n",
            " 48% 958/2015 [05:45<06:21,  2.77it/s]\u001b[A\n",
            " 48% 959/2015 [05:45<06:20,  2.77it/s]\u001b[A\n",
            " 48% 960/2015 [05:45<06:20,  2.77it/s]\u001b[A\n",
            " 48% 961/2015 [05:46<06:20,  2.77it/s]\u001b[A\n",
            " 48% 962/2015 [05:46<06:19,  2.77it/s]\u001b[A\n",
            " 48% 963/2015 [05:46<06:19,  2.77it/s]\u001b[A\n",
            " 48% 964/2015 [05:47<06:18,  2.77it/s]\u001b[A\n",
            " 48% 965/2015 [05:47<06:18,  2.77it/s]\u001b[A\n",
            " 48% 966/2015 [05:48<06:18,  2.77it/s]\u001b[A\n",
            " 48% 967/2015 [05:48<06:17,  2.77it/s]\u001b[A\n",
            " 48% 968/2015 [05:48<06:17,  2.77it/s]\u001b[A\n",
            " 48% 969/2015 [05:49<06:17,  2.77it/s]\u001b[A\n",
            " 48% 970/2015 [05:49<06:16,  2.77it/s]\u001b[A\n",
            " 48% 971/2015 [05:49<06:16,  2.77it/s]\u001b[A\n",
            " 48% 972/2015 [05:50<06:16,  2.77it/s]\u001b[A\n",
            " 48% 973/2015 [05:50<06:15,  2.77it/s]\u001b[A\n",
            " 48% 974/2015 [05:50<06:15,  2.77it/s]\u001b[A\n",
            " 48% 975/2015 [05:51<06:15,  2.77it/s]\u001b[A\n",
            " 48% 976/2015 [05:51<06:14,  2.77it/s]\u001b[A\n",
            " 48% 977/2015 [05:51<06:14,  2.77it/s]\u001b[A\n",
            " 49% 978/2015 [05:52<06:13,  2.77it/s]\u001b[A\n",
            " 49% 979/2015 [05:52<06:13,  2.77it/s]\u001b[A\n",
            " 49% 980/2015 [05:53<06:13,  2.77it/s]\u001b[A\n",
            " 49% 981/2015 [05:53<06:12,  2.77it/s]\u001b[A\n",
            " 49% 982/2015 [05:53<06:12,  2.77it/s]\u001b[A\n",
            " 49% 983/2015 [05:54<06:11,  2.77it/s]\u001b[A\n",
            " 49% 984/2015 [05:54<06:11,  2.77it/s]\u001b[A\n",
            " 49% 985/2015 [05:54<06:11,  2.77it/s]\u001b[A\n",
            " 49% 986/2015 [05:55<06:11,  2.77it/s]\u001b[A\n",
            " 49% 987/2015 [05:55<06:10,  2.77it/s]\u001b[A\n",
            " 49% 988/2015 [05:55<06:10,  2.77it/s]\u001b[A\n",
            " 49% 989/2015 [05:56<06:09,  2.77it/s]\u001b[A\n",
            " 49% 990/2015 [05:56<06:09,  2.77it/s]\u001b[A\n",
            " 49% 991/2015 [05:57<06:09,  2.77it/s]\u001b[A\n",
            " 49% 992/2015 [05:57<06:08,  2.77it/s]\u001b[A\n",
            " 49% 993/2015 [05:57<06:08,  2.77it/s]\u001b[A\n",
            " 49% 994/2015 [05:58<06:08,  2.77it/s]\u001b[A\n",
            " 49% 995/2015 [05:58<06:07,  2.77it/s]\u001b[A\n",
            " 49% 996/2015 [05:58<06:07,  2.77it/s]\u001b[A\n",
            " 49% 997/2015 [05:59<06:06,  2.77it/s]\u001b[A\n",
            " 50% 998/2015 [05:59<06:06,  2.77it/s]\u001b[A\n",
            " 50% 999/2015 [05:59<06:06,  2.77it/s]\u001b[A\n",
            " 50% 1000/2015 [06:00<06:06,  2.77it/s]\u001b[A\n",
            " 50% 1001/2015 [06:00<06:05,  2.77it/s]\u001b[A\n",
            " 50% 1002/2015 [06:01<06:05,  2.77it/s]\u001b[A\n",
            " 50% 1003/2015 [06:01<06:04,  2.77it/s]\u001b[A\n",
            " 50% 1004/2015 [06:01<06:04,  2.77it/s]\u001b[A\n",
            " 50% 1005/2015 [06:02<06:04,  2.77it/s]\u001b[A\n",
            " 50% 1006/2015 [06:02<06:03,  2.77it/s]\u001b[A\n",
            " 50% 1007/2015 [06:02<06:03,  2.77it/s]\u001b[A\n",
            " 50% 1008/2015 [06:03<06:03,  2.77it/s]\u001b[A\n",
            " 50% 1009/2015 [06:03<06:02,  2.77it/s]\u001b[A\n",
            " 50% 1010/2015 [06:03<06:02,  2.77it/s]\u001b[A\n",
            " 50% 1011/2015 [06:04<06:02,  2.77it/s]\u001b[A\n",
            " 50% 1012/2015 [06:04<06:01,  2.77it/s]\u001b[A\n",
            " 50% 1013/2015 [06:04<06:01,  2.77it/s]\u001b[A\n",
            " 50% 1014/2015 [06:05<06:00,  2.77it/s]\u001b[A\n",
            " 50% 1015/2015 [06:05<06:00,  2.77it/s]\u001b[A\n",
            " 50% 1016/2015 [06:06<06:00,  2.77it/s]\u001b[A\n",
            " 50% 1017/2015 [06:06<05:59,  2.77it/s]\u001b[A\n",
            " 51% 1018/2015 [06:06<05:59,  2.77it/s]\u001b[A\n",
            " 51% 1019/2015 [06:07<05:59,  2.77it/s]\u001b[A\n",
            " 51% 1020/2015 [06:07<05:58,  2.77it/s]\u001b[A\n",
            " 51% 1021/2015 [06:07<05:58,  2.77it/s]\u001b[A\n",
            " 51% 1022/2015 [06:08<05:58,  2.77it/s]\u001b[A\n",
            " 51% 1023/2015 [06:08<05:57,  2.77it/s]\u001b[A\n",
            " 51% 1024/2015 [06:08<05:57,  2.77it/s]\u001b[A\n",
            " 51% 1025/2015 [06:09<05:57,  2.77it/s]\u001b[A\n",
            " 51% 1026/2015 [06:09<05:56,  2.77it/s]\u001b[A\n",
            " 51% 1027/2015 [06:10<05:56,  2.77it/s]\u001b[A\n",
            " 51% 1028/2015 [06:10<05:55,  2.77it/s]\u001b[A\n",
            " 51% 1029/2015 [06:10<05:55,  2.77it/s]\u001b[A\n",
            " 51% 1030/2015 [06:11<05:55,  2.77it/s]\u001b[A\n",
            " 51% 1031/2015 [06:11<05:55,  2.77it/s]\u001b[A\n",
            " 51% 1032/2015 [06:11<05:54,  2.77it/s]\u001b[A\n",
            " 51% 1033/2015 [06:12<05:54,  2.77it/s]\u001b[A\n",
            " 51% 1034/2015 [06:12<05:53,  2.77it/s]\u001b[A\n",
            " 51% 1035/2015 [06:12<05:53,  2.77it/s]\u001b[A\n",
            " 51% 1036/2015 [06:13<05:52,  2.77it/s]\u001b[A\n",
            " 51% 1037/2015 [06:13<05:52,  2.77it/s]\u001b[A\n",
            " 52% 1038/2015 [06:13<05:52,  2.77it/s]\u001b[A\n",
            " 52% 1039/2015 [06:14<05:51,  2.78it/s]\u001b[A\n",
            " 52% 1040/2015 [06:14<05:51,  2.78it/s]\u001b[A\n",
            " 52% 1041/2015 [06:15<05:50,  2.78it/s]\u001b[A\n",
            " 52% 1042/2015 [06:15<05:50,  2.77it/s]\u001b[A\n",
            " 52% 1043/2015 [06:15<05:50,  2.77it/s]\u001b[A\n",
            " 52% 1044/2015 [06:16<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1045/2015 [06:16<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1046/2015 [06:16<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1047/2015 [06:17<05:49,  2.77it/s]\u001b[A\n",
            " 52% 1048/2015 [06:17<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1049/2015 [06:17<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1050/2015 [06:18<05:48,  2.77it/s]\u001b[A\n",
            " 52% 1051/2015 [06:18<05:47,  2.77it/s]\u001b[A\n",
            " 52% 1052/2015 [06:19<05:47,  2.77it/s]\u001b[A\n",
            " 52% 1053/2015 [06:19<05:46,  2.77it/s]\u001b[A\n",
            " 52% 1054/2015 [06:19<05:46,  2.77it/s]\u001b[A\n",
            " 52% 1055/2015 [06:20<05:46,  2.77it/s]\u001b[A\n",
            " 52% 1056/2015 [06:20<05:45,  2.77it/s]\u001b[A\n",
            " 52% 1057/2015 [06:20<05:45,  2.77it/s]\u001b[A\n",
            " 53% 1058/2015 [06:21<05:45,  2.77it/s]\u001b[A\n",
            " 53% 1059/2015 [06:21<05:44,  2.77it/s]\u001b[A\n",
            " 53% 1060/2015 [06:21<05:44,  2.77it/s]\u001b[A\n",
            " 53% 1061/2015 [06:22<05:44,  2.77it/s]\u001b[A\n",
            " 53% 1062/2015 [06:22<05:43,  2.77it/s]\u001b[A\n",
            " 53% 1063/2015 [06:23<05:43,  2.77it/s]\u001b[A\n",
            " 53% 1064/2015 [06:23<05:43,  2.77it/s]\u001b[A\n",
            " 53% 1065/2015 [06:23<05:42,  2.77it/s]\u001b[A\n",
            " 53% 1066/2015 [06:24<05:42,  2.77it/s]\u001b[A\n",
            " 53% 1067/2015 [06:24<05:41,  2.77it/s]\u001b[A\n",
            " 53% 1068/2015 [06:24<05:41,  2.77it/s]\u001b[A\n",
            " 53% 1069/2015 [06:25<05:41,  2.77it/s]\u001b[A\n",
            " 53% 1070/2015 [06:25<05:40,  2.77it/s]\u001b[A\n",
            " 53% 1071/2015 [06:25<05:40,  2.77it/s]\u001b[A\n",
            " 53% 1072/2015 [06:26<05:40,  2.77it/s]\u001b[A\n",
            " 53% 1073/2015 [06:26<05:39,  2.77it/s]\u001b[A\n",
            " 53% 1074/2015 [06:26<05:39,  2.77it/s]\u001b[A\n",
            " 53% 1075/2015 [06:27<05:39,  2.77it/s]\u001b[A\n",
            " 53% 1076/2015 [06:27<05:38,  2.77it/s]\u001b[A\n",
            " 53% 1077/2015 [06:28<05:38,  2.77it/s]\u001b[A\n",
            " 53% 1078/2015 [06:28<05:38,  2.77it/s]\u001b[A\n",
            " 54% 1079/2015 [06:28<05:37,  2.77it/s]\u001b[A\n",
            " 54% 1080/2015 [06:29<05:37,  2.77it/s]\u001b[A\n",
            " 54% 1081/2015 [06:29<05:37,  2.77it/s]\u001b[A\n",
            " 54% 1082/2015 [06:29<05:36,  2.77it/s]\u001b[A\n",
            " 54% 1083/2015 [06:30<05:36,  2.77it/s]\u001b[A\n",
            " 54% 1084/2015 [06:30<05:36,  2.77it/s]\u001b[A\n",
            " 54% 1085/2015 [06:30<05:35,  2.77it/s]\u001b[A\n",
            " 54% 1086/2015 [06:31<05:35,  2.77it/s]\u001b[A\n",
            " 54% 1087/2015 [06:31<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1088/2015 [06:32<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1089/2015 [06:32<05:34,  2.77it/s]\u001b[A\n",
            " 54% 1090/2015 [06:32<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1091/2015 [06:33<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1092/2015 [06:33<05:33,  2.77it/s]\u001b[A\n",
            " 54% 1093/2015 [06:33<05:32,  2.77it/s]\u001b[A\n",
            " 54% 1094/2015 [06:34<05:32,  2.77it/s]\u001b[A\n",
            " 54% 1095/2015 [06:34<05:32,  2.77it/s]\u001b[A\n",
            " 54% 1096/2015 [06:34<05:31,  2.77it/s]\u001b[A\n",
            " 54% 1097/2015 [06:35<05:31,  2.77it/s]\u001b[A\n",
            " 54% 1098/2015 [06:35<05:30,  2.77it/s]\u001b[A\n",
            " 55% 1099/2015 [06:36<05:30,  2.77it/s]\u001b[A\n",
            " 55% 1100/2015 [06:36<05:30,  2.77it/s]\u001b[A\n",
            " 55% 1101/2015 [06:36<05:29,  2.77it/s]\u001b[A\n",
            " 55% 1102/2015 [06:37<05:29,  2.77it/s]\u001b[A\n",
            " 55% 1103/2015 [06:37<05:29,  2.77it/s]\u001b[A\n",
            " 55% 1104/2015 [06:37<05:28,  2.77it/s]\u001b[A\n",
            " 55% 1105/2015 [06:38<05:28,  2.77it/s]\u001b[A\n",
            " 55% 1106/2015 [06:38<05:28,  2.77it/s]\u001b[A\n",
            " 55% 1107/2015 [06:38<05:27,  2.77it/s]\u001b[A\n",
            " 55% 1108/2015 [06:39<05:27,  2.77it/s]\u001b[A\n",
            " 55% 1109/2015 [06:39<05:27,  2.77it/s]\u001b[A\n",
            " 55% 1110/2015 [06:39<05:26,  2.77it/s]\u001b[A\n",
            " 55% 1111/2015 [06:40<05:26,  2.77it/s]\u001b[A\n",
            " 55% 1112/2015 [06:40<05:26,  2.77it/s]\u001b[A\n",
            " 55% 1113/2015 [06:41<05:25,  2.77it/s]\u001b[A\n",
            " 55% 1114/2015 [06:41<05:25,  2.77it/s]\u001b[A\n",
            " 55% 1115/2015 [06:41<05:25,  2.77it/s]\u001b[A\n",
            " 55% 1116/2015 [06:42<05:24,  2.77it/s]\u001b[A\n",
            " 55% 1117/2015 [06:42<05:24,  2.77it/s]\u001b[A\n",
            " 55% 1118/2015 [06:42<05:24,  2.77it/s]\u001b[A\n",
            " 56% 1119/2015 [06:43<05:23,  2.77it/s]\u001b[A\n",
            " 56% 1120/2015 [06:43<05:23,  2.77it/s]\u001b[A\n",
            " 56% 1121/2015 [06:43<05:22,  2.77it/s]\u001b[A\n",
            " 56% 1122/2015 [06:44<05:22,  2.77it/s]\u001b[A\n",
            " 56% 1123/2015 [06:44<05:22,  2.77it/s]\u001b[A\n",
            " 56% 1124/2015 [06:45<05:21,  2.77it/s]\u001b[A\n",
            " 56% 1125/2015 [06:45<05:21,  2.77it/s]\u001b[A\n",
            " 56% 1126/2015 [06:45<05:20,  2.77it/s]\u001b[A\n",
            " 56% 1127/2015 [06:46<05:20,  2.77it/s]\u001b[A\n",
            " 56% 1128/2015 [06:46<05:20,  2.77it/s]\u001b[A\n",
            " 56% 1129/2015 [06:46<05:19,  2.77it/s]\u001b[A\n",
            " 56% 1130/2015 [06:47<05:19,  2.77it/s]\u001b[A\n",
            " 56% 1131/2015 [06:47<05:18,  2.77it/s]\u001b[A\n",
            " 56% 1132/2015 [06:47<05:18,  2.77it/s]\u001b[A\n",
            " 56% 1133/2015 [06:48<05:18,  2.77it/s]\u001b[A\n",
            " 56% 1134/2015 [06:48<05:17,  2.77it/s]\u001b[A\n",
            " 56% 1135/2015 [06:48<05:17,  2.77it/s]\u001b[A\n",
            " 56% 1136/2015 [06:49<05:16,  2.77it/s]\u001b[A\n",
            " 56% 1137/2015 [06:49<05:16,  2.77it/s]\u001b[A\n",
            " 56% 1138/2015 [06:50<05:16,  2.77it/s]\u001b[A\n",
            " 57% 1139/2015 [06:50<05:15,  2.77it/s]\u001b[A\n",
            " 57% 1140/2015 [06:50<05:15,  2.77it/s]\u001b[A\n",
            " 57% 1141/2015 [06:51<05:15,  2.77it/s]\u001b[A\n",
            " 57% 1142/2015 [06:51<05:14,  2.77it/s]\u001b[A\n",
            " 57% 1143/2015 [06:51<05:14,  2.77it/s]\u001b[A\n",
            " 57% 1144/2015 [06:52<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1145/2015 [06:52<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1146/2015 [06:52<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1147/2015 [06:53<05:13,  2.77it/s]\u001b[A\n",
            " 57% 1148/2015 [06:53<05:12,  2.77it/s]\u001b[A\n",
            " 57% 1149/2015 [06:54<05:12,  2.77it/s]\u001b[A\n",
            " 57% 1150/2015 [06:54<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1151/2015 [06:54<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1152/2015 [06:55<05:11,  2.77it/s]\u001b[A\n",
            " 57% 1153/2015 [06:55<05:10,  2.77it/s]\u001b[A\n",
            " 57% 1154/2015 [06:55<05:10,  2.77it/s]\u001b[A\n",
            " 57% 1155/2015 [06:56<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1156/2015 [06:56<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1157/2015 [06:56<05:09,  2.77it/s]\u001b[A\n",
            " 57% 1158/2015 [06:57<05:09,  2.77it/s]\u001b[A\n",
            " 58% 1159/2015 [06:57<05:08,  2.77it/s]\u001b[A\n",
            " 58% 1160/2015 [06:58<05:08,  2.77it/s]\u001b[A\n",
            " 58% 1161/2015 [06:58<05:08,  2.77it/s]\u001b[A\n",
            " 58% 1162/2015 [06:58<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1163/2015 [06:59<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1164/2015 [06:59<05:07,  2.77it/s]\u001b[A\n",
            " 58% 1165/2015 [06:59<05:06,  2.77it/s]\u001b[A\n",
            " 58% 1166/2015 [07:00<05:06,  2.77it/s]\u001b[A\n",
            " 58% 1167/2015 [07:00<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1168/2015 [07:00<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1169/2015 [07:01<05:05,  2.77it/s]\u001b[A\n",
            " 58% 1170/2015 [07:01<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1171/2015 [07:01<05:04,  2.77it/s]\u001b[A\n",
            " 58% 1172/2015 [07:02<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1173/2015 [07:02<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1174/2015 [07:03<05:03,  2.77it/s]\u001b[A\n",
            " 58% 1175/2015 [07:03<05:02,  2.77it/s]\u001b[A\n",
            " 58% 1176/2015 [07:03<05:02,  2.77it/s]\u001b[A\n",
            " 58% 1177/2015 [07:04<05:02,  2.77it/s]\u001b[A\n",
            " 58% 1178/2015 [07:04<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1179/2015 [07:04<05:01,  2.77it/s]\u001b[A\n",
            " 59% 1180/2015 [07:05<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1181/2015 [07:05<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1182/2015 [07:05<05:00,  2.77it/s]\u001b[A\n",
            " 59% 1183/2015 [07:06<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1184/2015 [07:06<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1185/2015 [07:07<04:59,  2.77it/s]\u001b[A\n",
            " 59% 1186/2015 [07:07<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1187/2015 [07:07<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1188/2015 [07:08<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1189/2015 [07:08<04:58,  2.77it/s]\u001b[A\n",
            " 59% 1190/2015 [07:08<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1191/2015 [07:09<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1192/2015 [07:09<04:57,  2.77it/s]\u001b[A\n",
            " 59% 1193/2015 [07:09<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1194/2015 [07:10<04:56,  2.77it/s]\u001b[A\n",
            " 59% 1195/2015 [07:10<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1196/2015 [07:10<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1197/2015 [07:11<04:55,  2.77it/s]\u001b[A\n",
            " 59% 1198/2015 [07:11<04:54,  2.77it/s]\u001b[A\n",
            " 60% 1199/2015 [07:12<04:54,  2.77it/s]\u001b[A\n",
            " 60% 1200/2015 [07:12<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1201/2015 [07:12<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1202/2015 [07:13<04:53,  2.77it/s]\u001b[A\n",
            " 60% 1203/2015 [07:13<04:52,  2.77it/s]\u001b[A\n",
            " 60% 1204/2015 [07:13<04:52,  2.77it/s]\u001b[A\n",
            " 60% 1205/2015 [07:14<04:52,  2.77it/s]\u001b[A\n",
            " 60% 1206/2015 [07:14<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1207/2015 [07:14<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1208/2015 [07:15<04:51,  2.77it/s]\u001b[A\n",
            " 60% 1209/2015 [07:15<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1210/2015 [07:16<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1211/2015 [07:16<04:50,  2.77it/s]\u001b[A\n",
            " 60% 1212/2015 [07:16<04:49,  2.77it/s]\u001b[A\n",
            " 60% 1213/2015 [07:17<04:49,  2.77it/s]\u001b[A\n",
            " 60% 1214/2015 [07:17<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1215/2015 [07:17<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1216/2015 [07:18<04:48,  2.77it/s]\u001b[A\n",
            " 60% 1217/2015 [07:18<04:47,  2.77it/s]\u001b[A\n",
            " 60% 1218/2015 [07:18<04:47,  2.77it/s]\u001b[A\n",
            " 60% 1219/2015 [07:19<04:47,  2.77it/s]\u001b[A\n",
            " 61% 1220/2015 [07:19<04:46,  2.77it/s]\u001b[A\n",
            " 61% 1221/2015 [07:20<04:46,  2.77it/s]\u001b[A\n",
            " 61% 1222/2015 [07:20<04:46,  2.77it/s]\u001b[A\n",
            " 61% 1223/2015 [07:20<04:45,  2.77it/s]\u001b[A\n",
            " 61% 1224/2015 [07:21<04:45,  2.77it/s]\u001b[A\n",
            " 61% 1225/2015 [07:21<04:45,  2.77it/s]\u001b[A\n",
            " 61% 1226/2015 [07:21<04:44,  2.77it/s]\u001b[A\n",
            " 61% 1227/2015 [07:22<04:44,  2.77it/s]\u001b[A\n",
            " 61% 1228/2015 [07:22<04:43,  2.77it/s]\u001b[A\n",
            " 61% 1229/2015 [07:22<04:43,  2.77it/s]\u001b[A\n",
            " 61% 1230/2015 [07:23<04:43,  2.77it/s]\u001b[A\n",
            " 61% 1231/2015 [07:23<04:42,  2.77it/s]\u001b[A\n",
            " 61% 1232/2015 [07:23<04:42,  2.77it/s]\u001b[A\n",
            " 61% 1233/2015 [07:24<04:42,  2.77it/s]\u001b[A\n",
            " 61% 1234/2015 [07:24<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1235/2015 [07:25<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1236/2015 [07:25<04:41,  2.77it/s]\u001b[A\n",
            " 61% 1237/2015 [07:25<04:40,  2.77it/s]\u001b[A\n",
            " 61% 1238/2015 [07:26<04:40,  2.77it/s]\u001b[A\n",
            " 61% 1239/2015 [07:26<04:40,  2.77it/s]\u001b[A\n",
            " 62% 1240/2015 [07:26<04:39,  2.77it/s]\u001b[A\n",
            " 62% 1241/2015 [07:27<04:39,  2.77it/s]\u001b[A\n",
            " 62% 1242/2015 [07:27<04:39,  2.77it/s]\u001b[A\n",
            " 62% 1243/2015 [07:27<04:38,  2.77it/s]\u001b[A\n",
            " 62% 1244/2015 [07:28<04:38,  2.77it/s]\u001b[A\n",
            " 62% 1245/2015 [07:28<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1246/2015 [07:29<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1247/2015 [07:29<04:37,  2.77it/s]\u001b[A\n",
            " 62% 1248/2015 [07:29<04:36,  2.77it/s]\u001b[A\n",
            " 62% 1249/2015 [07:30<04:36,  2.77it/s]\u001b[A\n",
            " 62% 1250/2015 [07:30<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1251/2015 [07:30<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1252/2015 [07:31<04:35,  2.77it/s]\u001b[A\n",
            " 62% 1253/2015 [07:31<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1254/2015 [07:31<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1255/2015 [07:32<04:34,  2.77it/s]\u001b[A\n",
            " 62% 1256/2015 [07:32<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1257/2015 [07:33<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1258/2015 [07:33<04:33,  2.77it/s]\u001b[A\n",
            " 62% 1259/2015 [07:33<04:32,  2.77it/s]\u001b[A\n",
            " 63% 1260/2015 [07:34<04:32,  2.77it/s]\u001b[A\n",
            " 63% 1261/2015 [07:34<04:31,  2.77it/s]\u001b[A\n",
            " 63% 1262/2015 [07:34<04:31,  2.77it/s]\u001b[A\n",
            " 63% 1263/2015 [07:35<04:31,  2.77it/s]\u001b[A\n",
            " 63% 1264/2015 [07:35<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1265/2015 [07:35<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1266/2015 [07:36<04:30,  2.77it/s]\u001b[A\n",
            " 63% 1267/2015 [07:36<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1268/2015 [07:36<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1269/2015 [07:37<04:29,  2.77it/s]\u001b[A\n",
            " 63% 1270/2015 [07:37<04:28,  2.77it/s]\u001b[A\n",
            " 63% 1271/2015 [07:38<04:28,  2.77it/s]\u001b[A\n",
            " 63% 1272/2015 [07:38<04:28,  2.77it/s]\u001b[A\n",
            " 63% 1273/2015 [07:38<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1274/2015 [07:39<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1275/2015 [07:39<04:27,  2.77it/s]\u001b[A\n",
            " 63% 1276/2015 [07:39<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1277/2015 [07:40<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1278/2015 [07:40<04:26,  2.77it/s]\u001b[A\n",
            " 63% 1279/2015 [07:40<04:25,  2.77it/s]\u001b[A\n",
            " 64% 1280/2015 [07:41<04:25,  2.77it/s]\u001b[A\n",
            " 64% 1281/2015 [07:41<04:24,  2.77it/s]\u001b[A\n",
            " 64% 1282/2015 [07:42<04:24,  2.77it/s]\u001b[A\n",
            " 64% 1283/2015 [07:42<04:24,  2.77it/s]\u001b[A\n",
            " 64% 1284/2015 [07:42<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1285/2015 [07:43<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1286/2015 [07:43<04:23,  2.77it/s]\u001b[A\n",
            " 64% 1287/2015 [07:43<04:22,  2.77it/s]\u001b[A\n",
            " 64% 1288/2015 [07:44<04:22,  2.77it/s]\u001b[A\n",
            " 64% 1289/2015 [07:44<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1290/2015 [07:44<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1291/2015 [07:45<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1292/2015 [07:45<04:21,  2.77it/s]\u001b[A\n",
            " 64% 1293/2015 [07:45<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1294/2015 [07:46<04:20,  2.77it/s]\u001b[A\n",
            " 64% 1295/2015 [07:46<04:19,  2.77it/s]\u001b[A\n",
            " 64% 1296/2015 [07:47<04:19,  2.77it/s]\u001b[A\n",
            " 64% 1297/2015 [07:47<04:19,  2.77it/s]\u001b[A\n",
            " 64% 1298/2015 [07:47<04:18,  2.77it/s]\u001b[A\n",
            " 64% 1299/2015 [07:48<04:18,  2.77it/s]\u001b[A\n",
            " 65% 1300/2015 [07:48<04:18,  2.77it/s]\u001b[A\n",
            " 65% 1301/2015 [07:48<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1302/2015 [07:49<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1303/2015 [07:49<04:17,  2.77it/s]\u001b[A\n",
            " 65% 1304/2015 [07:49<04:16,  2.77it/s]\u001b[A\n",
            " 65% 1305/2015 [07:50<04:16,  2.77it/s]\u001b[A\n",
            " 65% 1306/2015 [07:50<04:16,  2.77it/s]\u001b[A\n",
            " 65% 1307/2015 [07:51<04:15,  2.77it/s]\u001b[A\n",
            " 65% 1308/2015 [07:51<04:15,  2.77it/s]\u001b[A\n",
            " 65% 1309/2015 [07:51<04:14,  2.77it/s]\u001b[A\n",
            " 65% 1310/2015 [07:52<04:14,  2.77it/s]\u001b[A\n",
            " 65% 1311/2015 [07:52<04:13,  2.77it/s]\u001b[A\n",
            " 65% 1312/2015 [07:52<04:13,  2.77it/s]\u001b[A\n",
            " 65% 1313/2015 [07:53<04:13,  2.77it/s]\u001b[A\n",
            " 65% 1314/2015 [07:53<04:12,  2.77it/s]\u001b[A\n",
            " 65% 1315/2015 [07:53<04:12,  2.77it/s]\u001b[A\n",
            " 65% 1316/2015 [07:54<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1317/2015 [07:54<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1318/2015 [07:55<04:11,  2.77it/s]\u001b[A\n",
            " 65% 1319/2015 [07:55<04:11,  2.77it/s]\u001b[A\n",
            " 66% 1320/2015 [07:55<04:10,  2.77it/s]\u001b[A\n",
            " 66% 1321/2015 [07:56<04:10,  2.77it/s]\u001b[A\n",
            " 66% 1322/2015 [07:56<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1323/2015 [07:56<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1324/2015 [07:57<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1325/2015 [07:57<04:09,  2.77it/s]\u001b[A\n",
            " 66% 1326/2015 [07:57<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1327/2015 [07:58<04:08,  2.77it/s]\u001b[A\n",
            " 66% 1328/2015 [07:58<04:07,  2.77it/s]\u001b[A\n",
            " 66% 1329/2015 [07:58<04:07,  2.77it/s]\u001b[A\n",
            " 66% 1330/2015 [07:59<04:07,  2.77it/s]\u001b[A\n",
            " 66% 1331/2015 [07:59<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1332/2015 [08:00<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1333/2015 [08:00<04:06,  2.77it/s]\u001b[A\n",
            " 66% 1334/2015 [08:00<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1335/2015 [08:01<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1336/2015 [08:01<04:05,  2.77it/s]\u001b[A\n",
            " 66% 1337/2015 [08:01<04:04,  2.77it/s]\u001b[A\n",
            " 66% 1338/2015 [08:02<04:04,  2.77it/s]\u001b[A\n",
            " 66% 1339/2015 [08:02<04:04,  2.77it/s]\u001b[A\n",
            " 67% 1340/2015 [08:02<04:03,  2.77it/s]\u001b[A\n",
            " 67% 1341/2015 [08:03<04:03,  2.77it/s]\u001b[A\n",
            " 67% 1342/2015 [08:03<04:02,  2.77it/s]\u001b[A\n",
            " 67% 1343/2015 [08:04<04:02,  2.77it/s]\u001b[A\n",
            " 67% 1344/2015 [08:04<04:02,  2.77it/s]\u001b[A\n",
            " 67% 1345/2015 [08:04<04:01,  2.77it/s]\u001b[A\n",
            " 67% 1346/2015 [08:05<04:01,  2.77it/s]\u001b[A\n",
            " 67% 1347/2015 [08:05<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1348/2015 [08:05<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1349/2015 [08:06<04:00,  2.77it/s]\u001b[A\n",
            " 67% 1350/2015 [08:06<03:59,  2.77it/s]\u001b[A\n",
            " 67% 1351/2015 [08:06<03:59,  2.77it/s]\u001b[A\n",
            " 67% 1352/2015 [08:07<03:59,  2.77it/s]\u001b[A\n",
            " 67% 1353/2015 [08:07<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1354/2015 [08:08<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1355/2015 [08:08<03:58,  2.77it/s]\u001b[A\n",
            " 67% 1356/2015 [08:08<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1357/2015 [08:09<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1358/2015 [08:09<03:57,  2.77it/s]\u001b[A\n",
            " 67% 1359/2015 [08:09<03:56,  2.77it/s]\u001b[A\n",
            " 67% 1360/2015 [08:10<03:56,  2.77it/s]\u001b[A\n",
            " 68% 1361/2015 [08:10<03:56,  2.77it/s]\u001b[A\n",
            " 68% 1362/2015 [08:10<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1363/2015 [08:11<03:55,  2.77it/s]\u001b[A\n",
            " 68% 1364/2015 [08:11<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1365/2015 [08:11<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1366/2015 [08:12<03:54,  2.77it/s]\u001b[A\n",
            " 68% 1367/2015 [08:12<03:53,  2.77it/s]\u001b[A\n",
            " 68% 1368/2015 [08:13<03:53,  2.77it/s]\u001b[A\n",
            " 68% 1369/2015 [08:13<03:52,  2.77it/s]\u001b[A\n",
            " 68% 1370/2015 [08:13<03:52,  2.77it/s]\u001b[A\n",
            " 68% 1371/2015 [08:14<03:52,  2.77it/s]\u001b[A\n",
            " 68% 1372/2015 [08:14<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1373/2015 [08:14<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1374/2015 [08:15<03:51,  2.77it/s]\u001b[A\n",
            " 68% 1375/2015 [08:15<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1376/2015 [08:15<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1377/2015 [08:16<03:50,  2.77it/s]\u001b[A\n",
            " 68% 1378/2015 [08:16<03:49,  2.77it/s]\u001b[A\n",
            " 68% 1379/2015 [08:17<03:49,  2.77it/s]\u001b[A\n",
            " 68% 1380/2015 [08:17<03:49,  2.77it/s]\u001b[A\n",
            " 69% 1381/2015 [08:17<03:48,  2.77it/s]\u001b[A\n",
            " 69% 1382/2015 [08:18<03:48,  2.77it/s]\u001b[A\n",
            " 69% 1383/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1384/2015 [08:18<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1385/2015 [08:19<03:47,  2.77it/s]\u001b[A\n",
            " 69% 1386/2015 [08:19<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1387/2015 [08:19<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1388/2015 [08:20<03:46,  2.77it/s]\u001b[A\n",
            " 69% 1389/2015 [08:20<03:45,  2.77it/s]\u001b[A\n",
            " 69% 1390/2015 [08:20<03:45,  2.77it/s]\u001b[A\n",
            " 69% 1391/2015 [08:21<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1392/2015 [08:21<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1393/2015 [08:22<03:44,  2.77it/s]\u001b[A\n",
            " 69% 1394/2015 [08:22<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1395/2015 [08:22<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1396/2015 [08:23<03:43,  2.77it/s]\u001b[A\n",
            " 69% 1397/2015 [08:23<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1398/2015 [08:23<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1399/2015 [08:24<03:42,  2.77it/s]\u001b[A\n",
            " 69% 1400/2015 [08:24<03:41,  2.77it/s]\u001b[A\n",
            " 70% 1401/2015 [08:24<03:41,  2.77it/s]\u001b[A\n",
            " 70% 1402/2015 [08:25<03:41,  2.77it/s]\u001b[A\n",
            " 70% 1403/2015 [08:25<03:40,  2.77it/s]\u001b[A\n",
            " 70% 1404/2015 [08:26<03:40,  2.77it/s]\u001b[A\n",
            " 70% 1405/2015 [08:26<03:39,  2.78it/s]\u001b[A\n",
            " 70% 1406/2015 [08:26<03:39,  2.78it/s]\u001b[A\n",
            " 70% 1407/2015 [08:27<03:39,  2.78it/s]\u001b[A\n",
            " 70% 1408/2015 [08:27<03:38,  2.78it/s]\u001b[A\n",
            " 70% 1409/2015 [08:27<03:38,  2.78it/s]\u001b[A\n",
            " 70% 1410/2015 [08:28<03:38,  2.77it/s]\u001b[A\n",
            " 70% 1411/2015 [08:28<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1412/2015 [08:28<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1413/2015 [08:29<03:37,  2.77it/s]\u001b[A\n",
            " 70% 1414/2015 [08:29<03:36,  2.77it/s]\u001b[A\n",
            " 70% 1415/2015 [08:30<03:36,  2.77it/s]\u001b[A\n",
            " 70% 1416/2015 [08:30<03:35,  2.77it/s]\u001b[A\n",
            " 70% 1417/2015 [08:30<03:35,  2.77it/s]\u001b[A\n",
            " 70% 1418/2015 [08:31<03:35,  2.77it/s]\u001b[A\n",
            " 70% 1419/2015 [08:31<03:34,  2.77it/s]\u001b[A\n",
            " 70% 1420/2015 [08:31<03:34,  2.77it/s]\u001b[A\n",
            " 71% 1421/2015 [08:32<03:34,  2.77it/s]\u001b[A\n",
            " 71% 1422/2015 [08:32<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1423/2015 [08:32<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1424/2015 [08:33<03:33,  2.77it/s]\u001b[A\n",
            " 71% 1425/2015 [08:33<03:32,  2.77it/s]\u001b[A\n",
            " 71% 1426/2015 [08:33<03:32,  2.77it/s]\u001b[A\n",
            " 71% 1427/2015 [08:34<03:32,  2.77it/s]\u001b[A\n",
            " 71% 1428/2015 [08:34<03:31,  2.77it/s]\u001b[A\n",
            " 71% 1429/2015 [08:35<03:31,  2.77it/s]\u001b[A\n",
            " 71% 1430/2015 [08:35<03:31,  2.77it/s]\u001b[A\n",
            " 71% 1431/2015 [08:35<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1432/2015 [08:36<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1433/2015 [08:36<03:30,  2.77it/s]\u001b[A\n",
            " 71% 1434/2015 [08:36<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1435/2015 [08:37<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1436/2015 [08:37<03:29,  2.77it/s]\u001b[A\n",
            " 71% 1437/2015 [08:37<03:28,  2.77it/s]\u001b[A\n",
            " 71% 1438/2015 [08:38<03:28,  2.77it/s]\u001b[A\n",
            " 71% 1439/2015 [08:38<03:27,  2.77it/s]\u001b[A\n",
            " 71% 1440/2015 [08:39<03:27,  2.77it/s]\u001b[A\n",
            " 72% 1441/2015 [08:39<03:27,  2.77it/s]\u001b[A\n",
            " 72% 1442/2015 [08:39<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1443/2015 [08:40<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1444/2015 [08:40<03:26,  2.77it/s]\u001b[A\n",
            " 72% 1445/2015 [08:40<03:25,  2.77it/s]\u001b[A\n",
            " 72% 1446/2015 [08:41<03:25,  2.77it/s]\u001b[A\n",
            " 72% 1447/2015 [08:41<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1448/2015 [08:41<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1449/2015 [08:42<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1450/2015 [08:42<03:24,  2.77it/s]\u001b[A\n",
            " 72% 1451/2015 [08:42<03:23,  2.77it/s]\u001b[A\n",
            " 72% 1452/2015 [08:43<03:23,  2.77it/s]\u001b[A\n",
            " 72% 1453/2015 [08:43<03:22,  2.77it/s]\u001b[A\n",
            " 72% 1454/2015 [08:44<03:22,  2.77it/s]\u001b[A\n",
            " 72% 1455/2015 [08:44<03:22,  2.77it/s]\u001b[A\n",
            " 72% 1456/2015 [08:44<03:21,  2.77it/s]\u001b[A\n",
            " 72% 1457/2015 [08:45<03:21,  2.77it/s]\u001b[A\n",
            " 72% 1458/2015 [08:45<03:20,  2.77it/s]\u001b[A\n",
            " 72% 1459/2015 [08:45<03:20,  2.77it/s]\u001b[A\n",
            " 72% 1460/2015 [08:46<03:20,  2.77it/s]\u001b[A\n",
            " 73% 1461/2015 [08:46<03:19,  2.77it/s]\u001b[A\n",
            " 73% 1462/2015 [08:46<03:19,  2.77it/s]\u001b[A\n",
            " 73% 1463/2015 [08:47<03:19,  2.77it/s]\u001b[A\n",
            " 73% 1464/2015 [08:47<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1465/2015 [08:48<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1466/2015 [08:48<03:18,  2.77it/s]\u001b[A\n",
            " 73% 1467/2015 [08:48<03:17,  2.77it/s]\u001b[A\n",
            " 73% 1468/2015 [08:49<03:17,  2.77it/s]\u001b[A\n",
            " 73% 1469/2015 [08:49<03:17,  2.77it/s]\u001b[A\n",
            " 73% 1470/2015 [08:49<03:16,  2.77it/s]\u001b[A\n",
            " 73% 1471/2015 [08:50<03:16,  2.77it/s]\u001b[A\n",
            " 73% 1472/2015 [08:50<03:16,  2.77it/s]\u001b[A\n",
            " 73% 1473/2015 [08:50<03:15,  2.77it/s]\u001b[A\n",
            " 73% 1474/2015 [08:51<03:15,  2.77it/s]\u001b[A\n",
            " 73% 1475/2015 [08:51<03:14,  2.77it/s]\u001b[A\n",
            " 73% 1476/2015 [08:52<03:14,  2.77it/s]\u001b[A\n",
            " 73% 1477/2015 [08:52<03:14,  2.77it/s]\u001b[A\n",
            " 73% 1478/2015 [08:52<03:13,  2.77it/s]\u001b[A\n",
            " 73% 1479/2015 [08:53<03:13,  2.77it/s]\u001b[A\n",
            " 73% 1480/2015 [08:53<03:13,  2.77it/s]\u001b[A\n",
            " 73% 1481/2015 [08:53<03:12,  2.77it/s]\u001b[A\n",
            " 74% 1482/2015 [08:54<03:12,  2.77it/s]\u001b[A\n",
            " 74% 1483/2015 [08:54<03:12,  2.77it/s]\u001b[A\n",
            " 74% 1484/2015 [08:54<03:11,  2.77it/s]\u001b[A\n",
            " 74% 1485/2015 [08:55<03:11,  2.77it/s]\u001b[A\n",
            " 74% 1486/2015 [08:55<03:10,  2.77it/s]\u001b[A\n",
            " 74% 1487/2015 [08:55<03:10,  2.77it/s]\u001b[A\n",
            " 74% 1488/2015 [08:56<03:10,  2.77it/s]\u001b[A\n",
            " 74% 1489/2015 [08:56<03:09,  2.77it/s]\u001b[A\n",
            " 74% 1490/2015 [08:57<03:09,  2.77it/s]\u001b[A\n",
            " 74% 1491/2015 [08:57<03:09,  2.77it/s]\u001b[A\n",
            " 74% 1492/2015 [08:57<03:08,  2.77it/s]\u001b[A\n",
            " 74% 1493/2015 [08:58<03:08,  2.77it/s]\u001b[A\n",
            " 74% 1494/2015 [08:58<03:07,  2.77it/s]\u001b[A\n",
            " 74% 1495/2015 [08:58<03:07,  2.77it/s]\u001b[A\n",
            " 74% 1496/2015 [08:59<03:07,  2.77it/s]\u001b[A\n",
            " 74% 1497/2015 [08:59<03:06,  2.77it/s]\u001b[A\n",
            " 74% 1498/2015 [08:59<03:06,  2.77it/s]\u001b[A\n",
            " 74% 1499/2015 [09:00<03:06,  2.77it/s]\u001b[A\n",
            " 74% 1500/2015 [09:00<03:05,  2.77it/s]\u001b[A\n",
            " 74% 1501/2015 [09:01<03:05,  2.77it/s]\u001b[A\n",
            " 75% 1502/2015 [09:01<03:05,  2.77it/s]\u001b[A\n",
            " 75% 1503/2015 [09:01<03:04,  2.77it/s]\u001b[A\n",
            " 75% 1504/2015 [09:02<03:04,  2.77it/s]\u001b[A\n",
            " 75% 1505/2015 [09:02<03:03,  2.77it/s]\u001b[A\n",
            " 75% 1506/2015 [09:02<03:03,  2.77it/s]\u001b[A\n",
            " 75% 1507/2015 [09:03<03:03,  2.77it/s]\u001b[A\n",
            " 75% 1508/2015 [09:03<03:02,  2.77it/s]\u001b[A\n",
            " 75% 1509/2015 [09:03<03:02,  2.77it/s]\u001b[A\n",
            " 75% 1510/2015 [09:04<03:02,  2.77it/s]\u001b[A\n",
            " 75% 1511/2015 [09:04<03:01,  2.77it/s]\u001b[A\n",
            " 75% 1512/2015 [09:05<03:01,  2.77it/s]\u001b[A\n",
            " 75% 1513/2015 [09:05<03:00,  2.77it/s]\u001b[A\n",
            " 75% 1514/2015 [09:05<03:00,  2.77it/s]\u001b[A\n",
            " 75% 1515/2015 [09:06<03:00,  2.77it/s]\u001b[A\n",
            " 75% 1516/2015 [09:06<02:59,  2.77it/s]\u001b[A\n",
            " 75% 1517/2015 [09:06<02:59,  2.77it/s]\u001b[A\n",
            " 75% 1518/2015 [09:07<02:59,  2.77it/s]\u001b[A\n",
            " 75% 1519/2015 [09:07<02:58,  2.77it/s]\u001b[A\n",
            " 75% 1520/2015 [09:07<02:58,  2.77it/s]\u001b[A\n",
            " 75% 1521/2015 [09:08<02:58,  2.77it/s]\u001b[A\n",
            " 76% 1522/2015 [09:08<02:57,  2.77it/s]\u001b[A\n",
            " 76% 1523/2015 [09:08<02:57,  2.77it/s]\u001b[A\n",
            " 76% 1524/2015 [09:09<02:57,  2.77it/s]\u001b[A\n",
            " 76% 1525/2015 [09:09<02:56,  2.77it/s]\u001b[A\n",
            " 76% 1526/2015 [09:10<02:56,  2.77it/s]\u001b[A\n",
            " 76% 1527/2015 [09:10<02:56,  2.77it/s]\u001b[A\n",
            " 76% 1528/2015 [09:10<02:55,  2.77it/s]\u001b[A\n",
            " 76% 1529/2015 [09:11<02:55,  2.77it/s]\u001b[A\n",
            " 76% 1530/2015 [09:11<02:55,  2.77it/s]\u001b[A\n",
            " 76% 1531/2015 [09:11<02:54,  2.77it/s]\u001b[A\n",
            " 76% 1532/2015 [09:12<02:54,  2.77it/s]\u001b[A\n",
            " 76% 1533/2015 [09:12<02:53,  2.77it/s]\u001b[A\n",
            " 76% 1534/2015 [09:12<02:53,  2.77it/s]\u001b[A\n",
            " 76% 1535/2015 [09:13<02:53,  2.77it/s]\u001b[A\n",
            " 76% 1536/2015 [09:13<02:52,  2.77it/s]\u001b[A\n",
            " 76% 1537/2015 [09:14<02:52,  2.77it/s]\u001b[A\n",
            " 76% 1538/2015 [09:14<02:52,  2.77it/s]\u001b[A\n",
            " 76% 1539/2015 [09:14<02:51,  2.77it/s]\u001b[A\n",
            " 76% 1540/2015 [09:15<02:51,  2.77it/s]\u001b[A\n",
            " 76% 1541/2015 [09:15<02:50,  2.77it/s]\u001b[A\n",
            " 77% 1542/2015 [09:15<02:50,  2.77it/s]\u001b[A\n",
            " 77% 1543/2015 [09:16<02:50,  2.77it/s]\u001b[A\n",
            " 77% 1544/2015 [09:16<02:49,  2.77it/s]\u001b[A\n",
            " 77% 1545/2015 [09:16<02:49,  2.77it/s]\u001b[A\n",
            " 77% 1546/2015 [09:17<02:49,  2.77it/s]\u001b[A\n",
            " 77% 1547/2015 [09:17<02:48,  2.77it/s]\u001b[A\n",
            " 77% 1548/2015 [09:17<02:48,  2.77it/s]\u001b[A\n",
            " 77% 1549/2015 [09:18<02:48,  2.77it/s]\u001b[A\n",
            " 77% 1550/2015 [09:18<02:47,  2.77it/s]\u001b[A\n",
            " 77% 1551/2015 [09:19<02:47,  2.77it/s]\u001b[A\n",
            " 77% 1552/2015 [09:19<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1553/2015 [09:19<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1554/2015 [09:20<02:46,  2.77it/s]\u001b[A\n",
            " 77% 1555/2015 [09:20<02:45,  2.77it/s]\u001b[A\n",
            " 77% 1556/2015 [09:20<02:45,  2.77it/s]\u001b[A\n",
            " 77% 1557/2015 [09:21<02:45,  2.77it/s]\u001b[A\n",
            " 77% 1558/2015 [09:21<02:44,  2.77it/s]\u001b[A\n",
            " 77% 1559/2015 [09:21<02:44,  2.77it/s]\u001b[A\n",
            " 77% 1560/2015 [09:22<02:44,  2.77it/s]\u001b[A\n",
            " 77% 1561/2015 [09:22<02:43,  2.77it/s]\u001b[A\n",
            " 78% 1562/2015 [09:23<02:43,  2.77it/s]\u001b[A\n",
            " 78% 1563/2015 [09:23<02:43,  2.77it/s]\u001b[A\n",
            " 78% 1564/2015 [09:23<02:42,  2.77it/s]\u001b[A\n",
            " 78% 1565/2015 [09:24<02:42,  2.77it/s]\u001b[A\n",
            " 78% 1566/2015 [09:24<02:42,  2.77it/s]\u001b[A\n",
            " 78% 1567/2015 [09:24<02:41,  2.77it/s]\u001b[A\n",
            " 78% 1568/2015 [09:25<02:41,  2.77it/s]\u001b[A\n",
            " 78% 1569/2015 [09:25<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1570/2015 [09:25<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1571/2015 [09:26<02:40,  2.77it/s]\u001b[A\n",
            " 78% 1572/2015 [09:26<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1573/2015 [09:27<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1574/2015 [09:27<02:39,  2.77it/s]\u001b[A\n",
            " 78% 1575/2015 [09:27<02:38,  2.77it/s]\u001b[A\n",
            " 78% 1576/2015 [09:28<02:38,  2.77it/s]\u001b[A\n",
            " 78% 1577/2015 [09:28<02:37,  2.77it/s]\u001b[A\n",
            " 78% 1578/2015 [09:28<02:37,  2.77it/s]\u001b[A\n",
            " 78% 1579/2015 [09:29<02:37,  2.77it/s]\u001b[A\n",
            " 78% 1580/2015 [09:29<02:36,  2.77it/s]\u001b[A\n",
            " 78% 1581/2015 [09:29<02:36,  2.77it/s]\u001b[A\n",
            " 79% 1582/2015 [09:30<02:36,  2.77it/s]\u001b[A\n",
            " 79% 1583/2015 [09:30<02:35,  2.77it/s]\u001b[A\n",
            " 79% 1584/2015 [09:30<02:35,  2.77it/s]\u001b[A\n",
            " 79% 1585/2015 [09:31<02:35,  2.77it/s]\u001b[A\n",
            " 79% 1586/2015 [09:31<02:34,  2.77it/s]\u001b[A\n",
            " 79% 1587/2015 [09:32<02:34,  2.77it/s]\u001b[A\n",
            " 79% 1588/2015 [09:32<02:34,  2.77it/s]\u001b[A\n",
            " 79% 1589/2015 [09:32<02:33,  2.77it/s]\u001b[A\n",
            " 79% 1590/2015 [09:33<02:33,  2.77it/s]\u001b[A\n",
            " 79% 1591/2015 [09:33<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1592/2015 [09:33<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1593/2015 [09:34<02:32,  2.77it/s]\u001b[A\n",
            " 79% 1594/2015 [09:34<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1595/2015 [09:34<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1596/2015 [09:35<02:31,  2.77it/s]\u001b[A\n",
            " 79% 1597/2015 [09:35<02:30,  2.77it/s]\u001b[A\n",
            " 79% 1598/2015 [09:36<02:30,  2.77it/s]\u001b[A\n",
            " 79% 1599/2015 [09:36<02:30,  2.77it/s]\u001b[A\n",
            " 79% 1600/2015 [09:36<02:29,  2.77it/s]\u001b[A\n",
            " 79% 1601/2015 [09:37<02:29,  2.77it/s]\u001b[A\n",
            " 80% 1602/2015 [09:37<02:29,  2.77it/s]\u001b[A\n",
            " 80% 1603/2015 [09:37<02:28,  2.77it/s]\u001b[A\n",
            " 80% 1604/2015 [09:38<02:28,  2.77it/s]\u001b[A\n",
            " 80% 1605/2015 [09:38<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1606/2015 [09:38<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1607/2015 [09:39<02:27,  2.77it/s]\u001b[A\n",
            " 80% 1608/2015 [09:39<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1609/2015 [09:39<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1610/2015 [09:40<02:26,  2.77it/s]\u001b[A\n",
            " 80% 1611/2015 [09:40<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1612/2015 [09:41<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1613/2015 [09:41<02:25,  2.77it/s]\u001b[A\n",
            " 80% 1614/2015 [09:41<02:24,  2.77it/s]\u001b[A\n",
            " 80% 1615/2015 [09:42<02:24,  2.77it/s]\u001b[A\n",
            " 80% 1616/2015 [09:42<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1617/2015 [09:42<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1618/2015 [09:43<02:23,  2.77it/s]\u001b[A\n",
            " 80% 1619/2015 [09:43<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1620/2015 [09:43<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1621/2015 [09:44<02:22,  2.77it/s]\u001b[A\n",
            " 80% 1622/2015 [09:44<02:21,  2.77it/s]\u001b[A\n",
            " 81% 1623/2015 [09:45<02:21,  2.77it/s]\u001b[A\n",
            " 81% 1624/2015 [09:45<02:21,  2.77it/s]\u001b[A\n",
            " 81% 1625/2015 [09:45<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1626/2015 [09:46<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1627/2015 [09:46<02:20,  2.77it/s]\u001b[A\n",
            " 81% 1628/2015 [09:46<02:19,  2.77it/s]\u001b[A\n",
            " 81% 1629/2015 [09:47<02:19,  2.77it/s]\u001b[A\n",
            " 81% 1630/2015 [09:47<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1631/2015 [09:47<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1632/2015 [09:48<02:18,  2.77it/s]\u001b[A\n",
            " 81% 1633/2015 [09:48<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1634/2015 [09:49<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1635/2015 [09:49<02:17,  2.77it/s]\u001b[A\n",
            " 81% 1636/2015 [09:49<02:16,  2.77it/s]\u001b[A\n",
            " 81% 1637/2015 [09:50<02:16,  2.77it/s]\u001b[A\n",
            " 81% 1638/2015 [09:50<02:16,  2.77it/s]\u001b[A\n",
            " 81% 1639/2015 [09:50<02:15,  2.77it/s]\u001b[A\n",
            " 81% 1640/2015 [09:51<02:15,  2.77it/s]\u001b[A\n",
            " 81% 1641/2015 [09:51<02:14,  2.77it/s]\u001b[A\n",
            " 81% 1642/2015 [09:51<02:14,  2.77it/s]\u001b[A\n",
            " 82% 1643/2015 [09:52<02:14,  2.77it/s]\u001b[A\n",
            " 82% 1644/2015 [09:52<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1645/2015 [09:52<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1646/2015 [09:53<02:13,  2.77it/s]\u001b[A\n",
            " 82% 1647/2015 [09:53<02:12,  2.77it/s]\u001b[A\n",
            " 82% 1648/2015 [09:54<02:12,  2.77it/s]\u001b[A\n",
            " 82% 1649/2015 [09:54<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1650/2015 [09:54<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1651/2015 [09:55<02:11,  2.77it/s]\u001b[A\n",
            " 82% 1652/2015 [09:55<02:10,  2.77it/s]\u001b[A\n",
            " 82% 1653/2015 [09:55<02:10,  2.77it/s]\u001b[A\n",
            " 82% 1654/2015 [09:56<02:10,  2.77it/s]\u001b[A\n",
            " 82% 1655/2015 [09:56<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1656/2015 [09:56<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1657/2015 [09:57<02:09,  2.77it/s]\u001b[A\n",
            " 82% 1658/2015 [09:57<02:08,  2.77it/s]\u001b[A\n",
            " 82% 1659/2015 [09:58<02:08,  2.77it/s]\u001b[A\n",
            " 82% 1660/2015 [09:58<02:08,  2.77it/s]\u001b[A\n",
            " 82% 1661/2015 [09:58<02:07,  2.77it/s]\u001b[A\n",
            " 82% 1662/2015 [09:59<02:07,  2.77it/s]\u001b[A\n",
            " 83% 1663/2015 [09:59<02:07,  2.77it/s]\u001b[A\n",
            " 83% 1664/2015 [09:59<02:06,  2.77it/s]\u001b[A\n",
            " 83% 1665/2015 [10:00<02:06,  2.77it/s]\u001b[A\n",
            " 83% 1666/2015 [10:00<02:05,  2.77it/s]\u001b[A\n",
            " 83% 1667/2015 [10:00<02:05,  2.77it/s]\u001b[A\n",
            " 83% 1668/2015 [10:01<02:05,  2.77it/s]\u001b[A\n",
            " 83% 1669/2015 [10:01<02:04,  2.77it/s]\u001b[A\n",
            " 83% 1670/2015 [10:01<02:04,  2.77it/s]\u001b[A\n",
            " 83% 1671/2015 [10:02<02:04,  2.77it/s]\u001b[A\n",
            " 83% 1672/2015 [10:02<02:03,  2.77it/s]\u001b[A\n",
            " 83% 1673/2015 [10:03<02:03,  2.77it/s]\u001b[A\n",
            " 83% 1674/2015 [10:03<02:02,  2.77it/s]\u001b[A\n",
            " 83% 1675/2015 [10:03<02:02,  2.77it/s]\u001b[A\n",
            " 83% 1676/2015 [10:04<02:02,  2.77it/s]\u001b[A\n",
            " 83% 1677/2015 [10:04<02:01,  2.77it/s]\u001b[A\n",
            " 83% 1678/2015 [10:04<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1679/2015 [10:05<02:01,  2.78it/s]\u001b[A\n",
            " 83% 1680/2015 [10:05<02:00,  2.78it/s]\u001b[A\n",
            " 83% 1681/2015 [10:05<02:00,  2.77it/s]\u001b[A\n",
            " 83% 1682/2015 [10:06<02:00,  2.77it/s]\u001b[A\n",
            " 84% 1683/2015 [10:06<01:59,  2.77it/s]\u001b[A\n",
            " 84% 1684/2015 [10:07<01:59,  2.77it/s]\u001b[A\n",
            " 84% 1685/2015 [10:07<01:58,  2.77it/s]\u001b[A\n",
            " 84% 1686/2015 [10:07<01:58,  2.77it/s]\u001b[A\n",
            " 84% 1687/2015 [10:08<01:58,  2.77it/s]\u001b[A\n",
            " 84% 1688/2015 [10:08<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1689/2015 [10:08<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1690/2015 [10:09<01:57,  2.77it/s]\u001b[A\n",
            " 84% 1691/2015 [10:09<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1692/2015 [10:09<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1693/2015 [10:10<01:56,  2.77it/s]\u001b[A\n",
            " 84% 1694/2015 [10:10<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1695/2015 [10:11<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1696/2015 [10:11<01:55,  2.77it/s]\u001b[A\n",
            " 84% 1697/2015 [10:11<01:54,  2.77it/s]\u001b[A\n",
            " 84% 1698/2015 [10:12<01:54,  2.77it/s]\u001b[A\n",
            " 84% 1699/2015 [10:12<01:54,  2.77it/s]\u001b[A\n",
            " 84% 1700/2015 [10:12<01:53,  2.77it/s]\u001b[A\n",
            " 84% 1701/2015 [10:13<01:53,  2.77it/s]\u001b[A\n",
            " 84% 1702/2015 [10:13<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1703/2015 [10:13<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1704/2015 [10:14<01:52,  2.77it/s]\u001b[A\n",
            " 85% 1705/2015 [10:14<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1706/2015 [10:14<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1707/2015 [10:15<01:51,  2.77it/s]\u001b[A\n",
            " 85% 1708/2015 [10:15<01:50,  2.77it/s]\u001b[A\n",
            " 85% 1709/2015 [10:16<01:50,  2.77it/s]\u001b[A\n",
            " 85% 1710/2015 [10:16<01:50,  2.77it/s]\u001b[A\n",
            " 85% 1711/2015 [10:16<01:49,  2.77it/s]\u001b[A\n",
            " 85% 1712/2015 [10:17<01:49,  2.77it/s]\u001b[A\n",
            " 85% 1713/2015 [10:17<01:49,  2.77it/s]\u001b[A\n",
            " 85% 1714/2015 [10:17<01:48,  2.77it/s]\u001b[A\n",
            " 85% 1715/2015 [10:18<01:48,  2.77it/s]\u001b[A\n",
            " 85% 1716/2015 [10:18<01:47,  2.77it/s]\u001b[A\n",
            " 85% 1717/2015 [10:18<01:47,  2.77it/s]\u001b[A\n",
            " 85% 1718/2015 [10:19<01:47,  2.77it/s]\u001b[A\n",
            " 85% 1719/2015 [10:19<01:46,  2.77it/s]\u001b[A\n",
            " 85% 1720/2015 [10:20<01:46,  2.77it/s]\u001b[A\n",
            " 85% 1721/2015 [10:20<01:46,  2.77it/s]\u001b[A\n",
            " 85% 1722/2015 [10:20<01:45,  2.77it/s]\u001b[A\n",
            " 86% 1723/2015 [10:21<01:45,  2.77it/s]\u001b[A\n",
            " 86% 1724/2015 [10:21<01:45,  2.77it/s]\u001b[A\n",
            " 86% 1725/2015 [10:21<01:44,  2.77it/s]\u001b[A\n",
            " 86% 1726/2015 [10:22<01:44,  2.77it/s]\u001b[A\n",
            " 86% 1727/2015 [10:22<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1728/2015 [10:22<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1729/2015 [10:23<01:43,  2.77it/s]\u001b[A\n",
            " 86% 1730/2015 [10:23<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1731/2015 [10:24<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1732/2015 [10:24<01:42,  2.77it/s]\u001b[A\n",
            " 86% 1733/2015 [10:24<01:41,  2.77it/s]\u001b[A\n",
            " 86% 1734/2015 [10:25<01:41,  2.77it/s]\u001b[A\n",
            " 86% 1735/2015 [10:25<01:41,  2.77it/s]\u001b[A\n",
            " 86% 1736/2015 [10:25<01:40,  2.77it/s]\u001b[A\n",
            " 86% 1737/2015 [10:26<01:40,  2.77it/s]\u001b[A\n",
            " 86% 1738/2015 [10:26<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1739/2015 [10:26<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1740/2015 [10:27<01:39,  2.77it/s]\u001b[A\n",
            " 86% 1741/2015 [10:27<01:38,  2.77it/s]\u001b[A\n",
            " 86% 1742/2015 [10:27<01:38,  2.77it/s]\u001b[A\n",
            " 87% 1743/2015 [10:28<01:38,  2.77it/s]\u001b[A\n",
            " 87% 1744/2015 [10:28<01:37,  2.77it/s]\u001b[A\n",
            " 87% 1745/2015 [10:29<01:37,  2.77it/s]\u001b[A\n",
            " 87% 1746/2015 [10:29<01:37,  2.77it/s]\u001b[A\n",
            " 87% 1747/2015 [10:29<01:36,  2.77it/s]\u001b[A\n",
            " 87% 1748/2015 [10:30<01:36,  2.77it/s]\u001b[A\n",
            " 87% 1749/2015 [10:30<01:35,  2.77it/s]\u001b[A\n",
            " 87% 1750/2015 [10:30<01:35,  2.77it/s]\u001b[A\n",
            " 87% 1751/2015 [10:31<01:35,  2.77it/s]\u001b[A\n",
            " 87% 1752/2015 [10:31<01:34,  2.77it/s]\u001b[A\n",
            " 87% 1753/2015 [10:31<01:34,  2.77it/s]\u001b[A\n",
            " 87% 1754/2015 [10:32<01:34,  2.77it/s]\u001b[A\n",
            " 87% 1755/2015 [10:32<01:33,  2.77it/s]\u001b[A\n",
            " 87% 1756/2015 [10:33<01:33,  2.77it/s]\u001b[A\n",
            " 87% 1757/2015 [10:33<01:33,  2.77it/s]\u001b[A\n",
            " 87% 1758/2015 [10:33<01:32,  2.77it/s]\u001b[A\n",
            " 87% 1759/2015 [10:34<01:32,  2.77it/s]\u001b[A\n",
            " 87% 1760/2015 [10:34<01:31,  2.77it/s]\u001b[A\n",
            " 87% 1761/2015 [10:34<01:31,  2.77it/s]\u001b[A\n",
            " 87% 1762/2015 [10:35<01:31,  2.77it/s]\u001b[A\n",
            " 87% 1763/2015 [10:35<01:30,  2.77it/s]\u001b[A\n",
            " 88% 1764/2015 [10:35<01:30,  2.77it/s]\u001b[A\n",
            " 88% 1765/2015 [10:36<01:30,  2.77it/s]\u001b[A\n",
            " 88% 1766/2015 [10:36<01:29,  2.77it/s]\u001b[A\n",
            " 88% 1767/2015 [10:36<01:29,  2.77it/s]\u001b[A\n",
            " 88% 1768/2015 [10:37<01:29,  2.77it/s]\u001b[A\n",
            " 88% 1769/2015 [10:37<01:28,  2.77it/s]\u001b[A\n",
            " 88% 1770/2015 [10:38<01:28,  2.77it/s]\u001b[A\n",
            " 88% 1771/2015 [10:38<01:28,  2.77it/s]\u001b[A\n",
            " 88% 1772/2015 [10:38<01:27,  2.77it/s]\u001b[A\n",
            " 88% 1773/2015 [10:39<01:27,  2.77it/s]\u001b[A\n",
            " 88% 1774/2015 [10:39<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1775/2015 [10:39<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1776/2015 [10:40<01:26,  2.77it/s]\u001b[A\n",
            " 88% 1777/2015 [10:40<01:25,  2.77it/s]\u001b[A\n",
            " 88% 1778/2015 [10:40<01:25,  2.77it/s]\u001b[A\n",
            " 88% 1779/2015 [10:41<01:25,  2.77it/s]\u001b[A\n",
            " 88% 1780/2015 [10:41<01:24,  2.77it/s]\u001b[A\n",
            " 88% 1781/2015 [10:42<01:24,  2.77it/s]\u001b[A\n",
            " 88% 1782/2015 [10:42<01:24,  2.77it/s]\u001b[A\n",
            " 88% 1783/2015 [10:42<01:23,  2.77it/s]\u001b[A\n",
            " 89% 1784/2015 [10:43<01:23,  2.77it/s]\u001b[A\n",
            " 89% 1785/2015 [10:43<01:22,  2.77it/s]\u001b[A\n",
            " 89% 1786/2015 [10:43<01:22,  2.77it/s]\u001b[A\n",
            " 89% 1787/2015 [10:44<01:22,  2.77it/s]\u001b[A\n",
            " 89% 1788/2015 [10:44<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1789/2015 [10:44<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1790/2015 [10:45<01:21,  2.77it/s]\u001b[A\n",
            " 89% 1791/2015 [10:45<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1792/2015 [10:46<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1793/2015 [10:46<01:20,  2.77it/s]\u001b[A\n",
            " 89% 1794/2015 [10:46<01:19,  2.77it/s]\u001b[A\n",
            " 89% 1795/2015 [10:47<01:19,  2.77it/s]\u001b[A\n",
            " 89% 1796/2015 [10:47<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1797/2015 [10:47<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1798/2015 [10:48<01:18,  2.77it/s]\u001b[A\n",
            " 89% 1799/2015 [10:48<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1800/2015 [10:48<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1801/2015 [10:49<01:17,  2.77it/s]\u001b[A\n",
            " 89% 1802/2015 [10:49<01:16,  2.77it/s]\u001b[A\n",
            " 89% 1803/2015 [10:49<01:16,  2.77it/s]\u001b[A\n",
            " 90% 1804/2015 [10:50<01:16,  2.77it/s]\u001b[A\n",
            " 90% 1805/2015 [10:50<01:15,  2.77it/s]\u001b[A\n",
            " 90% 1806/2015 [10:51<01:15,  2.77it/s]\u001b[A\n",
            " 90% 1807/2015 [10:51<01:15,  2.77it/s]\u001b[A\n",
            " 90% 1808/2015 [10:51<01:14,  2.77it/s]\u001b[A\n",
            " 90% 1809/2015 [10:52<01:14,  2.77it/s]\u001b[A\n",
            " 90% 1810/2015 [10:52<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1811/2015 [10:52<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1812/2015 [10:53<01:13,  2.77it/s]\u001b[A\n",
            " 90% 1813/2015 [10:53<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1814/2015 [10:53<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1815/2015 [10:54<01:12,  2.77it/s]\u001b[A\n",
            " 90% 1816/2015 [10:54<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1817/2015 [10:55<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1818/2015 [10:55<01:11,  2.77it/s]\u001b[A\n",
            " 90% 1819/2015 [10:55<01:10,  2.77it/s]\u001b[A\n",
            " 90% 1820/2015 [10:56<01:10,  2.77it/s]\u001b[A\n",
            " 90% 1821/2015 [10:56<01:09,  2.77it/s]\u001b[A\n",
            " 90% 1822/2015 [10:56<01:09,  2.77it/s]\u001b[A\n",
            " 90% 1823/2015 [10:57<01:09,  2.77it/s]\u001b[A\n",
            " 91% 1824/2015 [10:57<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1825/2015 [10:57<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1826/2015 [10:58<01:08,  2.77it/s]\u001b[A\n",
            " 91% 1827/2015 [10:58<01:07,  2.77it/s]\u001b[A\n",
            " 91% 1828/2015 [10:58<01:07,  2.77it/s]\u001b[A\n",
            " 91% 1829/2015 [10:59<01:07,  2.77it/s]\u001b[A\n",
            " 91% 1830/2015 [10:59<01:06,  2.77it/s]\u001b[A\n",
            " 91% 1831/2015 [11:00<01:06,  2.77it/s]\u001b[A\n",
            " 91% 1832/2015 [11:00<01:06,  2.77it/s]\u001b[A\n",
            " 91% 1833/2015 [11:00<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1834/2015 [11:01<01:05,  2.77it/s]\u001b[A\n",
            " 91% 1835/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1836/2015 [11:01<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1837/2015 [11:02<01:04,  2.77it/s]\u001b[A\n",
            " 91% 1838/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1839/2015 [11:02<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1840/2015 [11:03<01:03,  2.77it/s]\u001b[A\n",
            " 91% 1841/2015 [11:03<01:02,  2.77it/s]\u001b[A\n",
            " 91% 1842/2015 [11:04<01:02,  2.77it/s]\u001b[A\n",
            " 91% 1843/2015 [11:04<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1844/2015 [11:04<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1845/2015 [11:05<01:01,  2.77it/s]\u001b[A\n",
            " 92% 1846/2015 [11:05<01:00,  2.77it/s]\u001b[A\n",
            " 92% 1847/2015 [11:05<01:00,  2.77it/s]\u001b[A\n",
            " 92% 1848/2015 [11:06<01:00,  2.77it/s]\u001b[A\n",
            " 92% 1849/2015 [11:06<00:59,  2.77it/s]\u001b[A\n",
            " 92% 1850/2015 [11:06<00:59,  2.77it/s]\u001b[A\n",
            " 92% 1851/2015 [11:07<00:59,  2.77it/s]\u001b[A\n",
            " 92% 1852/2015 [11:07<00:58,  2.77it/s]\u001b[A\n",
            " 92% 1853/2015 [11:07<00:58,  2.77it/s]\u001b[A\n",
            " 92% 1854/2015 [11:08<00:58,  2.77it/s]\u001b[A\n",
            " 92% 1855/2015 [11:08<00:57,  2.77it/s]\u001b[A\n",
            " 92% 1856/2015 [11:09<00:57,  2.77it/s]\u001b[A\n",
            " 92% 1857/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1858/2015 [11:09<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1859/2015 [11:10<00:56,  2.77it/s]\u001b[A\n",
            " 92% 1860/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1861/2015 [11:10<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1862/2015 [11:11<00:55,  2.77it/s]\u001b[A\n",
            " 92% 1863/2015 [11:11<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1864/2015 [11:11<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1865/2015 [11:12<00:54,  2.77it/s]\u001b[A\n",
            " 93% 1866/2015 [11:12<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1867/2015 [11:13<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1868/2015 [11:13<00:53,  2.77it/s]\u001b[A\n",
            " 93% 1869/2015 [11:13<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1870/2015 [11:14<00:52,  2.77it/s]\u001b[A\n",
            " 93% 1871/2015 [11:14<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1872/2015 [11:14<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1873/2015 [11:15<00:51,  2.77it/s]\u001b[A\n",
            " 93% 1874/2015 [11:15<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1875/2015 [11:15<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1876/2015 [11:16<00:50,  2.77it/s]\u001b[A\n",
            " 93% 1877/2015 [11:16<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1878/2015 [11:17<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1879/2015 [11:17<00:49,  2.77it/s]\u001b[A\n",
            " 93% 1880/2015 [11:17<00:48,  2.77it/s]\u001b[A\n",
            " 93% 1881/2015 [11:18<00:48,  2.77it/s]\u001b[A\n",
            " 93% 1882/2015 [11:18<00:47,  2.77it/s]\u001b[A\n",
            " 93% 1883/2015 [11:18<00:47,  2.77it/s]\u001b[A\n",
            " 93% 1884/2015 [11:19<00:47,  2.77it/s]\u001b[A\n",
            " 94% 1885/2015 [11:19<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1886/2015 [11:19<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1887/2015 [11:20<00:46,  2.77it/s]\u001b[A\n",
            " 94% 1888/2015 [11:20<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1889/2015 [11:20<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1890/2015 [11:21<00:45,  2.77it/s]\u001b[A\n",
            " 94% 1891/2015 [11:21<00:44,  2.77it/s]\u001b[A\n",
            " 94% 1892/2015 [11:22<00:44,  2.77it/s]\u001b[A\n",
            " 94% 1893/2015 [11:22<00:44,  2.77it/s]\u001b[A\n",
            " 94% 1894/2015 [11:22<00:43,  2.77it/s]\u001b[A\n",
            " 94% 1895/2015 [11:23<00:43,  2.77it/s]\u001b[A\n",
            " 94% 1896/2015 [11:23<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1897/2015 [11:23<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1898/2015 [11:24<00:42,  2.77it/s]\u001b[A\n",
            " 94% 1899/2015 [11:24<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1900/2015 [11:24<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1901/2015 [11:25<00:41,  2.77it/s]\u001b[A\n",
            " 94% 1902/2015 [11:25<00:40,  2.77it/s]\u001b[A\n",
            " 94% 1903/2015 [11:26<00:40,  2.77it/s]\u001b[A\n",
            " 94% 1904/2015 [11:26<00:40,  2.77it/s]\u001b[A\n",
            " 95% 1905/2015 [11:26<00:39,  2.77it/s]\u001b[A\n",
            " 95% 1906/2015 [11:27<00:39,  2.77it/s]\u001b[A\n",
            " 95% 1907/2015 [11:27<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1908/2015 [11:27<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1909/2015 [11:28<00:38,  2.77it/s]\u001b[A\n",
            " 95% 1910/2015 [11:28<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1911/2015 [11:28<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1912/2015 [11:29<00:37,  2.77it/s]\u001b[A\n",
            " 95% 1913/2015 [11:29<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1914/2015 [11:30<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1915/2015 [11:30<00:36,  2.77it/s]\u001b[A\n",
            " 95% 1916/2015 [11:30<00:35,  2.77it/s]\u001b[A\n",
            " 95% 1917/2015 [11:31<00:35,  2.77it/s]\u001b[A\n",
            " 95% 1918/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1919/2015 [11:31<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1920/2015 [11:32<00:34,  2.77it/s]\u001b[A\n",
            " 95% 1921/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1922/2015 [11:32<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1923/2015 [11:33<00:33,  2.77it/s]\u001b[A\n",
            " 95% 1924/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1925/2015 [11:33<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1926/2015 [11:34<00:32,  2.77it/s]\u001b[A\n",
            " 96% 1927/2015 [11:34<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1928/2015 [11:35<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1929/2015 [11:35<00:31,  2.77it/s]\u001b[A\n",
            " 96% 1930/2015 [11:35<00:30,  2.77it/s]\u001b[A\n",
            " 96% 1931/2015 [11:36<00:30,  2.77it/s]\u001b[A\n",
            " 96% 1932/2015 [11:36<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1933/2015 [11:36<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1934/2015 [11:37<00:29,  2.77it/s]\u001b[A\n",
            " 96% 1935/2015 [11:37<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1936/2015 [11:37<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1937/2015 [11:38<00:28,  2.77it/s]\u001b[A\n",
            " 96% 1938/2015 [11:38<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1939/2015 [11:39<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1940/2015 [11:39<00:27,  2.77it/s]\u001b[A\n",
            " 96% 1941/2015 [11:39<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1942/2015 [11:40<00:26,  2.77it/s]\u001b[A\n",
            " 96% 1943/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 96% 1944/2015 [11:40<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1945/2015 [11:41<00:25,  2.77it/s]\u001b[A\n",
            " 97% 1946/2015 [11:41<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1947/2015 [11:41<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1948/2015 [11:42<00:24,  2.77it/s]\u001b[A\n",
            " 97% 1949/2015 [11:42<00:23,  2.77it/s]\u001b[A\n",
            " 97% 1950/2015 [11:42<00:23,  2.77it/s]\u001b[A\n",
            " 97% 1951/2015 [11:43<00:23,  2.77it/s]\u001b[A\n",
            " 97% 1952/2015 [11:43<00:22,  2.77it/s]\u001b[A\n",
            " 97% 1953/2015 [11:44<00:22,  2.77it/s]\u001b[A\n",
            " 97% 1954/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1955/2015 [11:44<00:21,  2.78it/s]\u001b[A\n",
            " 97% 1956/2015 [11:45<00:21,  2.77it/s]\u001b[A\n",
            " 97% 1957/2015 [11:45<00:20,  2.77it/s]\u001b[A\n",
            " 97% 1958/2015 [11:45<00:20,  2.77it/s]\u001b[A\n",
            " 97% 1959/2015 [11:46<00:20,  2.77it/s]\u001b[A\n",
            " 97% 1960/2015 [11:46<00:19,  2.77it/s]\u001b[A\n",
            " 97% 1961/2015 [11:46<00:19,  2.77it/s]\u001b[A\n",
            " 97% 1962/2015 [11:47<00:19,  2.78it/s]\u001b[A\n",
            " 97% 1963/2015 [11:47<00:18,  2.78it/s]\u001b[A\n",
            " 97% 1964/2015 [11:48<00:18,  2.77it/s]\u001b[A\n",
            " 98% 1965/2015 [11:48<00:18,  2.77it/s]\u001b[A\n",
            " 98% 1966/2015 [11:48<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1967/2015 [11:49<00:17,  2.77it/s]\u001b[A\n",
            " 98% 1968/2015 [11:49<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1969/2015 [11:49<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1970/2015 [11:50<00:16,  2.77it/s]\u001b[A\n",
            " 98% 1971/2015 [11:50<00:15,  2.77it/s]\u001b[A\n",
            " 98% 1972/2015 [11:50<00:15,  2.77it/s]\u001b[A\n",
            " 98% 1973/2015 [11:51<00:15,  2.77it/s]\u001b[A\n",
            " 98% 1974/2015 [11:51<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1975/2015 [11:52<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1976/2015 [11:52<00:14,  2.77it/s]\u001b[A\n",
            " 98% 1977/2015 [11:52<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1978/2015 [11:53<00:13,  2.77it/s]\u001b[A\n",
            " 98% 1979/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1980/2015 [11:53<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1981/2015 [11:54<00:12,  2.77it/s]\u001b[A\n",
            " 98% 1982/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1983/2015 [11:54<00:11,  2.77it/s]\u001b[A\n",
            " 98% 1984/2015 [11:55<00:11,  2.77it/s]\u001b[A\n",
            " 99% 1985/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1986/2015 [11:55<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1987/2015 [11:56<00:10,  2.77it/s]\u001b[A\n",
            " 99% 1988/2015 [11:56<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1989/2015 [11:57<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1990/2015 [11:57<00:09,  2.77it/s]\u001b[A\n",
            " 99% 1991/2015 [11:57<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1992/2015 [11:58<00:08,  2.77it/s]\u001b[A\n",
            " 99% 1993/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1994/2015 [11:58<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1995/2015 [11:59<00:07,  2.77it/s]\u001b[A\n",
            " 99% 1996/2015 [11:59<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1997/2015 [11:59<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1998/2015 [12:00<00:06,  2.77it/s]\u001b[A\n",
            " 99% 1999/2015 [12:00<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2000/2015 [12:01<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2001/2015 [12:01<00:05,  2.77it/s]\u001b[A\n",
            " 99% 2002/2015 [12:01<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2003/2015 [12:02<00:04,  2.77it/s]\u001b[A\n",
            " 99% 2004/2015 [12:02<00:03,  2.77it/s]\u001b[A\n",
            "100% 2005/2015 [12:02<00:03,  2.77it/s]\u001b[A\n",
            "100% 2006/2015 [12:03<00:03,  2.77it/s]\u001b[A\n",
            "100% 2007/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2008/2015 [12:03<00:02,  2.77it/s]\u001b[A\n",
            "100% 2009/2015 [12:04<00:02,  2.77it/s]\u001b[A\n",
            "100% 2010/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2011/2015 [12:04<00:01,  2.77it/s]\u001b[A\n",
            "100% 2012/2015 [12:05<00:01,  2.77it/s]\u001b[A\n",
            "100% 2013/2015 [12:05<00:00,  2.77it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6645021438598633, 'eval_acc': 0.7760492519528664, 'eval_f1': 0.7760492519528664, 'eval_precision': 0.7760492519528664, 'eval_recall': 0.7760492519528664, 'eval_runtime': 726.5034, 'eval_samples_per_second': 41.585, 'eval_steps_per_second': 2.774, 'epoch': 40.0}\n",
            "100% 680/680 [8:25:34<00:00,  2.24s/it]\n",
            "100% 2015/2015 [12:06<00:00,  2.77it/s]\u001b[A\n",
            "                                       \u001b[ASaving model checkpoint to results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/adapter_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/config.json\n",
            "Model weights saved in results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-680/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442 (score: 0.6447117924690247).\n",
            "Loading best adapter(s) from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442 (score: 0.6447117924690247).\n",
            "Loading module configuration from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/adapter_config.json\n",
            "Overwriting existing adapter 'mlm'.\n",
            "Loading module weights from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/pytorch_adapter.bin\n",
            "Loading module configuration from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/head_config.json\n",
            "Overwriting existing head 'mlm'\n",
            "Adding head 'mlm' with config {'head_type': 'classification', 'num_labels': 5, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4}, 'use_pooler': False, 'bias': True}.\n",
            "Loading module weights from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/pytorch_model_head.bin\n",
            "Loading best adapter fusion(s) from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442 (score: 0.6447117924690247).\n",
            "Loading module configuration from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/adapter_fusion_config.json\n",
            "Overwriting existing adapter fusion module 'mlm'\n",
            "An AdapterFusion config has already been set and will NOT be overwritten\n",
            "Loading module weights from results/rct-sample_rct-sample_chemprot_rct_3/checkpoint-442/mlm/pytorch_model_adapter_fusion.bin\n",
            "{'train_runtime': 30348.3436, 'train_samples_per_second': 0.659, 'train_steps_per_second': 0.022, 'train_loss': 0.6345928416532629, 'epoch': 40.0}\n",
            "100% 680/680 [8:25:48<00:00, 44.63s/it]\n",
            "Saving model checkpoint to results/rct-sample_rct-sample_chemprot_rct_3/\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/adapter_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/head_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/rct-sample_rct-sample_chemprot_rct_3/mlm/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in results/rct-sample_rct-sample_chemprot_rct_3/config.json\n",
            "Model weights saved in results/rct-sample_rct-sample_chemprot_rct_3/pytorch_model.bin\n",
            "tokenizer config file saved in results/rct-sample_rct-sample_chemprot_rct_3/tokenizer_config.json\n",
            "Special tokens file saved in results/rct-sample_rct-sample_chemprot_rct_3/special_tokens_map.json\n",
            "08/04/2021 05:19:16 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30212\n",
            "  Batch size = 15\n",
            "100% 2015/2015 [12:06<00:00,  2.77it/s]\n",
            "08/04/2021 05:31:22 - INFO - __main__ - ***** Eval results *****\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_loss = 0.6447117924690247\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_acc = 0.7751886667549318\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_f1 = 0.7751886667549318\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_precision = 0.7751886667549318\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_recall = 0.7751886667549318\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_runtime = 726.6018\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_samples_per_second = 41.58\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   eval_steps_per_second = 2.773\n",
            "08/04/2021 05:31:22 - INFO - __main__ -   epoch = 40.0\n",
            "08/04/2021 05:31:22 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30135\n",
            "  Batch size = 15\n",
            "100% 2009/2009 [12:04<00:00,  2.77it/s]\n",
            "08/04/2021 05:43:27 - INFO - __main__ - ***** Test results {} *****\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_loss = 0.6720022559165955\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_acc = 0.7674133067861291\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_f1 = 0.7674133067861291\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_precision = 0.7674133067861291\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_recall = 0.7674133067861291\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_runtime = 724.6452\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_samples_per_second = 41.586\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   eval_steps_per_second = 2.772\n",
            "08/04/2021 05:43:27 - INFO - __main__ -   epoch = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}