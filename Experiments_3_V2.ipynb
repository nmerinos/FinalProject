{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of tapos_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhLmNzrS-GgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0289bbed-9f1c-46ad-c89f-6cdfd39a7629"
      },
      "source": [
        "!pip install -U adapter-transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-2.1.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 15.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, adapter-transformers\n",
            "Successfully installed adapter-transformers-2.1.2 huggingface-hub-0.0.15 sacremoses-0.0.45 tokenizers-0.10.3\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.15)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 24.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting tqdm>=4.42\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.7.0 tqdm-4.62.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k1927vw5um6d"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def jsonl2txt(path):\n",
        "    df = pd.DataFrame()\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            df = df.append(json.loads(line), ignore_index=True)\n",
        "    df.text.to_csv(path.replace(\"jsonl\", \"txt\"), header=False, index=False)\n",
        "jsonl2txt('../data/ag_train.jsonl')\n",
        "jsonl2txt('../data/ag_dev.jsonl')\n",
        "jsonl2txt('../data/ag_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JyjZPE-Eum6e"
      },
      "source": [
        "jsonl2txt('../data/hyperpartisan_news_dev.jsonl')\n",
        "jsonl2txt('../data/hyperpartisan_news_train.jsonl')\n",
        "jsonl2txt('../data/hyperpartisan_news_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "MjOMvFYSum6e"
      },
      "source": [
        "jsonl2txt('../data/rct-sample_dev.jsonl')\n",
        "jsonl2txt('../data/rct-sample_train.jsonl')\n",
        "jsonl2txt('../data/rct-sample_test.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "k3tNjTt1um6f"
      },
      "source": [
        "jsonl2txt('../data/chemprot_dev.jsonl')\n",
        "jsonl2txt('../data/chemprot_test.jsonl')\n",
        "jsonl2txt('../data/chemprot_train.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "RCws4UK1um6f"
      },
      "source": [
        "jsonl2txt('../data/sciie_dev.jsonl')\n",
        "jsonl2txt('../data/sciie_test.jsonl')\n",
        "jsonl2txt('../data/sciie_train.jsonl')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJa7VcLOJyEC"
      },
      "source": [
        "**V2-1: Creating new adapter using hyperpartisan news dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uQxrEYnwm3M",
        "outputId": "086222e4-819a-445b-ae8a-9ff17404ea13"
      },
      "source": [
        "!python3 run_mlm.py \\\n",
        "--train_file data/hyperpartisan_news_train.txt \\\n",
        "--line_by_line \\\n",
        "--validation_file data/hyperpartisan_news_dev.txt \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--mlm_probability 0.15 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir results/adapters/hyperpartisan_news \\\n",
        "--train_adapter \\\n",
        "--num_train_epochs 100 \\\n",
        "--learning_rate 1e-4 \\\n",
        "--logging_steps 50 \\\n",
        "--per_gpu_train_batch_size 8 \\\n",
        "--per_gpu_eval_batch_size 8 \\\n",
        "--gradient_accumulation_steps 8  \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 07:30:51.133206: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 07:30:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/02/2021 07:30:52 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=50,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=8,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=results/adapters/hyperpartisan_news/runs/Aug02_07-30-52_748c09106651,\n",
            "logging_first_step=False,\n",
            "logging_steps=50,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "output_dir=results/adapters/hyperpartisan_news,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=hyperpartisan_news,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=results/adapters/hyperpartisan_news,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/02/2021 07:30:52 - WARNING - datasets.builder -   Using custom data configuration default-e3a6b820522218be\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-e3a6b820522218be/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-e3a6b820522218be/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1590] 2021-08-02 07:30:53,135 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjex9xp7s\n",
            "Downloading: 100% 481/481 [00:00<00:00, 492kB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 07:30:53,420 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|file_utils.py:1602] 2021-08-02 07:30:53,420 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:531] 2021-08-02 07:30:53,420 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-02 07:30:53,421 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:427] 2021-08-02 07:30:53,704 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:531] 2021-08-02 07:30:53,985 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:569] 2021-08-02 07:30:53,986 >> Model config RobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-08-02 07:30:54,261 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9pse3gq1\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 4.67MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 07:30:54,735 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1602] 2021-08-02 07:30:54,735 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1590] 2021-08-02 07:30:55,021 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzcc55vap\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.53MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 07:30:55,436 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1602] 2021-08-02 07:30:55,437 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1590] 2021-08-02 07:30:55,859 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcce64tdn\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 5.64MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 07:30:56,378 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|file_utils.py:1602] 2021-08-02 07:30:56,379 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-08-02 07:30:57,212 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1590] 2021-08-02 07:30:57,544 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjnk0mz3i\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 61.8MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-08-02 07:31:05,873 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|file_utils.py:1602] 2021-08-02 07:31:05,873 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1163] 2021-08-02 07:31:05,874 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:1349] 2021-08-02 07:31:07,597 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1358] 2021-08-02 07:31:07,598 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:260] 2021-08-02 07:31:07,607 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00,  1.21ba/s]\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00, 11.46ba/s]\n",
            "[INFO|trainer.py:547] 2021-08-02 07:31:20,515 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:760] 2021-08-02 07:31:20,522 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:760] 2021-08-02 07:31:20,522 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1199] 2021-08-02 07:31:20,528 >> ***** Running training *****\n",
            "[INFO|trainer.py:1200] 2021-08-02 07:31:20,528 >>   Num examples = 516\n",
            "[INFO|trainer.py:1201] 2021-08-02 07:31:20,528 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1202] 2021-08-02 07:31:20,528 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1203] 2021-08-02 07:31:20,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1204] 2021-08-02 07:31:20,529 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1205] 2021-08-02 07:31:20,529 >>   Total optimization steps = 800\n",
            "[WARNING|training_args.py:760] 2021-08-02 07:31:20,534 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:774] 2021-08-02 07:31:20,534 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 1.2231, 'learning_rate': 9.375e-05, 'epoch': 6.25}\n",
            "{'loss': 1.2096, 'learning_rate': 8.75e-05, 'epoch': 12.49}\n",
            "{'loss': 1.1839, 'learning_rate': 8.125000000000001e-05, 'epoch': 18.74}\n",
            "{'loss': 1.1655, 'learning_rate': 7.500000000000001e-05, 'epoch': 24.98}\n",
            "{'loss': 1.1635, 'learning_rate': 6.875e-05, 'epoch': 31.25}\n",
            "{'loss': 1.1506, 'learning_rate': 6.25e-05, 'epoch': 37.49}\n",
            "{'loss': 1.1497, 'learning_rate': 5.6250000000000005e-05, 'epoch': 43.74}\n",
            "{'loss': 1.1394, 'learning_rate': 5e-05, 'epoch': 49.98}\n",
            "{'loss': 1.1349, 'learning_rate': 4.375e-05, 'epoch': 56.25}\n",
            "{'loss': 1.1319, 'learning_rate': 3.7500000000000003e-05, 'epoch': 62.49}\n",
            " 62% 500/800 [55:46<33:35,  6.72s/it][INFO|trainer.py:1989] 2021-08-02 08:27:07,183 >> Saving model checkpoint to results/adapters/hyperpartisan_news/checkpoint-500\n",
            "[INFO|loading.py:59] 2021-08-02 08:27:07,184 >> Configuration saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 08:27:07,196 >> Module weights saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-02 08:27:07,198 >> Configuration saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 08:27:07,488 >> Module weights saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-02 08:27:07,494 >> Configuration saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 08:27:07,885 >> Module weights saved in results/adapters/hyperpartisan_news/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-02 08:27:07,886 >> tokenizer config file saved in results/adapters/hyperpartisan_news/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-02 08:27:07,886 >> Special tokens file saved in results/adapters/hyperpartisan_news/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.1275, 'learning_rate': 3.125e-05, 'epoch': 68.74}\n",
            "{'loss': 1.1324, 'learning_rate': 2.5e-05, 'epoch': 74.98}\n",
            "{'loss': 1.1274, 'learning_rate': 1.8750000000000002e-05, 'epoch': 81.25}\n",
            "{'loss': 1.1142, 'learning_rate': 1.25e-05, 'epoch': 87.49}\n",
            "{'loss': 1.1258, 'learning_rate': 6.25e-06, 'epoch': 93.74}\n",
            "{'loss': 1.1174, 'learning_rate': 0.0, 'epoch': 99.98}\n",
            "100% 800/800 [1:29:24<00:00,  6.69s/it][INFO|trainer.py:1403] 2021-08-02 09:00:44,568 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5364.0398, 'train_samples_per_second': 9.62, 'train_steps_per_second': 0.149, 'train_loss': 1.1498042917251587, 'epoch': 99.98}\n",
            "100% 800/800 [1:29:24<00:00,  6.71s/it]\n",
            "[INFO|trainer.py:1989] 2021-08-02 09:00:44,570 >> Saving model checkpoint to results/adapters/hyperpartisan_news\n",
            "[INFO|loading.py:59] 2021-08-02 09:00:44,571 >> Configuration saved in results/adapters/hyperpartisan_news/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 09:00:44,581 >> Module weights saved in results/adapters/hyperpartisan_news/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-08-02 09:00:44,581 >> Configuration saved in results/adapters/hyperpartisan_news/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 09:00:44,879 >> Module weights saved in results/adapters/hyperpartisan_news/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-08-02 09:00:44,886 >> Configuration saved in results/adapters/hyperpartisan_news/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-08-02 09:00:45,284 >> Module weights saved in results/adapters/hyperpartisan_news/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-08-02 09:00:45,284 >> tokenizer config file saved in results/adapters/hyperpartisan_news/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-08-02 09:00:45,285 >> Special tokens file saved in results/adapters/hyperpartisan_news/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      99.98\n",
            "  train_loss               =     1.1498\n",
            "  train_runtime            = 1:29:24.03\n",
            "  train_samples            =        516\n",
            "  train_samples_per_second =       9.62\n",
            "  train_steps_per_second   =      0.149\n",
            "08/02/2021 09:00:45 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:547] 2021-08-02 09:00:45,380 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[WARNING|training_args.py:774] 2021-08-02 09:00:45,383 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2239] 2021-08-02 09:00:45,383 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2241] 2021-08-02 09:00:45,383 >>   Num examples = 64\n",
            "[INFO|trainer.py:2244] 2021-08-02 09:00:45,383 >>   Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  2.42it/s][WARNING|training_args.py:774] 2021-08-02 09:00:48,739 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 8/8 [00:03<00:00,  2.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      99.98\n",
            "  eval_loss               =     1.0274\n",
            "  eval_runtime            = 0:00:03.35\n",
            "  eval_samples            =         64\n",
            "  eval_samples_per_second =     19.072\n",
            "  eval_steps_per_second   =      2.384\n",
            "  perplexity              =     2.7938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dg143vWGIa0",
        "outputId": "14fa875f-a4a3-418c-df63-ad48adbb399b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/dl_final_project/gatech_deep_final\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lIPHKpcGg4g",
        "outputId": "564abfad-cb27-4461-dc3b-1b813027847f"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/dl_final_project/gatech_deep_final\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dl_final_project/gatech_deep_final\n",
            "'~'\t       run_mlm.py\n",
            " data\t       run_multiple_choice_downstream_task.py\n",
            " experiments   run_multiple_choice.py\n",
            " __pycache__   tapos_test.ipynb\n",
            " README.md     tapos-training\n",
            " results       utils_multiple_choice.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IjsYQbQKBNo"
      },
      "source": [
        "V2-2: Use new adapter hyperpartisan on hyperpartisan dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9KEN7NMG8mK",
        "outputId": "cabc0bec-0233-4db3-bc7c-7c947c9b3618"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 09:07:05.115631: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 09:07:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1950\n",
            "{'loss': 0.6411, 'learning_rate': 1.4871794871794874e-05, 'epoch': 7.69}\n",
            "{'loss': 0.5857, 'learning_rate': 9.743589743589744e-06, 'epoch': 15.38}\n",
            "{'loss': 0.5437, 'learning_rate': 4.615384615384616e-06, 'epoch': 23.08}\n",
            "100% 1950/1950 [25:36<00:00,  1.45it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1536.2842, 'train_samples_per_second': 10.076, 'train_steps_per_second': 1.269, 'train_loss': 0.574947282840044, 'epoch': 30.0}\n",
            "100% 1950/1950 [25:36<00:00,  1.27it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter/special_tokens_map.json\n",
            "08/02/2021 09:32:53 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.21it/s]\n",
            "08/02/2021 09:32:56 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_loss = 0.5679441094398499\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_acc = 0.6875\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_f1 = 0.6875\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_precision = 0.6875\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_recall = 0.6875\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_runtime = 2.826\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_samples_per_second = 22.647\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   eval_steps_per_second = 2.831\n",
            "08/02/2021 09:32:56 - INFO - __main__ -   epoch = 30.0\n",
            "08/02/2021 09:32:56 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.51it/s]\n",
            "08/02/2021 09:32:59 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_loss = 0.5332592725753784\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_acc = 0.7230769230769231\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_f1 = 0.723076923076923\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_precision = 0.7230769230769231\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_recall = 0.7230769230769231\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_runtime = 2.9262\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_samples_per_second = 22.213\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   eval_steps_per_second = 3.076\n",
            "08/02/2021 09:32:59 - INFO - __main__ -   epoch = 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBI80cOLBJwe",
        "outputId": "5c5e51db-a566-43ab-e868-ab3d174da99c"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter3/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 18:01:59.173418: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 18:02:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/02/2021 18:02:03 - INFO - filelock - Lock 139966987825616 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "Downloading: 100% 481/481 [00:00<00:00, 518kB/s]\n",
            "08/02/2021 18:02:03 - INFO - filelock - Lock 139966987825616 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "08/02/2021 18:02:04 - INFO - filelock - Lock 139966987740944 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 4.38MB/s]\n",
            "08/02/2021 18:02:04 - INFO - filelock - Lock 139966987740944 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "08/02/2021 18:02:05 - INFO - filelock - Lock 139966987737424 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 2.31MB/s]\n",
            "08/02/2021 18:02:05 - INFO - filelock - Lock 139966987737424 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "08/02/2021 18:02:05 - INFO - filelock - Lock 139966987740944 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 5.05MB/s]\n",
            "08/02/2021 18:02:06 - INFO - filelock - Lock 139966987740944 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "08/02/2021 18:02:07 - INFO - filelock - Lock 139966987736912 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 60.5MB/s]\n",
            "08/02/2021 18:02:16 - INFO - filelock - Lock 139966987736912 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3870\n",
            "{'loss': 0.6518, 'learning_rate': 1.7416020671834626e-05, 'epoch': 3.88}\n",
            "{'loss': 0.6021, 'learning_rate': 1.4832041343669253e-05, 'epoch': 7.75}\n",
            "{'loss': 0.5464, 'learning_rate': 1.2248062015503876e-05, 'epoch': 11.63}\n",
            "{'loss': 0.5219, 'learning_rate': 9.664082687338502e-06, 'epoch': 15.5}\n",
            "{'loss': 0.4856, 'learning_rate': 7.080103359173127e-06, 'epoch': 19.38}\n",
            "{'loss': 0.4554, 'learning_rate': 4.4961240310077525e-06, 'epoch': 23.26}\n",
            "{'loss': 0.4394, 'learning_rate': 1.9121447028423773e-06, 'epoch': 27.13}\n",
            "100% 3870/3870 [27:22<00:00,  2.30it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1642.443, 'train_samples_per_second': 9.425, 'train_steps_per_second': 2.356, 'train_loss': 0.5197704305328448, 'epoch': 30.0}\n",
            "100% 3870/3870 [27:22<00:00,  2.36it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter3/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter3/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter3/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter3/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter3/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter3/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter3/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter3/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter3/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter3/special_tokens_map.json\n",
            "08/02/2021 18:29:54 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.17it/s]\n",
            "08/02/2021 18:29:57 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_loss = 0.5466850399971008\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_acc = 0.765625\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_f1 = 0.765625\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_precision = 0.765625\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_recall = 0.765625\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_runtime = 2.8616\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_samples_per_second = 22.365\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   eval_steps_per_second = 2.796\n",
            "08/02/2021 18:29:57 - INFO - __main__ -   epoch = 30.0\n",
            "08/02/2021 18:29:57 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.46it/s]\n",
            "08/02/2021 18:30:00 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_loss = 0.5211206674575806\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_acc = 0.7538461538461538\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_f1 = 0.7538461538461538\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_precision = 0.7538461538461538\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_recall = 0.7538461538461538\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_runtime = 2.9632\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_samples_per_second = 21.936\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   eval_steps_per_second = 3.037\n",
            "08/02/2021 18:30:00 - INFO - __main__ -   epoch = 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR3dUl5xKNVS"
      },
      "source": [
        "V2-3: Tuning new adapter hyperpartisan on hyperpartisan dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gkU4SfYJ8dC",
        "outputId": "3f5a545a-8935-47e8-861b-a7f2d58cc530"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 45 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter4/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 18:38:23.538676: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 18:38:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5805\n",
            "{'loss': 0.6528, 'learning_rate': 1.827734711455642e-05, 'epoch': 3.88}\n",
            "{'loss': 0.6005, 'learning_rate': 1.6554694229112837e-05, 'epoch': 7.75}\n",
            "{'loss': 0.5404, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
            "{'loss': 0.5013, 'learning_rate': 1.3109388458225669e-05, 'epoch': 15.5}\n",
            "{'loss': 0.4507, 'learning_rate': 1.1386735572782086e-05, 'epoch': 19.38}\n",
            "{'loss': 0.4019, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
            "{'loss': 0.3836, 'learning_rate': 7.941429801894918e-06, 'epoch': 27.13}\n",
            "{'loss': 0.3686, 'learning_rate': 6.218776916451336e-06, 'epoch': 31.01}\n",
            "{'loss': 0.3515, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
            "{'loss': 0.3414, 'learning_rate': 2.773471145564169e-06, 'epoch': 38.76}\n",
            "{'loss': 0.3308, 'learning_rate': 1.0508182601205856e-06, 'epoch': 42.64}\n",
            "100% 5805/5805 [41:05<00:00,  2.35it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2465.7973, 'train_samples_per_second': 9.417, 'train_steps_per_second': 2.354, 'train_loss': 0.44262419313731427, 'epoch': 45.0}\n",
            "100% 5805/5805 [41:05<00:00,  2.35it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter4/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter4/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter4/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter4/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter4/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter4/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter4/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter4/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter4/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter4/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter4/special_tokens_map.json\n",
            "08/02/2021 19:19:40 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.20it/s]\n",
            "08/02/2021 19:19:43 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_loss = 0.5527108907699585\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_acc = 0.78125\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_f1 = 0.78125\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_precision = 0.78125\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_recall = 0.78125\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_runtime = 2.831\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_samples_per_second = 22.607\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   eval_steps_per_second = 2.826\n",
            "08/02/2021 19:19:43 - INFO - __main__ -   epoch = 45.0\n",
            "08/02/2021 19:19:43 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.50it/s]\n",
            "08/02/2021 19:19:46 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_loss = 0.5941903591156006\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_acc = 0.8153846153846154\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_f1 = 0.8153846153846154\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_precision = 0.8153846153846154\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_recall = 0.8153846153846154\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_runtime = 2.9355\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_samples_per_second = 22.143\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   eval_steps_per_second = 3.066\n",
            "08/02/2021 19:19:46 - INFO - __main__ -   epoch = 45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ33Kox3Vu1I",
        "outputId": "f5671741-e8d9-4e0e-b48c-869323da4769"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 45 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter5/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 19:28:59.472598: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 19:29:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1935\n",
            "{'loss': 0.6344, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
            "{'loss': 0.5607, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
            "{'loss': 0.5057, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
            "100% 1935/1935 [40:15<00:00,  1.24s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2415.8555, 'train_samples_per_second': 9.612, 'train_steps_per_second': 0.801, 'train_loss': 0.5442752256565931, 'epoch': 45.0}\n",
            "100% 1935/1935 [40:15<00:00,  1.25s/it]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter5/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter5/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter5/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter5/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter5/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter5/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter5/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter5/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter5/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter5/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter5/special_tokens_map.json\n",
            "08/02/2021 20:09:26 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.20it/s]\n",
            "08/02/2021 20:09:29 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_loss = 0.5412304401397705\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_acc = 0.75\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_f1 = 0.75\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_precision = 0.75\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_recall = 0.75\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_runtime = 2.8236\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_samples_per_second = 22.666\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   eval_steps_per_second = 2.833\n",
            "08/02/2021 20:09:29 - INFO - __main__ -   epoch = 45.0\n",
            "08/02/2021 20:09:29 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.50it/s]\n",
            "08/02/2021 20:09:32 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_loss = 0.5108768343925476\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_acc = 0.7384615384615385\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_f1 = 0.7384615384615385\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_precision = 0.7384615384615385\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_recall = 0.7384615384615385\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_runtime = 2.9302\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_samples_per_second = 22.183\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   eval_steps_per_second = 3.072\n",
            "08/02/2021 20:09:32 - INFO - __main__ -   epoch = 45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhJwXiAWfNQn",
        "outputId": "93194fc0-7c8c-4e1b-b8f5-3b030fd66c6b"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter6/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 20:10:19.745583: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 20:10:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 860\n",
            "{'loss': 0.6389, 'learning_rate': 8.372093023255815e-06, 'epoch': 11.63}\n",
            "100% 860/860 [17:57<00:00,  1.26s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1077.1477, 'train_samples_per_second': 9.581, 'train_steps_per_second': 0.798, 'train_loss': 0.622131400884584, 'epoch': 20.0}\n",
            "100% 860/860 [17:57<00:00,  1.25s/it]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter6/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter6/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter6/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter6/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter6/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter6/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter6/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter6/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter6/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter6/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter6/special_tokens_map.json\n",
            "08/02/2021 20:28:28 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.19it/s]\n",
            "08/02/2021 20:28:31 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_loss = 0.624457836151123\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_acc = 0.59375\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_f1 = 0.59375\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_precision = 0.59375\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_recall = 0.59375\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_runtime = 2.8396\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_samples_per_second = 22.539\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   eval_steps_per_second = 2.817\n",
            "08/02/2021 20:28:31 - INFO - __main__ -   epoch = 20.0\n",
            "08/02/2021 20:28:31 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.47it/s]\n",
            "08/02/2021 20:28:33 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_loss = 0.6200564503669739\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_acc = 0.5846153846153846\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_f1 = 0.5846153846153846\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_precision = 0.5846153846153846\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_recall = 0.5846153846153846\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_runtime = 2.9529\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_samples_per_second = 22.012\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   eval_steps_per_second = 3.048\n",
            "08/02/2021 20:28:33 - INFO - __main__ -   epoch = 20.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pyCLzCekLFu",
        "outputId": "82632847-424e-4391-eca5-e0424565efcd"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter6/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 20:31:59.132446: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 20:32:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2580\n",
            "{'loss': 0.6533, 'learning_rate': 1.612403100775194e-05, 'epoch': 3.88}\n",
            "{'loss': 0.6087, 'learning_rate': 1.2248062015503876e-05, 'epoch': 7.75}\n",
            "{'loss': 0.5566, 'learning_rate': 8.372093023255815e-06, 'epoch': 11.63}\n",
            "{'loss': 0.5426, 'learning_rate': 4.4961240310077525e-06, 'epoch': 15.5}\n",
            "{'loss': 0.5281, 'learning_rate': 6.201550387596899e-07, 'epoch': 19.38}\n",
            "100% 2580/2580 [18:18<00:00,  2.34it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1098.7075, 'train_samples_per_second': 9.393, 'train_steps_per_second': 2.348, 'train_loss': 0.5760715987331183, 'epoch': 20.0}\n",
            "100% 2580/2580 [18:18<00:00,  2.35it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter7/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter7/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter7/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter7/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter7/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter7/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter7/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter7/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter7/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter7/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter7/special_tokens_map.json\n",
            "08/02/2021 20:50:28 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:02<00:00,  3.22it/s]\n",
            "08/02/2021 20:50:31 - INFO - __main__ - ***** Eval results *****\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_loss = 0.57426917552948\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_acc = 0.703125\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_f1 = 0.703125\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_precision = 0.703125\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_recall = 0.703125\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_runtime = 2.8151\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_samples_per_second = 22.734\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   eval_steps_per_second = 2.842\n",
            "08/02/2021 20:50:31 - INFO - __main__ -   epoch = 20.0\n",
            "08/02/2021 20:50:31 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  3.49it/s]\n",
            "08/02/2021 20:50:34 - INFO - __main__ - ***** Test results {} *****\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_loss = 0.5410309433937073\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_acc = 0.7076923076923077\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_f1 = 0.7076923076923077\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_precision = 0.7076923076923077\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_recall = 0.7076923076923077\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_runtime = 2.9379\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_samples_per_second = 22.125\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   eval_steps_per_second = 3.063\n",
            "08/02/2021 20:50:34 - INFO - __main__ -   epoch = 20.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0tYjMbG07xV",
        "outputId": "657ab20d-b315-40e6-bab9-bfa6eee13263"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/dl_final_project/gatech_deep_final\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/dl_final_project/gatech_deep_final\n",
            "'~'\t       run_mlm.py\n",
            " data\t       run_multiple_choice_downstream_task.py\n",
            " experiments   run_multiple_choice.py\n",
            " __pycache__   tapos_test.ipynb\n",
            " README.md     tapos-training\n",
            " results       utils_multiple_choice.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wExnK6GSBQmE"
      },
      "source": [
        "V2-Ag: New adapter hyperpartisan on ag news dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n8K2RpVpSBs",
        "outputId": "74da93f0-ad4d-440e-d855-1e6ae400a941"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/ag_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 2 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter9/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 21:45:52.630385: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/02/2021 21:45:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/02/2021 21:45:57 - INFO - filelock - Lock 140064468737872 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "Downloading: 100% 481/481 [00:00<00:00, 444kB/s]\n",
            "08/02/2021 21:45:58 - INFO - filelock - Lock 140064468737872 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "08/02/2021 21:45:59 - INFO - filelock - Lock 140064616791248 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 4.44MB/s]\n",
            "08/02/2021 21:45:59 - INFO - filelock - Lock 140064616791248 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "08/02/2021 21:45:59 - INFO - filelock - Lock 140064616791568 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.53MB/s]\n",
            "08/02/2021 21:46:00 - INFO - filelock - Lock 140064616791568 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "08/02/2021 21:46:00 - INFO - filelock - Lock 140064616842448 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 5.30MB/s]\n",
            "08/02/2021 21:46:01 - INFO - filelock - Lock 140064616842448 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "08/02/2021 21:46:02 - INFO - filelock - Lock 140064616882512 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 48.3MB/s]\n",
            "08/02/2021 21:46:12 - INFO - filelock - Lock 140064616882512 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 115000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 28750\n",
            "{'loss': 1.2754, 'learning_rate': 1.965217391304348e-05, 'epoch': 0.03}\n",
            "{'loss': 0.4226, 'learning_rate': 1.9304347826086957e-05, 'epoch': 0.07}\n",
            "{'loss': 0.3528, 'learning_rate': 1.8956521739130434e-05, 'epoch': 0.1}\n",
            "{'loss': 0.345, 'learning_rate': 1.8608695652173912e-05, 'epoch': 0.14}\n",
            "{'loss': 0.3357, 'learning_rate': 1.8260869565217393e-05, 'epoch': 0.17}\n",
            "{'loss': 0.324, 'learning_rate': 1.791304347826087e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3241, 'learning_rate': 1.756521739130435e-05, 'epoch': 0.24}\n",
            "{'loss': 0.336, 'learning_rate': 1.721739130434783e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3249, 'learning_rate': 1.6869565217391307e-05, 'epoch': 0.31}\n",
            "{'loss': 0.3421, 'learning_rate': 1.6521739130434785e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3451, 'learning_rate': 1.6173913043478262e-05, 'epoch': 0.38}\n",
            "{'loss': 0.3235, 'learning_rate': 1.582608695652174e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3296, 'learning_rate': 1.5478260869565217e-05, 'epoch': 0.45}\n",
            "{'loss': 0.3159, 'learning_rate': 1.5130434782608697e-05, 'epoch': 0.49}\n",
            "{'loss': 0.3262, 'learning_rate': 1.4782608695652174e-05, 'epoch': 0.52}\n",
            "{'loss': 0.3486, 'learning_rate': 1.4434782608695654e-05, 'epoch': 0.56}\n",
            "{'loss': 0.3169, 'learning_rate': 1.4086956521739133e-05, 'epoch': 0.59}\n",
            "{'loss': 0.3132, 'learning_rate': 1.373913043478261e-05, 'epoch': 0.63}\n",
            "{'loss': 0.3194, 'learning_rate': 1.3391304347826088e-05, 'epoch': 0.66}\n",
            "{'loss': 0.326, 'learning_rate': 1.3043478260869566e-05, 'epoch': 0.7}\n",
            "{'loss': 0.3091, 'learning_rate': 1.2695652173913045e-05, 'epoch': 0.73}\n",
            "{'loss': 0.3227, 'learning_rate': 1.2347826086956523e-05, 'epoch': 0.77}\n",
            "{'loss': 0.3256, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n",
            "{'loss': 0.3427, 'learning_rate': 1.1652173913043478e-05, 'epoch': 0.83}\n",
            "{'loss': 0.3074, 'learning_rate': 1.1304347826086957e-05, 'epoch': 0.87}\n",
            "{'loss': 0.3067, 'learning_rate': 1.0956521739130435e-05, 'epoch': 0.9}\n",
            "{'loss': 0.322, 'learning_rate': 1.0608695652173914e-05, 'epoch': 0.94}\n",
            "{'loss': 0.3423, 'learning_rate': 1.0260869565217393e-05, 'epoch': 0.97}\n",
            "{'loss': 0.3096, 'learning_rate': 9.913043478260871e-06, 'epoch': 1.01}\n",
            "{'loss': 0.3203, 'learning_rate': 9.565217391304349e-06, 'epoch': 1.04}\n",
            "{'loss': 0.3155, 'learning_rate': 9.217391304347826e-06, 'epoch': 1.08}\n",
            "{'loss': 0.31, 'learning_rate': 8.869565217391306e-06, 'epoch': 1.11}\n",
            "{'loss': 0.3164, 'learning_rate': 8.521739130434783e-06, 'epoch': 1.15}\n",
            "{'loss': 0.2952, 'learning_rate': 8.173913043478263e-06, 'epoch': 1.18}\n",
            "{'loss': 0.3371, 'learning_rate': 7.82608695652174e-06, 'epoch': 1.22}\n",
            "{'loss': 0.3016, 'learning_rate': 7.478260869565218e-06, 'epoch': 1.25}\n",
            "{'loss': 0.2966, 'learning_rate': 7.130434782608696e-06, 'epoch': 1.29}\n",
            "{'loss': 0.3188, 'learning_rate': 6.782608695652174e-06, 'epoch': 1.32}\n",
            "{'loss': 0.2975, 'learning_rate': 6.434782608695652e-06, 'epoch': 1.36}\n",
            "{'loss': 0.34, 'learning_rate': 6.086956521739132e-06, 'epoch': 1.39}\n",
            "{'loss': 0.3169, 'learning_rate': 5.739130434782609e-06, 'epoch': 1.43}\n",
            "{'loss': 0.3081, 'learning_rate': 5.391304347826088e-06, 'epoch': 1.46}\n",
            "{'loss': 0.302, 'learning_rate': 5.043478260869565e-06, 'epoch': 1.5}\n",
            "{'loss': 0.3204, 'learning_rate': 4.695652173913044e-06, 'epoch': 1.53}\n",
            "{'loss': 0.2995, 'learning_rate': 4.347826086956522e-06, 'epoch': 1.57}\n",
            "{'loss': 0.3079, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.6}\n",
            "{'loss': 0.3306, 'learning_rate': 3.6521739130434787e-06, 'epoch': 1.63}\n",
            "{'loss': 0.303, 'learning_rate': 3.3043478260869567e-06, 'epoch': 1.67}\n",
            "{'loss': 0.2952, 'learning_rate': 2.956521739130435e-06, 'epoch': 1.7}\n",
            "{'loss': 0.3298, 'learning_rate': 2.6086956521739132e-06, 'epoch': 1.74}\n",
            "{'loss': 0.3109, 'learning_rate': 2.2608695652173913e-06, 'epoch': 1.77}\n",
            "{'loss': 0.3291, 'learning_rate': 1.9130434782608697e-06, 'epoch': 1.81}\n",
            "{'loss': 0.2923, 'learning_rate': 1.565217391304348e-06, 'epoch': 1.84}\n",
            "{'loss': 0.3308, 'learning_rate': 1.2173913043478262e-06, 'epoch': 1.88}\n",
            "{'loss': 0.3029, 'learning_rate': 8.695652173913044e-07, 'epoch': 1.91}\n",
            "{'loss': 0.3178, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.95}\n",
            "{'loss': 0.3179, 'learning_rate': 1.7391304347826088e-07, 'epoch': 1.98}\n",
            "100% 28750/28750 [3:46:24<00:00,  2.12it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 13585.0267, 'train_samples_per_second': 16.93, 'train_steps_per_second': 2.116, 'train_loss': 0.3383432325280231, 'epoch': 2.0}\n",
            "100% 28750/28750 [3:46:24<00:00,  2.12it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter9/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter9/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter9/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter9/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter9/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter9/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter9/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter9/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter9/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter9/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter9/special_tokens_map.json\n",
            "08/03/2021 01:33:02 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 8\n",
            "100% 625/625 [02:04<00:00,  5.02it/s]\n",
            "08/03/2021 01:35:06 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_loss = 0.2811973989009857\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_acc = 0.9104\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_f1 = 0.9103999999999999\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_precision = 0.9104\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_recall = 0.9104\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_runtime = 124.6934\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_samples_per_second = 40.098\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   eval_steps_per_second = 5.012\n",
            "08/03/2021 01:35:06 - INFO - __main__ -   epoch = 2.0\n",
            "08/03/2021 01:35:06 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 7600\n",
            "  Batch size = 8\n",
            "100% 950/950 [03:09<00:00,  5.02it/s]\n",
            "08/03/2021 01:38:16 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_loss = 0.3070048391819\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_acc = 0.8997368421052632\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_f1 = 0.8997368421052632\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_precision = 0.8997368421052632\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_recall = 0.8997368421052632\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_runtime = 189.5867\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_samples_per_second = 40.087\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   eval_steps_per_second = 5.011\n",
            "08/03/2021 01:38:16 - INFO - __main__ -   epoch = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIuBAq1HReVY",
        "outputId": "7162f5b2-d34b-4f90-b5f4-41efcfde9378"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcDXLiyQRq_k",
        "outputId": "99d107f2-b9c0-4fa1-fbef-1b364b931a26"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/dl_final_project/gatech_deep_final\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dl_final_project/gatech_deep_final\n",
            "'~'\t       run_mlm.py\n",
            " data\t       run_multiple_choice_downstream_task.py\n",
            " experiments   run_multiple_choice.py\n",
            " __pycache__   tapos_test.ipynb\n",
            " README.md     tapos-training\n",
            " results       utils_multiple_choice.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k77husAUKj9G"
      },
      "source": [
        "V2-4: New adapter hyperpartisan on imdb dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGZistq9RwaU",
        "outputId": "47432513-a43a-4f8e-e775-a7509240418c"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/imdb_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 2 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter10/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 04:39:47.182260: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 04:39:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/03/2021 04:39:53 - INFO - filelock - Lock 140679781742288 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "Downloading: 100% 481/481 [00:00<00:00, 455kB/s]\n",
            "08/03/2021 04:39:53 - INFO - filelock - Lock 140679781742288 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
            "08/03/2021 04:39:54 - INFO - filelock - Lock 140679781741072 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 3.52MB/s]\n",
            "08/03/2021 04:39:55 - INFO - filelock - Lock 140679781741072 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
            "08/03/2021 04:39:55 - INFO - filelock - Lock 140679781723984 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 2.66MB/s]\n",
            "08/03/2021 04:39:56 - INFO - filelock - Lock 140679781723984 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "08/03/2021 04:39:56 - INFO - filelock - Lock 140679781764112 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 4.11MB/s]\n",
            "08/03/2021 04:39:57 - INFO - filelock - Lock 140679781764112 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n",
            "08/03/2021 04:39:58 - INFO - filelock - Lock 140679781644368 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 61.4MB/s]\n",
            "08/03/2021 04:40:06 - INFO - filelock - Lock 140679781644368 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 20000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5000\n",
            "{'loss': 0.685, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4134, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2255, 'learning_rate': 1.4e-05, 'epoch': 0.6}\n",
            "{'loss': 0.226, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n",
            "{'loss': 0.2035, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
            "{'loss': 0.2293, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.2}\n",
            "{'loss': 0.2088, 'learning_rate': 6e-06, 'epoch': 1.4}\n",
            "{'loss': 0.1974, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.6}\n",
            "{'loss': 0.2025, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.8}\n",
            "{'loss': 0.2281, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "100% 5000/5000 [21:38<00:00,  3.85it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1298.7705, 'train_samples_per_second': 30.798, 'train_steps_per_second': 3.85, 'train_loss': 0.2819547653198242, 'epoch': 2.0}\n",
            "100% 5000/5000 [21:38<00:00,  3.85it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter10/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter10/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter10/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter10/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter10/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter10/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter10/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter10/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter10/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter10/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter10/special_tokens_map.json\n",
            "08/03/2021 05:02:13 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 8\n",
            "100% 625/625 [01:06<00:00,  9.34it/s]\n",
            "08/03/2021 05:03:20 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_loss = 0.1982172131538391\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_acc = 0.9272\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_f1 = 0.9271999999999999\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_precision = 0.9272\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_recall = 0.9272\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_runtime = 67.0291\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_samples_per_second = 74.594\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   eval_steps_per_second = 9.324\n",
            "08/03/2021 05:03:20 - INFO - __main__ -   epoch = 2.0\n",
            "08/03/2021 05:03:20 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n",
            "100% 3125/3125 [05:35<00:00,  9.33it/s]\n",
            "08/03/2021 05:08:55 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_loss = 0.18865720927715302\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_acc = 0.92968\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_f1 = 0.92968\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_precision = 0.92968\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_recall = 0.92968\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_runtime = 335.2273\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_samples_per_second = 74.576\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   eval_steps_per_second = 9.322\n",
            "08/03/2021 05:08:55 - INFO - __main__ -   epoch = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFXvIA7mKuPx"
      },
      "source": [
        "**V2**-5: New adapter hyperpartisan on amazon dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnjaaiMtapHj",
        "outputId": "dc048998-cb04-4d92-8bec-f4b3f36f510e"
      },
      "source": [
        " !python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/amazon_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 2 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter11/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 05:09:59.831977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 05:10:01 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 115251\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 28814\n",
            "{'loss': 0.4449, 'learning_rate': 1.9652946484347888e-05, 'epoch': 0.03}\n",
            "{'loss': 0.4158, 'learning_rate': 1.9305892968695775e-05, 'epoch': 0.07}\n",
            "{'loss': 0.4187, 'learning_rate': 1.895883945304366e-05, 'epoch': 0.1}\n",
            "{'loss': 0.3906, 'learning_rate': 1.8611785937391548e-05, 'epoch': 0.14}\n",
            "{'loss': 0.4023, 'learning_rate': 1.826473242173943e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3873, 'learning_rate': 1.7917678906087318e-05, 'epoch': 0.21}\n",
            "{'loss': 0.4049, 'learning_rate': 1.7570625390435208e-05, 'epoch': 0.24}\n",
            "{'loss': 0.3857, 'learning_rate': 1.7223571874783095e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3838, 'learning_rate': 1.687651835913098e-05, 'epoch': 0.31}\n",
            "{'loss': 0.373, 'learning_rate': 1.6529464843478868e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3952, 'learning_rate': 1.618241132782675e-05, 'epoch': 0.38}\n",
            "{'loss': 0.3743, 'learning_rate': 1.5835357812174638e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3525, 'learning_rate': 1.5488304296522524e-05, 'epoch': 0.45}\n",
            "{'loss': 0.3697, 'learning_rate': 1.5141250780870411e-05, 'epoch': 0.49}\n",
            "{'loss': 0.3731, 'learning_rate': 1.4794197265218298e-05, 'epoch': 0.52}\n",
            "{'loss': 0.3649, 'learning_rate': 1.4447143749566184e-05, 'epoch': 0.56}\n",
            "{'loss': 0.357, 'learning_rate': 1.410009023391407e-05, 'epoch': 0.59}\n",
            "{'loss': 0.3598, 'learning_rate': 1.3753036718261956e-05, 'epoch': 0.62}\n",
            "{'loss': 0.3705, 'learning_rate': 1.3405983202609842e-05, 'epoch': 0.66}\n",
            "{'loss': 0.3529, 'learning_rate': 1.305892968695773e-05, 'epoch': 0.69}\n",
            "{'loss': 0.3568, 'learning_rate': 1.2711876171305617e-05, 'epoch': 0.73}\n",
            "{'loss': 0.3863, 'learning_rate': 1.2364822655653504e-05, 'epoch': 0.76}\n",
            "{'loss': 0.3489, 'learning_rate': 1.2017769140001389e-05, 'epoch': 0.8}\n",
            "{'loss': 0.345, 'learning_rate': 1.1670715624349276e-05, 'epoch': 0.83}\n",
            "{'loss': 0.3698, 'learning_rate': 1.1323662108697162e-05, 'epoch': 0.87}\n",
            "{'loss': 0.3614, 'learning_rate': 1.0976608593045049e-05, 'epoch': 0.9}\n",
            "{'loss': 0.3546, 'learning_rate': 1.0629555077392934e-05, 'epoch': 0.94}\n",
            "{'loss': 0.3408, 'learning_rate': 1.028250156174082e-05, 'epoch': 0.97}\n",
            "{'loss': 0.3547, 'learning_rate': 9.935448046088709e-06, 'epoch': 1.01}\n",
            "{'loss': 0.3415, 'learning_rate': 9.588394530436594e-06, 'epoch': 1.04}\n",
            "{'loss': 0.3514, 'learning_rate': 9.24134101478448e-06, 'epoch': 1.08}\n",
            "{'loss': 0.3656, 'learning_rate': 8.894287499132367e-06, 'epoch': 1.11}\n",
            "{'loss': 0.3629, 'learning_rate': 8.547233983480254e-06, 'epoch': 1.15}\n",
            "{'loss': 0.3439, 'learning_rate': 8.20018046782814e-06, 'epoch': 1.18}\n",
            "{'loss': 0.3625, 'learning_rate': 7.853126952176027e-06, 'epoch': 1.21}\n",
            "{'loss': 0.3698, 'learning_rate': 7.5060734365239125e-06, 'epoch': 1.25}\n",
            "{'loss': 0.3594, 'learning_rate': 7.159019920871799e-06, 'epoch': 1.28}\n",
            "{'loss': 0.3727, 'learning_rate': 6.811966405219686e-06, 'epoch': 1.32}\n",
            "{'loss': 0.3576, 'learning_rate': 6.4649128895675716e-06, 'epoch': 1.35}\n",
            "{'loss': 0.372, 'learning_rate': 6.117859373915458e-06, 'epoch': 1.39}\n",
            "{'loss': 0.356, 'learning_rate': 5.770805858263344e-06, 'epoch': 1.42}\n",
            "{'loss': 0.3529, 'learning_rate': 5.4237523426112314e-06, 'epoch': 1.46}\n",
            "{'loss': 0.3473, 'learning_rate': 5.076698826959118e-06, 'epoch': 1.49}\n",
            "{'loss': 0.359, 'learning_rate': 4.729645311307004e-06, 'epoch': 1.53}\n",
            "{'loss': 0.3544, 'learning_rate': 4.3825917956548905e-06, 'epoch': 1.56}\n",
            "{'loss': 0.3486, 'learning_rate': 4.035538280002777e-06, 'epoch': 1.6}\n",
            "{'loss': 0.3368, 'learning_rate': 3.6884847643506633e-06, 'epoch': 1.63}\n",
            "{'loss': 0.3592, 'learning_rate': 3.3414312486985495e-06, 'epoch': 1.67}\n",
            "{'loss': 0.3495, 'learning_rate': 2.9943777330464357e-06, 'epoch': 1.7}\n",
            "{'loss': 0.3623, 'learning_rate': 2.6473242173943228e-06, 'epoch': 1.74}\n",
            "{'loss': 0.3538, 'learning_rate': 2.300270701742209e-06, 'epoch': 1.77}\n",
            "{'loss': 0.3545, 'learning_rate': 1.953217186090095e-06, 'epoch': 1.8}\n",
            "{'loss': 0.364, 'learning_rate': 1.6061636704379816e-06, 'epoch': 1.84}\n",
            "{'loss': 0.358, 'learning_rate': 1.259110154785868e-06, 'epoch': 1.87}\n",
            "{'loss': 0.3524, 'learning_rate': 9.120566391337544e-07, 'epoch': 1.91}\n",
            "{'loss': 0.362, 'learning_rate': 5.650031234816409e-07, 'epoch': 1.94}\n",
            "{'loss': 0.3593, 'learning_rate': 2.1794960782952734e-07, 'epoch': 1.98}\n",
            "100% 28814/28814 [2:04:55<00:00,  4.61it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7495.1282, 'train_samples_per_second': 30.754, 'train_steps_per_second': 3.844, 'train_loss': 0.36625977261521364, 'epoch': 2.0}\n",
            "100% 28814/28814 [2:04:55<00:00,  3.84it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter11/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter11/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter11/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter11/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter11/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter11/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter11/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter11/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter11/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter11/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter11/special_tokens_map.json\n",
            "08/03/2021 07:15:44 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 8\n",
            "100% 625/625 [01:07<00:00,  9.29it/s]\n",
            "08/03/2021 07:16:51 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_loss = 0.3461466431617737\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_acc = 0.8572\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_f1 = 0.8572\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_precision = 0.8572\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_recall = 0.8572\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_runtime = 67.4071\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_samples_per_second = 74.176\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   eval_steps_per_second = 9.272\n",
            "08/03/2021 07:16:51 - INFO - __main__ -   epoch = 2.0\n",
            "08/03/2021 07:16:51 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n",
            "100% 3125/3125 [05:37<00:00,  9.26it/s]\n",
            "08/03/2021 07:22:29 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_loss = 0.3433789312839508\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_acc = 0.85548\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_f1 = 0.85548\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_precision = 0.85548\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_recall = 0.85548\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_runtime = 337.4349\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_samples_per_second = 74.088\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   eval_steps_per_second = 9.261\n",
            "08/03/2021 07:22:29 - INFO - __main__ -   epoch = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5H2EJee7Y3k",
        "outputId": "9a9e4716-61b7-43db-8512-d815d156a890"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/rct-sample_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter12/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 07:33:19.868497: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 07:33:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 126\n",
            "100% 126/126 [00:32<00:00,  4.46it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 32.4566, 'train_samples_per_second': 30.81, 'train_steps_per_second': 3.882, 'train_loss': 1.5355623033311632, 'epoch': 2.0}\n",
            "100% 126/126 [00:32<00:00,  3.89it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter12/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter12/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter12/special_tokens_map.json\n",
            "08/03/2021 07:34:11 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30212\n",
            "  Batch size = 8\n",
            "100% 3777/3777 [06:42<00:00,  9.38it/s]\n",
            "08/03/2021 07:40:54 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_loss = 1.5194153785705566\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_acc = 0.3257314974182444\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_f1 = 0.3257314974182444\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_precision = 0.3257314974182444\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_recall = 0.3257314974182444\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_runtime = 402.592\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_samples_per_second = 75.044\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   eval_steps_per_second = 9.382\n",
            "08/03/2021 07:40:54 - INFO - __main__ -   epoch = 2.0\n",
            "08/03/2021 07:40:54 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30135\n",
            "  Batch size = 8\n",
            "100% 3767/3767 [06:41<00:00,  9.37it/s]\n",
            "08/03/2021 07:47:36 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_loss = 1.5211257934570312\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_acc = 0.32231624357059896\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_f1 = 0.32231624357059896\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_precision = 0.32231624357059896\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_recall = 0.32231624357059896\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_runtime = 401.9752\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_samples_per_second = 74.967\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   eval_steps_per_second = 9.371\n",
            "08/03/2021 07:47:36 - INFO - __main__ -   epoch = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjYr61FaK84I"
      },
      "source": [
        "V2-5: New adapter hyperpartisan on rct_sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msdEt_Fk_Ar5",
        "outputId": "1a3a6119-a54f-4028-d20b-6aa52d291d4d"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/rct-sample_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter12/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 07:48:19.285803: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 07:48:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1890\n",
            "{'loss': 1.4486, 'learning_rate': 1.470899470899471e-05, 'epoch': 7.94}\n",
            "{'loss': 0.9948, 'learning_rate': 9.417989417989418e-06, 'epoch': 15.87}\n",
            "{'loss': 0.6689, 'learning_rate': 4.126984126984127e-06, 'epoch': 23.81}\n",
            "100% 1890/1890 [08:06<00:00,  4.46it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 486.3149, 'train_samples_per_second': 30.844, 'train_steps_per_second': 3.886, 'train_loss': 0.9474650065104167, 'epoch': 30.0}\n",
            "100% 1890/1890 [08:06<00:00,  3.89it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter12/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter12/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter12/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter12/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter12/special_tokens_map.json\n",
            "08/03/2021 07:56:41 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30212\n",
            "  Batch size = 8\n",
            "100% 3777/3777 [06:43<00:00,  9.37it/s]\n",
            "08/03/2021 08:03:25 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_loss = 0.674564778804779\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_acc = 0.7343108698530385\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_f1 = 0.7343108698530384\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_precision = 0.7343108698530385\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_recall = 0.7343108698530385\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_runtime = 403.1917\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_samples_per_second = 74.932\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   eval_steps_per_second = 9.368\n",
            "08/03/2021 08:03:25 - INFO - __main__ -   epoch = 30.0\n",
            "08/03/2021 08:03:25 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30135\n",
            "  Batch size = 8\n",
            "100% 3767/3767 [06:42<00:00,  9.36it/s]\n",
            "08/03/2021 08:10:07 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_loss = 0.6918657422065735\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_acc = 0.7289198606271777\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_f1 = 0.7289198606271778\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_precision = 0.7289198606271777\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_recall = 0.7289198606271777\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_runtime = 402.5701\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_samples_per_second = 74.857\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   eval_steps_per_second = 9.357\n",
            "08/03/2021 08:10:07 - INFO - __main__ -   epoch = 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWz6ICw5LLdC"
      },
      "source": [
        "V2-6: New adapter hyperpartisan on citation-intent dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOOI5HujHwBx",
        "outputId": "14b360a0-ee54-47ef-9c0b-cba385610c6e"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/citation-intent_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--output_dir results/hyperpartisan_news-new-adapter13/ \\\n",
        "--task_name mlm \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 08:33:20.841876: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 08:33:22 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1688\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6330\n",
            "{'loss': 1.336, 'learning_rate': 1.8420221169036335e-05, 'epoch': 2.37}\n",
            "{'loss': 1.188, 'learning_rate': 1.6840442338072673e-05, 'epoch': 4.74}\n",
            "{'loss': 1.0226, 'learning_rate': 1.5260663507109007e-05, 'epoch': 7.11}\n",
            "{'loss': 0.9503, 'learning_rate': 1.368088467614534e-05, 'epoch': 9.48}\n",
            "{'loss': 0.9114, 'learning_rate': 1.2101105845181676e-05, 'epoch': 11.85}\n",
            "{'loss': 0.886, 'learning_rate': 1.052132701421801e-05, 'epoch': 14.22}\n",
            "{'loss': 0.8515, 'learning_rate': 8.941548183254345e-06, 'epoch': 16.59}\n",
            "{'loss': 0.8352, 'learning_rate': 7.36176935229068e-06, 'epoch': 18.96}\n",
            "{'loss': 0.8155, 'learning_rate': 5.7819905213270145e-06, 'epoch': 21.33}\n",
            "{'loss': 0.8143, 'learning_rate': 4.20221169036335e-06, 'epoch': 23.7}\n",
            "{'loss': 0.7866, 'learning_rate': 2.6224328593996843e-06, 'epoch': 26.07}\n",
            "{'loss': 0.7949, 'learning_rate': 1.042654028436019e-06, 'epoch': 28.44}\n",
            "100% 6330/6330 [27:23<00:00,  3.84it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1643.137, 'train_samples_per_second': 30.819, 'train_steps_per_second': 3.852, 'train_loss': 0.9254023836687277, 'epoch': 30.0}\n",
            "100% 6330/6330 [27:23<00:00,  3.85it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news-new-adapter13/\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter13/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter13/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter13/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter13/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter13/mlm/head_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter13/mlm/pytorch_model_head.bin\n",
            "Configuration saved in results/hyperpartisan_news-new-adapter13/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news-new-adapter13/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news-new-adapter13/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news-new-adapter13/special_tokens_map.json\n",
            "08/03/2021 09:00:55 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 114\n",
            "  Batch size = 8\n",
            "100% 15/15 [00:01<00:00, 10.45it/s]\n",
            "08/03/2021 09:00:57 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_loss = 0.6833904981613159\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_acc = 0.7368421052631579\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_f1 = 0.7368421052631579\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_precision = 0.7368421052631579\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_recall = 0.7368421052631579\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_runtime = 1.541\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_samples_per_second = 73.979\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   eval_steps_per_second = 9.734\n",
            "08/03/2021 09:00:57 - INFO - __main__ -   epoch = 30.0\n",
            "08/03/2021 09:00:57 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 139\n",
            "  Batch size = 8\n",
            "100% 18/18 [00:01<00:00, 10.21it/s]\n",
            "08/03/2021 09:00:59 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_loss = 0.8098021745681763\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_acc = 0.697841726618705\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_f1 = 0.697841726618705\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_precision = 0.697841726618705\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_recall = 0.697841726618705\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_runtime = 1.87\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_samples_per_second = 74.331\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   eval_steps_per_second = 9.626\n",
            "08/03/2021 09:00:59 - INFO - __main__ -   epoch = 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pABNyo8QayLt"
      },
      "source": [
        "Best Performance with Macro F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ULvbzWcawqd",
        "outputId": "895d4ee2-e865-4048-8327-aac648dea30a"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 45 \\\n",
        "--output_dir results/hyperpartisan_news_adapterhub_adapters10/ \\\n",
        "--task_name hb3 \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/ag/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--seed 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 09:55:13.995554: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 09:55:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1935\n",
            "{'loss': 0.3113, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
            "{'loss': 0.0705, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
            "{'loss': 0.0361, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
            "100% 1935/1935 [17:25<00:00,  1.85it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1045.7096, 'train_samples_per_second': 22.205, 'train_steps_per_second': 1.85, 'train_loss': 0.11029264169145925, 'epoch': 45.0}\n",
            "100% 1935/1935 [17:25<00:00,  1.85it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters10/\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/multinli/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/multinli/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/qqp/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/qqp/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/scitail/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/scitail/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters10/multinli,qqp,scitail,mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters10/multinli,qqp,scitail,mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters10/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters10/special_tokens_map.json\n",
            "08/03/2021 10:13:04 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:01<00:00,  7.62it/s]\n",
            "08/03/2021 10:13:06 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_loss = 1.163870930671692\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_acc = 0.8125\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_f1 = 0.7922077922077922\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_precision = 0.8357487922705313\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_recall = 0.7813765182186234\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_runtime = 1.1967\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_samples_per_second = 53.481\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   eval_steps_per_second = 6.685\n",
            "08/03/2021 10:13:06 - INFO - __main__ -   epoch = 45.0\n",
            "08/03/2021 10:13:06 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:01<00:00,  8.32it/s]\n",
            "08/03/2021 10:13:07 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_loss = 0.2544547915458679\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_acc = 0.9692307692307692\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_f1 = 0.9679487179487178\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_precision = 0.975\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_recall = 0.962962962962963\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_runtime = 1.2325\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_samples_per_second = 52.738\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   eval_steps_per_second = 7.302\n",
            "08/03/2021 10:13:07 - INFO - __main__ -   epoch = 45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSwQeoDdhGrz"
      },
      "source": [
        "only new two adapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOuOog7pgny1",
        "outputId": "c9aada54-3ef7-4fbd-c9f4-30cbb45adb81"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 45 \\\n",
        "--output_dir results/hyperpartisan_news_adapterhub_adapters11/ \\\n",
        "--task_name hb3 \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/ag/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--seed 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 10:17:11.513196: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 10:17:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1935\n",
            "{'loss': 0.52, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
            "{'loss': 0.3279, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
            "{'loss': 0.2607, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
            "100% 1935/1935 [12:01<00:00,  2.69it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 721.561, 'train_samples_per_second': 32.18, 'train_steps_per_second': 2.682, 'train_loss': 0.33886110369857275, 'epoch': 45.0}\n",
            "100% 1935/1935 [12:01<00:00,  2.68it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters11/\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters11/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters11/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters11/special_tokens_map.json\n",
            "08/03/2021 10:29:24 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:00<00:00, 10.61it/s]\n",
            "08/03/2021 10:29:25 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_loss = 0.770103931427002\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_acc = 0.796875\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_f1 = 0.7772423025435075\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_precision = 0.8099415204678362\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_recall = 0.7682186234817814\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_runtime = 0.8587\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_samples_per_second = 74.536\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   eval_steps_per_second = 9.317\n",
            "08/03/2021 10:29:25 - INFO - __main__ -   epoch = 45.0\n",
            "08/03/2021 10:29:25 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:00<00:00, 11.63it/s]\n",
            "08/03/2021 10:29:26 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_loss = 0.5413658618927002\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_acc = 0.8615384615384616\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_f1 = 0.8526077097505669\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_precision = 0.8731501057082452\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_recall = 0.8440545808966862\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_runtime = 0.882\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_samples_per_second = 73.696\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   eval_steps_per_second = 10.204\n",
            "08/03/2021 10:29:26 - INFO - __main__ -   epoch = 45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0-I0RB9kCBe",
        "outputId": "9d34adfb-6109-4d91-8ec7-94419d502d68"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/hyperpartisan_news_ \\\n",
        "--max_seq_length 512 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 45 \\\n",
        "--output_dir results/hyperpartisan_news_adapterhub_adapters12/ \\\n",
        "--task_name hb3 \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/ag/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--seed 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 10:30:34.303826: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 10:30:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Overwriting existing adapter 'mlm'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 516\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1935\n",
            "{'loss': 0.52, 'learning_rate': 1.4832041343669253e-05, 'epoch': 11.63}\n",
            "{'loss': 0.3279, 'learning_rate': 9.664082687338502e-06, 'epoch': 23.26}\n",
            "{'loss': 0.2607, 'learning_rate': 4.4961240310077525e-06, 'epoch': 34.88}\n",
            "100% 1935/1935 [12:01<00:00,  2.68it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 721.2181, 'train_samples_per_second': 32.196, 'train_steps_per_second': 2.683, 'train_loss': 0.33886110369857275, 'epoch': 45.0}\n",
            "100% 1935/1935 [12:01<00:00,  2.68it/s]\n",
            "Saving model checkpoint to results/hyperpartisan_news_adapterhub_adapters12/\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/adapter_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/pytorch_adapter.bin\n",
            "Configuration saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/adapter_fusion_config.json\n",
            "Module weights saved in results/hyperpartisan_news_adapterhub_adapters12/mlm/pytorch_model_adapter_fusion.bin\n",
            "tokenizer config file saved in results/hyperpartisan_news_adapterhub_adapters12/tokenizer_config.json\n",
            "Special tokens file saved in results/hyperpartisan_news_adapterhub_adapters12/special_tokens_map.json\n",
            "08/03/2021 10:42:46 - INFO - __main__ - *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 64\n",
            "  Batch size = 8\n",
            "100% 8/8 [00:00<00:00, 10.59it/s]\n",
            "08/03/2021 10:42:47 - INFO - __main__ - ***** Eval results *****\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_loss = 0.770103931427002\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_acc = 0.796875\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_f1 = 0.796875\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_precision = 0.796875\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_recall = 0.796875\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_runtime = 0.8601\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_samples_per_second = 74.407\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   eval_steps_per_second = 9.301\n",
            "08/03/2021 10:42:47 - INFO - __main__ -   epoch = 45.0\n",
            "08/03/2021 10:42:47 - INFO - root - *** Test ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 65\n",
            "  Batch size = 8\n",
            "100% 9/9 [00:00<00:00, 11.65it/s]\n",
            "08/03/2021 10:42:48 - INFO - __main__ - ***** Test results {} *****\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_loss = 0.5413658618927002\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_acc = 0.8615384615384616\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_f1 = 0.8615384615384615\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_precision = 0.8615384615384616\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_recall = 0.8615384615384616\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_runtime = 0.8813\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_samples_per_second = 73.754\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   eval_steps_per_second = 10.212\n",
            "08/03/2021 10:42:48 - INFO - __main__ -   epoch = 45.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLJevII3rlEe"
      },
      "source": [
        "all adapters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJVlpRnarfj4",
        "outputId": "ee79bc08-01ab-423f-82ca-7f713ea8f312"
      },
      "source": [
        "!python3 run_multiple_choice.py \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--data_dir data/ag_ \\\n",
        "--max_seq_length 128 \\\n",
        "--per_device_train_batch_size 12 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 10 \\\n",
        "--output_dir results/hyperpartisan_news_adapterhub_adapters15/ \\\n",
        "--task_name hb3 \\\n",
        "--do_predict \\\n",
        "--model_name_or_path roberta-base \\\n",
        "--adapter_1 results/adapters/hyperpartisan_news/mlm \\\n",
        "--load_best_model_at_end \\\n",
        "--seed 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-03 17:25:22.528849: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/03/2021 17:25:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1055: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 115000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 95840\n",
            "{'loss': 0.581, 'learning_rate': 1.9895659432387312e-05, 'epoch': 0.05}\n",
            "{'loss': 0.3667, 'learning_rate': 1.9791318864774626e-05, 'epoch': 0.1}\n",
            "{'loss': 0.3254, 'learning_rate': 1.968697829716194e-05, 'epoch': 0.16}\n",
            "{'loss': 0.3323, 'learning_rate': 1.958263772954925e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3378, 'learning_rate': 1.9478297161936563e-05, 'epoch': 0.26}\n",
            "{'loss': 0.328, 'learning_rate': 1.9373956594323873e-05, 'epoch': 0.31}\n",
            "{'loss': 0.3297, 'learning_rate': 1.9269616026711187e-05, 'epoch': 0.37}\n",
            "{'loss': 0.314, 'learning_rate': 1.9165275459098497e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3313, 'learning_rate': 1.906093489148581e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2877, 'learning_rate': 1.8956594323873125e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2999, 'learning_rate': 1.8852253756260435e-05, 'epoch': 0.57}\n",
            "{'loss': 0.3069, 'learning_rate': 1.874791318864775e-05, 'epoch': 0.63}\n",
            "{'loss': 0.2943, 'learning_rate': 1.864357262103506e-05, 'epoch': 0.68}\n",
            "{'loss': 0.2893, 'learning_rate': 1.8539232053422373e-05, 'epoch': 0.73}\n",
            "{'loss': 0.2837, 'learning_rate': 1.8434891485809686e-05, 'epoch': 0.78}\n",
            "{'loss': 0.3118, 'learning_rate': 1.8330550918196996e-05, 'epoch': 0.83}\n",
            "{'loss': 0.2764, 'learning_rate': 1.8226210350584307e-05, 'epoch': 0.89}\n",
            "{'loss': 0.2959, 'learning_rate': 1.812186978297162e-05, 'epoch': 0.94}\n",
            "{'loss': 0.2902, 'learning_rate': 1.8017529215358934e-05, 'epoch': 0.99}\n",
            "{'loss': 0.2914, 'learning_rate': 1.7913188647746244e-05, 'epoch': 1.04}\n",
            "{'loss': 0.2816, 'learning_rate': 1.7808848080133558e-05, 'epoch': 1.1}\n",
            "{'loss': 0.2735, 'learning_rate': 1.7704507512520868e-05, 'epoch': 1.15}\n",
            "{'loss': 0.2824, 'learning_rate': 1.7600166944908182e-05, 'epoch': 1.2}\n",
            "{'loss': 0.2798, 'learning_rate': 1.7495826377295492e-05, 'epoch': 1.25}\n",
            "{'loss': 0.2687, 'learning_rate': 1.7391485809682806e-05, 'epoch': 1.3}\n",
            "{'loss': 0.2785, 'learning_rate': 1.728714524207012e-05, 'epoch': 1.36}\n",
            "{'loss': 0.2658, 'learning_rate': 1.718280467445743e-05, 'epoch': 1.41}\n",
            "{'loss': 0.2678, 'learning_rate': 1.7078464106844743e-05, 'epoch': 1.46}\n",
            "{'loss': 0.2534, 'learning_rate': 1.6974123539232054e-05, 'epoch': 1.51}\n",
            "{'loss': 0.2718, 'learning_rate': 1.6869782971619367e-05, 'epoch': 1.57}\n",
            "{'loss': 0.2588, 'learning_rate': 1.676544240400668e-05, 'epoch': 1.62}\n",
            "{'loss': 0.2718, 'learning_rate': 1.666110183639399e-05, 'epoch': 1.67}\n",
            "{'loss': 0.268, 'learning_rate': 1.65567612687813e-05, 'epoch': 1.72}\n",
            "{'loss': 0.2647, 'learning_rate': 1.6452420701168615e-05, 'epoch': 1.77}\n",
            "{'loss': 0.2674, 'learning_rate': 1.634808013355593e-05, 'epoch': 1.83}\n",
            "{'loss': 0.2505, 'learning_rate': 1.624373956594324e-05, 'epoch': 1.88}\n",
            "{'loss': 0.2701, 'learning_rate': 1.6139398998330553e-05, 'epoch': 1.93}\n",
            "{'loss': 0.277, 'learning_rate': 1.6035058430717863e-05, 'epoch': 1.98}\n",
            "{'loss': 0.2594, 'learning_rate': 1.5930717863105177e-05, 'epoch': 2.03}\n",
            "{'loss': 0.261, 'learning_rate': 1.5826377295492487e-05, 'epoch': 2.09}\n",
            "{'loss': 0.2667, 'learning_rate': 1.57220367278798e-05, 'epoch': 2.14}\n",
            "{'loss': 0.2477, 'learning_rate': 1.5617696160267114e-05, 'epoch': 2.19}\n",
            "{'loss': 0.2487, 'learning_rate': 1.5513355592654425e-05, 'epoch': 2.24}\n",
            "{'loss': 0.2473, 'learning_rate': 1.540901502504174e-05, 'epoch': 2.3}\n",
            "{'loss': 0.2525, 'learning_rate': 1.530467445742905e-05, 'epoch': 2.35}\n",
            "{'loss': 0.2518, 'learning_rate': 1.520033388981636e-05, 'epoch': 2.4}\n",
            "{'loss': 0.261, 'learning_rate': 1.5095993322203674e-05, 'epoch': 2.45}\n",
            "{'loss': 0.2562, 'learning_rate': 1.4991652754590986e-05, 'epoch': 2.5}\n",
            "{'loss': 0.2715, 'learning_rate': 1.4887312186978298e-05, 'epoch': 2.56}\n",
            "{'loss': 0.2584, 'learning_rate': 1.4782971619365612e-05, 'epoch': 2.61}\n",
            "{'loss': 0.254, 'learning_rate': 1.4678631051752922e-05, 'epoch': 2.66}\n",
            "{'loss': 0.2442, 'learning_rate': 1.4574290484140234e-05, 'epoch': 2.71}\n",
            "{'loss': 0.2583, 'learning_rate': 1.4469949916527548e-05, 'epoch': 2.77}\n",
            "{'loss': 0.2622, 'learning_rate': 1.436560934891486e-05, 'epoch': 2.82}\n",
            "{'loss': 0.2573, 'learning_rate': 1.4261268781302172e-05, 'epoch': 2.87}\n",
            "{'loss': 0.2463, 'learning_rate': 1.4156928213689482e-05, 'epoch': 2.92}\n",
            "{'loss': 0.2571, 'learning_rate': 1.4052587646076796e-05, 'epoch': 2.97}\n",
            "{'loss': 0.257, 'learning_rate': 1.3948247078464108e-05, 'epoch': 3.03}\n",
            "{'loss': 0.239, 'learning_rate': 1.384390651085142e-05, 'epoch': 3.08}\n",
            "{'loss': 0.26, 'learning_rate': 1.3739565943238733e-05, 'epoch': 3.13}\n",
            "{'loss': 0.2491, 'learning_rate': 1.3635225375626045e-05, 'epoch': 3.18}\n",
            "{'loss': 0.26, 'learning_rate': 1.3530884808013355e-05, 'epoch': 3.23}\n",
            "{'loss': 0.2465, 'learning_rate': 1.3426544240400669e-05, 'epoch': 3.29}\n",
            "{'loss': 0.2453, 'learning_rate': 1.3322203672787981e-05, 'epoch': 3.34}\n",
            "{'loss': 0.2516, 'learning_rate': 1.3217863105175293e-05, 'epoch': 3.39}\n",
            "{'loss': 0.2456, 'learning_rate': 1.3113522537562607e-05, 'epoch': 3.44}\n",
            "{'loss': 0.2406, 'learning_rate': 1.3009181969949917e-05, 'epoch': 3.5}\n",
            "{'loss': 0.2537, 'learning_rate': 1.2904841402337229e-05, 'epoch': 3.55}\n",
            "{'loss': 0.257, 'learning_rate': 1.2800500834724543e-05, 'epoch': 3.6}\n",
            "{'loss': 0.2344, 'learning_rate': 1.2696160267111855e-05, 'epoch': 3.65}\n",
            "{'loss': 0.2698, 'learning_rate': 1.2591819699499167e-05, 'epoch': 3.7}\n",
            "{'loss': 0.249, 'learning_rate': 1.2487479131886477e-05, 'epoch': 3.76}\n",
            "{'loss': 0.2439, 'learning_rate': 1.238313856427379e-05, 'epoch': 3.81}\n",
            "{'loss': 0.2474, 'learning_rate': 1.2278797996661102e-05, 'epoch': 3.86}\n",
            "{'loss': 0.233, 'learning_rate': 1.2174457429048414e-05, 'epoch': 3.91}\n",
            "{'loss': 0.2479, 'learning_rate': 1.2070116861435728e-05, 'epoch': 3.96}\n",
            "{'loss': 0.2421, 'learning_rate': 1.196577629382304e-05, 'epoch': 4.02}\n",
            "{'loss': 0.2466, 'learning_rate': 1.186143572621035e-05, 'epoch': 4.07}\n",
            "{'loss': 0.2393, 'learning_rate': 1.1757095158597664e-05, 'epoch': 4.12}\n",
            "{'loss': 0.2338, 'learning_rate': 1.1652754590984976e-05, 'epoch': 4.17}\n",
            "{'loss': 0.2379, 'learning_rate': 1.1548414023372288e-05, 'epoch': 4.23}\n",
            "{'loss': 0.2395, 'learning_rate': 1.1444073455759602e-05, 'epoch': 4.28}\n",
            "{'loss': 0.241, 'learning_rate': 1.1339732888146912e-05, 'epoch': 4.33}\n",
            "{'loss': 0.2364, 'learning_rate': 1.1235392320534224e-05, 'epoch': 4.38}\n",
            "{'loss': 0.2478, 'learning_rate': 1.1131051752921537e-05, 'epoch': 4.43}\n",
            "{'loss': 0.2382, 'learning_rate': 1.102671118530885e-05, 'epoch': 4.49}\n",
            "{'loss': 0.256, 'learning_rate': 1.0922370617696161e-05, 'epoch': 4.54}\n",
            "{'loss': 0.2287, 'learning_rate': 1.0818030050083472e-05, 'epoch': 4.59}\n",
            "{'loss': 0.223, 'learning_rate': 1.0713689482470785e-05, 'epoch': 4.64}\n",
            "{'loss': 0.2525, 'learning_rate': 1.0609348914858097e-05, 'epoch': 4.7}\n",
            "{'loss': 0.243, 'learning_rate': 1.050500834724541e-05, 'epoch': 4.75}\n",
            "{'loss': 0.2381, 'learning_rate': 1.0400667779632723e-05, 'epoch': 4.8}\n",
            "{'loss': 0.2412, 'learning_rate': 1.0296327212020035e-05, 'epoch': 4.85}\n",
            "{'loss': 0.2453, 'learning_rate': 1.0191986644407345e-05, 'epoch': 4.9}\n",
            "{'loss': 0.2355, 'learning_rate': 1.0087646076794659e-05, 'epoch': 4.96}\n",
            "{'loss': 0.2466, 'learning_rate': 9.98330550918197e-06, 'epoch': 5.01}\n",
            " 50% 48071/95840 [7:05:59<7:04:41,  1.87it/s]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}