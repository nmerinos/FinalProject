# FinalProject
Pre-training allows reusing existing networks. Fine-tuning while freezing the original model allows the domain task to be learned with a much smaller set of parameters. The authors of the paper [Gururangan et al. (2020)](https://arxiv.org/abs/2004.10964)  have demonstrated that SoTA model performance can be achieved with a fraction of the parameters.  We want to replicate the paperâ€™s results using adapter transformers. This should reduce the training time required when it comes to Task-Adaptive Pretraining, while achieving similar results. This project aims to demonstrate the value of transformer adapters.
